   headers=headers,
        )
        r.raise_for_status()
    except Exception as e:
        host = _host_of(url)
        _mark_bad(host)
        log.warning(
            "fetch_error",
            extra={"url": url, "host": host, "detail": str(e), "timeoutSec": timeout},
        )
        raise ExternalServiceError(service="fetch", url=url, detail=str(e)) from e

    ctype = (r.headers.get("content-type") or "").lower()

    raw_bytes = await _read_capped_bytes(r, cap_bytes)
    enc = r.encoding or "utf-8"
    raw_text = raw_bytes.decode(enc, errors="ignore")
    txt = _extract_text_from_html(raw_text, str(r.url))
    txt = (txt or "").strip().replace("\r", "")
    if len(txt) > cap_chars:
        txt = txt[:cap_chars]

    MIN_USEFUL_CHARS = 80
    host_final = _host_of(str(r.url))
    if ("text/html" not in ctype) or (len(txt) < MIN_USEFUL_CHARS):
        _mark_bad(host_final)
    else:
        _mark_good(host_final)

    if telemetry is not None:
        telemetry.update(
            {
                "reqUrl": url,
                "finalUrl": str(r.url),
                "status": int(r.status_code),
                "elapsedSec": round(time.perf_counter() - t0, 6),
                "bytes": len(raw_bytes),
                "chars": len(txt),
                "timeoutSec": timeout,
                "capBytes": cap_bytes,
                "capChars": cap_chars,
                "contentType": ctype,
                "cooldownFails": _BAD_HOSTS.get(host_final, (0, 0.0))[0]
                if host_final in _BAD_HOSTS
                else 0,
            }
        )
    return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: list[str],
    per_timeout_s: float | None = None,
    cap_chars: int | None = None,
    cap_bytes: int | None = None,
    max_parallel: int | None = None,
    telemetry: dict[str, Any] | None = None,
    stop_ev: asyncio.Event | None = None,
):
    t_total0 = time.perf_counter()
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))
    tel_items: list[dict[str, Any]] = []

    async def _one(u: str):
        item_tel: dict[str, Any] = {"reqUrl": u}
        host = _host_of(u)

        # global fast cancel
        if stop_ev is not None and stop_ev.is_set():
            item_tel.update({"ok": False, "skipped": True, "skipReason": "cancelled", "host": host})
            tel_items.append(item_tel)
            return u, None

        # Skip hosts on cooldown
        if _is_on_cooldown(host):
            item_tel.update(
                {"ok": False, "skipped": True, "skipReason": "cooldown", "host": host}
            )
            tel_items.append(item_tel)
            return u, None

        t0 = time.perf_counter()
        async with sem:
            # check again after waiting
            if stop_ev is not None and stop_ev.is_set():
                item_tel.update({"ok": False, "skipped": True, "skipReason": "cancelled", "host": host})
                tel_items.append(item_tel)
                return u, None
            try:
                res = await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                    telemetry=item_tel,
                    stop_ev=stop_ev,
                )
                item_tel.setdefault("elapsedSec", round(time.perf_counter() - t0, 6))
                item_tel["ok"] = True
                item_tel["host"] = host
                tel_items.append(item_tel)
                return u, res
            except Exception as e:
                _mark_bad(host)
                item_tel.update(
                    {
                        "ok": False,
                        "errorType": type(e).__name__,
                        "errorMsg": str(e),
                        "elapsedSec": round(time.perf_counter() - t0, 6),
                        "timeoutSec": (
                            float(per_timeout_s) if per_timeout_s is not None else _timeout()
                        ),
                        "capBytes": (int(cap_bytes) if cap_bytes is not None else _max_bytes()),
                        "capChars": (int(cap_chars) if cap_chars is not None else _max_chars()),
                        "host": host,
                    }
                )
                tel_items.append(item_tel)
                return u, None

    # If cancelled before starting, short-circuit
    if stop_ev is not None and stop_ev.is_set():
        if telemetry is not None:
            telemetry.update(
                {
                    "totalSec": round(time.perf_counter() - t_total0, 6),
                    "requested": len(urls),
                    "ok": 0,
                    "miss": len(urls),
                    "items": [],
                    "settings": {
                        "userAgent": _ua(),
                        "defaultTimeoutSec": _timeout(),
                        "defaultCapChars": _max_chars(),
                        "defaultCapBytes": _max_bytes(),
                        "maxParallel": _max_parallel() if max_parallel is None else int(max_parallel),
                    },
                    "cancelled": True,
                }
            )
        return [(u, None) for u in urls]

    tasks = [_one(u) for u in urls]
    results = await asyncio.gather(*tasks)

    if telemetry is not None:
        ok_cnt = sum(1 for it in tel_items if it.get("ok"))
        telemetry.update(
            {
                "totalSec": round(time.perf_counter() - t_total0, 6),
                "requested": len(urls),
                "ok": ok_cnt,
                "miss": len(urls) - ok_cnt,
                "items": tel_items,
                "settings": {
                    "userAgent": _ua(),
                    "defaultTimeoutSec": _timeout(),
                    "defaultCapChars": _max_chars(),
                    "defaultCapBytes": _max_bytes(),
                    "maxParallel": _max_parallel() if max_parallel is None else int(max_parallel),
                },
            }
        )

    return results

# ===== aimodel/file_read/web/orchestrator.py =====

from __future__ import annotations

from ..core.logging import get_logger

log = get_logger(__name__)
import time
from collections import defaultdict
from typing import Any
import asyncio

from ..core.request_ctx import get_x_id
from .brave import BraveProvider
from .orchestrator_common import (_as_bool, _as_float, _as_int, _as_str,
                                  _dedupe_by_host, _fetch_round, _head_tail,
                                  _host, condense_doc, content_quality_score,
                                  score_hit)
from .provider import SearchHit


async def build_web_block(
    query: str,
    k: int | None = None,
    per_url_timeout_s: float | None = None,
    stop_ev: asyncio.Event | None = None,
) -> tuple[str | None, dict[str, Any]]:
    tel: dict[str, Any] = {"query": (query or "").strip()}

    # fast-cancel
    if stop_ev is not None and stop_ev.is_set():
        tel["cancelled"] = True
        return (None, tel)

    cfg_k = int(k) if k is not None else _as_int("web_orch_default_k")
    total_char_budget = _as_int("web_orch_total_char_budget")
    per_doc_budget = _as_int("web_orch_per_doc_char_budget")
    max_parallel = _as_int("web_orch_max_parallel_fetch")
    overfetch_factor = _as_float("web_orch_overfetch_factor")
    overfetch_min_extra = _as_int("web_orch_overfetch_min_extra")
    enable_js_retry = _as_bool("web_orch_enable_js_retry")
    js_avg_q_thresh = _as_float("web_orch_js_retry_avg_q")
    js_low_q_thresh = _as_float("web_orch_js_retry_low_q")
    js_lowish_ratio = _as_float("web_orch_js_retry_lowish_ratio")
    js_timeout_add = _as_float("web_orch_js_retry_timeout_add")
    js_timeout_cap = _as_float("web_orch_js_retry_timeout_cap")
    js_parallel_delta = _as_int("web_orch_js_retry_parallel_delta")
    js_min_parallel = _as_int("web_orch_js_retry_min_parallel")
    header_tpl = _as_str("web_block_header")
    sep_str = _as_str("web_orch_block_separator")
    min_chunk_after = _as_int("web_orch_min_chunk_after_shrink")
    min_block_reserve = _as_int("web_orch_min_block_reserve")
    per_timeout = (
        float(per_url_timeout_s)
        if per_url_timeout_s is not None
        else _as_float("web_fetch_timeout_sec")
    )
    start_time = time.perf_counter()
    provider = BraveProvider()
    overfetch = max(cfg_k + overfetch_min_extra, int(round(cfg_k * overfetch_factor)))
    tel["search"] = {"requestedK": cfg_k, "overfetch": overfetch}
    t0 = time.perf_counter()

    # cancel before search
    if stop_ev is not None and stop_ev.is_set():
        tel["cancelled"] = True
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return (None, tel)

    try:
        hits: list[SearchHit] = await provider.search(
            query, k=overfetch, telemetry=tel["search"], xid=get_x_id()
        )
    except Exception as e:
        tel["error"] = {"stage": "search", "type": type(e).__name__, "msg": str(e)}
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        log.error("[web-block] (empty) due to search error: %s", tel["error"])
        return (None, tel)

    # cancel after search
    if stop_ev is not None and stop_ev.is_set():
        tel["cancelled"] = True
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return (None, tel)

    tel["search"]["elapsedSecTotal"] = round(time.perf_counter() - t0, 6)
    if not hits:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        log.debug("[web-block] (empty) — no hits")
        return (None, tel)

    seen_urls = set()
    scored: list[tuple[int, SearchHit]] = []
    for h in hits:
        u = (h.url or "").strip()
        if not u or u in seen_urls:
            continue
        seen_urls.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
    tel["scoring"] = {"inputHits": len(hits), "scored": len(scored)}
    if not scored:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        log.info("[web-block] (empty) — no scored hits")
        return (None, tel)
    prefetch = max(cfg_k * 2, cfg_k + 6)
    top_hits = _dedupe_by_host(scored, prefetch)
    tel["scoring"]["picked"] = len(top_hits)
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]
    t_f = time.perf_counter()
    tel["fetch1"] = {}

    # cancel before fetch
    if stop_ev is not None and stop_ev.is_set():
        tel["cancelled"] = True
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return (None, tel)

    results = await _fetch_round(
        urls,
        meta,
        per_url_timeout_s=per_timeout,
        max_parallel=max_parallel,
        use_js=False,
        telemetry=tel["fetch1"],
        stop_ev=stop_ev,
    )
    tel["fetch1"]["roundSec"] = round(time.perf_counter() - t_f, 6)

    # cancel after fetch
    if stop_ev is not None and stop_ev.is_set():
        tel["cancelled"] = True
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return (None, tel)

    texts: list[tuple[str, str, str]] = []
    quality_scores: list[float] = []
    for original_url, res in results:
        if not res:
            continue
        final_url, status, text = res
        title = next((t for t, u in meta if u == original_url), final_url)
        qscore = content_quality_score(text or "")
        quality_scores.append(qscore)
        if text:
            texts.append((title, final_url, text))
    tel["fetch1"]["docs"] = {
        "ok": len(texts),
        "qAvg": sum(quality_scores) / len(quality_scores) if quality_scores else 0.0,
    }

    try_js = False
    if enable_js_retry and quality_scores:
        avg_q = sum(quality_scores) / len(quality_scores)
        lowish = sum(1 for q in quality_scores if q < js_low_q_thresh)
        if avg_q < js_avg_q_thresh or lowish / max(1, len(quality_scores)) >= js_lowish_ratio:
            try_js = True
        tel["jsRetry"] = {
            "considered": True,
            "triggered": try_js,
            "avgQ": round(avg_q, 4),
            "lowishRatio": round(lowish / max(1, len(quality_scores)) * 1.0, 4),
            "thresholds": {
                "avg": js_avg_q_thresh,
                "low": js_low_q_thresh,
                "ratio": js_lowish_ratio,
            },
        }
    else:
        tel["jsRetry"] = {"considered": bool(enable_js_retry), "triggered": False}

    if try_js:
        # cancel before JS retry
        if stop_ev is not None and stop_ev.is_set():
            tel["cancelled"] = True
            tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
            return (None, tel)
        js_timeout = min(per_timeout + js_timeout_add, js_timeout_cap)
        js_parallel = max(js_min_parallel, max_parallel + js_parallel_delta)
        tel["fetch2"] = {"timeoutSec": js_timeout, "maxParallel": js_parallel}
        results_js = await _fetch_round(
            urls,
            meta,
            per_url_timeout_s=js_timeout,
            max_parallel=js_parallel,
            use_js=True,
            telemetry=tel["fetch2"],
            stop_ev=stop_ev,
        )
        if stop_ev is not None and stop_ev.is_set():
            tel["cancelled"] = True
            tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
            return (None, tel)
        texts_js: list[tuple[str, str, str]] = []
        for original_url, res in results_js:
            if not res:
                continue
            final_url, status, text = res
            title = next((t for t, u in meta if u == original_url), final_url)
            if text:
                texts_js.append((title, final_url, text))
        if texts_js:
            texts = texts_js

    if not texts:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        log.info("[web-block] (empty) — no fetched texts")
        return (None, tel)

    texts.sort(key=lambda t: content_quality_score(t[2]), reverse=True)
    by_host: dict[str, list[tuple[str, str, str]]] = defaultdict(list)
    for title, url, text in texts:
        by_host[_host(url)].append((title, url, text))
    for h in by_host:
        by_host[h].sort(key=lambda x: content_quality_score(x[2]), reverse=True)
    hosts_ordered = sorted(
        by_host.keys(), key=lambda h: content_quality_score(by_host[h][0][2]), reverse=True
    )
    header = header_tpl.format(query=query)
    sep = sep_str
    available = max(min_block_reserve, total_char_budget - len(header) - len(sep))
    min_hosts = max(1, min(_as_int("web_orch_min_hosts"), len(hosts_ordered)))
    per_host_quota = max(min_chunk_after * 2, available // max(min_hosts, cfg_k))
    per_host_quota = min(per_host_quota, per_doc_budget)
    block_parts: list[str] = []
    used = 0
    included_hosts: list[str] = []
    for h in hosts_ordered:
        if stop_ev is not None and stop_ev.is_set():
            tel["cancelled"] = True
            tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
            return (None, tel)
        title, url, text = by_host[h][0]
        chunk = condense_doc(title, url, text, max_chars=per_host_quota)
        sep_len = len(sep) if block_parts else 0
        if used + sep_len + len(chunk) > available:
            rem = available - used - sep_len
            if rem > min_chunk_after:
                chunk = _head_tail(chunk, rem)
            else:
                break
        block_parts.append(chunk)
        included_hosts.append(h)
        used += sep_len + len(chunk)
        if len(included_hosts) >= min_hosts and used >= int(available * 0.66):
            break
    layer = 1
    while used < available:
        if stop_ev is not None and stop_ev.is_set():
            tel["cancelled"] = True
            tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
            return (None, tel)
        added_any = False
        for h in hosts_ordered:
            if layer >= len(by_host[h]):
                continue
            title, url, text = by_host[h][layer]
            sep_len = len(sep) if block_parts else 0
            chunk = condense_doc(title, url, text, max_chars=per_doc_budget)
            if used + sep_len + len(chunk) > available:
                rem = available - used - sep_len
                if rem <= min_chunk_after:
                    continue
                chunk = _head_tail(chunk, rem)
                if used + sep_len + len(chunk) > available:
                    continue
            block_parts.append(chunk)
            used += sep_len + len(chunk)
            added_any = True
            if used >= available:
                break
        if not added_any:
            break
        layer += 1
    body = sep.join(block_parts)
    block = f"{header}{sep}{body}" if body else header
    tel["assembly"] = {
        "chunksPicked": len(block_parts),
        "chars": len(block),
        "available": available,
        "headerChars": len(header),
        "hostsIncluded": len(included_hosts),
        "perHostQuota": per_host_quota,
    }
    tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
    log.info("[web-block] -------- BEGIN --------")
    log.info(block)
    log.info("[web-block] --------  END  --------")
    try:
        srcs = [{"title": t, "url": u} for t, u, _ in texts[:10]]
        log.info("[web-block] sources: %s", srcs)
    except Exception as _e:
        log.info("[web-block] sources: <unavailable> %s", str(_e))
    return (block, tel)

# ===== aimodel/file_read/web/orchestrator_common.py =====

from __future__ import annotations

import re
from typing import Any
from urllib.parse import urlparse
import asyncio

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..utils.text import clean_ws
from .fetch import fetch_many
from .provider import SearchHit

log = get_logger(__name__)


def _req(key: str):
    return SETTINGS[key]


def _as_int(key: str) -> int:
    return int(_req(key))


def _as_float(key: str) -> float:
    return float(_req(key))


def _as_bool(key: str) -> bool:
    return bool(_req(key))


def _as_str(key: str) -> str:
    v = _req(key)
    return "" if v is None else str(v)


def _host(url: str) -> str:
    h = (urlparse(url).hostname or "").lower()
    pref = _as_str("web_orch_www_prefix")
    return h[len(pref) :] if pref and h.startswith(pref) else h


def _tokens(s: str) -> list[str]:
    return re.findall(r"\w+", (s or "").lower())


def _head_tail(text: str, max_chars: int) -> str:
    t = text or ""
    if max_chars <= 0 or len(t) <= max_chars:
        return clean_ws(t)

    ellipsis = _as_str("web_orch_ellipsis")
    reserve = len(ellipsis) * 2
    avail = max(0, max_chars - reserve)
    if avail <= 0:
        return clean_ws(t[:max_chars])

    head_len = max(1, int(avail * 0.4))
    mid_len = max(1, int(avail * 0.2))
    tail_len = max(1, avail - head_len - mid_len)

    n = len(t)
    h0, h1 = 0, min(head_len, n)
    head = t[h0:h1]

    m0 = max(0, (n // 2) - (mid_len // 2))
    m1 = min(n, m0 + mid_len)
    if m0 < h1 and (h1 + mid_len) <= n:
        m0, m1 = h1, min(n, h1 + mid_len)
    mid = t[m0:m1]

    t0 = max(m1, n - tail_len)
    tail = t[t0:n] if tail_len > 0 else ""

    return clean_ws(head + ellipsis + mid + ellipsis + tail)


def condense_doc(title: str, url: str, text: str, *, max_chars: int) -> str:
    body = _head_tail(text or "", max_chars)
    safe_title = clean_ws(title or url)
    bullet = _as_str("web_orch_bullet_prefix") or "- "
    indent = _as_str("web_orch_indent_prefix") or "  "
    return f"{bullet}{safe_title}\n{indent}{url}\n{indent}{body}"


def score_hit(hit: SearchHit, query: str) -> int:
    w_exact = _as_int("web_orch_score_w_exact")
    w_substr = _as_int("web_orch_score_w_substr")
    w_title_full = _as_int("web_orch_score_w_title_full")
    w_title_part = _as_int("web_orch_score_w_title_part")
    w_snip_touch = _as_int("web_orch_score_w_snip_touch")
    score = 0
    q = (query or "").strip().lower()
    title = (hit.title or "").strip()
    snippet = (hit.snippet or "").strip()
    title_l = title.lower()
    snip_l = snippet.lower()
    if q:
        if title_l == q:
            score += w_exact
        elif q in title_l:
            score += w_substr
    qtoks = _tokens(q)
    if qtoks:
        cov_title = sum(1 for t in qtoks if t in title_l)
        if cov_title == len(qtoks):
            score += w_title_full
        elif cov_title > 0:
            score += w_title_part
        if any(t in snip_l for t in qtoks):
            score += w_snip_touch
    return score


def _type_ratio(text: str, sub: str) -> float:
    if not text:
        return 1.0
    cnt = text.lower().count(sub)
    return float(cnt) / max(1, len(text))


def content_quality_score(text: str) -> float:
    if not text:
        return 0.0
    t = text.strip()
    n = len(t)
    len_div = _as_float("web_orch_q_len_norm_divisor")
    w_len = _as_float("web_orch_q_len_weight")
    w_div = _as_float("web_orch_q_diversity_weight")
    length_score = min(1.0, n / len_div) if len_div > 0 else 0.0
    toks = _tokens(t)
    if not toks:
        return 0.1 * length_score
    uniq = len(set(toks))
    diversity = uniq / max(1.0, float(len(toks)))
    pen = 0.0
    for rule in _req("web_orch_q_penalties"):
        token = str(rule.get("token") or "")
        mult = float(rule.get("mult") or 0.0)
        cap = float(rule.get("cap") or 1.0)
        pen += min(cap, _type_ratio(t, token) * mult)
    raw = (w_len * length_score) + (w_div * diversity) - pen
    return max(0.0, min(1.0, raw))


def _dedupe_by_host(scored_hits: list[tuple[int, SearchHit]], k: int) -> list[SearchHit]:
    picked: list[SearchHit] = []
    seen_hosts = set()
    for s, h in sorted(scored_hits, key=lambda x: x[0], reverse=True):
        u = (h.url or "").strip()
        if not u:
            continue
        host = _host(u)
        if host in seen_hosts:
            continue
        seen_hosts.add(host)
        picked.append(h)
        if len(picked) >= k:
            break
    return picked


async def _fetch_round(
    urls: list[str],
    meta: list[tuple[str, str]],
    per_url_timeout_s: float,
    max_parallel: int,
    use_js: bool = False,
    telemetry: dict[str, Any] | None = None,
    stop_ev: asyncio.Event | None = None,
) -> list[tuple[str, tuple[str, int, str] | None]]:
    fetch_fn = fetch_many
    if use_js:
        try:
            from . import fetch as _fetch_mod
            fetch_fn = getattr(_fetch_mod, "fetch_many_js", fetch_many)
        except Exception:
            fetch_fn = fetch_many

    # fast-cancel before spawning requests
    if stop_ev is not None and stop_ev.is_set():
        if telemetry is not None:
            telemetry.update({"cancelled": True})
        return [(u, None) for u in urls]

    cap_mult = _as_float("web_orch_fetch_cap_multiplier")
    per_doc_budget = _as_int("web_orch_per_doc_char_budget")
    fetch_max_chars = _as_int("web_fetch_max_chars")
    per_doc_cap = min(int(per_doc_budget * cap_mult), fetch_max_chars)

    results = await fetch_fn(
        urls,
        per_timeout_s=per_url_timeout_s,
        cap_chars=per_doc_cap,
        max_parallel=max_parallel,
        telemetry=telemetry,
        # NOTE: fetch_many doesn't accept stop_ev; we short-circuit above instead
    )
    return results

# ===== aimodel/file_read/web/provider.py =====

from __future__ import annotations

from dataclasses import dataclass

from ..core.logging import get_logger

log = get_logger(__name__)


@dataclass
class SearchHit:
    title: str
    url: str
    snippet: str | None = None
    rank: int = 0


class SearchProvider:
    async def search(self, query: str, k: int = 3) -> list[SearchHit]:
        raise NotImplementedError

# ===== aimodel/file_read/web/query_summarizer.py =====

# aimodel/file_read/web/query_summarizer.py
from __future__ import annotations

import asyncio
import re
import time
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..utils.streaming import safe_token_count_messages

log = get_logger(__name__)


def _tokens(s: str) -> set[str]:
    return set(re.findall(r"\w+", (s or "").lower()))


def _as_list(v) -> list:
    if v is None:
        return []
    if isinstance(v, (list, tuple)):
        return list(v)
    return [v]


def summarize_query(
    llm: Any,
    user_text: str,
    *,
    stop_ev: asyncio.Event | None = None,
) -> tuple[str, dict[str, Any]]:
    telemetry: dict[str, Any] = {}
    txt = (user_text or "").strip()

    if stop_ev and stop_ev.is_set():
        telemetry["cancelledAt"] = "start"
        return txt, telemetry

    bypass_enabled = SETTINGS.get("query_sum_bypass_short_enabled")
    short_chars = SETTINGS.get("query_sum_short_max_chars")
    short_words = SETTINGS.get("query_sum_short_max_words")
    if bypass_enabled is True and isinstance(short_chars, int) and isinstance(short_words, int):
        if len(txt) <= short_chars and len(txt.split()) <= short_words:
            telemetry.update({"bypass": True})
            return txt, telemetry
    telemetry.update({"bypass": False})

    prompt = SETTINGS.get("query_sum_prompt")
    if isinstance(prompt, str) and "{text}" in prompt:
        params = {}
        max_tokens = SETTINGS.get("query_sum_max_tokens")
        if isinstance(max_tokens, int):
            params["max_tokens"] = max_tokens
        temperature = SETTINGS.get("query_sum_temperature")
        if isinstance(temperature, (int, float)):
            params["temperature"] = float(temperature)
        top_p = SETTINGS.get("query_sum_top_p")
        if isinstance(top_p, (int, float)):
            params["top_p"] = float(top_p)
        stops = _as_list(SETTINGS.get("query_sum_stop"))
        if stops:
            params["stop"] = [str(s) for s in stops if isinstance(s, str)]
        params["stream"] = False

        if stop_ev and stop_ev.is_set():
            telemetry["cancelledAt"] = "before_llm"
            return txt, telemetry

        t_start = time.perf_counter()
        out = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt.format(text=txt)}],
            **params,
        )
        elapsed = time.perf_counter() - t_start
        result = (out["choices"][0]["message"]["content"] or "").strip()
        in_tokens = (
            safe_token_count_messages(llm, [{"role": "user", "content": prompt.format(text=txt)}])
            or 0
        )
        out_tokens = safe_token_count_messages(llm, [{"role": "assistant", "content": result}]) or 0
        telemetry.update(
            {
                "elapsedSec": round(elapsed, 4),
                "inputTokens": in_tokens,
                "outputTokens": out_tokens,
            }
        )
    else:
        return txt, telemetry

    if stop_ev and stop_ev.is_set():
        telemetry["cancelledAt"] = "after_llm"
        return txt, telemetry

    overlap_enabled = SETTINGS.get("query_sum_overlap_check_enabled")
    j_min = SETTINGS.get("query_sum_overlap_jaccard_min")
    if overlap_enabled is True and isinstance(j_min, (int, float)):
        src_toks = _tokens(txt)
        out_toks = _tokens(result)
        if not result or not out_toks:
            telemetry.update({"overlapRetained": True, "overlapScore": 0.0})
            return txt, telemetry
        jaccard = (
            (len(src_toks & out_toks) / len(src_toks | out_toks)) if (src_toks or out_toks) else 1.0
        )
        telemetry.update({"overlapScore": round(jaccard, 4)})
        if jaccard < float(j_min):
            telemetry.update({"overlapRetained": True})
            return txt, telemetry
        telemetry.update({"overlapRetained": False})
        return result, telemetry

    return result, telemetry

# ===== aimodel/file_read/web/router_ai.py =====

from __future__ import annotations

import json
import re
import time
from typing import Any
import asyncio 
from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..utils.streaming import safe_token_count_messages
from ..utils.text import strip_wrappers as _strip_wrappers

log = get_logger(__name__)


def _force_json(s: str) -> dict:
    if not s:
        return {}
    raw = s.strip()
    try:
        cf = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", raw, re.IGNORECASE)
        if cf:
            raw = cf.group(1).strip()
    except Exception:
        pass
    try:
        v = json.loads(raw)
        print("[_force_json] parsed whole raw")
        return v if isinstance(v, dict) else {}
    except Exception:
        pass
    try:
        m = None
        for m in re.finditer(
            r"\{[^{}]*\"need\"\s*:\s*(?:true|false|\"true\"|\"false\")[^{}]*\}", raw, re.IGNORECASE
        ):
            pass
        if m:
            frag = m.group(0)
            v = json.loads(frag)
            print("[_force_json] parsed frag with need field")
            return v if isinstance(v, dict) else {}
    except Exception:
        pass
    try:
        last = None
        for last in re.finditer(r"\{[\s\S]*\}", raw):
            pass
        if last:
            frag = last.group(0)
            v = json.loads(frag)
            print("[_force_json] parsed last {} block")
            return v if isinstance(v, dict) else {}
    except Exception:
        pass
    print("[_force_json] failed to parse JSON")
    return {}


def decide_web(llm: Any, user_text: str) -> tuple[bool, str | None, dict[str, Any]]:
    telemetry: dict[str, Any] = {}
    try:
        if not user_text or not user_text.strip():
            print("[decide_web] empty user_text")
            return (False, None, telemetry)
        t_start = time.perf_counter()
        t_raw = user_text.strip()
        if SETTINGS.get("router_strip_wrappers_enabled") is True:
            core_text = _strip_wrappers(
                t_raw,
                trim_whitespace=SETTINGS.get("router_trim_whitespace") is True,
                split_on_blank=SETTINGS.get("router_strip_split_on_blank") is True,
                header_regex=SETTINGS.get("router_strip_header_regex"),
            )
        else:
            core_text = t_raw.strip() if SETTINGS.get("router_trim_whitespace") is True else t_raw
        print(f"[decide_web] core_text={core_text[:100]!r}")
        prompt_tpl = SETTINGS.get("router_decide_prompt")
        if not isinstance(prompt_tpl, str) or not prompt_tpl.strip():
            print("[decide_web] no prompt template")
            return (False, None, telemetry)
        the_prompt = _safe_prompt_format(prompt_tpl, text=core_text)
        params = {
            "max_tokens": SETTINGS.get("router_decide_max_tokens"),
            "temperature": SETTINGS.get("router_decide_temperature"),
            "top_p": SETTINGS.get("router_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}
        print(f"[decide_web] sending prompt, params={params}")
        raw_out_obj = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (
            raw_out_obj.get("choices", [{}])[0].get("message", {}).get("content") or ""
        ).strip()
        print(f"[decide_web] raw llm output={text_out[:200]!r}")
        telemetry["rawRouterOut"] = text_out[:2000]
        data = _force_json(text_out) or {}
        print(f"[decide_web] parsed data={data}")
        need_val = data.get("need", None)
        if isinstance(need_val, str):
            nv = need_val.strip().lower()
            if nv in ("true", "yes", "y", "1"):
                need_val = True
            elif nv in ("false", "no", "n", "0"):
                need_val = False
        if isinstance(need_val, bool):
            need = need_val
            parsed_ok = True
        else:
            parsed_ok = False
            need_default = SETTINGS.get("router_default_need_when_invalid")
            need = bool(need_default) if isinstance(need_default, bool) else False
        query_field = data.get("query", "")
        try:
            if SETTINGS.get("router_strip_wrappers_enabled") is True:
                query = _strip_wrappers(
                    str(query_field or "").strip(),
                    trim_whitespace=SETTINGS.get("router_trim_whitespace") is True,
                    split_on_blank=SETTINGS.get("router_strip_split_on_blank") is True,
                    header_regex=SETTINGS.get("router_strip_header_regex"),
                )
            else:
                query = str(query_field or "").strip()
        except Exception:
            query = ""
        if not need:
            query = None
        t_elapsed = time.perf_counter() - t_start
        in_tokens = safe_token_count_messages(llm, [{"role": "user", "content": the_prompt}]) or 0
        out_tokens = (
            safe_token_count_messages(llm, [{"role": "assistant", "content": text_out}]) or 0
        )
        telemetry.update(
            {
                "needed": bool(need),
                "routerQuery": query if need else None,
                "elapsedSec": round(t_elapsed, 4),
                "inputTokens": in_tokens,
                "outputTokens": out_tokens,
                "parsedOk": parsed_ok,
            }
        )
        print(f"[decide_web] result need={need}, query={query}")
        return (need, query, telemetry)
    except Exception as e:
        print(f"[decide_web] error: {e}")
        return (False, None, telemetry)


async def decide_web_and_fetch(
    llm: Any, user_text: str, *, k: int = 3, stop_ev: asyncio.Event | None = None
) -> tuple[str | None, dict[str, Any]]:
    telemetry: dict[str, Any] = {}
    need, proposed_q, tel_decide = decide_web(llm, (user_text or "").strip())
    telemetry.update(tel_decide)
    if not need:
        return None, telemetry

    # cancel before work
    if stop_ev is not None and stop_ev.is_set():
        telemetry["cancelled"] = True
        return None, telemetry

    from .orchestrator import build_web_block
    from .query_summarizer import summarize_query

    base_query = (proposed_q or user_text).strip()
    try:
        q_summary, tel_sum = summarize_query(llm, base_query)
        telemetry["summarizer"] = tel_sum
        q_summary = (q_summary or "").strip() or base_query
    except Exception:
        q_summary = base_query

    # cancel before fetch
    if stop_ev is not None and stop_ev.is_set():
        telemetry["cancelled"] = True
        return None, telemetry

    t_start = time.perf_counter()
    try:
        block, tel_orch = await build_web_block(q_summary, k=k, stop_ev=stop_ev)
        telemetry["orchestrator"] = tel_orch or {}
    except Exception:
        block = None
    telemetry.update(
        {
            "fetchElapsedSec": round(time.perf_counter() - t_start, 4),
            "blockChars": len(block) if block else 0,
        }
    )
    return (block or None, telemetry)



def _safe_prompt_format(tpl: str, **kwargs) -> str:
    marker = "__ROUTER_TEXT_FIELD__"
    tmp = tpl.replace("{text}", marker)
    tmp = tmp.replace("{", "{{").replace("}", "}}")
    tmp = tmp.replace(marker, "{text}")
    return tmp.format(**kwargs)

# ===== aimodel/file_read/workers/model_worker.py =====

# aimodel/file_read/workers/model_worker.py
from __future__ import annotations

import asyncio
import os
import signal
import socket
import subprocess
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple

import httpx

from ..core.logging import get_logger

log = get_logger(__name__)


@dataclass
class WorkerInfo:
    id: str
    port: int
    model_path: str
    process: subprocess.Popen
    status: str = "loading"  # loading | ready | stopped
    host_bind: str = "127.0.0.1"    # where uvicorn binds
    host_client: str = "127.0.0.1"  # where the proxy should dial


class ModelWorkerSupervisor:
    def __init__(self):
        self._workers: Dict[str, WorkerInfo] = {}

    # ---------------------------
    # Utilities
    # ---------------------------
    def _find_free_port(self, host: str) -> int:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.bind((host, 0))
        port = s.getsockname()[1]
        s.close()
        return port

    def _is_worker_ready(self, *args) -> bool:
        """
        Back-compat:
          - _is_worker_ready(port)
          - _is_worker_ready(host, port)
        """
        if len(args) == 1:
            host, port = "127.0.0.1", args[0]
        elif len(args) == 2:
            host, port = args
        else:
            return False

        try:
            r = httpx.get(f"http://{host}:{port}/api/worker/health", timeout=1.0)
            if r.status_code != 200:
                return False
            data = r.json()
            return bool(data.get("ok"))
        except Exception:
            return False

    async def _wait_ready(self, wid: str, host: str, port: int, timeout_s: float = 120.0) -> bool:
        t0 = time.time()
        while time.time() - t0 < timeout_s:
            info = self._workers.get(wid)
            if not info:
                return False
            # if worker process died, bail
            if info.process and (info.process.poll() is not None):
                return False
            if self._is_worker_ready(host, port):
                return True
            await asyncio.sleep(0.25)
        return False

    # ---------------------------
    # Introspection
    # ---------------------------
    def get_worker(self, wid: str) -> Optional[WorkerInfo]:
        return self._workers.get(wid)

    def get_addr(self, wid: str) -> Optional[Tuple[str, int]]:
        info = self._workers.get(wid)
        if not info:
            return None
        return (info.host_client, info.port)

    # Back-compat helper for existing code
    def get_port(self, wid: str) -> Optional[int]:
        info = self._workers.get(wid)
        return info.port if info else None

    def list(self) -> list[dict]:
        """
        Returns a JSON-serializable snapshot of known workers.
        Also self-heals status: if a worker finished loading after
        spawn wait, flip status to 'ready' here.
        """
        out = []
        for w in self._workers.values():
            # reflect process death
            if getattr(w, "process", None) and (w.process.poll() is not None):
                w.status = "stopped"
            else:
                # probe health to upgrade from 'loading' -> 'ready'
                if w.status != "ready":
                    try:
                        if self._is_worker_ready(w.host_client, w.port):
                            w.status = "ready"
                    except Exception:
                        pass

            out.append(
                {
                    "id": w.id,
                    "port": w.port,
                    "model_path": w.model_path,
                    "status": w.status,
                    "pid": (w.process.pid if getattr(w, "process", None) else None),
                }
            )
        return out

    # Older name some routes used
    def list_workers(self) -> list[dict]:
        return self.list()

    # ---------------------------
    # Lifecycle
    # ---------------------------
    async def spawn_worker(self, model_path: str) -> WorkerInfo:
        """Spawn a new worker process to serve a model."""
        host_bind = os.getenv("LM_WORKER_BIND_HOST", "127.0.0.1")
        host_client = os.getenv("LM_WORKER_CLIENT_HOST", "127.0.0.1")

        wid = os.urandom(4).hex()
        port = self._find_free_port(host_bind)

        # repo root = parent of the 'aimodel' package dir
        repo_root = Path(__file__).resolve().parents[3]
        app_module = "aimodel.file_read.workers.worker_entry:app"

        log.info(f"[workers] spawning {wid} on {host_bind}:{port} for {model_path}")
        cmd = [
            sys.executable,
            "-m",
            "uvicorn",
            app_module,
            "--host",
            host_bind,
            "--port",
            str(port),
            "--log-level",
            "info",
        ]
        log.info(f"[workers] cwd={repo_root} cmd={' '.join(cmd)}")

        env = os.environ.copy()
        env["MODEL_PATH"] = model_path
        env["PYTHONPATH"] = str(repo_root) + os.pathsep + env.get("PYTHONPATH", "")
        debug = env.get("LM_WORKER_DEBUG", "0") == "1"

        proc = subprocess.Popen(
            cmd,
            cwd=str(repo_root),
            env=env,
            stdout=None if debug else subprocess.DEVNULL,
            stderr=None if debug else subprocess.DEVNULL,
        )

        time.sleep(0.05)
        if proc.poll() is not None:
            raise RuntimeError("worker exited immediately; enable LM_WORKER_DEBUG=1 to see logs")

        info = WorkerInfo(
            id=wid,
            port=port,
            model_path=model_path,
            process=proc,
            status="loading",
            host_bind=host_bind,
            host_client=host_client,
        )
        # register immediately so UI can show "loading"
        self._workers[wid] = info

        ready = await self._wait_ready(wid, host_client, port)
        info.status = "ready" if ready else "loading"
        return info

    async def stop_worker(self, wid: str) -> bool:
        """Stop a specific worker."""
        info = self._workers.get(wid)
        if not info:
            return False
        log.info(f"[workers] stopping worker {wid}")
        try:
            if info.process and (info.process.poll() is None):
                info.process.send_signal(signal.SIGTERM)
                try:
                    info.process.wait(timeout=10)
                except subprocess.TimeoutExpired:
                    info.process.kill()
        except Exception as e:
            log.warning(f"[workers] error stopping {wid}: {e}")
        info.status = "stopped"
        # leave record so UI can show it as stopped; caller can prune if desired
        return True

    async def stop_all(self) -> int:
        """Stop all workers."""
        n = 0
        for wid in list(self._workers.keys()):
            try:
                ok = await self.stop_worker(wid)
                if ok:
                    n += 1
            except Exception as e:
                log.warning(f"[workers] stop_all error on {wid}: {e}")
        return n


# Singleton supervisor
supervisor = ModelWorkerSupervisor()

# ===== aimodel/file_read/workers/retitle_worker.py =====

from __future__ import annotations

import asyncio
import logging
import re
from pathlib import Path
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..runtime import model_runtime as MR
from ..services.cancel import GEN_SEMAPHORE, is_active
from ..store.base import now_iso
from ..store.chats import _load_chat
from ..store.index import load_index, save_index

log = get_logger(__name__)


def S(key: str):
    return SETTINGS[key]


# Key the queue by user+session to avoid collisions across users
def _key(uid: str, session_id: str) -> str:
    return f"{uid}:{session_id}"


# pending snapshot: { root, uid, session_id, messages, job_seq }
_PENDING: dict[str, dict[str, Any]] = {}
_ENQUEUED: set[str] = set()
_queue: asyncio.Queue[str] = asyncio.Queue(maxsize=int(S("retitle_queue_maxsize")))
_lock = asyncio.Lock()


def _preview(s: str) -> str:
    n = int(S("retitle_preview_chars"))
    ell = S("retitle_preview_ellipsis")
    s = s or ""
    return (s[:n] + ell) if len(s) > n else s


def _is_substantial(text: str) -> bool:
    t = (text or "").strip()
    min_chars = int(S("retitle_min_substantial_chars"))
    require_alpha = bool(S("retitle_require_alpha"))
    if len(t) < min_chars:
        return False
    return (re.search(r"[A-Za-z]", t) is not None) if require_alpha else True


def _pick_source(messages: list[dict]) -> str | None:
    if not messages:
        return None
    min_user_len = int(S("retitle_min_user_chars"))
    for m in reversed(messages):
        if m.get("role") == "user":
            txt = (m.get("content") or "").strip()
            if len(txt) >= min_user_len and _is_substantial(txt):
                return txt
    for m in reversed(messages):
        if m.get("role") == "assistant":
            txt = (m.get("content") or "").strip()
            if _is_substantial(txt):
                return txt
    return None


def _sanitize_title(s: str) -> str:
    if not s:
        return ""
    s = s.strip()
    drop_prefix_re = S("retitle_sanitize_drop_prefix_regex")
    if drop_prefix_re:
        s = re.sub(drop_prefix_re, "", s)
    if bool(S("retitle_sanitize_strip_quotes")):
        s = s.strip().strip('"').strip("'").strip()
    replace_not_allowed_re = S("retitle_sanitize_replace_not_allowed_regex")
    replace_with = S("retitle_sanitize_replace_with")
    if replace_not_allowed_re:
        s = re.sub(replace_not_allowed_re, replace_with, s)
    s = re.sub(r"\s+", " ", s).strip()
    max_words = int(S("retitle_sanitize_max_words"))
    max_chars = int(S("retitle_sanitize_max_chars"))
    if max_words > 0:
        words = s.split()
        s = " ".join(words[:max_words])
    if max_chars > 0 and len(s) > max_chars:
        s = s[:max_chars].rstrip()
    return s


def _make_title(llm, src: str) -> str:
    hard = SETTINGS.get("retitle_llm_hard_prefix") or ""
    sys_extra = SETTINGS.get("retitle_llm_sys_inst") or ""
    sys = f"{hard}\n\n{sys_extra}".strip()
    user_text = f"{S('retitle_user_prefix')}{src}{S('retitle_user_suffix')}"
    messages = [
        {"role": "system", "content": sys},
        {"role": "user", "content": user_text},
    ]
    out = llm.create_chat_completion(
        messages=messages,
        max_tokens=int(S("retitle_llm_max_tokens")),
        temperature=float(S("retitle_llm_temperature")),
        top_p=float(S("retitle_llm_top_p")),
        stream=False,
        stop=S("retitle_llm_stop"),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip().strip('"').strip("'")
    strip_regex = SETTINGS.get("retitle_strip_regex")
    if strip_regex:
        raw = re.sub(strip_regex, "", raw).strip()
    raw = re.sub(r"^`{1,3}|`{1,3}$", "", raw).strip()
    raw = re.sub(r"[.:;,\-\s]+$", "", raw)
    return raw


async def start_worker():
    while True:
        key = await _queue.get()
        try:
            await _process_session(key)
        except Exception:
            logging.exception("Retitle worker failed")
        finally:
            _queue.task_done()


def _extract_job(snapshot: dict) -> tuple[list[dict], int]:
    msgs = snapshot.get("messages") or []
    job_seq = int(snapshot.get("job_seq") or 0)
    return msgs, job_seq


async def _process_session(key: str):
    if not bool(S("retitle_enable")):
        return

    # small grace to avoid racing while generation is still streaming
    await asyncio.sleep(int(S("retitle_grace_ms")) / 1000.0)

    # backoff if the chat is still active
    waited = 0
    backoff = int(S("retitle_active_backoff_start_ms"))
    backoff_max = int(S("retitle_active_backoff_max_ms"))
    backoff_total = int(S("retitle_active_backoff_total_ms"))
    growth = float(S("retitle_active_backoff_growth"))
    # The is_active flag is keyed by session id; strip uid part
    _, session_id = key.split(":", 1)
    while is_active(session_id) and waited < backoff_total:
        await asyncio.sleep(backoff / 1000.0)
        waited += backoff
        backoff = min(int(backoff * growth), backoff_max)

    async with _lock:
        snapshot = _PENDING.pop(key, None)
        _ENQUEUED.discard(key)
    if not snapshot:
        return

    root: Path = snapshot["root"]
    uid: str = snapshot["uid"]
    session_id: str = snapshot["session_id"]
    messages, job_seq = _extract_job(snapshot)

    # Check current seq to avoid retitling after more messages arrived
    try:
        cur_seq = int((_load_chat(root, uid, session_id) or {}).get("seq") or 0)
    except Exception:
        cur_seq = job_seq
    if cur_seq > job_seq:
        return

    src = _pick_source(messages) or ""
    if not src.strip():
        return

    async with GEN_SEMAPHORE:
        llm = MR.get_llm()
        try:
            title_raw = await asyncio.to_thread(_make_title, llm, src)
        except Exception as e:
            logging.exception("retitle: LLM error: %s", e)
            return
        finally:
            try:
                llm.reset()
            except Exception:
                pass

    title = _sanitize_title(title_raw) if bool(S("retitle_enable_sanitize")) else title_raw
    if not title:
        return

    # Update encrypted per-user index
    idx = load_index(root, uid)
    row = next(
        (r for r in idx if r.get("sessionId") == session_id and r.get("ownerUid") == uid), None
    )
    if not row:
        return
    if (row.get("title") or "").strip() == title:
        return

    row["title"] = title
    row["updatedAt"] = now_iso()
    save_index(root, uid, idx)


def enqueue(
    root: Path, uid: str, session_id: str, messages: list[dict], *, job_seq: int | None = None
):
    if not session_id or not uid:
        return
    if not isinstance(messages, list):
        messages = []
    if job_seq is None:
        try:
            job_seq = max(int(m.get("id") or 0) for m in messages) if messages else 0
        except Exception:
            job_seq = 0

    k = _key(uid, session_id)
    snap = {
        "root": root,
        "uid": uid,
        "session_id": session_id,
        "messages": messages,
        "job_seq": int(job_seq),
    }

    async def _put():
        async with _lock:
            _PENDING[k] = snap
            if k not in _ENQUEUED:
                _ENQUEUED.add(k)
                try:
                    _queue.put_nowait(k)
                except Exception as e:
                    logging.warning(f"Failed to enqueue retitle: {e}")

    try:
        loop = asyncio.get_running_loop()
        loop.create_task(_put())
    except RuntimeError:
        asyncio.run(_put())

# ===== aimodel/file_read/workers/worker_entry.py =====

# aimodel/file_read/workers/worker_entry.py
from __future__ import annotations

import os
import signal
import sys
import gc
from dataclasses import dataclass
from typing import Any, Dict, Optional

from fastapi import FastAPI, HTTPException, Request, Body
from fastapi.responses import StreamingResponse

# Keep this worker self-contained: load llama.cpp directly here
try:
    from llama_cpp import Llama  # type: ignore
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed in worker") from e

# import your pipeline + schemas from the repo (PYTHONPATH is set by the supervisor)
from aimodel.file_read.core.schemas import ChatBody
from aimodel.file_read.services.generate_flow import generate_stream_flow, cancel_session as _cancel

def _log(msg: str):
    print(f"[worker] {msg}", flush=True)

def _env_num(name: str, typ, default=None):
    v = os.getenv(name, "")
    if not v:
        return default
    try:
        return typ(v)
    except Exception:
        return default

@dataclass
class WorkerCfg:
    model_path: str
    n_ctx: int = 4096
    n_threads: int = _env_num("N_THREADS", int, 8) or 8
    n_gpu_layers: int = _env_num("N_GPU_LAYERS", int, 40) or 40
    n_batch: int = _env_num("N_BATCH", int, 256) or 256
    rope_freq_base: Optional[float] = _env_num("ROPE_FREQ_BASE", float, None)
    rope_freq_scale: Optional[float] = _env_num("ROPE_FREQ_SCALE", float, None)

    @staticmethod
    def from_env() -> "WorkerCfg":
        mp = (os.getenv("MODEL_PATH") or "").strip()
        if not mp:
            raise RuntimeError("MODEL_PATH env is required for worker")
        return WorkerCfg(
            model_path=mp,
            n_ctx=_env_num("N_CTX", int, 4096) or 4096,
        )

app = FastAPI(title="LocalMind Model Worker", version="0.1")

_llm: Optional[Llama] = None
_cfg: Optional[WorkerCfg] = None

def _build_kwargs(cfg: WorkerCfg) -> Dict[str, Any]:
    kw: Dict[str, Any] = dict(
        model_path=cfg.model_path,
        n_ctx=cfg.n_ctx,
        n_threads=cfg.n_threads,
        n_gpu_layers=cfg.n_gpu_layers,
        n_batch=cfg.n_batch,
    )
    if cfg.rope_freq_base is not None:
        kw["rope_freq_base"] = cfg.rope_freq_base
    if cfg.rope_freq_scale is not None:
        kw["rope_freq_scale"] = cfg.rope_freq_scale
    return kw

def _attach_introspection(llm: Llama):
    def get_last_timings():
        for attr in ("get_timings", "timings", "perf"):
            try:
                obj = getattr(llm, attr, None)
                v = obj() if callable(obj) else obj
                if isinstance(v, dict):
                    return v
            except Exception:
                pass
        return None
    try:
        llm.get_last_timings = get_last_timings  # type: ignore[attr-defined]
    except Exception:
        pass

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            try:
                del _llm
            finally:
                _llm = None
                gc.collect()
    except Exception:
        _llm = None
        gc.collect()

def _patch_main_runtime_with_worker_llm(llm: Llama):
    """
    Make the shared pipeline use THIS worker's Llama.
    """
    try:
        from aimodel.file_read.runtime import model_runtime as MR

        # Replace getters to point at the worker's LLM
        def _ensure_ready():
            # no-op; worker initializes the model at startup
            return True

        def _get_llm():
            return llm

        # Patch
        MR.ensure_ready = _ensure_ready          # type: ignore[attr-defined]
        MR.get_llm = _get_llm                    # type: ignore[attr-defined]
        # If MR keeps a private handle, set it for good measure
        try:
            setattr(MR, "_LLM", llm)
        except Exception:
            pass

        _log("patched model_runtime to use worker LLM")
    except Exception as e:
        _log(f"failed to patch model_runtime: {e}")

@app.on_event("startup")
def _startup():
    global _llm, _cfg
    _cfg = WorkerCfg.from_env()
    kw = _build_kwargs(_cfg)
    _log(f"startup cwd={os.getcwd()} py={sys.version.split()[0]}")
    _log(f"MODEL_PATH={_cfg.model_path}")
    _log(f"kwargs={kw}")
    _llm = Llama(**kw)
    _attach_introspection(_llm)
    _patch_main_runtime_with_worker_llm(_llm)
    _log("llama initialized OK")

@app.get("/api/worker/health")
def worker_health():
    if _cfg is None:
        return {"ok": False}
    name = os.path.basename(_cfg.model_path)
    return {
        "ok": _llm is not None,
        "model": name,
        "path": _cfg.model_path,
        "n_ctx": _cfg.n_ctx,
        "n_threads": _cfg.n_threads,
        "n_gpu_layers": _cfg.n_gpu_layers,
        "n_batch": _cfg.n_batch,
    }

@app.get("/api/worker/diag")
def worker_diag():
    return {
        "cwd": os.getcwd(),
        "python": sys.version,
        "env": {
            "MODEL_PATH": os.getenv("MODEL_PATH"),
            "N_CTX": os.getenv("N_CTX"),
            "N_THREADS": os.getenv("N_THREADS"),
            "N_GPU_LAYERS": os.getenv("N_GPU_LAYERS"),
            "N_BATCH": os.getenv("N_BATCH"),
        },
        "llm_ready": _llm is not None,
    }

@app.on_event("shutdown")
def _shutdown():
    _close_llm()

def _sigterm(_signum, _frame):
    try:
        _close_llm()
    finally:
        os._exit(0)  # fast exit to release VRAM
signal.signal(signal.SIGTERM, _sigterm)

# ---------- ONE streaming route using your existing pipeline ----------
@app.post("/api/worker/generate/stream")
async def worker_generate_stream(request: Request, data: ChatBody = Body(...)) -> StreamingResponse:
    if _llm is None:
        raise HTTPException(status_code=503, detail="Model not ready")
    # Run your full pipeline INSIDE the worker
    return await generate_stream_flow(data, request)

# Cancel stays local to the worker process
@app.post("/api/worker/cancel/{session_id}")
async def worker_cancel(session_id: str):
    return await _cancel(session_id)

@app.post("/api/worker/shutdown")
def worker_shutdown():
    try:
        _close_llm()
    finally:
        os._exit(0)

# ===== frontend/src/api/admins.ts =====

import { request } from "../services/http";

export type AdminState = {
  hasAdmin: boolean;
  isAdmin: boolean; // use this to gate admin UI
  isAdminRaw?: boolean; // optional: listed but maybe not Pro
  ownerUid?: string | null;
  ownerEmail?: string | null;

  guestEnabled: boolean; // guest toggle in settings
  canSelfPromote: boolean; // show self-promote banner

  me: { uid: string; email: string; pro: boolean };
};

export const getAdminState = () => request<AdminState>("/api/admins/state");

export const selfPromote = () =>
  request<{ ok: boolean }>("/api/admins/self-promote", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
  });

export const setGuestEnabled = (enabled: boolean) =>
  request<{ ok: boolean; enabled: boolean }>("/api/admins/guest", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ enabled }),
  });

# ===== frontend/src/api/billing.ts =====

// frontend/src/file_read/api/billing.ts
import { buildUrl, getJSON } from "../services/http";

export type BillingStatus = { status: string; current_period_end: number };

export async function getBillingStatus() {
  // getJSON already wraps fetch with credentials: "include"
  return getJSON<BillingStatus>("/billing/status");
}

export async function startCheckout(priceId?: string) {
  const r = await fetch(buildUrl("/billing/checkout"), {
    method: "POST",
    credentials: "include",
    headers: { Accept: "application/json", "Content-Type": "application/json" },
    body: JSON.stringify({ price_id: priceId }),
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json() as Promise<{ url: string }>;
}

export async function openPortal() {
  const r = await fetch(buildUrl("/billing/portal"), {
    method: "POST",
    credentials: "include",
    headers: { Accept: "application/json" },
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json() as Promise<{ url: string }>;
}

# ===== frontend/src/api/devices.ts =====

// frontend/src/file_read/api/devices.ts
export type DeviceRec = {
  id: string;
  name?: string | null;
  platform?: string;
  appVersion?: string;
  lastSeen?: string;
  isCurrent?: boolean;
  exp?: number | null;
};

export async function listDevices(): Promise<DeviceRec[]> {
  const r = await fetch("/api/devices", { credentials: "include" });
  if (!r.ok) throw new Error(await r.text());
  return r.json();
}

export async function revokeDevice(id: string): Promise<{ ok: boolean }> {
  const r = await fetch(`/api/devices/${encodeURIComponent(id)}`, {
    method: "DELETE",
    credentials: "include",
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json();
}

export async function renameDevice(deviceId: string, name: string): Promise<void> {
  const r = await fetch(`/api/devices/rename`, {
    method: "POST",
    credentials: "include",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ deviceId, name }),
  });
  if (!r.ok) throw new Error(await r.text());
}

export async function recheckActivation(): Promise<void> {
  await fetch("/api/activation/recheck", { method: "POST", credentials: "include" }).catch(() => {});
}

# ===== frontend/src/api/license.ts =====

import { buildUrl } from "../services/http";

export async function refreshLicense(force = false) {
  const r = await fetch(
    buildUrl(`/license/refresh?force=${force ? "true" : "false"}`),
    {
      method: "POST",
      credentials: "include",
    },
  );
  if (!r.ok) throw new Error(await r.text());
  return r.json();
}

export async function applyLicense(license: string) {
  const r = await fetch(buildUrl(`/license/apply`), {
    method: "POST",
    credentials: "include",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ license }),
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json();
}

# ===== frontend/src/api/models.ts =====

export type ModelFile = {
  path: string;
  sizeBytes: number;
  name: string;
  rel: string;
  // optional metadata your backend may return
  mtime?: number;
  arch?: string | null;
  paramsB?: number | null;
  quant?: string | null;
  format?: string | null;
};

export type ModelsResponse = {
  available: ModelFile[];
  current: { loaded: boolean; config?: any | null } | null;
  settings?: any;
};

const BASE = "/api/models";

export async function getModels(): Promise<ModelsResponse> {
  const res = await fetch(`${BASE}`, { credentials: "include" });
  if (!res.ok) throw new Error(`getModels ${res.status}`);
  return res.json();
}

// ✅ Define a body type that includes resetDefaults
export type LoadBody = {
  modelPath: string;
  nCtx?: number;
  nThreads?: number;
  nGpuLayers?: number;
  nBatch?: number;
  ropeFreqBase?: number | null;
  ropeFreqScale?: number | null;
  resetDefaults?: boolean; // ← NEW
};

export async function loadModel(body: LoadBody) {
  const res = await fetch(`${BASE}/load`, {
    method: "POST",
    credentials: "include",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
  });
  if (!res.ok) {
    const j = await res.json().catch(() => null);
    const msg = (j && (j.error || j.detail)) || `loadModel ${res.status}`;
    throw new Error(msg);
  }
  return res.json();
}

export async function unloadModel() {
  const res = await fetch(`${BASE}/unload`, {
    method: "POST",
    credentials: "include",
  });
  if (!res.ok) throw new Error(`unloadModel ${res.status}`);
  return res.json();
}

export async function getModelHealth(): Promise<{ ok: boolean; loaded: boolean; config: any }> {
  const res = await fetch(`${BASE}/health`, { credentials: "include" });
  if (!res.ok) throw new Error(`health ${res.status}`);
  return res.json();
}

// (optional) convenience helper if you want a single call to reset to defaults:
export const loadModelWithDefaults = (modelPath: string) =>
  loadModel({ modelPath, resetDefaults: true });

export async function cancelModelLoad() {
  const res = await fetch(`/api/models/cancel-load`, {
    method: "POST",
    credentials: "include",
  });
  if (!res.ok) throw new Error(`cancelModelLoad ${res.status}`);
  return res.json();
}

# ===== frontend/src/api/modelWorkers.ts =====

import { getJSON, postJSON } from "../services/http";

export type WorkerRow = {
  id: string;
  port: number;
  model_path: string;
  status: "loading" | "ready" | "stopped";
};

export type InspectResp = {
  ok: boolean;
  workers: WorkerRow[];
  active: string | null;
  // server may also include a "system" snapshot; we don't require it here
};

export async function inspectWorkers(): Promise<InspectResp> {
  return getJSON("/api/model-workers/inspect");
}

export async function listWorkers(): Promise<InspectResp> {
  return inspectWorkers();
}

export async function spawnWorker(modelPath: string) {
  return postJSON("/api/model-workers/spawn", { modelPath });
}

export async function activateWorker(id: string) {
  return postJSON(`/api/model-workers/activate/${encodeURIComponent(id)}`, {});
}

export async function killWorker(id: string) {
  return postJSON(`/api/model-workers/kill/${encodeURIComponent(id)}`, {});
}

export async function killAllWorkers() {
  return postJSON("/api/model-workers/kill-all", {});
}

// Active worker health via proxy (requires auth; your http helpers should attach it)
export async function getActiveWorkerHealth(): Promise<any> {
  return getJSON("/api/aiw/health");
}

# ===== frontend/src/api/system.ts =====

// frontend/src/file_read/api/system.ts
import { getJSON } from "../services/http";

export type Resources = {
  os?: string; // optional (your backend returns 'platform' instead)
  cpuPct: number | null;
  ram: { total: number | null; used: number | null; free: number | null };
  vram: { total: number | null; used: number | null };// UI only needs total/used
  gpus: { index: number; name: string; total: number; used: number; free: number }[];
};

// Server shape (what your backend now returns)
type ServerResources = {
  cpu?: { countPhysical?: number; countLogical?: number; percent?: number };
  ram?: { totalBytes?: number; availableBytes?: number; usedBytes?: number; percent?: number };
  gpus?: {
    index: number;
    name: string;
    memoryTotalBytes: number;
    memoryUsedBytes: number;
    memoryFreeBytes: number;
    utilPercent: number | null;
  }[];
  platform?: string;
  gpuSource?: string;
};

function normalizeResources(s: ServerResources): Resources {
  const cpuPct = s?.cpu?.percent ?? null;

  const ramTotal = s?.ram?.totalBytes ?? null;
  const ramUsed  = s?.ram?.usedBytes ?? null;
  const ramFree  = s?.ram?.availableBytes ?? null;

  const gpus = (s?.gpus ?? []).map(g => ({
    index: g.index,
    name: g.name,
    total: g.memoryTotalBytes,
    used:  g.memoryUsedBytes,
    free:  g.memoryFreeBytes,
  }));

  const vramTotal = gpus.reduce((acc, g) => acc + (g.total || 0), 0) || null;
  const vramUsed  = gpus.reduce((acc, g) => acc + (g.used  || 0), 0) || null;

  return {
    os: s.platform,     // optional; your UI doesn’t really use it
    cpuPct,
    ram: { total: ramTotal, used: ramUsed, free: ramFree },
    vram: { total: vramTotal, used: vramUsed },
    gpus,
  };
}

export async function getResources(): Promise<Resources> {
  // IMPORTANT: include /api prefix
  const raw = await getJSON<ServerResources>("/api/system/resources");
  return normalizeResources(raw);
}

export type WorkerRow = {
  id: string;
  port: number;
  model_path: string;
  status: "loading" | "ready" | "stopped";
  health?: {
    ok: boolean;
    model: string;
    path: string;
    n_ctx: number;
    n_threads: number;
    n_gpu_layers: number;
    n_batch: number;
  } | null;
};

export const inspectWorkers = () =>
  getJSON<{ ok: boolean; workers: WorkerRow[]; active: string | null }>("/api/model-workers/inspect");

# ===== frontend/src/App.tsx =====

import AgentRunner from "./pages/AgentRunner";
import { useAuth } from "./auth/AuthContext";
import SignIn from "./auth/SignIn";
import SignUp from "./auth/SignUp";
import ForgotPassword from "./auth/ForgotPassword";
import { useState } from "react";

export default function App() {
  const { user, loading } = useAuth();
  const [mode, setMode] = useState<"signin" | "signup" | "forgot">("signin");

  // BYPASS: render app even if not signed in
  if (import.meta.env.VITE_BYPASS_AUTH === "true") {
    return (
      <main className="bg-gray-50 h-screen overflow-hidden">
        <AgentRunner />
      </main>
    );
  }

  if (loading) {
    return (
      <main className="min-h-screen grid place-items-center">Loading…</main>
    );
  }

  if (!user) {
    return (
      <main className="min-h-screen grid place-items-center bg-gray-50 px-4">
        <div className="w-full max-w-sm bg-white p-6 rounded-2xl shadow">
          {mode === "signin" && <SignIn />}
          {mode === "signup" && <SignUp />}
          {mode === "forgot" && <ForgotPassword />}

          <div className="mt-4 text-sm text-center text-gray-700">
            {mode !== "signin" && (
              <button
                onClick={() => setMode("signin")}
                className="underline mx-2"
              >
                Sign in
              </button>
            )}
            {mode !== "signup" && (
              <button
                onClick={() => setMode("signup")}
                className="underline mx-2"
              >
                Create account
              </button>
            )}
            {mode !== "forgot" && (
              <button
                onClick={() => setMode("forgot")}
                className="underline mx-2"
              >
                Forgot password
              </button>
            )}
          </div>
        </div>
      </main>
    );
  }

  return (
    <main className="bg-gray-50 h-screen overflow-hidden">
      <AgentRunner />
    </main>
  );
}

# ===== frontend/src/auth/AuthContext.tsx =====

// frontend/src/file_read/auth/AuthContext.tsx
import React, {
  createContext,
  useContext,
  useEffect,
  useMemo,
  useState,
} from "react";
import { getJSON } from "../services/http";
import { refreshLicense } from "../api/license";

type LocalUser = { email?: string; name?: string };

type Ctx = {
  user: LocalUser | null;
  loading: boolean;
  refreshMe: () => Promise<void>;
  logout: () => Promise<void>;
};

const AuthContext = createContext<Ctx>({
  user: null,
  loading: true,
  refreshMe: async () => {},
  logout: async () => {},
});

export const AuthProvider: React.FC<{ children: React.ReactNode }> = ({
  children,
}) => {
  const [user, setUser] = useState<LocalUser | null>(null);
  const [loading, setLoading] = useState(true);
  const [justLoggedOutAt, setJustLoggedOutAt] = useState<number>(0);

  const hardLogoutLocal = () => {
    try {
      localStorage.removeItem("profile_email");
    } catch {}
    setUser(null);
  };

  const logout = async () => {
    try {
      await fetch("/api/auth/logout", {
        method: "POST",
        credentials: "include",
      });
    } catch {}
    hardLogoutLocal();
    setJustLoggedOutAt(Date.now());
  };

  const refreshMe = async () => {
    try {
      const me = await getJSON<LocalUser>("/auth/me");
      setUser(me || null);
      if (me?.email) {
        try {
          localStorage.setItem("profile_email", me.email);
        } catch {}
        try {
          window.dispatchEvent(new Event("auth:ready"));
        } catch {}
      }
      await refreshLicense().catch(() => {});
    } catch {
      hardLogoutLocal();
    }
  };
  // Initial load
  useEffect(() => {
    let cancelled = false;
    (async () => {
      try {
        if (Date.now() - justLoggedOutAt < 800) return;
        const me = await getJSON<LocalUser>("/auth/me");
        if (!cancelled) {
          setUser(me || null);
          if (me?.email) {
            try {
              localStorage.setItem("profile_email", me.email);
            } catch {}
            try {
              window.dispatchEvent(new Event("auth:ready"));
            } catch {}
          }
        }
      } catch {
        if (!cancelled) hardLogoutLocal();
      } finally {
        if (!cancelled) setLoading(false);
      }
    })();
    return () => {
      cancelled = true;
    };
  }, [justLoggedOutAt]);

  const ctx = useMemo<Ctx>(
    () => ({ user, loading, refreshMe, logout }),
    [user, loading],
  );

  return <AuthContext.Provider value={ctx}>{children}</AuthContext.Provider>;
};

export const useAuth = () => useContext(AuthContext);

# ===== frontend/src/auth/ForgotPassword.tsx =====

// frontend/src/file_read/auth/ForgotPassword.tsx
import { useState } from "react";
import { postJSON } from "../services/http";

export default function ForgotPassword() {
  const [email, setEmail] = useState("");
  const [sent, setSent] = useState(false);
  const [err, setErr] = useState<string | null>(null);
  const [submitting, setSubmitting] = useState(false);

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    setErr(null);
    setSent(false);
    setSubmitting(true);
    try {
      // Backend should implement: POST /auth/forgot  { email }
      // Behavior suggestion:
      //  - If SMTP configured: send email with reset link/token
      //  - If offline: generate token and print reset URL to server logs (admin shares it)
      await postJSON("/auth/forgot", { email: email.trim().toLowerCase() });
      setSent(true);
    } catch (e: any) {
      // Common cases: 404 if endpoint not implemented, or 400/422 for bad email
      const msg = (e?.message as string) || "";
      if (/HTTP 404/i.test(msg)) {
        setErr(
          "Password reset isn’t enabled on this box. Ask the admin to reset your password.",
        );
      } else {
        setErr(
          msg.replace(/^HTTP \d+\s*–\s*/, "") || "Could not send reset request",
        );
      }
    } finally {
      setSubmitting(false);
    }
  }

  return (
    <form onSubmit={onSubmit} className="space-y-4">
      <h1 className="text-xl font-semibold text-center">Reset password</h1>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Email</label>
        <input
          value={email}
          onChange={(e) => setEmail(e.target.value)}
          type="email"
          inputMode="email"
          autoComplete="email"
          required
          className="w-full rounded-lg border border-gray-300 px-3 py-2 outline-none focus:ring-2 focus:ring-black"
          placeholder="you@example.com"
        />
      </div>

      {sent && (
        <div className="text-sm text-green-700 bg-green-50 border border-green-200 rounded-lg px-3 py-2">
          If password reset is enabled, a link has been sent (or printed in the
          server logs if email isn’t configured). Contact your admin if you
          don’t receive it.
        </div>
      )}

      {err && (
        <div className="text-sm text-red-600 bg-red-50 border border-red-200 rounded-lg px-3 py-2">
          {err}
        </div>
      )}

      <button
        type="submit"
        disabled={submitting}
        className="w-full rounded-lg bg-black text-white py-2.5 font-medium disabled:opacity-60"
      >
        {submitting ? "Sending…" : "Send reset request"}
      </button>
    </form>
  );
}

# ===== frontend/src/auth/localAuth.ts =====

import { buildUrl } from "../services/http";

export async function localRegister(email: string, password: string) {
  const r = await fetch(buildUrl("/auth/register"), {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    credentials: "include",
    body: JSON.stringify({ email, password }),
  });
  if (!r.ok) throw new Error(await r.text());
}

export async function localLogin(email: string, password: string) {
  const r = await fetch(buildUrl("/auth/login"), {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    credentials: "include",
    body: JSON.stringify({ email, password }),
  });
  if (!r.ok) throw new Error(await r.text());
  try {
    localStorage.setItem("profile_email", email);
  } catch {}
}

export async function localLogout() {
  await fetch(buildUrl("/auth/logout"), {
    method: "POST",
    credentials: "include",
  });
}

# ===== frontend/src/auth/SignIn.tsx =====

import { useState } from "react";
import { localLogin, localRegister } from "./localAuth";
import { useAuth } from "./AuthContext";

export default function SignIn() {
  const { refreshMe } = useAuth();
  const [email, setEmail] = useState("");
  const [pw, setPw] = useState("");
  const [err, setErr] = useState<string | null>(null);
  const [mode, setMode] = useState<"signin" | "signup">("signin");
  const [busy, setBusy] = useState(false);

  async function submit(e: React.FormEvent) {
    e.preventDefault();
    setErr(null);
    setBusy(true);
    try {
      const em = email.trim().toLowerCase();
      if (!em) throw new Error("Email is required");
      if (mode === "signup") {
        if (pw.length < 6)
          throw new Error("Password must be at least 6 characters");
        await localRegister(em, pw);
      }
      await localLogin(em, pw);
      await refreshMe();
    } catch (e: any) {
      setErr(e?.message || "Auth failed");
    } finally {
      setBusy(false);
    }
  }

  return (
    <form onSubmit={submit} className="space-y-3">
      <h1 className="text-lg font-semibold">
        {mode === "signin" ? "Sign in" : "Create account"}
      </h1>
      <input
        className="w-full border rounded px-3 py-2"
        placeholder="Email"
        value={email}
        onChange={(e) => setEmail(e.target.value)}
      />
      <input
        className="w-full border rounded px-3 py-2"
        placeholder="Password"
        type="password"
        value={pw}
        onChange={(e) => setPw(e.target.value)}
      />
      {err && <div className="text-sm text-red-600">{err}</div>}
      <button
        disabled={busy}
        className="w-full rounded bg-black text-white py-2"
      >
        {busy ? "Please wait…" : mode === "signin" ? "Sign in" : "Sign up"}
      </button>
      <div className="text-sm text-center">
        {mode === "signin" ? (
          <button
            type="button"
            className="underline"
            onClick={() => setMode("signup")}
          >
            Create account
          </button>
        ) : (
          <button
            type="button"
            className="underline"
            onClick={() => setMode("signin")}
          >
            Have an account? Sign in
          </button>
        )}
      </div>
    </form>
  );
}

# ===== frontend/src/auth/SignUp.tsx =====

import { useState } from "react";
import { Eye, EyeOff } from "lucide-react";
import { localRegister, localLogin } from "./localAuth";
import { useAuth } from "./AuthContext";

export default function SignUp() {
  const { refreshMe } = useAuth();
  const [email, setEmail] = useState("");
  const [pw, setPw] = useState("");
  const [confirmPw, setConfirmPw] = useState("");
  const [err, setErr] = useState<string | null>(null);
  const [submitting, setSubmitting] = useState(false);
  const [showPw, setShowPw] = useState(false);
  const [showConfirm, setShowConfirm] = useState(false);

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    setErr(null);
    const em = email.trim().toLowerCase();
    if (!em) return setErr("Email is required");
    if (pw.length < 6) return setErr("Password must be at least 6 characters");
    if (pw !== confirmPw) return setErr("Passwords do not match");
    setSubmitting(true);
    try {
      await localRegister(em, pw);
      await localLogin(em, pw);
      await refreshMe();
    } catch (e: any) {
      setErr(e?.message || "Sign up failed");
      setSubmitting(false);
    }
  }

  return (
    <form onSubmit={onSubmit} className="space-y-4">
      <h1 className="text-xl font-semibold text-center">Create account</h1>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Email</label>
        <input
          value={email}
          onChange={(e) => setEmail(e.target.value)}
          type="email"
          inputMode="email"
          autoComplete="email"
          required
          className="w-full rounded-lg border border-gray-300 px-3 py-2 outline-none focus:ring-2 focus:ring-black"
          placeholder="you@example.com"
        />
      </div>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Password</label>
        <div className="relative">
          <input
            value={pw}
            onChange={(e) => setPw(e.target.value)}
            type={showPw ? "text" : "password"}
            autoComplete="new-password"
            minLength={6}
            required
            className="w-full rounded-lg border border-gray-300 px-3 py-2 pr-10 outline-none focus:ring-2 focus:ring-black"
            placeholder="At least 6 characters"
          />
          <button
            type="button"
            onClick={() => setShowPw((s) => !s)}
            className="absolute inset-y-0 right-0 flex items-center pr-3 text-gray-500"
            aria-label={showPw ? "Hide password" : "Show password"}
          >
            {showPw ? <EyeOff size={18} /> : <Eye size={18} />}
          </button>
        </div>
      </div>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Confirm Password</label>
        <div className="relative">
          <input
            value={confirmPw}
            onChange={(e) => setConfirmPw(e.target.value)}
            type={showConfirm ? "text" : "password"}
            autoComplete="new-password"
            minLength={6}
            required
            className="w-full rounded-lg border border-gray-300 px-3 py-2 pr-10 outline-none focus:ring-2 focus:ring-black"
            placeholder="Re-enter password"
          />
          <button
            type="button"
            onClick={() => setShowConfirm((s) => !s)}
            className="absolute inset-y-0 right-0 flex items-center pr-3 text-gray-500"
            aria-label={
              showConfirm ? "Hide confirm password" : "Show confirm password"
            }
          >
            {showConfirm ? <EyeOff size={18} /> : <Eye size={18} />}
          </button>
        </div>
      </div>

      {err && (
        <div className="text-sm text-red-600 bg-red-50 border border-red-200 rounded-lg px-3 py-2">
          {err}
        </div>
      )}

      <button
        type="submit"
        disabled={submitting}
        className="w-full rounded-lg bg-black text-white py-2.5 font-medium disabled:opacity-60"
      >
        {submitting ? "Creating…" : "Create account"}
      </button>
    </form>
  );
}

# ===== frontend/src/components/AdminBanner.tsx =====

import { useEffect, useState } from "react";
import { getAdminState, selfPromote } from "../api/admins";

export default function AdminBanner() {
  const [can, setCan] = useState(false);
  const [busy, setBusy] = useState(false);
  const [err, setErr] = useState<string | null>(null);

  useEffect(() => {
    let cancelled = false;
    (async () => {
      try {
        const s = await getAdminState();
        if (!cancelled) setCan(!!s.canSelfPromote);
      } catch {
        /* ignore */
      }
    })();
    return () => {
      cancelled = true;
    };
  }, []);

  if (!can) return null;

  return (
    <div className="rounded-lg border border-amber-300 bg-amber-50 p-3 text-sm">
      <div className="font-medium text-amber-800">Admin setup</div>
      <div className="text-amber-800/90 mt-1">
        No admins exist yet. Click below to make yourself the first admin
        (requires Pro).
      </div>

      <button
        disabled={busy}
        onClick={async () => {
          setBusy(true);
          setErr(null);
          try {
            await selfPromote();
            location.reload();
          } catch (e: any) {
            setErr(e?.message || "Failed to self-promote");
          } finally {
            setBusy(false);
          }
        }}
        className="mt-2 inline-flex items-center rounded bg-black px-3 py-1.5 text-white disabled:opacity-60"
      >
        {busy ? "Promoting…" : "Make me admin"}
      </button>

      {err && <div className="mt-2 text-xs text-red-600">{err}</div>}
    </div>
  );
}

# ===== frontend/src/components/AdminManagement.tsx =====

import { useEffect, useState } from "react";
import { getAdminState } from "../api/admins";

export default function AdminManagement() {
  const [state, setState] = useState<Awaited<
    ReturnType<typeof getAdminState>
  > | null>(null);
  const [err, setErr] = useState<string | null>(null);

  async function reload() {
    setErr(null);
    try {
      const s = await getAdminState();
      setState(s);
    } catch (e: any) {
      setErr(e?.message || "Failed to load admin state");
    }
  }

  useEffect(() => {
    reload();
  }, []);

  if (!state) return null;

  // Try to show the current admin (support both old & new shapes)
  const ownerFromList = (state as any).admins?.[0] as
    | { uid?: string; email?: string }
    | undefined;
  const ownerEmail =
    (state as any).ownerEmail ??
    ownerFromList?.email ??
    (state.isAdmin ? state.me.email : null);

  const ownerUid =
    (state as any).ownerUid ??
    (state as any).primaryUid ??
    ownerFromList?.uid ??
    (state.isAdmin ? state.me.uid : null);

  const hasAdmin =
    typeof (state as any).hasAdmin === "boolean"
      ? (state as any).hasAdmin
      : !!(state as any).hasAdmins; // <— add (state as any) here

  return (
    <div className="rounded-2xl border p-4">
      <div className="font-semibold mb-3">Admin status</div>

      <div className="text-sm space-y-2">
        <div>
          <span className="text-gray-600 mr-1">Admin account:</span>
          <b>
            {ownerEmail
              ? `${ownerEmail} (${ownerUid ?? "—"})`
              : hasAdmin
                ? "Unknown"
                : "—"}
          </b>
        </div>

        {state.isAdmin && (
          <div className="text-green-700">
            You are the admin on this device. Admin features are enabled for
            you.
          </div>
        )}

        {!state.isAdmin && hasAdmin && (
          <div className="text-gray-700">
            This device already has an admin. Admin controls are available only
            to that account.
          </div>
        )}

        {!hasAdmin && (
          <div className="text-amber-700">
            No admin yet. Use the “Make me admin” banner above (requires Pro) to
            claim admin.
          </div>
        )}
      </div>

      {err && <div className="mt-2 text-xs text-red-600">{err}</div>}
    </div>
  );
}

# ===== frontend/src/components/AdminScopeToggle.tsx =====

import { useEffect, useState } from "react";
import { getAdminState } from "../api/admins";
import { getAdminChatScope, setAdminChatScope } from "../hooks/adminChatsApi";
import type { AdminChatScope } from "../hooks/adminChatsApi";

export default function AdminScopeToggle() {
  const [isAdmin, setIsAdmin] = useState(false);
  const [scope, setScope] = useState<AdminChatScope>(() => getAdminChatScope());

  useEffect(() => {
    let cancelled = false;
    (async () => {
      try {
        const s = await getAdminState();
        if (!cancelled) setIsAdmin(!!s.isAdmin);
      } catch {
        /* ignore */
      }
    })();
    return () => {
      cancelled = true;
    };
  }, []);

  if (!isAdmin) return null;

  function choose(next: AdminChatScope) {
    setScope(next);
    setAdminChatScope(next); // persists + dispatches "admin:scope"
  }

  return (
    <div
      className="inline-flex rounded-lg border overflow-hidden text-xs"
      role="group"
      aria-label="Admin chat scope"
    >
      <button
        onClick={() => choose("mine")}
        className={`px-2 py-1 ${scope === "mine" ? "bg-black text-white" : "bg-white hover:bg-gray-50"}`}
        title="Show only your own chats"
      >
        Mine
      </button>
      <button
        onClick={() => choose("all")}
        className={`px-2 py-1 ${scope === "all" ? "bg-black text-white" : "bg-white hover:bg-gray-50"}`}
        title="Show all users’ chats"
      >
        All
      </button>
    </div>
  );
}

# ===== frontend/src/components/AssistantMetrics.tsx =====

// frontend/src/file_read/components/chat/AssistantMetrics.tsx
import { Info } from "lucide-react";
import MetricsHoverCard from "./MetricsHoverCard";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function AssistantMetrics({
  status,
  runJson,
  flat,
  align = "right",
}: {
  status: string;
  runJson?: RunJson | null;
  flat?: GenMetrics | null;
  align?: "left" | "right";
}) {
  return (
    <div className="mt-2 flex justify-start">
      <div className="inline-flex items-center gap-1 px-2 py-1 rounded-full bg-white border shadow-sm text-[11px] text-gray-600">
        <Info className="w-3.5 h-3.5 opacity-70" />
        <span className="truncate max-w-[70vw] sm:max-w-none">
          {status || "Run details"}
        </span>
        <MetricsHoverCard
          data={
            runJson ??
            (flat
              ? {
                  stats: {
                    stopReason: flat.stop_reason ?? null,
                    tokensPerSecond: flat.tok_per_sec ?? null,
                    timeToFirstTokenSec:
                      flat.ttft_ms != null
                        ? Math.max(0, flat.ttft_ms) / 1000
                        : null,
                    totalTimeSec: null,
                    promptTokensCount: flat.input_tokens_est ?? null,
                    predictedTokensCount: flat.output_tokens ?? null,
                    totalTokensCount: flat.total_tokens_est ?? null,
                  },
                }
              : null)
          }
          title="Run JSON"
          align={align}
          compact
        />
      </div>
    </div>
  );
}

# ===== frontend/src/components/Budget/BudgetBar.tsx =====

// frontend/src/file_read/components/Budget/BudgetBar.tsx

import { useState } from "react";
import {
  type RunJson,
  getNormalizedBudget,
  getRagTelemetry,
  getWebTelemetry,
  getTimingMetrics,
  getPackTelemetry,
  getThroughput,
} from "../../shared/lib/runjson";
import { RagPanel, WebPanel, TimingPanel } from "./BudgetBarPanelsExtras";
import { num, PackPanel } from "./BudgetBarPanelsCore";

import { ChevronDown, ChevronUp } from "lucide-react";

function pct(n: number, d: number) {
  if (!Number.isFinite(n) || !Number.isFinite(d) || d <= 0) return 0;
  return Math.max(0, Math.min(100, (n / d) * 100));
}

export default function BudgetBar({ runJson }: { runJson?: RunJson | null }) {
  const nb = getNormalizedBudget(runJson ?? undefined);
  if (!nb) return null;

  const [open, setOpen] = useState(false);

  const rag = getRagTelemetry(runJson ?? undefined) as any | null;
  const web = getWebTelemetry(runJson ?? undefined) as any | null;
  const pack = getPackTelemetry(runJson ?? undefined) as any | null;
  const timing = getTimingMetrics(runJson ?? undefined) as any | null;
  const tps = getThroughput(runJson ?? undefined);

  const breakdown =
    (runJson as any)?.budget_view?.breakdown ??
    (runJson as any)?.stats?.budget?.breakdown ??
    null;

  const modelCtx = num(nb.modelCtx);
  const clampMargin = num(nb.clampMargin);
  const inputTokensEst = num(nb.inputTokensEst);
  const outBudgetChosen = num(nb.outBudgetChosen);
  const outActual = num(runJson?.stats?.predictedTokensCount);
  const outShown = outActual || outBudgetChosen;

  const used = inputTokensEst + outShown + clampMargin;
  const fullPct = pct(used, modelCtx);

  const ragDelta = Math.max(
    0,
    num(rag?.ragTokensAdded) ||
      num(rag?.blockTokens) ||
      num(rag?.blockTokensApprox) ||
      num(rag?.sessionOnlyTokensApprox),
  );
  const ragWasInjected = !!(rag?.injected || rag?.sessionOnly || ragDelta > 0);
  const ragPctOfInput =
    inputTokensEst > 0 ? Math.round((ragDelta / inputTokensEst) * 100) : 0;
  const ragBlockBuildTime =
    rag?.injectBuildSec ?? rag?.blockBuildSec ?? rag?.sessionOnlyBuildSec;

  const webRouteSec = web?.elapsedSec;
  const webFetchSec = web?.fetchElapsedSec;
  const webInjectSec = web?.injectElapsedSec;
  const webPre =
    num((web as any)?.breakdown?.totalWebPreTtftSec) ||
    num(webRouteSec) + num(webFetchSec) + num(webInjectSec);

  const packPackSec = num(pack?.packSec);
  const packSummarySec = num(pack?.summarySec);
  const packFinalTrimSec = num(pack?.finalTrimSec);
  const packCompressSec = num(pack?.compressSec);
  const packSummaryTokens = num(pack?.summaryTokensApprox);
  const packSummaryUsedLLM = !!pack?.summaryUsedLLM;

  const droppedMsgs = num((pack as any)?.finalTrimDroppedMsgs);
  const droppedApproxTok = num((pack as any)?.finalTrimDroppedApproxTokens);
  const sumShrinkFrom = num((pack as any)?.finalTrimSummaryShrunkFromChars);
  const sumShrinkTo = num((pack as any)?.finalTrimSummaryShrunkToChars);
  const sumShrinkDropped = num((pack as any)?.finalTrimSummaryDroppedChars);
  const rolledPeeledMsgs = num((pack as any)?.rollPeeledMsgs);
  const rollNewSummaryTokens = num((pack as any)?.rollNewSummaryTokensApprox);

  const engine = timing?.engine || null;
  const engineLoadSec = num(engine?.loadSec);
  const enginePromptSec = num(engine?.promptSec);
  const engineEvalSec = num(engine?.evalSec);
  const enginePromptN = engine?.promptN;
  const engineEvalN = engine?.evalN;

  const preModelSec = num(timing?.preModelSec);
  const modelQueueSec = num(timing?.modelQueueSec);

  const preAccountedFromBackend = num(breakdown?.preTtftAccountedSec);
  const accountedFallback =
    webPre +
    num(rag?.routerDecideSec) +
    num(ragBlockBuildTime) +
    packPackSec +
    packSummarySec +
    packFinalTrimSec +
    packCompressSec +
    preModelSec +
    modelQueueSec;
  const accounted = preAccountedFromBackend || accountedFallback;

  const unattributed =
    breakdown && Number.isFinite(breakdown.unattributedTtftSec)
      ? num(breakdown.unattributedTtftSec)
      : Math.max(0, num(timing?.ttftSec) - accounted);

  return (
    <div className="px-3 py-2 border-t bg-white/90 backdrop-blur sticky bottom-0 z-40">
      <div className="flex items-center gap-2 text-[11px] text-gray-700">
        <button
          type="button"
          onClick={() => setOpen((v) => !v)}
          aria-expanded={open ? "true" : "false"}
          className="shrink-0 inline-flex items-center gap-1 px-2 h-6 rounded border bg-white hover:bg-gray-50"
          title={open ? "Hide details" : "Show details"}
        >
          {open ? (
            <ChevronDown className="w-3.5 h-3.5" />
          ) : (
            <ChevronUp className="w-3.5 h-3.5" />
          )}
          <span className="hidden sm:inline">Details</span>
        </button>

        <div
          className="flex-1 h-1.5 rounded bg-gray-200 overflow-hidden"
          title={`Context ${fullPct.toFixed(1)}%`}
        >
          <div className="h-1.5 bg-black" style={{ width: `${fullPct}%` }} />
        </div>

        <div className="whitespace-nowrap hidden xs:block">
          In: <span className="font-medium">{inputTokensEst}</span>
        </div>
        <div className="whitespace-nowrap hidden xs:block">
          Out: <span className="font-medium">{outShown}</span>
        </div>
        <div className="whitespace-nowrap hidden sm:block">
          Ctx: <span className="font-medium">{modelCtx}</span>
        </div>
        <div className="whitespace-nowrap text-gray-500 hidden md:block">
          {`Context is ${fullPct.toFixed(1)}% full`}
        </div>
      </div>

      {open && (
        <div className="mt-2 max-h-40 sm:max-h-48 md:max-h-56 overflow-y-auto pr-1 pb-1 -mr-1">
          {pack && (
            <PackPanel
              pack={pack}
              packPackSec={packPackSec}
              packSummarySec={packSummarySec}
              packFinalTrimSec={packFinalTrimSec}
              packCompressSec={packCompressSec}
              packSummaryTokens={packSummaryTokens}
              packSummaryUsedLLM={packSummaryUsedLLM}
              droppedMsgs={droppedMsgs}
              droppedApproxTok={droppedApproxTok}
              sumShrinkFrom={sumShrinkFrom}
              sumShrinkTo={sumShrinkTo}
              sumShrinkDropped={sumShrinkDropped}
              rolledPeeledMsgs={rolledPeeledMsgs}
              rollNewSummaryTokens={rollNewSummaryTokens}
            />
          )}

          {rag && (
            <RagPanel
              rag={rag}
              ragWasInjected={ragWasInjected}
              ragBlockBuildTime={ragBlockBuildTime}
              ragDelta={ragDelta}
              ragPctOfInput={ragPctOfInput}
              inputTokensEst={inputTokensEst}
            />
          )}

          {web && <WebPanel web={web} />}

          {timing && (
            <TimingPanel
              timing={timing}
              enginePromptSec={enginePromptSec}
              engineEvalSec={engineEvalSec}
              engineLoadSec={engineLoadSec}
              enginePromptN={enginePromptN}
              engineEvalN={engineEvalN}
              preModelSec={preModelSec}
              modelQueueSec={modelQueueSec}
              unattributed={unattributed}
              encodeTps={tps?.encodeTps ?? null}
              decodeTps={tps?.decodeTps ?? null}
              overallTps={tps?.overallTps ?? null}
            />
          )}
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/components/Budget/BudgetBarPanelsCore.tsx =====

// frontend/src/file_read/components/Budget/BudgetBarPanelsCore.tsx
export const num = (v: unknown) =>
  typeof v === "number" && Number.isFinite(v) ? v : 0;

export function fmtSec(v?: number) {
  if (v == null || !Number.isFinite(v)) return "—";
  if (v < 0.01) return "<0.01s";
  return `${v.toFixed(2)}s`;
}

export function fmtTps(v?: number | null) {
  if (v == null || !Number.isFinite(v)) return "—";
  if (v < 1) return v.toFixed(2);
  if (v < 10) return v.toFixed(1);
  return Math.round(v).toString();
}

type PackPanelProps = {
  pack: any;
  packPackSec: number;
  packSummarySec: number;
  packFinalTrimSec: number;
  packCompressSec: number;
  packSummaryTokens: number;
  packSummaryUsedLLM: boolean;
  droppedMsgs: number;
  droppedApproxTok: number;
  sumShrinkFrom: number;
  sumShrinkTo: number;
  sumShrinkDropped: number;
  rolledPeeledMsgs: number;
  rollNewSummaryTokens: number;
};

export function PackPanel({
  pack,
  packPackSec,
  packSummarySec,
  packFinalTrimSec,
  packCompressSec,
  packSummaryTokens,
  packSummaryUsedLLM,
  droppedMsgs,
  droppedApproxTok,
  sumShrinkFrom,
  sumShrinkTo,
  sumShrinkDropped,
  rolledPeeledMsgs,
  rollNewSummaryTokens,
}: PackPanelProps) {
  return (
    <div className="mt-2 text-[11px] text-gray-700">
      <div className="flex flex-wrap items-center gap-x-4 gap-y-1">
        {"packSec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            pack {fmtSec(packPackSec)}
          </span>
        )}
        {"summarySec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            summary {fmtSec(packSummarySec)}{" "}
            {packSummaryUsedLLM ? "(llm)" : "(fast)"}
          </span>
        )}
        {"finalTrimSec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            trim {fmtSec(packFinalTrimSec)}
          </span>
        )}
        {"compressSec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            compress {fmtSec(packCompressSec)}
          </span>
        )}
        {"summaryTokensApprox" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            sumTokens≈<b>{packSummaryTokens}</b>
          </span>
        )}
        {"packedChars" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            packed chars=<b>{(pack as any).packedChars}</b>
          </span>
        )}
        {"messages" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            msgs=<b>{(pack as any).messages}</b>
          </span>
        )}

        {(droppedMsgs > 0 || droppedApproxTok > 0) && (
          <span className="px-1.5 py-0.5 rounded bg-red-50 border border-red-200 text-red-700">
            dropped msgs=<b>{droppedMsgs}</b>
            {droppedApproxTok ? (
              <>
                {" "}
                / ≈<b>{droppedApproxTok}</b> tok
              </>
            ) : null}
          </span>
        )}

        {sumShrinkDropped > 0 && (
          <span className="px-1.5 py-0.5 rounded bg-amber-50 border border-amber-200 text-amber-800">
            summary shrink {sumShrinkFrom}→{sumShrinkTo} chars (−
            <b>{sumShrinkDropped}</b>)
          </span>
        )}

        {rolledPeeledMsgs > 0 && (
          <span className="px-1.5 py-0.5 rounded bg-blue-50 border border-blue-200 text-blue-800">
            rolled: <b>{rolledPeeledMsgs}</b> msgs → +sum≈
            <b>{rollNewSummaryTokens}</b> tok
          </span>
        )}
      </div>
    </div