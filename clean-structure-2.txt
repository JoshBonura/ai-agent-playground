()
    _dbg("final_len=", len(final))
    return final

# ===== aimodel/file_read/rag/ingest/pdf_ingest.py =====

# aimodel/file_read/rag/ingest/pdf_ingest.py
from __future__ import annotations
from typing import Tuple
import io
from ...core.settings import SETTINGS
from .ocr import is_bad_text, ocr_pdf
from .common import _utf8

def _dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[pdf_ingest]", *args, flush=True)
    except Exception:
        pass

def extract_pdf(data: bytes) -> Tuple[str, str]:
    print("[pdf_ingest] ENTER extract_pdf", flush=True)

    S = SETTINGS.effective
    OCR_ENABLED = bool(S().get("pdf_ocr_enable", False))
    OCR_MODE = str(S().get("pdf_ocr_mode", "auto")).lower()   # auto | force | never
    WHEN_BAD  = bool(S().get("pdf_ocr_when_bad", True))
    DPI       = int(S().get("pdf_ocr_dpi", 300))
    MAX_PAGES = int(S().get("pdf_ocr_max_pages", 0))

    # Unconditional config echo so we KNOW what the server is using
    print(f"[pdf_ingest] cfg ocr_enabled={OCR_ENABLED} mode={OCR_MODE} when_bad={WHEN_BAD} dpi={DPI} max_pages={MAX_PAGES}", flush=True)

    def _do_ocr() -> str:
        print("[pdf_ingest] OCR_CALL begin", flush=True)  # <-- undeniable marker
        txt = (ocr_pdf(data) or "").strip()
        print(f"[pdf_ingest] OCR_CALL end text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
        return txt

    # FORCE: run OCR up front
    if OCR_ENABLED and OCR_MODE == "force":
        _dbg("mode=force -> OCR first")
        ocr_txt = _do_ocr()
        if ocr_txt:
            print("[pdf_ingest] EXIT (force OCR success)", flush=True)
            return ocr_txt, "text/plain"
        _dbg("mode=force -> OCR empty, trying text extract")

    # Try embedded text (pdfminer)
    txt = ""
    try:
        from pdfminer.high_level import extract_text
        txt = (extract_text(io.BytesIO(data)) or "").strip()
        print(f"[pdf_ingest] pdfminer text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
    except Exception as e:
        print(f"[pdf_ingest] pdfminer ERROR {repr(e)}", flush=True)
        txt = ""

    # AUTO: OCR if missing/weak text
    if OCR_ENABLED and OCR_MODE != "never":
        try_ocr = (not txt) or (WHEN_BAD and is_bad_text(txt))
        print(f"[pdf_ingest] auto-eval try_ocr={try_ocr} has_text={bool(txt)} is_bad={(is_bad_text(txt) if txt else 'n/a')}", flush=True)
        if try_ocr:
            ocr_txt = _do_ocr()
            if ocr_txt:
                print("[pdf_ingest] EXIT (auto OCR success)", flush=True)
                return ocr_txt, "text/plain"

    # Fallback: PyPDF2
    if not txt:
        try:
            from PyPDF2 import PdfReader
            r = PdfReader(io.BytesIO(data))
            pages = [(p.extract_text() or "").strip() for p in r.pages]
            txt = "\n\n".join([p for p in pages if p]).strip()
            print(f"[pdf_ingest] pypdf2 text_len={len(txt)} preview={repr((txt or '')[:120])}", flush=True)
        except Exception as e2:
            print(f"[pdf_ingest] pypdf2 ERROR {repr(e2)}", flush=True)
            txt = _utf8(data)
            print(f"[pdf_ingest] bytes-fallback text_len={len(txt)}", flush=True)

    final = (txt.strip() if txt else "")
    print(f"[pdf_ingest] EXIT (returned_len={len(final)})", flush=True)
    return final, "text/plain"

# ===== aimodel/file_read/rag/ingest/ppt_ingest.py =====

# OCR-enabled PPT ingest (minimal changes per request)
from __future__ import annotations
from typing import Tuple, List
import io, re
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile(r"[ \t]+")
def _squeeze(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _clip(s: str, limit: int) -> str:
    if limit > 0 and len(s) > limit:
        return s[:limit] + "…"
    return s

def _shape_text(shape) -> List[str]:
    out: List[str] = []
    if getattr(shape, "has_text_frame", False):
        for p in shape.text_frame.paragraphs:
            txt = _squeeze("".join(r.text for r in p.runs))
            if txt:
                out.append(txt)
    if getattr(shape, "has_table", False):
        tbl = shape.table
        for r in tbl.rows:
            cells = [_squeeze(c.text) for c in r.cells]
            if any(cells):
                out.append(" | ".join(c for c in cells if c))
    if getattr(shape, "shape_type", None) and str(shape.shape_type) == "GROUP":
        try:
            for sh in getattr(shape, "shapes", []):
                out.extend(_shape_text(sh))
        except Exception:
            pass
    try:
        S = SETTINGS.effective
        if bool(S().get("pptx_ocr_images", False)):
            is_pic = getattr(shape, "shape_type", None)
            if is_pic and "PICTURE" in str(is_pic):
                img = getattr(shape, "image", None)
                blob = getattr(img, "blob", None) if img is not None else None
                if blob and len(blob) >= int(S().get("ocr_min_image_bytes", 16384)):
                    print("[OCR] candidate image size:", len(blob))
                    t = (ocr_image_bytes(blob) or "").strip()
                    print("[OCR] result:", repr(t[:200]))
                    if t:
                        out.append(t)
    except Exception as e:
        print("[OCR] error:", e)
    return out

def extract_pptx(data: bytes) -> Tuple[str, str]:
    from pptx import Presentation
    S = SETTINGS.effective
    USE_MD = bool(S().get("pptx_use_markdown_headings", True))
    INCLUDE_NOTES = bool(S().get("pptx_include_notes", True))
    INCLUDE_TABLES = bool(S().get("pptx_include_tables", True))
    DROP_EMPTY = bool(S().get("pptx_drop_empty_lines", True))
    MAX_PARA = int(S().get("pptx_para_max_chars", 0))
    NUMBER_SLIDES = bool(S().get("pptx_number_slides", True))

    prs = Presentation(io.BytesIO(data))
    lines: List[str] = []

    for i, slide in enumerate(prs.slides, start=1):
        title = ""
        try:
            if getattr(slide, "shapes", None):
                for sh in slide.shapes:
                    if getattr(sh, "is_placeholder", False) and str(getattr(sh, "placeholder_format", "").type).lower().endswith("title"):
                        title = _squeeze(getattr(sh, "text", "") or "")
                        break
        except Exception:
            pass

        head = f"Slide {i}" + (f": {title}" if title else "")
        if USE_MD:
            lines.append(("## " if NUMBER_SLIDES else "## ") + head)
        else:
            lines.append(head)

        body: List[str] = []
        for sh in getattr(slide, "shapes", []):
            if getattr(sh, "has_table", False) and not INCLUDE_TABLES:
                continue
            body.extend(_shape_text(sh))

        for t in body:
            t = _clip(t, MAX_PARA)
            if t or not DROP_EMPTY:
                lines.append(t)

        if INCLUDE_NOTES:
            try:
                notes = slide.notes_slide
                if notes and getattr(notes, "notes_text_frame", None):
                    note_txt = _squeeze(notes.notes_text_frame.text)
                    if note_txt:
                        lines.append("")
                        lines.append("### Notes")
                        for ln in note_txt.splitlines():
                            ln = _squeeze(ln)
                            if ln or not DROP_EMPTY:
                                lines.append(_clip(ln, MAX_PARA))
            except Exception:
                pass

        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

def extract_ppt(data: bytes) -> Tuple[str, str]:
    from .doc_binary_ingest import _generic_ole_text
    S = SETTINGS.effective
    DROP_EMPTY = bool(S().get("ppt_drop_empty_lines", True))
    DEDUPE = bool(S().get("ppt_dedupe_lines", True))
    MAX_PARA = int(S().get("ppt_max_line_chars", 600))
    MIN_ALPHA = float(S().get("ppt_min_alpha_ratio", 0.4))
    MAX_PUNCT = float(S().get("ppt_max_punct_ratio", 0.5))
    TOKEN_MAX = int(S().get("ppt_token_max_chars", 40))

    raw = _generic_ole_text(data)
    if not raw:
        try:
            raw = data.decode("utf-8", errors="ignore")
        except Exception:
            raw = ""

    out: List[str] = []
    seen = set()
    for ln in (raw.splitlines() if raw else []):
        s = _squeeze(ln)
        if not s and DROP_EMPTY:
            continue
        if MAX_PARA > 0 and len(s) > MAX_PARA:
            s = s[:MAX_PARA] + "…"
        if s:
            letters = sum(1 for c in s if c.isalpha())
            alen = max(1, len(s))
            if letters / alen < MIN_ALPHA:
                continue
            punct = sum(1 for c in s if not c.isalnum() and not c.isspace())
            if punct / alen > MAX_PUNCT:
                continue
            if " " not in s and len(s) <= TOKEN_MAX and re.fullmatch(r"[\w.\-]+", s):
                continue
        if DEDUPE:
            if s in seen:
                continue
            seen.add(s)
        if s or not DROP_EMPTY:
            out.append(s)

    text = "\n".join(out).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
from datetime import datetime, date, time
from ...core.settings import SETTINGS

import re

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_xls(data: bytes) -> Tuple[str, str]:
    """
    Extract text from legacy .xls using xlrd and emit a format compatible with
    extract_excel() (headers/rows or key/values) so downstream RAG stays consistent.
    """
    try:
        import xlrd  # BSD-licensed
    except Exception:
        # If xlrd is not installed, return plaintext to avoid 500s.
        return (data.decode("utf-8", errors="replace"), "text/plain")

    S = SETTINGS.effective

    # --- formatting/config (mirrors excel_ingest) ---
    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    EMIT_KEYVALUES = bool(S().get("excel_emit_key_values"))
    EMIT_CELL_ADDR = bool(S().get("excel_emit_cell_addresses"))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and isinstance(dt, datetime) and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    # --- open workbook ---
    try:
        book = xlrd.open_workbook(file_contents=data)
    except Exception:
        # Misnamed/garbled file: fail soft
        return (data.decode("utf-8", errors="replace"), "text/plain")

    datemode = book.datemode

    def xlrd_cell_to_py(cell):
        # xlrd types: 0 empty, 1 text, 2 number, 3 date, 4 boolean, 5 error, 6 blank
        ctype, value = cell.ctype, cell.value
        if ctype == xlrd.XL_CELL_DATE:
            try:
                return xlrd.xldate_as_datetime(value, datemode)
            except Exception:
                return value
        if ctype == xlrd.XL_CELL_NUMBER:
            return float(value)
        if ctype == xlrd.XL_CELL_BOOLEAN:
            return bool(value)
        return value

    lines: List[str] = []

    for sheet in book.sheets():
        nrows = min(sheet.nrows or 0, INFER_MAX_ROWS)
        ncols = min(sheet.ncols or 0, INFER_MAX_COLS)
        if nrows == 0 or ncols == 0:
            continue

        # header inference
        headers_raw = []
        header_fill = 0
        for c in range(ncols):
            v = xlrd_cell_to_py(sheet.cell(0, c))
            s = "" if v is None else str(v).strip()
            if s:
                header_fill += 1
            headers_raw.append(s)

        fill_ratio = header_fill / max(1, ncols)
        start_row = 1
        if fill_ratio < INFER_MIN_HEADER_FILL and nrows >= 2:
            headers_raw = []
            for c in range(ncols):
                v = xlrd_cell_to_py(sheet.cell(1, c))
                s = "" if v is None else str(v).strip()
                headers_raw.append(s)
            start_row = 2

        norm_headers = [normalize_header(h) for h in headers_raw]

        lines.append(f"# Sheet: {sheet.name}")

        # key/value mode (two-column heuristic)
        if EMIT_KEYVALUES and ncols == 2:
            textish = valueish = rows = 0
            for r in range(start_row, nrows):
                a = xlrd_cell_to_py(sheet.cell(r, 0))
                b = xlrd_cell_to_py(sheet.cell(r, 1))
                if a is None and b is None:
                    continue
                rows += 1
                if isinstance(a, str):
                    textish += 1
                if isinstance(b, (int, float, datetime, date, time)):
                    valueish += 1
            if rows >= 3 and textish / max(1, rows) >= 0.6 and valueish / max(1, rows) >= 0.6:
                lines.append("## Key/Values")
                for r in range(start_row, nrows):
                    k = fmt_val(xlrd_cell_to_py(sheet.cell(r, 0)))
                    v = fmt_val(xlrd_cell_to_py(sheet.cell(r, 1)))
                    if not k and not v:
                        continue
                    lines.append(f"- {k}: {v}" if k else f"- : {v}")
                lines.append("")
                continue  # done with this sheet

        # table mode
        lines.append("## Inferred Table")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))

        for r in range(start_row, nrows):
            row_vals: List[str] = []
            for c in range(ncols):
                val = fmt_val(xlrd_cell_to_py(sheet.cell(r, c)))
                if val:
                    row_vals.append(val if not EMIT_CELL_ADDR else f"{val}")
            if row_vals:
                lines.append("row: " + ", ".join(row_vals))
        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/retrieve.py =====

# ===== aimodel/file_read/rag/retrieve.py =====
from __future__ import annotations
from dataclasses import asdict, dataclass
from typing import List, Optional, Tuple, Dict, Any
import time

from ..core.settings import SETTINGS
from .store import search_vectors

_EMBEDDER = None
_EMBEDDER_NAME = None

def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        print(f"[RAG] sentence_transformers unavailable: {e}")
        return None, None

    model_name = str(SETTINGS.get("rag_embedding_model", "intfloat/e5-small-v2"))
    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            print(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return _EMBEDDER, _EMBEDDER_NAME

def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding encode failed: {e}")
        return []

def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    hits_sorted = sorted(hits, key=lambda h: float(h.get("score", 0.0)), reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out

def _first_nonempty(*vals: Any) -> str:
    for v in vals:
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

def _excelish_header(h: Dict[str, Any]) -> Optional[str]:
    meta = h.get("meta") or {}
    mime = _first_nonempty(h.get("mime"), meta.get("mime"))
    if mime not in {"text/excel+row", "text/excel+lines", "text/excel+cells"}:
        return None

    src = _first_nonempty(h.get("source"), meta.get("source"))
    sheet = _first_nonempty(h.get("sheet"), meta.get("sheet"))
    table = _first_nonempty(h.get("table"), meta.get("table"))
    row_id = _first_nonempty(h.get("row_id"), meta.get("row_id"))
    idx = _first_nonempty(str(h.get("chunkIndex") or ""), str(meta.get("chunkIndex") or ""))

    parts = [src]
    if sheet:
        parts.append(f"{sheet}")
    if table:
        parts.append(f"tbl {table}")
    if row_id:
        parts.append(f"row {row_id}")
    elif idx:
        parts.append(f"chunk {idx}")

    label = " — ".join(parts) if parts else None
    return f"- {label}" if label else None

def _default_header(h: Dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"

def _render_header(h: Dict[str, Any]) -> str:
    eh = _excelish_header(h)
    return eh if eh else _default_header(h)

def _trim_to_budget(lines: List[str], total_budget: int) -> str:
    out = []
    used = 0
    for i, ln in enumerate(lines):
        need = len(ln) + (1 if i > 0 else 0)
        if used + need > total_budget:
            break
        if i > 0:
            pass
        out.append(ln)
        used += need
    return "\n".join(out)

def make_rag_block(hits: List[dict], *, max_chars: int = 800) -> str:
    lines = ["Local knowledge:"]
    total_budget = int(SETTINGS.get("rag_total_char_budget", 2200))
    used = len(lines[0]) + 1

    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()

        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost

    return "\n".join(lines)

@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"

def _build_rag_block_core(query: str, *, session_id: str | None, k: int, session_only: bool) -> Tuple[Optional[str], RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode=("session-only" if session_only else "global"))
    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")

    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)

    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None, tel

    d = len(qvec)

    hits_chat: List[dict] = []
    hits_glob: List[dict] = []

    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)

    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)

    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)

    all_hits = hits_chat + ([] if session_only else hits_glob)

    if not all_hits:
        print("[RAG SEARCH] no hits")
        return None, tel

    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)

    t4 = time.perf_counter()
    block = make_rag_block(
        hits_top,
        max_chars=int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    )
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    print(f"[RAG BLOCK] chars={tel.blockChars}")
    return block, tel

def build_rag_block(query: str, session_id: str | None = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None
    k = int(SETTINGS.get("rag_top_k", 4))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False)
    return block

def build_rag_block_with_telemetry(query: str, session_id: str | None = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None, {}
    k = int(SETTINGS.get("rag_top_k", 4))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False)
    return block, asdict(tel)

def build_rag_block_session_only(query: str, session_id: Optional[str], *, k: Optional[int] = None) -> Optional[str]:
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k", SETTINGS.get("rag_top_k", 4)))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True)
    return block

def build_rag_block_session_only_with_telemetry(query: str, session_id: Optional[str], *, k: Optional[int] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None, {}
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k", SETTINGS.get("rag_top_k", 4)))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True)
    return block, asdict(tel)

# ===== aimodel/file_read/rag/router_ai.py =====

# ===== aimodel/file_read/rag/router_ai.py =====
from __future__ import annotations
from typing import Tuple, Optional, Any
import json, re, traceback, time
from ..core.settings import SETTINGS

def _dbg(msg: str):
    print(f"[RAG ROUTER] {msg}")

def _force_json_strict(s: str) -> dict:
    if not s:
        return {}
    try:
        v = json.loads(s)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass
    rgx = SETTINGS.get("router_rag_json_extract_regex")
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
                v = json.loads(cand)
                return v if isinstance(v, dict) else {}
        except Exception:
            pass
    return {}

def _strip_wrappers(text: str) -> str:
    t = text or ""
    if SETTINGS.get("router_rag_trim_whitespace") is True:
        t = t.strip()
    if SETTINGS.get("router_rag_strip_wrappers_enabled") is not True:
        return t
    head = t
    if SETTINGS.get("router_rag_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]
    pat = SETTINGS.get("router_rag_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

def _normalize_keys(d: dict) -> dict:
    return {str(k).strip().strip('"').strip("'").strip().lower(): v for k, v in d.items()}

def _as_bool(v) -> Optional[bool]:
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        s = v.strip().strip('"').strip("'").lower()
        if s in ("true", "yes", "y", "1"):  return True
        if s in ("false", "no", "n", "0"):  return False
    return None

def decide_rag(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    try:
        if not user_text or not user_text.strip():
            return (False, None)

        core_text = _strip_wrappers(user_text.strip())

        prompt_tpl = SETTINGS.get("router_rag_decide_prompt")
        if not isinstance(prompt_tpl, str) or ("$text" not in prompt_tpl and "{text}" not in prompt_tpl):
            _dbg("router_rag_decide_prompt missing/invalid")
            return (False, None)

        from string import Template
        if "$text" in prompt_tpl:
            the_prompt = Template(prompt_tpl).safe_substitute(text=core_text)
        else:
            the_prompt = prompt_tpl.format(text=core_text)

        params = {
            "max_tokens": SETTINGS.get("router_rag_decide_max_tokens"),
            "temperature": SETTINGS.get("router_rag_decide_temperature"),
            "top_p": SETTINGS.get("router_rag_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_rag_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}

        raw = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw.get("choices", [{}])[0].get("message", {}).get("content") or "").strip()

        data = _force_json_strict(text_out)
        if not isinstance(data, dict):
            return (False, None)
        data = _normalize_keys(data)

        need_raw = data.get("need")
        need_bool = _as_bool(need_raw) if not isinstance(need_raw, bool) else need_raw
        if need_bool is None:
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)

        need = bool(need_bool)
        query_field = data.get("query", "")
        query = _strip_wrappers(str(query_field or "").strip()) if need else None
        return (need, query)

    except Exception as e:
        _dbg(f"FATAL {type(e).__name__}: {e}")
        traceback.print_exc()
        need_default = SETTINGS.get("router_rag_default_need_when_invalid")
        return (bool(need_default) if isinstance(need_default, bool) else False, None)

# ===== aimodel/file_read/rag/schemas.py =====

from pydantic import BaseModel, Field
from typing import Optional, List, Literal, Dict

class SearchReq(BaseModel):
    query: str
    sessionId: Optional[str] = None
    kChat: int = 6
    kGlobal: int = 4
    hybrid_alpha: float = 0.5  # 0..1, higher = semantic weight

class ItemRow(BaseModel):
    id: str
    sessionId: Optional[str]
    source: str
    title: Optional[str]
    mime: Optional[str]
    size: Optional[int]
    createdAt: str
    meta: Dict[str, str] = Field(default_factory=dict)

class SearchHit(BaseModel):
    id: str
    text: str
    score: float
    source: Optional[str] = None   # <-- safer
    title: Optional[str] = None
    sessionId: Optional[str] = None
    url: Optional[str] = None

# ===== aimodel/file_read/rag/search.py =====

from __future__ import annotations
from typing import List, Dict, Optional
import numpy as np
from .store import search_vectors

def reciprocal_rank_fusion(results: List[List[Dict]], k: int = 60) -> List[Dict]:
    # results = [list_from_chat, list_from_global]
    scores: Dict[str, float] = {}
    lookup: Dict[str, Dict] = {}
    for lst in results:
        for rank, r in enumerate(lst, start=1):
            rid = r["id"]
            scores[rid] = scores.get(rid, 0.0) + 1.0 / (k + rank)
            lookup[rid] = r
    fused = [{"score": s, **lookup[rid]} for rid, s in scores.items()]
    fused.sort(key=lambda x: x["score"], reverse=True)
    return fused

def merge_chat_first(chat_hits: List[Dict], global_hits: List[Dict], alpha: float = 0.5) -> List[Dict]:
    # alpha weights semantic scores; RRF stabilizes positions
    fused = reciprocal_rank_fusion([chat_hits, global_hits])
    return fused

# ===== aimodel/file_read/rag/store.py =====

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import faiss, json, os, time, hashlib
import numpy as np
from ..store.base import APP_DIR   
import shutil

BASE = APP_DIR / "rag"

def _ns_dir(session_id: Optional[str]) -> Path:
    if session_id:
        return BASE / "by_session" / session_id
    return BASE / "global"

def _paths(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / "index.faiss", d / "meta.jsonl"

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def _load_index(dim: int, p: Path) -> faiss.Index:
    if p.exists():
        return faiss.read_index(str(p))
    return faiss.IndexFlatIP(dim)

def _save_index(idx: faiss.Index, p: Path) -> None:
    faiss.write_index(idx, str(p))

def add_vectors(session_id: Optional[str], embeds: np.ndarray, metas: List[Dict], dim: int):
    idx_path, meta_path = _paths(session_id)
    idx = _load_index(dim, idx_path)

    # ensure index type
    if not isinstance(idx, faiss.IndexFlatIP):
        idx = faiss.IndexFlatIP(dim) if idx.ntotal == 0 else idx

    # normalize vectors
    embeds = _norm(embeds)

    # ✅ read existing ids to avoid duplicates
    existing_ids = set()
    if meta_path.exists():
        with meta_path.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    existing_ids.add(j["id"])
                except:
                    pass

    start = idx.ntotal
    new_embeds = []
    new_metas = []

    for i, m in enumerate(metas):
        if m["id"] in existing_ids:
            continue
        m["row"] = start + len(new_embeds)
        new_embeds.append(embeds[i])
        new_metas.append(m)

    if new_embeds:
        idx.add(np.vstack(new_embeds))
        _save_index(idx, idx_path)
        with meta_path.open("a", encoding="utf-8") as f:
            for m in new_metas:
                f.write(json.dumps(m, ensure_ascii=False) + "\n")

def search_vectors(session_id: Optional[str], query_vec: np.ndarray, topk: int, dim: int) -> List[Dict]:
    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return []

    idx = _load_index(dim, idx_path)

    # ✅ ensure numpy array for reshape
    query_vec = np.asarray(query_vec, dtype="float32")
    q = _norm(query_vec.reshape(1, -1))

    D, I = idx.search(q, topk)
    out: List[Dict] = []
    rows: Dict[int, Dict] = {}
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                j = json.loads(line)
                rows[int(j["row"])] = j
            except:
                pass
    for score, row in zip(D[0].tolist(), I[0].tolist()):
        if row < 0:
            continue
        m = rows.get(row)
        if not m:
            continue
        m = dict(m)
        m["score"] = float(score)
        out.append(m)
    return out

def search_similar(qvec: List[float] | np.ndarray, *, k: int = 5, session_id: Optional[str] = None) -> List[Dict]:
    """
    Compatibility wrapper used by retrieve.py.
    qvec: a single embedding vector (list or np.ndarray)
    """
    arr = np.asarray(qvec, dtype="float32")
    dim = int(arr.shape[-1])
    return search_vectors(session_id, arr, k, dim)

def add_texts(
    texts: List[str],
    metas: List[Dict],
    *,
    session_id: Optional[str],
    embed_fn,  # callable: List[str] -> np.ndarray[float32]
) -> int:
    if not texts:
        return 0
    vecs = embed_fn(texts)  # should return (n, d) float32
    if not isinstance(vecs, np.ndarray):
        vecs = np.asarray(vecs, dtype="float32")
    dim = int(vecs.shape[-1])
    add_vectors(session_id, vecs, metas, dim)
    return len(texts)

def delete_namespace(session_id: str) -> bool:
    """
    Hard-delete all RAG data for a given session: the by_session/<sessionId> folder.
    Returns True if it existed and was removed.
    """
    d = _ns_dir(session_id)
    try:
        if d.exists():
            shutil.rmtree(d, ignore_errors=True)
            return True
        return False
    except Exception:
        return False

# ===== aimodel/file_read/rag/uploads.py =====

from __future__ import annotations
from typing import Dict, List, Optional, Iterable, Tuple
from pathlib import Path
import json, os
import numpy as np
import faiss

from .store import _ns_dir  # reuse your namespace layout

_META_FN = "meta.jsonl"
_INDEX_FN = "index.faiss"

# ---------- Read-only helpers (NO mkdir) ----------
def _meta_path_ro(session_id: Optional[str]) -> Path:
    return _ns_dir(session_id) / _META_FN

def _index_path_ro(session_id: Optional[str]) -> Path:
    return _ns_dir(session_id) / _INDEX_FN

# ---------- Mutating helpers (mkdir when writing) ----------
def _paths_mut(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / _INDEX_FN, d / _META_FN

def _read_meta(meta_path: Path) -> List[dict]:
    if not meta_path.exists():
        return []
    out: List[dict] = []
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                j = json.loads(line)
                if isinstance(j, dict):
                    out.append(j)
            except:
                pass
    return out

def _write_meta(meta_path: Path, rows: List[dict]) -> None:
    tmp = meta_path.with_suffix(".jsonl.tmp")
    with tmp.open("w", encoding="utf-8") as f:
        for j in rows:
            f.write(json.dumps(j, ensure_ascii=False) + "\n")
    tmp.replace(meta_path)

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def list_sources(session_id: Optional[str], include_global: bool = True) -> List[dict]:
    def _agg(ns: Optional[str]) -> Dict[str, int]:
        mp = _meta_path_ro(ns)  # read-only, no mkdir
        agg: Dict[str, int] = {}
        for j in _read_meta(mp):
            src = (j.get("source") or "").strip()
            if not src:
                continue
            agg[src] = agg.get(src, 0) + 1
        return agg

    rows: List[dict] = []
    # session first
    if session_id is not None:
        for src, n in _agg(session_id).items():
            rows.append({"source": src, "sessionId": session_id, "chunks": n})
    if include_global:
        for src, n in _agg(None).items():
            rows.append({"source": src, "sessionId": None, "chunks": n})
    return rows

def hard_delete_source(source: str, *, session_id: Optional[str], embedder) -> dict:
    """
    Remove all chunks for `source` in the given namespace and REBUILD the FAISS index.
    `embedder`: callable(List[str]) -> np.ndarray[float32] (same one used on ingest).
    """
    idx_path, meta_path = _paths_mut(session_id)  # mutating path (mkdir allowed)
    rows = _read_meta(meta_path)
    if not rows:
        return {"ok": True, "removed": 0, "remaining": 0}

    keep: List[dict] = []
    removed = 0
    for j in rows:
        if str(j.get("source") or "").strip() == source:
            removed += 1
        else:
            keep.append(j)

    if removed == 0:
        return {"ok": True, "removed": 0, "remaining": len(keep)}

    # Reassign contiguous row ids for the kept entries
    for i, j in enumerate(keep):
        j["row"] = i

    if len(keep) == 0:
        # No rows left: drop index; empty meta file
        if idx_path.exists():
            try:
                idx_path.unlink()
            except:
                pass
        _write_meta(meta_path, [])
        return {"ok": True, "removed": removed, "remaining": 0}

    # Re-embed kept texts in batches
    texts = [str(j.get("text") or "") for j in keep]
    B = 128  # batch size
    parts: List[np.ndarray] = []
    for i in range(0, len(texts), B):
        vec = embedder(texts[i:i + B])
        if not isinstance(vec, np.ndarray):
            vec = np.asarray(vec, dtype="float32")
        parts.append(vec.astype("float32"))
    embeds = np.vstack(parts)
    embeds = _norm(embeds)

    dim = int(embeds.shape[-1])
    new_index = faiss.IndexFlatIP(dim)
    new_index.add(embeds)

    # Save rebuilt index + meta
    faiss.write_index(new_index, str(idx_path))
    _write_meta(meta_path, keep)

    return {"ok": True, "removed": removed, "remaining": len(keep)}

# ===== aimodel/file_read/rag/worker.py =====



# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs==9.5.4
httpx==0.27.0
# HTML parsing / readability (permissive)
lxml==6.0.1
readability-lxml==0.8.1
selectolax==0.3.21
beautifulsoup4==4.12.3

# RAG
faiss-cpu==1.8.0.post1
numpy==1.26.4
sentence-transformers==3.0.1
tzlocal==5.2
openpyxl==3.1.5 
python-multipart==0.0.20

python-docx==1.1.2
PyYAML==6.0.2
toml==0.10.2
pdfminer.six==20240706

PyPDF2==3.0.1
pandas==2.2.2

striprtf==0.0.26

python-pptx==0.6.23

pytesseract==0.3.13
pypdfium2==4.30.0
Pillow==10.4.0



# ===== aimodel/file_read/runtime/__init__.py =====



# ===== aimodel/file_read/runtime/model_runtime.py =====

# aimodel/file_read/model_runtime.py
from __future__ import annotations
import os
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Optional, List

from ..adaptive.config.paths import read_settings, write_settings

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath","")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None,"") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None,"") else None),
        )

_runtime_lock = RLock()
_llm: Optional[Llama] = None
_cfg: Optional[ModelConfig] = None

def _build_kwargs(cfg: ModelConfig) -> Dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _attach_introspection(llm: Llama) -> None:
    def get_last_timings():
        try:
            t = getattr(llm, "get_timings", None)
            if callable(t):
                v = t()
                if isinstance(v, dict):
                    return v
        except Exception:
            pass
        try:
            v = getattr(llm, "timings", None)
            if isinstance(v, dict):
                return v
        except Exception:
            pass
        try:
            v = getattr(llm, "perf", None)
            if isinstance(v, dict):
                return v
        except Exception:
            pass
        return None
    try:
        setattr(llm, "get_last_timings", get_last_timings)
    except Exception:
        pass

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            _llm = None
    except Exception:
        _llm = None

def current_model_info() -> Dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
        }

def ensure_ready() -> None:
    global _llm, _cfg
    with _runtime_lock:
        if _llm is not None:
            return
        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected. Load one via /models/load or set LOCALAI_MODEL_PATH.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")
        _llm = Llama(**_build_kwargs(cfg))
        _attach_introspection(_llm)
        _cfg = cfg

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: Dict[str, Any]) -> Dict[str, Any]:
    global _llm, _cfg
    with _runtime_lock:
        s = read_settings()
        s.update({k:v for k,v in config_patch.items() if v is not None})
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")
        _close_llm()
        _llm = Llama(**_build_kwargs(cfg))
        _attach_introspection(_llm)
        _cfg = cfg
        write_settings(asdict(cfg))
        return current_model_info()

def unload_model() -> None:
    global _llm
    with _runtime_lock:
        _close_llm()

def list_local_models() -> List[Dict[str, Any]]:
    s = read_settings()
    root = Path(s.get("modelsDir") or "")
    root.mkdir(parents=True, exist_ok=True)
    out: List[Dict[str, Any]] = []
    for p in root.rglob("*.gguf"):
        try:
            out.append({
                "path": str(p.resolve()),
                "sizeBytes": p.stat().st_size,
                "name": p.name,
                "rel": str(p.relative_to(root)),
            })
        except Exception:
            pass
    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/services/attachments.py =====

from __future__ import annotations
from typing import Any, Iterable, Optional, List


def att_get(att: Any, key: str, default=None):
    """Safe accessor for dicts OR Pydantic models."""
    try:
        return att.get(key, default)          # dict-like
    except AttributeError:
        return getattr(att, key, default)     # model-like


def join_attachment_names(attachments: Optional[Iterable[Any]]) -> str:
    if not attachments:
        return ""
    names: List[str] = [att_get(a, "name") for a in attachments]  # type: ignore[list-item]
    names = [n for n in names if n]
    return ", ".join(names)

# ===== aimodel/file_read/services/budget.py =====

# aimodel/file_read/services/budget.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
from .context_window import current_n_ctx, estimate_tokens

@dataclass
class TurnBudget:
    # INPUT
    n_ctx: int
    input_tokens_est: Optional[int]
    requested_out_tokens: int
    clamped_out_tokens: int
    clamp_margin: int
    reserved_system_tokens: Optional[int] = None

    # DERIVED
    available_for_out_tokens: Optional[int] = None
    headroom_tokens: Optional[int] = None
    overage_tokens: Optional[int] = None

    # EXPLANATION
    reason: str = "ok"

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

def analyze_budget(
    llm: Any,
    messages: List[Dict[str, str]],
    *,
    requested_out_tokens: int,
    clamp_margin: int,
    reserved_system_tokens: Optional[int] = None,
) -> TurnBudget:
    n_ctx = current_n_ctx()
    try:
        inp = estimate_tokens(llm, messages)
    except Exception:
        inp = None

    # available = n_ctx - input - clamp_margin (mirrors clamp_out_budget logic)
    if inp is None:
        available = None
        clamped = requested_out_tokens  # best effort; clamp_out_budget will finalize
        headroom = None
        overage = None
        reason = "input_tokens_unknown"
    else:
        available = max(16, n_ctx - inp - clamp_margin)
        clamped = max(16, min(requested_out_tokens, available))
        headroom = max(0, available - clamped)
        overage = max(0, requested_out_tokens - available)
        reason = "ok" if overage == 0 else "requested_exceeds_available"

    return TurnBudget(
        n_ctx=n_ctx,
        input_tokens_est=inp,
        requested_out_tokens=requested_out_tokens,
        clamped_out_tokens=clamped,
        clamp_margin=clamp_margin,
        reserved_system_tokens=reserved_system_tokens,
        available_for_out_tokens=available,
        headroom_tokens=headroom,
        overage_tokens=overage,
        reason=reason,
    )

# ===== aimodel/file_read/services/cancel.py =====

from __future__ import annotations
import asyncio
from threading import Event
from typing import Dict
from ..core.settings import SETTINGS

eff = SETTINGS.effective()
GEN_SEMAPHORE = asyncio.Semaphore(int(eff["gen_semaphore_permits"]))
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

from __future__ import annotations
from typing import List, Dict, Optional, Tuple, Any
from ..utils.streaming import safe_token_count_messages
from ..runtime.model_runtime import current_model_info
from ..core.settings import SETTINGS

def estimate_tokens(llm, messages: List[Dict[str,str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    eff = SETTINGS.effective()
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or eff["nctx_fallback"])
    except Exception:
        return int(eff["nctx_fallback"])

def clamp_out_budget(
    *, llm, messages: List[Dict[str,str]], requested_out: int, margin: int = 32
) -> Tuple[int, int]:
    eff = SETTINGS.effective()
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = int(eff["token_estimate_fallback"])
    n_ctx = current_n_ctx()
    available = max(int(eff["min_out_tokens"]), n_ctx - prompt_est - margin)
    safe_out = max(int(eff["min_out_tokens"]), min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

def compute_budget_view(
    llm,
    messages: List[Dict[str, str]],
    requested_out: Optional[int] = None,
) -> Dict[str, Any]:
    """
    Returns a structured snapshot of the token budget for this request.
    Uses your existing safe_token_count + nCtx logic to stay consistent
    with clamp_out_budget().
    """
    eff = SETTINGS.effective()
    n_ctx = current_n_ctx()
    margin = int(eff.get("clamp_margin", 32))
    min_out = int(eff.get("min_out_tokens", 16))
    default_out = int(eff.get("out_budget", 512))
    req_out = int(requested_out or default_out)

    # input estimate (with fallback identical to clamp_out_budget)
    inp_opt = estimate_tokens(llm, messages)
    if inp_opt is None:
        try:
            from ..utils.streaming import safe_token_count_messages
            prompt_est = safe_token_count_messages(llm, messages)
        except Exception:
            prompt_est = int(eff["token_estimate_fallback"])
    else:
        prompt_est = int(inp_opt)

    # capacity available for output per current policy (same formula as clamp_out_budget)
    available = max(min_out, n_ctx - prompt_est - margin)

    # chosen output budget (respect requested/default but never exceed available)
    out_budget_chosen = max(min_out, min(req_out, available))

    # diagnostics
    over_by_tokens = max(0, (prompt_est + req_out + margin) - n_ctx)
    usable_ctx = max(0, n_ctx - margin)

    return {
        # model/window
        "modelCtx": n_ctx,
        "clampMargin": margin,
        "usableCtx": usable_ctx,

        # input side
        "inputTokensEst": prompt_est,

        # output side
        "outBudgetChosen": out_budget_chosen,
        "outBudgetDefault": default_out,
        "outBudgetRequested": req_out,
        "outBudgetMaxAllowed": max(min_out, available),

        # heads-up flags
        "overByTokens": over_by_tokens,   # >0 means request (req_out) would overflow if granted
        "minOutTokens": min_out,

        # timing slot (filled by caller/streaming_worker if they measure queue wait)
        "queueWaitSec": None,
    }

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations
from typing import AsyncGenerator, AsyncIterator, Dict
from dataclasses import asdict
from fastapi.responses import StreamingResponse
import time

from ..core.settings import SETTINGS
from ..utils.streaming import RUNJSON_START, RUNJSON_END
from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .streaming_worker import run_stream as _run_stream
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream

from .generate_pipeline import prepare_generation_with_telemetry

async def generate_stream_flow(data, request) -> StreamingResponse:
    prep = await prepare_generation_with_telemetry(data)
    eff = SETTINGS.effective(session_id=prep.session_id)
    stop_ev = cancel_event(prep.session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        q_start = time.perf_counter()
        async with GEN_SEMAPHORE:
            try:
                q_wait = time.perf_counter() - q_start
                if isinstance(prep.budget_view, dict):
                    prep.budget_view["queueWaitSec"] = round(q_wait, 3)
            except Exception:
                pass

            mark_active(prep.session_id, +1)
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
                s = chunk_bytes.decode("utf-8", errors="ignore")
                if RUNJSON_START in s and RUNJSON_END in s:
                    return
                if s.strip() == eff["stopped_line_marker"]:
                    return
                out_buf.extend(chunk_bytes)

            try:
                async for chunk in run_stream(
                    llm=prep.llm,
                    messages=prep.packed,
                    out_budget=prep.out_budget,
                    stop_ev=stop_ev,
                    request=request,
                    temperature=prep.temperature,
                    top_p=prep.top_p,
                    input_tokens_est=prep.input_tokens_est,
                    t0_request=prep.t_request_start,
                    budget_view=prep.budget_view,
                ):
                    if isinstance(chunk, (bytes, bytearray)):
                        _accum_visible(chunk)
                    else:
                        _accum_visible(chunk.encode("utf-8"))
                    yield chunk
            finally:
                try:
                    full_text = out_buf.decode("utf-8", errors="ignore").strip()
                    start = full_text.find(RUNJSON_START)
                    if start != -1:
                        end = full_text.find(RUNJSON_END, start)
                        if end != -1:
                            full_text = (full_text[:start] + full_text[end + len(RUNJSON_END):]).strip()
                    if full_text:
                        prep.st["recent"].append({"role": "assistant", "content": full_text})
                except Exception:
                    pass

                try:
                    from ..store import apply_pending_for
                    apply_pending_for(prep.session_id)
                except Exception:
                    pass

                try:
                    from ..store import list_messages as store_list_messages
                    from ..workers.retitle_worker import enqueue as enqueue_retitle
                    msgs = store_list_messages(prep.session_id)
                    last_seq = max((int(m.id) for m in msgs), default=0)
                    enqueue_retitle(prep.session_id, [asdict(m) for m in msgs], job_seq=last_seq)
                except Exception:
                    pass

                mark_active(prep.session_id, -1)

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "Connection": "keep-alive"},
    )

async def cancel_session(session_id: str) -> Dict[str, bool]:
    from .cancel import cancel_event
    cancel_event(session_id).set()
    return {"ok": True}

async def cancel_session_alias(session_id: str) -> Dict[str, bool]:
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/generate_pipeline.py =====

# aimodel/file_read/services/generate_pipeline.py
# main generate pipeline orchestrator; no inline comments per request
from __future__ import annotations
import time
from typing import Any, Dict, List, Optional
from ..core.settings import SETTINGS
from ..runtime.model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody
from .session_io import handle_incoming, persist_summary
from .packing import build_system_text, pack_with_rollup, maybe_inject_rag_block
from .prompt_utils import chars_len
from .router_text import compose_router_text
from .attachments import att_get
from .budget import analyze_budget
from .context_window import clamp_out_budget
from ..web.router_ai import decide_web_and_fetch
from ..rag.router_ai import decide_rag
from ..rag.retrieve import build_rag_block_session_only
from ..core.packing_memory_core import PACK_TELEMETRY, SUMMARY_TEL
from .generate_pipeline_support import (
    Prep,
    _bool,
    _tok_count,
    _approx_block_tokens,
    _diff_find_inserted_block,
    _web_breakdown,
    _web_unattributed,
    _enforce_fit,
)

async def prepare_generation_with_telemetry(data: ChatBody) -> Prep:
    ensure_ready()
    llm = get_llm()
    t_request_start = time.perf_counter()
    eff0 = SETTINGS.effective()
    session_id = data.sessionId or eff0["default_session_id"]
    eff = SETTINGS.effective(session_id=session_id)
    temperature = float(eff["default_temperature"] if getattr(data, "temperature") is None else data.temperature)
    top_p = float(eff["default_top_p"] if getattr(data, "top_p") is None else data.top_p)
    out_budget_req = int(eff["default_max_tokens"] if getattr(data, "max_tokens") is None else data.max_tokens)
    auto_web = _bool(eff["default_auto_web"])
    if getattr(data, "autoWeb") is not None:
        auto_web = _bool(data.autoWeb)
    web_k = int(eff["default_web_k"] if getattr(data, "webK") is None else data.webK)
    web_k = max(int(eff["web_k_min"]), min(web_k, int(eff["web_k_max"])))
    auto_rag = _bool(eff["default_auto_rag"])
    if getattr(data, "autoRag") is not None:
        auto_rag = _bool(data.autoRag)
    model_ctx = int(eff["model_ctx"])
    incoming = [
        {
            "role": m.role,
            "content": m.content,
            "attachments": getattr(m, "attachments", None),
        }
        for m in (data.messages or [])
    ]
    latest_user = next((m for m in reversed(incoming) if m["role"] == "user"), {})
    latest_user_text = (latest_user.get("content") or "").strip()
    atts = (latest_user.get("attachments") or [])
    has_atts = bool(atts)
    if not latest_user_text and has_atts:
        names = [att_get(a, "name") for a in atts]
        names = [n for n in names if n]
        latest_user_text = "User uploaded: " + (", ".join(names) if names else "files")
    st = handle_incoming(session_id, incoming)
    base_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    router_text = compose_router_text(
        st.get("recent", []),
        str(base_user_text or ""),
        st.get("summary", "") or "",
        tail_turns=int(eff["router_tail_turns"]),
        summary_chars=int(eff["router_summary_chars"]),
        max_chars=int(eff["router_max_chars"]),
    )
    telemetry: Dict[str, Any] = {"web": {}, "rag": {}, "pack": {}, "prepSec": round(time.perf_counter() - t_request_start, 6)}
    ephemeral_once: List[Dict[str, str]] = []
    telemetry["web"]["injectElapsedSec"] = 0.0
    telemetry["web"]["ephemeralBlocks"] = 0
    try:
        web_block: Optional[str] = None
        web_tel: Dict[str, Any] = {}
        if auto_web and not (has_atts and bool(eff.get("disable_global_rag_on_attachments"))):
            res = await decide_web_and_fetch(llm, router_text, k=web_k)
            if isinstance(res, tuple):
                web_block = res[0] if len(res) > 0 else None
                tel_candidate = res[1] if len(res) > 1 else None
                if isinstance(tel_candidate, dict):
                    web_tel = tel_candidate
            elif isinstance(res, str):
                web_block = res
            else:
                web_block = None
        telemetry["web"].update(web_tel or {})
        need_flag = (web_tel or {}).get("needed")
        injected_candidate = isinstance(web_block, str) and bool(web_block.strip())
        if (need_flag is True) and (not injected_candidate):
            try:
                from ..web.orchestrator import build_web_block
                fb = await build_web_block(router_text, k=web_k)
                if fb and fb.strip():
                    web_block = fb
                    injected_candidate = True
            except Exception:
                pass
        if injected_candidate:
            t0_inject = time.perf_counter()
            web_text = str(eff["web_block_preamble"]) + "\n\n" + web_block.strip()
            telemetry["web"]["blockChars"] = len(web_text)
            tok = _approx_block_tokens(llm, "assistant", web_text)
            if tok is not None:
                telemetry["web"]["blockTokensApprox"] = tok
            telemetry["web"]["injected"] = True
            ephemeral_once.append({"role": "assistant", "content": web_text, "_ephemeral": True})
            telemetry["web"]["injectElapsedSec"] = round(time.perf_counter() - t0_inject, 6)
        else:
            telemetry["web"]["injected"] = False
            telemetry["web"]["injectElapsedSec"] = 0.0
    except Exception:
        telemetry["web"].setdefault("injected", False)
        telemetry["web"].setdefault("injectElapsedSec", 0.0)
    telemetry["web"]["ephemeralBlocks"] = len(ephemeral_once)
    if has_atts and bool(eff.get("disable_global_rag_on_attachments")):
        att_names = [att_get(a, "name") for a in atts if att_get(a, "name")]
        query_for_atts = (base_user_text or "").strip() or " ".join(att_names) or "document"
        t0_att = time.perf_counter()
        try:
            att_block = build_rag_block_session_only(query_for_atts, session_id)
        except Exception:
            att_block = None
        if att_block:
            rag_text = str(eff["rag_block_preamble"]) + "\n\n" + att_block
            telemetry["rag"]["sessionOnly"] = True
            telemetry["rag"]["mode"] = "session-only"
            telemetry["rag"]["blockChars"] = len(rag_text)
            tok = _approx_block_tokens(llm, "assistant", rag_text)
            if tok is not None:
                telemetry["rag"]["sessionOnlyTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            ephemeral_once.append({"role": "assistant", "content": rag_text, "_ephemeral": True})
        else:
            telemetry["rag"]["sessionOnly"] = False
            telemetry["rag"].setdefault("injected", False)
        telemetry["rag"]["sessionOnlyBuildSec"] = round(time.perf_counter() - t0_att, 6)
    system_text = build_system_text()
    t_pack0 = time.perf_counter()
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )
    telemetry["pack"]["packSec"] = round(time.perf_counter() - t_pack0, 6)
    rag_router_allowed = not (has_atts and bool(eff.get("disable_global_rag_on_attachments")))
    if rag_router_allowed and bool(eff.get("rag_enabled", True)) and not ephemeral_once:
        rag_need = False
        rag_query: Optional[str] = None
        t_router0 = time.perf_counter()
        if auto_rag:
            try:
                rag_need, rag_query = decide_rag(llm, router_text)
            except Exception:
                rag_need, rag_query = (False, None)
        telemetry["rag"]["routerDecideSec"] = round(time.perf_counter() - t_router0, 6)
        telemetry["rag"]["routerNeeded"] = bool(rag_need)
        if rag_query is not None:
            telemetry["rag"]["routerQuery"] = rag_query
        skip_rag = bool(ephemeral_once) or (not rag_need)
        tokens_before = _tok_count(llm, packed)
        t_inject0 = time.perf_counter()
        res = maybe_inject_rag_block(
            packed,
            session_id=session_id,
            skip_rag=skip_rag,
            rag_query=rag_query,
        )
        telemetry["rag"]["injectBuildSec"] = round(time.perf_counter() - t_inject0, 6)
        if isinstance(res, tuple):
            packed2 = res[0]
            tel = res[1] if len(res) > 1 and isinstance(res[1], dict) else {}
            block_text = res[2] if len(res) > 2 and isinstance(res[2], str) else None
        else:
            packed2 = res
            tel = {}
            block_text = None
        if tel:
            telemetry["rag"].update(tel)
        if block_text:
            telemetry["rag"]["blockChars"] = len(block_text)
            tok = _approx_block_tokens(llm, "user", block_text)
            if tok is not None:
                telemetry["rag"]["blockTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or "global"
        else:
            inserted = _diff_find_inserted_block(packed, packed2)
            if inserted and isinstance(inserted.get("content"), str):
                text = inserted["content"]
                telemetry["rag"]["blockChars"] = len(text)
                tok = _approx_block_tokens(llm, "user", text)
                if tok is not None:
                    telemetry["rag"]["blockTokensApprox"] = tok
                telemetry["rag"]["injected"] = True
                telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or "global"
        tokens_after = _tok_count(llm, packed2)
        if tokens_before is not None:
            telemetry["rag"]["packedTokensBefore"] = tokens_before
        if tokens_after is not None:
            telemetry["rag"]["packedTokensAfter"] = tokens_after
        if tokens_before is not None and tokens_after is not None:
            telemetry["rag"]["ragTokensAdded"] = max(0, tokens_after - tokens_before)
        packed = packed2
    else:
        telemetry["rag"]["routerSkipped"] = True
        if ephemeral_once:
            telemetry["rag"]["routerSkippedReason"] = "ephemeral_block_present"
        elif not rag_router_allowed:
            telemetry["rag"]["routerSkippedReason"] = "attachments_disable_global"
        elif not bool(eff.get("rag_enabled", True)):
            telemetry["rag"]["routerSkippedReason"] = "rag_disabled"
    packed, out_budget_adj = _enforce_fit(llm, eff, packed, out_budget_req)
    packed_chars = chars_len(packed)
    telemetry["pack"]["packedChars"] = packed_chars
    telemetry["pack"]["messages"] = len(packed)
    telemetry["pack"]["summarySec"] = float(PACK_TELEMETRY.get("summarySec") or 0.0)
    telemetry["pack"]["summaryTokensApprox"] = int(PACK_TELEMETRY.get("summaryTokensApprox") or 0)
    telemetry["pack"]["summaryUsedLLM"] = bool(PACK_TELEMETRY.get("summaryUsedLLM") or False)
    telemetry["pack"]["finalTrimSec"] = float(PACK_TELEMETRY.get("finalTrimSec") or 0.0)
    telemetry["pack"]["compressSec"] = float(PACK_TELEMETRY.get("compressSec") or 0.0)
    telemetry["pack"]["packInputTokensApprox"] = int(PACK_TELEMETRY.get("packInputTokensApprox") or 0)
    telemetry["pack"]["packMsgs"] = int(PACK_TELEMETRY.get("packMsgs") or 0)
    telemetry["pack"]["finalTrimTokensBefore"] = int(PACK_TELEMETRY.get("finalTrimTokensBefore") or 0)
    telemetry["pack"]["finalTrimTokensAfter"] = int(PACK_TELEMETRY.get("finalTrimTokensAfter") or 0)
    telemetry["pack"]["finalTrimDroppedMsgs"] = int(PACK_TELEMETRY.get("finalTrimDroppedMsgs") or 0)
    telemetry["pack"]["finalTrimDroppedApproxTokens"] = int(PACK_TELEMETRY.get("finalTrimDroppedApproxTokens") or 0)
    telemetry["pack"]["finalTrimSummaryShrunkFromChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryShrunkFromChars") or 0)
    telemetry["pack"]["finalTrimSummaryShrunkToChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryShrunkToChars") or 0)
    telemetry["pack"]["finalTrimSummaryDroppedChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryDroppedChars") or 0)
    telemetry["pack"]["rollStartTokens"] = int(PACK_TELEMETRY.get("rollStartTokens") or 0)
    telemetry["pack"]["rollOverageTokens"] = int(PACK_TELEMETRY.get("rollOverageTokens") or 0)
    telemetry["pack"]["rollPeeledMsgs"] = int(PACK_TELEMETRY.get("rollPeeledMsgs") or 0)
    telemetry["pack"]["rollNewSummaryChars"] = int(PACK_TELEMETRY.get("rollNewSummaryChars") or 0)
    telemetry["pack"]["rollNewSummaryTokensApprox"] = int(PACK_TELEMETRY.get("rollNewSummaryTokensApprox") or 0)
    persist_summary(session_id, st["summary"])
    budget_view = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=out_budget_adj,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()
    wb = _web_breakdown(telemetry.get("web", {}))
    telemetry.setdefault("web", {})["breakdown"] = wb
    telemetry["web"]["breakdown"]["unattributedWebSec"] = _web_unattributed(telemetry.get("web", {}), wb)
    telemetry["web"]["breakdown"]["prepSec"] = float(telemetry.get("prepSec") or 0.0)
    budget_view.setdefault("web", {}).update(telemetry.get("web", {}))
    budget_view.setdefault("rag", {}).update(telemetry.get("rag", {}))
    budget_view.setdefault("pack", {}).update(telemetry.get("pack", {}))
    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_adj, margin=int(eff["clamp_margin"])
    )
    budget_view.setdefault("request", {})
    budget_view["request"]["outBudgetRequested"] = out_budget_adj
    budget_view["request"]["temperature"] = temperature
    budget_view["request"]["top_p"] = top_p
    return Prep(
        llm=llm,
        session_id=session_id,
        packed=packed,
        st=st,
        out_budget=out_budget,
        input_tokens_est=input_tokens_est,
        budget_view=budget_view,
        temperature=temperature,
        top_p=top_p,
        t_request_start=t_request_start,
    )

# ===== aimodel/file_read/services/generate_pipeline_support.py =====

# aimodel/file_read/services/generate_pipeline_support.py
# utility helpers for generate pipeline; no inline comments per request
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from ..utils.streaming import safe_token_count_messages

@dataclass
class Prep:
    llm: Any
    session_id: str
    packed: List[Dict[str, str]]
    st: Dict[str, Any]
    out_budget: int
    input_tokens_est: Optional[int]
    budget_view: Dict[str, Any]
    temperature: float
    top_p: float
    t_request_start: float

def _bool(v, default: bool = False) -> bool:
    try:
        return bool(v)
    except Exception:
        return bool(default)

def _tok_count(llm, messages: List[Dict[str, str]]) -> Optional[int]:
    try:
        return int(safe_token_count_messages(llm, messages))
    except Exception:
        return None

def _approx_block_tokens(llm, role: str, text: str) -> Optional[int]:
    return _tok_count(llm, [{"role": role, "content": text}])

def _diff_find_inserted_block(before: List[Dict[str, str]], after: List[Dict[str, str]]) -> Optional[Dict[str, str]]:
    if len(after) - len(before) != 1:
        return None
    i = 0
    while i < len(before) and before[i] == after[i]:
        i += 1
    if i < len(after):
        return after[i]
    return None

def _dump_msgs(label: str, msgs: List[Dict[str, str]], head_chars: int = 180):
    return

def _web_breakdown(web: Dict[str, Any]) -> Dict[str, float]:
    w = web or {}
    orch = w.get("orchestrator") or {}
    router = float(w.get("elapsedSec") or 0.0)
    summarize = float((w.get("summarizer") or {}).get("elapsedSec") or 0.0)
    inject = float(w.get("injectElapsedSec") or 0.0)
    search_total = 0.0
    s1 = (orch.get("search") or {})
    for k in ("elapsedSecTotal", "elapsedSec"):
        if isinstance(s1.get(k), (int, float)):
            search_total = float(s1[k])
            break
    fetch1 = float((orch.get("fetch1") or {}).get("totalSec") or 0.0)
    fetch2 = float((orch.get("fetch2") or {}).get("totalSec") or 0.0)
    orch_elapsed = float(orch.get("elapsedSec") or w.get("fetchElapsedSec") or 0.0)
    assemble = orch_elapsed - (search_total + fetch1 + fetch2)
    if assemble < 0:
        assemble = 0.0
    total_pre_ttft = router + summarize + orch_elapsed + inject
    return {
        "routerSec": round(router, 6),
        "summarizeSec": round(summarize, 6),
        "searchSec": round(search_total, 6),
        "fetchSec": round(fetch1, 6),
        "jsFetchSec": round(fetch2, 6),
        "assembleSec": round(assemble, 6),
        "orchestratorSec": round(orch_elapsed, 6),
        "injectSec": round(inject, 6),
        "totalWebPreTtftSec": round(total_pre_ttft, 6),
    }

def _web_unattributed(web: Dict[str, Any], breakdown: Dict[str, float]) -> float:
    total = float((web or {}).get("fetchElapsedSec") or 0.0)
    explained = float(breakdown.get("searchSec", 0.0)) + float(breakdown.get("fetchSec", 0.0)) + float(breakdown.get("jsFetchSec", 0.0)) + float(breakdown.get("assembleSec", 0.0))
    ua = total - explained
    return round(ua if ua > 0 else 0.0, 6)

def _enforce_fit(llm, eff: Dict[str, Any], packed: List[Dict[str, str]], out_budget_req: int) -> tuple[list[dict], int]:
    tok = _tok_count(llm, packed) or 0
    capacity = int(eff["model_ctx"]) - int(eff["clamp_margin"])
    def drop_one(px):
        keep_head = 2 if len(px) >= 2 and isinstance(px[1].get("content"), str) and px[1]["content"].startswith(eff["summary_header_prefix"]) else 1
        if len(px) > keep_head + 1:
            px.pop(keep_head)
            return True
        return False
    def remove_ephemeral_blocks(px):
        i = 0
        removed = False
        while i < len(px):
            m = px[i]
            if m.get("_ephemeral") is True:
                px.pop(i)
                removed = True
            else:
                i += 1
        return removed
    if tok + out_budget_req > capacity:
        if remove_ephemeral_blocks(packed):
            tok = _tok_count(llm, packed) or 0
    while tok + out_budget_req > capacity and drop_one(packed):
        tok = _tok_count(llm, packed) or 0
    if tok >= capacity:
        ob2 = 0
    else:
        ob2 = min(out_budget_req, max(0, capacity - tok))
    return packed, ob2

# ===== aimodel/file_read/services/packing.py =====

# ===== aimodel/file_read/services/packing.py =====
from __future__ import annotations
from typing import Tuple, List, Dict, Optional, Any
from ..rag.retrieve import build_rag_block_with_telemetry
from ..core.settings import SETTINGS
from ..core.packing_ops import build_system, pack_messages, roll_summary_if_needed

def build_system_text() -> str:
    eff = SETTINGS.effective()
    base = build_system(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
    )
    guidance = str(eff["packing_guidance"])
    return base + guidance

def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    eff = SETTINGS.effective()

    packed, input_budget = pack_messages(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
        summary=summary,
        recent=recent,
        max_ctx=max_ctx,
        out_budget=out_budget,
    )

    packed, new_summary = roll_summary_if_needed(
        packed=packed,
        recent=recent,
        summary=summary,
        input_budget=input_budget,
        system_text=system_text,
    )

    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

def maybe_inject_rag_block(
    messages: list[dict], *,
    session_id: str | None,
    skip_rag: bool = False,
    rag_query: str | None = None,
) -> tuple[list[dict], Optional[Dict[str, Any]], Optional[str]]:
    if skip_rag:
        return messages, None, None
    if not SETTINGS.get("rag_enabled", True):
        return messages, None, None
    if not messages or messages[-1].get("role") != "user":
        return messages, None, None

    user_q = rag_query or (messages[-1].get("content") or "")
    block, tel = build_rag_block_with_telemetry(user_q, session_id=session_id)
    if not block:
        print(f"[RAG INJECT] no hits (session={session_id}) q={(user_q or '')!r}")
        return messages, None, None

    print(f"[RAG INJECT] injecting (session={session_id}) chars={len(block)}")
    injected = messages[:-1] + [{"role": "user", "content": block}, messages[-1]]

    tel = dict(tel or {})
    tel["injected"] = True
    tel["mode"] = tel.get("mode") or "global"  # from telemetry
    return injected, tel, block

# ===== aimodel/file_read/services/prompt_utils.py =====

from __future__ import annotations
import json
from datetime import datetime
from typing import Dict, List


def now_str() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total


def dump_full_prompt(
    messages: List[Dict[str, object]],
    *,
    params: Dict[str, object],
    session_id: str,
) -> None:
    try:
        print(f"[{now_str()}] PROMPT DUMP BEGIN session={session_id} msgs={len(messages)}")
        print(json.dumps({"messages": messages, "params": params}, ensure_ascii=False, indent=2))
        print(f"[{now_str()}] PROMPT DUMP END   session={session_id}")
    except Exception as e:
        print(f"[{now_str()}] PROMPT DUMP ERROR session={session_id} err={type(e).__name__}: {e}")

# ===== aimodel/file_read/services/router_text.py =====

from __future__ import annotations
from typing import Optional, List
from ..core.settings import SETTINGS


def compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: Optional[int] = None,
    summary_chars: Optional[int] = None,
    max_chars: Optional[int] = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: List[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.packing_memory_core import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List

from ..core.settings import SETTINGS
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END,
    build_run_json, watch_disconnect,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int],  t0_request: Optional[float] = None, budget_view: Optional[dict] = None,
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=SETTINGS.stream_queue_maxsize)
    SENTINEL = object()

    def produce():
        t_start = t0_request or time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        t_call: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []
        stage: dict = {"queueWaitSec": None, "genSec": None}

        try:
            try:
                t_call = time.perf_counter()
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=SETTINGS.stream_top_k,
                    repeat_penalty=SETTINGS.stream_repeat_penalty,
                    stop=SETTINGS.stream_stop_strings,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(
                        SETTINGS.stream_retry_min_tokens,
                        int(out_budget * SETTINGS.stream_retry_fraction)
                    )
                    log.warning(
                        "generate: context overflow, retrying with max_tokens=%d",
                        retry_tokens
                    )
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=SETTINGS.stream_top_k,
                        repeat_penalty=SETTINGS.stream_repeat_penalty,
                        stop=SETTINGS.stream_stop_strings,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(SETTINGS.stream_backpressure_sleep_sec)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                out_text = "".join(out_parts)

                if t_first is not None and t_last is not None:
                    stage["genSec"] = round(t_last - t_first, 3)
                if t_start is not None and t_first is not None:
                    stage["ttftSec"] = round(t_first - t_start, 3)
                if t_start is not None and t_last is not None:
                    stage["totalSec"] = round(t_last - t_start, 3)

                if t_call is not None and t_start is not None:
                    stage["preModelSec"] = round(t_call - t_start, 6)
                if t_call is not None and t_first is not None:
                    stage["modelQueueSec"] = round(t_first - t_call, 6)

                if isinstance(budget_view, dict) and "queueWaitSec" in budget_view:
                    stage["queueWaitSec"] = budget_view.get("queueWaitSec")

                engine = None
                method_used = None
                try:
                    td = None
                    g_last = getattr(llm, "get_last_timings", None)
                    if callable(g_last):
                        method_used = "get_last_timings"
                        td = g_last()
                    if td is None:
                        g = getattr(llm, "get_timings", None)
                        if callable(g):
                            method_used = "get_timings"
                            td = g()
                    if td is None:
                        if isinstance(getattr(llm, "timings", None), dict):
                            method_used = "timings_attr"
                            td = getattr(llm, "timings")
                        elif isinstance(getattr(llm, "perf", None), dict):
                            method_used = "perf_attr"
                            td = getattr(llm, "perf")

                    if isinstance(td, dict):
                        def fms(v):
                            try:
                                return float(v) / 1000.0
                            except Exception:
                                return None
                        def to_i(v):
                            try:
                                return int(v)
                            except Exception:
                                return None
                        load_ms = td.get("load_ms") or td.get("loadMs")
                        prompt_ms = td.get("prompt_ms") or td.get("promptMs") or td.get("prefill_ms")
                        eval_ms = td.get("eval_ms") or td.get("evalMs") or td.get("decode_ms")
                        prompt_n = td.get("prompt_n") or td.get("promptN") or td.get("prompt_tokens")
                        eval_n = td.get("eval_n") or td.get("evalN") or td.get("eval_tokens")
                        engine = {}
                        x = fms(load_ms)
                        if x is not None:
                            engine["loadSec"] = round(x, 3)
                        x = fms(prompt_ms)
                        if x is not None:
                            engine["promptSec"] = round(x, 3)
                        x = fms(eval_ms)
                        if x is not None:
                            engine["evalSec"] = round(x, 3)
                        n = to_i(prompt_n)
                        if n is not None:
                            engine["promptN"] = n
                        n = to_i(eval_n)
                        if n is not None:
                            engine["evalN"] = n
                        try:
                            log.debug("llm timings method=%s keys=%s", method_used, list(td.keys()))
                        except Exception:
                            pass
                    else:
                        log.debug("llm timings unavailable")
                except Exception as e:
                    log.debug("llm timings probe error: %s", e)
                    engine = None
                if engine:
                    stage["engine"] = engine

                if isinstance(budget_view, dict):
                    ttft_val = float(stage.get("ttftSec") or 0.0)
                    pack = (budget_view.get("pack") or {})
                    rag = (budget_view.get("rag") or {})
                    web_bd = ((budget_view.get("web") or {}).get("breakdown") or {})
                    pack_sec = float(pack.get("packSec") or 0.0)
                    trim_sec = float(pack.get("finalTrimSec") or 0.0)
                    comp_sec = float(pack.get("compressSec") or 0.0)
                    rag_router = float(rag.get("routerDecideSec") or 0.0)
                    rag_block = float(
                        rag.get("injectBuildSec")
                        or rag.get("blockBuildSec")
                        or rag.get("sessionOnlyBuildSec")
                        or 0.0
                    )
                    prep_sec = float(web_bd.get("prepSec") or 0.0)
                    web_pre = float(web_bd.get("totalWebPreTtftSec") or 0.0)
                    model_queue = float(stage.get("modelQueueSec") or 0.0)
                    pre_accounted = pack_sec + trim_sec + comp_sec + rag_router + rag_block + web_pre + prep_sec + model_queue
                    unattr_ttft = max(0.0, ttft_val - pre_accounted)
                    budget_view.setdefault("breakdown", {})
                    budget_view["breakdown"].update({
                        "ttftSec": ttft_val,
                        "preTtftAccountedSec": round(pre_accounted, 6),
                        "unattributedTtftSec": round(unattr_ttft, 6),
                    })

                run_json = build_run_json(
                    request_cfg={"temperature": temperature, "top_p": top_p, "max_tokens": out_budget},
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                    budget_view=budget_view,
                    extra_timings=stage,
                    error_text=err_text,
                )
                if SETTINGS.runjson_emit:
                    q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass
            finally:
                try:
                    llm.reset()
                except Exception:
                    pass
                try:
                    q.put_nowait(SENTINEL)
                except Exception:
                    pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set() and SETTINGS.stream_emit_stopped_line:
            yield (f"\n{SETTINGS.stopped_line_marker}\n").encode("utf-8")
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=SETTINGS.stream_producer_join_timeout_sec)
        except Exception:
            pass

# ===== aimodel/file_read/services/warmup.py =====

# place at: aimodel/file_read/runtime/warmup.py  (or services/warmup.py if you chose Option B)

from __future__ import annotations
import asyncio, time
from typing import Optional, Dict, Any

from ..core.settings import SETTINGS
from ..runtime.model_runtime import ensure_ready, get_llm
from ..services.cancel import GEN_SEMAPHORE

_warmup_task: Optional[asyncio.Task] = None

async def _warmup_llm_once() -> Dict[str, Any]:
    t0 = time.perf_counter()
    ensure_ready()
    try:
        got = await asyncio.wait_for(GEN_SEMAPHORE.acquire(), timeout=0.01)
    except asyncio.TimeoutError:
        return {"llmSec": 0.0, "skipped": True}
    if not got:
        return {"llmSec": 0.0, "skipped": True}
    try:
        llm = get_llm()
        llm.create_chat_completion(messages=[{"role":"user","content":"ok"}], max_tokens=1, temperature=0.0, stream=False)
        return {"llmSec": round(time.perf_counter() - t0, 4), "skipped": False}
    except Exception:
        return {"llmSec": round(time.perf_counter() - t0, 4), "error": True}
    finally:
        try:
            GEN_SEMAPHORE.release()
        except Exception:
            pass

async def _warmup_web_once() -> Dict[str, Any]:
    tel: Dict[str, Any] = {"queries": []}
    if not bool(SETTINGS.get("web_enabled", True)):
        tel["skipped"] = True
        return tel
    k           = int(SETTINGS.get("warmup_web_k", 2))
    timeout_sec = float(SETTINGS.get("warmup_timeout_sec", 6))
    queries     = list(SETTINGS.get("warmup_queries") or [])
    try:
        from ..web.router_ai import decide_web_and_fetch
    except Exception:
        tel["error"] = "web_stack_unavailable"
        return tel
    llm = get_llm()
    t0 = time.perf_counter()
    for q in queries[:3]:
        try:
            block, web_tel = await asyncio.wait_for(decide_web_and_fetch(llm, q, k=k), timeout=timeout_sec)
            tel["queries"].append({"q": q, "chars": len(block or ""), **(web_tel or {})})
        except Exception:
            tel["queries"].append({"q": q, "error": True})
    tel["totalFetchSec"] = round(time.perf_counter() - t0, 4)
    return tel

async def warmup_once() -> Dict[str, Any]:
    eff = SETTINGS.effective()
    out: Dict[str, Any] = {}
    if bool(eff.get("warmup_llm_enabled", True)):
        out["llm"] = await _warmup_llm_once()
    if bool(eff.get("warmup_web_enabled", False)):
        out["web"] = await _warmup_web_once()
    return out

async def warmup_periodic(stop_ev: asyncio.Event) -> None:
    eff = SETTINGS.effective()
    interval = max(60, int(eff.get("warmup_on_interval_sec", 900)))
    jitter   = min(15, max(0, int(eff.get("warmup_jitter_sec", 5))))
    while not stop_ev.is_set():
        try:
            await warmup_once()
        except Exception:
            pass
        try:
            delay = interval + (asyncio.get_running_loop().time() % (jitter or 1))
            await asyncio.wait_for(stop_ev.wait(), timeout=delay)
        except asyncio.TimeoutError:
            continue

def start_warmup_runner(loop: asyncio.AbstractEventLoop) -> asyncio.Event:
    stop_ev = asyncio.Event()
    if bool(SETTINGS.get("warmup_enabled", True)):
        loop.create_task(warmup_periodic(stop_ev), name="warmup_periodic")
    return stop_ev

def stop_warmup_runner(stop_ev: Optional[asyncio.Event]) -> None:
    if stop_ev is not None:
        stop_ev.set()

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message, update_last, append_message,
    delete_message, delete_messages_batch, list_messages,
    list_paged, delete_batch,
    merge_chat, merge_chat_new, edit_message, set_summary, get_summary,  # ← add these
)
from .index import ChatMeta

__all__ = [
    # chats
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "merge_chat", "merge_chat_new", "edit_message",  # ← add these
    # index
    "ChatMeta",
    # pending
    "set_summary", "get_summary"
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile, threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..adaptive.config.paths import app_data_dir

# -------- Directories & Paths --------
APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"
PENDING_PATH = APP_DIR / "pending.json"              # NEW
OLD_PENDING_DELETES = APP_DIR / "pending_deletes.json"  # NEW

# -------- Lock for safe writes --------
_lock = threading.RLock()

# -------- Helpers --------
def now_iso() -> str:
    """UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    """Safely write JSON to a temp file then move into place."""
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    """Ensure app/chats directories exist and index.json is initialized."""
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    """Return path to chat file for a session ID."""
    return CHATS_DIR / f"{session_id}.json"

# -------- Exports --------
__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "PENDING_PATH",          # NEW
    "OLD_PENDING_DELETES",   # NEW
    "_lock",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

# ===== aimodel/file_read/store/chats.py =====
from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any

from ..core.settings import SETTINGS
from ..utils.streaming import strip_runjson
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta
from ..rag.store import delete_namespace as rag_delete_namespace


def _load_chat(session_id: str) -> Dict[str, Any]:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""  # backfill older files
        return data


@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str
    attachments: Optional[List[Dict]] = None   # ✅ added

def _normalize_attachments(atts: Optional[list[Any]]) -> Optional[list[dict]]:
    if not atts:
        return None
    out = []
    for a in atts:
        # convert dataclass/typed object to dict
        if isinstance(a, dict):
            out.append({
                "name": a.get("name"),
                "source": a.get("source"),
                "sessionId": a.get("sessionId"),
            })
        else:
            try:
                out.append({
                    "name": getattr(a, "name", None),
                    "source": getattr(a, "source", None),
                    "sessionId": getattr(a, "sessionId", None),
                })
            except Exception:
                continue
    return out or None

def upsert_on_first_message(session_id: str, title: s