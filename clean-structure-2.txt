()
GEN_SEMAPHORE = asyncio.Semaphore(int(eff["gen_semaphore_permits"]))
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

from __future__ import annotations
from typing import List, Dict, Optional, Tuple
from ..utils.streaming import safe_token_count_messages
from ..runtime.model_runtime import current_model_info
from ..core.settings import SETTINGS

def estimate_tokens(llm, messages: List[Dict[str,str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    eff = SETTINGS.effective()
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or eff["nctx_fallback"])
    except Exception:
        return int(eff["nctx_fallback"])

def clamp_out_budget(
    *, llm, messages: List[Dict[str,str]], requested_out: int, margin: int = 32
) -> Tuple[int, int]:
    eff = SETTINGS.effective()
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = int(eff["token_estimate_fallback"])
    n_ctx = current_n_ctx()
    available = max(int(eff["min_out_tokens"]), n_ctx - prompt_est - margin)
    safe_out = max(int(eff["min_out_tokens"]), min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

# ===== aimodel/file_read/services/generate_flow.py =====

from __future__ import annotations
import time
import json
from typing import AsyncGenerator, Optional, AsyncIterator
from fastapi.responses import StreamingResponse
from dataclasses import asdict

from ..core.settings import SETTINGS
from ..runtime.model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody

from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .session_io import handle_incoming, persist_summary
from .packing import build_system_text, pack_with_rollup
from .context_window import clamp_out_budget

from ..web.router_ai import decide_web_and_fetch
from ..rag.router_ai import decide_rag
from ..rag.retrieve import build_rag_block_session_only
from ..utils.streaming import RUNJSON_START, RUNJSON_END

from .streaming_worker import run_stream as _run_stream
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream  # type: ignore[assignment]

# local helpers
from .prompt_utils import now_str, chars_len, dump_full_prompt
from .router_text import compose_router_text
from .attachments import att_get


async def generate_stream_flow(data: ChatBody, request) -> StreamingResponse:
    ensure_ready()
    llm = get_llm()

    # ---- effective settings (no fallbacks) ----
    eff0 = SETTINGS.effective()
    session_id = data.sessionId or eff0["default_session_id"]
    eff = SETTINGS.effective(session_id=session_id)

    if not data.messages:
        return StreamingResponse(
            iter([eff["empty_messages_response"].encode("utf-8")]),
            media_type="text/plain"
        )

    # ---- request params with explicit override only ----
    temperature = float(eff["default_temperature"])
    if getattr(data, "temperature") is not None:
        temperature = float(data.temperature)

    top_p = float(eff["default_top_p"])
    if getattr(data, "top_p") is not None:
        top_p = float(data.top_p)

    out_budget_req = int(eff["default_max_tokens"])
    if getattr(data, "max_tokens") is not None:
        out_budget_req = int(data.max_tokens)

    auto_web = bool(eff["default_auto_web"])
    if getattr(data, "autoWeb") is not None:
        auto_web = bool(data.autoWeb)

    web_k = int(eff["default_web_k"])
    if getattr(data, "webK") is not None:
        web_k = int(data.webK)
    web_k = max(int(eff["web_k_min"]), min(web_k, int(eff["web_k_max"])))

    auto_rag = bool(eff["default_auto_rag"])
    if getattr(data, "autoRag") is not None:
        auto_rag = bool(data.autoRag)

    model_ctx = int(eff["model_ctx"])

    # üîé explicit RAG / attachments config print
    print(
        f"[{now_str()}] RAG CONFIG session={session_id} "
        f"param.autoRag={getattr(data, 'autoRag', None)!r} "
        f"default_auto_rag={eff['default_auto_rag']!r} "
        f"-> auto_rag={auto_rag} "
        f"rag_enabled={eff['rag_enabled']!r} "
        f"disable_global_rag_on_attachments={eff['disable_global_rag_on_attachments']!r}"
    )

    # ---- normalize inbound ----
    incoming = [
        {
            "role": m.role,
            "content": m.content,
            "attachments": getattr(m, "attachments", None),  # safe on message objects
        }
        for m in data.messages
    ]
    print(f"[{now_str()}] DEBUG incoming={json.dumps(incoming, default=str, ensure_ascii=False)}")

    latest_user = next((m for m in reversed(incoming) if m["role"] == "user"), {})
    latest_user_text = (latest_user.get("content") or "").strip()
    atts = (latest_user.get("attachments") or [])
    has_atts = bool(atts)

    if not latest_user_text and has_atts:
        names = [att_get(a, "name") for a in atts]
        names = [n for n in names if n]
        latest_user_text = "User uploaded: " + (", ".join(names) if names else "files")
        print(f"[{now_str()}] DEBUG fallback latest_user_text={latest_user_text!r}")
    else:
        print(f"[{now_str()}] DEBUG normal latest_user_text={latest_user_text!r}")

    print(f"[{now_str()}] GEN request START session={session_id} msgs_in={len(incoming)}")
    st = handle_incoming(session_id, incoming)

    # ---- router text once ----
    base_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    router_text = compose_router_text(
        st.get("recent", []),
        str(base_user_text or ""),
        st.get("summary", "") or "",
        tail_turns=int(eff["router_tail_turns"]),
        summary_chars=int(eff["router_summary_chars"]),
        max_chars=int(eff["router_max_chars"]),
    )

    # ---------------------- WEB INJECT ----------------------
    t0 = time.perf_counter()
    print(f"[{now_str()}] GEN web_inject START session={session_id} latest_user_text_chars={len(str(base_user_text or ''))}")
    try:
        block = None
        if auto_web and not (has_atts and bool(eff["disable_web_on_attachments"])):
            block = await decide_web_and_fetch(llm, router_text, k=web_k)

        print(f"[{now_str()}] ORCH web has_block={bool(block)} block_len={(len(block) if block else 0)}")

        if block:
            st["_ephemeral_web"] = (st.get("_ephemeral_web") or []) + [{
                "role": "assistant",
                "content": eff["web_block_preamble"] + "\n\n" + block,
            }]

        dt = time.perf_counter() - t0
        eph_cnt = len(st.get("_ephemeral_web") or [])
        print(f"[{now_str()}] GEN web_inject END   session={session_id} elapsed={dt:.3f}s ephemeral_blocks={eph_cnt}")
    except Exception as e:
        dt = time.perf_counter() - t0
        print(f"[{now_str()}] GEN web_inject ERROR session={session_id} elapsed={dt:.3f}s err={type(e).__name__}: {e}")

    # ---------------------- ATTACHMENT RETRIEVE (session-only) ----------------
    if has_atts and bool(eff["disable_global_rag_on_attachments"]):
        att_names = [att_get(a, "name") for a in atts if att_get(a, "name")]
        query_for_atts = (base_user_text or "").strip() or " ".join(att_names) or "document"
        print(f"[{now_str()}] ATTACHMENTS retrieve query={query_for_atts!r}")

        try:
            att_block = build_rag_block_session_only(query_for_atts, session_id)
        except Exception as e:
            print(f"[{now_str()}] ATTACHMENTS retrieve ERROR {type(e).__name__}: {e}")
            att_block = None

        if att_block:
            st["_ephemeral_web"] = (st.get("_ephemeral_web") or []) + [{
                "role": "assistant",
                "content": eff["rag_block_preamble"] + "\n\n" + att_block,
            }]
            print(f"[{now_str()}] ATTACHMENTS block injected chars={len(att_block)}")
        else:
            print(f"[{now_str()}] ATTACHMENTS no block injected")

    # ---------------------- PACK (web/attachments ephemeral included) ---------
    system_text = build_system_text()
    ephemeral_once = st.pop("_ephemeral_web", [])
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )

    # ---------------------- RAG ROUTER (normal turns only) --------------------
    rag_router_allowed = not (has_atts and bool(eff["disable_global_rag_on_attachments"]))
    if rag_router_allowed and bool(eff["rag_enabled"]):
        rag_need = False
        rag_query: Optional[str] = None

        if auto_rag:
            try:
                rag_need, rag_query = decide_rag(llm, router_text)
                print(f"[{now_str()}] RAG ROUTER need={rag_need} query={rag_query!r}")
            except Exception as e:
                print(f"[{now_str()}] RAG ROUTER ERROR {type(e).__name__}: {e}")
                # No fallback to settings; be conservative and skip.
                rag_need = False
                rag_query = None

        # If a web block was injected OR RAG router said no, skip RAG
        skip_rag = bool(ephemeral_once) or (not rag_need)

        from .packing import maybe_inject_rag_block
        packed = maybe_inject_rag_block(
            packed,
            session_id=session_id,
            skip_rag=skip_rag,
            rag_query=rag_query,
        )
    else:
        print(f"[{now_str()}] RAG ROUTER skipped (attachment turn or rag_enabled=False)")

    # ---------------------- STREAM SETUP --------------------------------------
    packed_chars = chars_len(packed)
    print(f"[{now_str()}] GEN pack READY session={session_id} msgs={len(packed)} chars={packed_chars} out_budget_req={out_budget_req}")

    dump_full_prompt(
        packed,
        params={"requested_out": out_budget_req, "temperature": temperature, "top_p": top_p},
        session_id=session_id,
    )

    persist_summary(session_id, st["summary"])

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_req, margin=int(eff["clamp_margin"])
    )
    print(f"[{now_str()}] GEN clamp_out_budget session={session_id} out_budget={out_budget} input_tokens_est={input_tokens_est}")

    stop_ev = cancel_event(session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        async with GEN_SEMAPHORE:
            mark_active(session_id, +1)
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
                s = chunk_bytes.decode("utf-8", errors="ignore")
                if RUNJSON_START in s and RUNJSON_END in s:
                    return
                if s.strip() == eff["stopped_line_marker"]:
                    return
                out_buf.extend(chunk_bytes)

            try:
                print(f"[{now_str()}] GEN run_stream START session={session_id} msgs={len(packed)} chars={packed_chars} out_budget={out_budget} tokens_in~={input_tokens_est}")
                async for chunk in run_stream(
                    llm=llm,
                    messages=packed,
                    out_budget=out_budget,
                    stop_ev=stop_ev,
                    request=request,
                    temperature=temperature,
                    top_p=top_p,
                    input_tokens_est=input_tokens_est,
                ):
                    if isinstance(chunk, (bytes, bytearray)):
                        _accum_visible(chunk)
                    else:
                        _accum_visible(chunk.encode("utf-8"))
                    yield chunk
            finally:
                try:
                    full_text = out_buf.decode("utf-8", errors="ignore").strip()
                    start = full_text.find(RUNJSON_START)
                    if start != -1:
                        end = full_text.find(RUNJSON_END, start)
                        if end != -1:
                            full_text = (full_text[:start] + full_text[end + len(RUNJSON_END):]).strip()
                    if full_text:
                        st["recent"].append({"role": "assistant", "content": full_text})
                        print(f"[{now_str()}] RECENT append assistant chars={len(full_text)}")
                except Exception:
                    pass
                try:
                    from ..store import apply_pending_for
                    apply_pending_for(session_id)
                except Exception:
                    pass
                try:
                    from ..store import list_messages as store_list_messages
                    from ..workers.retitle_worker import enqueue as enqueue_retitle
                    msgs = store_list_messages(session_id)
                    last_seq = max((int(m.id) for m in msgs), default=0)
                    enqueue_retitle(session_id, [asdict(m) for m in msgs], job_seq=last_seq)
                except Exception:
                    pass
                print(f"[{now_str()}] GEN run_stream END session={session_id}")
                mark_active(session_id, -1)

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "Connection": "keep-alive"},
    )


async def cancel_session(session_id: str):
    from .cancel import cancel_event
    cancel_event(session_id).set()
    return {"ok": True}


async def cancel_session_alias(session_id: str):
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/packing.py =====

# aimodel/file_read/services/packing.py
from __future__ import annotations
from typing import Tuple, List, Dict, Optional
from ..rag.retrieve import build_rag_block
from ..core.settings import SETTINGS
from ..core.memory import build_system, pack_messages, roll_summary_if_needed


def build_system_text() -> str:
    eff = SETTINGS.effective()
    base = build_system(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
    )
    guidance = str(eff["packing_guidance"])
    return base + guidance


def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    eff = SETTINGS.effective()

    packed, input_budget = pack_messages(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
        summary=summary,
        recent=recent,
        max_ctx=max_ctx,
        out_budget=out_budget,
    )

    packed, new_summary = roll_summary_if_needed(
        packed=packed,
        recent=recent,
        summary=summary,
        input_budget=input_budget,
        system_text=system_text,
    )

    # Inject ephemeral (web findings) BEFORE the last user message, so the final turn is still user.
    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

def maybe_inject_rag_block(messages: list[dict], *, session_id: str | None,
                           skip_rag: bool = False, rag_query: str | None = None) -> list[dict]:
    if skip_rag:
        return messages
    if not SETTINGS.get("rag_enabled", True):
        return messages
    if not messages or messages[-1].get("role") != "user":
        return messages

    user_q = rag_query or (messages[-1].get("content") or "")
    block = build_rag_block(user_q, session_id=session_id)

    if not block:
        print(f"[RAG INJECT] no hits (session={session_id}) q={(user_q or '')!r}")
        return messages

    print(f"[RAG INJECT] injecting (session={session_id}) chars={len(block)}")
    injected = messages[:-1] + [{"role": "user", "content": block}, messages[-1]]
    return injected

# ===== aimodel/file_read/services/prompt_utils.py =====

from __future__ import annotations
import json
from datetime import datetime
from typing import Dict, List


def now_str() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total


def dump_full_prompt(
    messages: List[Dict[str, object]],
    *,
    params: Dict[str, object],
    session_id: str,
) -> None:
    try:
        print(f"[{now_str()}] PROMPT DUMP BEGIN session={session_id} msgs={len(messages)}")
        print(json.dumps({"messages": messages, "params": params}, ensure_ascii=False, indent=2))
        print(f"[{now_str()}] PROMPT DUMP END   session={session_id}")
    except Exception as e:
        print(f"[{now_str()}] PROMPT DUMP ERROR session={session_id} err={type(e).__name__}: {e}")

# ===== aimodel/file_read/services/router_text.py =====

from __future__ import annotations
from typing import Optional, List
from ..core.settings import SETTINGS


def compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: Optional[int] = None,
    summary_chars: Optional[int] = None,
    max_chars: Optional[int] = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: List[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.memory import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List

from ..core.settings import SETTINGS
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END,
    build_run_json, watch_disconnect,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int]
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=SETTINGS.stream_queue_maxsize)
    SENTINEL = object()

    def produce():
        t_start = time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []

        try:
            try:
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=SETTINGS.stream_top_k,
                    repeat_penalty=SETTINGS.stream_repeat_penalty,
                    stop=SETTINGS.stream_stop_strings,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(
                        SETTINGS.stream_retry_min_tokens,
                        int(out_budget * SETTINGS.stream_retry_fraction)
                    )
                    log.warning(
                        "generate: context overflow, retrying with max_tokens=%d",
                        retry_tokens
                    )
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=SETTINGS.stream_top_k,
                        repeat_penalty=SETTINGS.stream_repeat_penalty,
                        stop=SETTINGS.stream_stop_strings,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(SETTINGS.stream_backpressure_sleep_sec)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                llm.reset()
            except Exception:
                pass

            try:
                out_text = "".join(out_parts)
                run_json = build_run_json(
                    request_cfg={
                        "temperature": temperature,
                        "top_p": top_p,
                        "max_tokens": out_budget
                    },
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                )
                if SETTINGS.runjson_emit:
                    q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass

            try:
                q.put_nowait(SENTINEL)
            except Exception:
                pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set() and SETTINGS.stream_emit_stopped_line:
            yield (f"\n{SETTINGS.stopped_line_marker}\n").encode("utf-8")
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=SETTINGS.stream_producer_join_timeout_sec)
        except Exception:
            pass

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message, update_last, append_message,
    delete_message, delete_messages_batch, list_messages,
    list_paged, delete_batch,
    merge_chat, merge_chat_new, edit_message, set_summary, get_summary,  # ‚Üê add these
)
from .index import ChatMeta

__all__ = [
    # chats
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "merge_chat", "merge_chat_new", "edit_message",  # ‚Üê add these
    # index
    "ChatMeta",
    # pending
    "set_summary", "get_summary"
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile, threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..adaptive.config.paths import app_data_dir

# -------- Directories & Paths --------
APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"
PENDING_PATH = APP_DIR / "pending.json"              # NEW
OLD_PENDING_DELETES = APP_DIR / "pending_deletes.json"  # NEW

# -------- Lock for safe writes --------
_lock = threading.RLock()

# -------- Helpers --------
def now_iso() -> str:
    """UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    """Safely write JSON to a temp file then move into place."""
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    """Ensure app/chats directories exist and index.json is initialized."""
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    """Return path to chat file for a session ID."""
    return CHATS_DIR / f"{session_id}.json"

# -------- Exports --------
__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "PENDING_PATH",          # NEW
    "OLD_PENDING_DELETES",   # NEW
    "_lock",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

# ===== aimodel/file_read/store/chats.py =====
from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any

from ..core.settings import SETTINGS
from ..utils.streaming import strip_runjson
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta


def _load_chat(session_id: str) -> Dict[str, Any]:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""  # backfill older files
        return data


@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str
    attachments: Optional[List[Dict]] = None   # ‚úÖ added

def _normalize_attachments(atts: Optional[list[Any]]) -> Optional[list[dict]]:
    if not atts:
        return None
    out = []
    for a in atts:
        # convert dataclass/typed object to dict
        if isinstance(a, dict):
            out.append({
                "name": a.get("name"),
                "source": a.get("source"),
                "sessionId": a.get("sessionId"),
            })
        else:
            try:
                out.append({
                    "name": getattr(a, "name", None),
                    "source": getattr(a, "source", None),
                    "sessionId": getattr(a, "sessionId", None),
                })
            except Exception:
                continue
    return out or None

def upsert_on_first_message(session_id: str, title: str) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or SETTINGS["chat_default_title"]),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row)
    save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)


def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)


def append_message(session_id: str, role: str, content: str, attachments: Optional[list[Any]] = None) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq,
        "sessionId": session_id,
        "role": role,
        "content": content,
        "createdAt": now_iso(),
    }
    norm_atts = _normalize_attachments(attachments)
    if norm_atts:
        msg["attachments"] = norm_atts   # ‚úÖ now JSON-serializable

    data["messages"].append(msg)
    data["seq"] = seq
    _save_chat(session_id, data)

    # update index meta as before...
    ...
    return ChatMessageRow(
        id=seq,
        sessionId=session_id,
        role=role,
        content=content,
        createdAt=msg["createdAt"],
        attachments=norm_atts,
    )



def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1


def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted


def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    rows: List[ChatMessageRow] = []
    for m in data.get("messages", []):
        rows.append(ChatMessageRow(
            id=m["id"],
            sessionId=m["sessionId"],
            role=m["role"],
            content=m["content"],
            createdAt=m.get("createdAt"),
            attachments=m.get("attachments", []),  # ‚úÖ safe default
        ))
    return rows


def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    min_size = int(SETTINGS["chat_page_min_size"])
    max_size = int(SETTINGS["chat_page_max_size"])
    size = max(min_size, min(max_size, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag


def delete_batch(session_ids: List[str]) -> List[str]:
    for sid in session_ids:
        try:
            chat_path(sid).unlink(missing_ok=True)
        except Exception:
            pass
    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids


def merge_chat(source_id: str, target_id: str):
    source_msgs = list_messages(source_id)
    target_msgs = list_messages(target_id)

    merged = []
    for m in source_msgs:
        row = append_message(target_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    for m in target_msgs:
        row = append_message(target_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    return merged


def _save_chat(session_id: str, data: Dict[str, Any]):
    atomic_write(chat_path(session_id), data)


def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)


def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")


def merge_chat_new(source_id: str, target_id: Optional[str] = None):
    from uuid import uuid4
    new_id = str(uuid4())
    upsert_on_first_message(new_id, SETTINGS["chat_merged_title"])

    merged = []
    for m in list_messages(source_id):
        row = append_message(new_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    if target_id:
        for m in list_messages(target_id):
            row = append_message(new_id, m.role, m.content, attachments=m.attachments)
            merged.append(row)

    return new_id, merged


def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            # normalize attachments
            if "attachments" in m and m["attachments"] is not None:
                norm = []
                for a in m["attachments"]:
                    if hasattr(a, "dict"):
                        norm.append(a.dict())
                    elif isinstance(a, dict):
                        norm.append(a)
                    else:
                        norm.append(dict(a))
                m["attachments"] = norm
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(
        id=updated["id"],
        sessionId=updated["sessionId"],
        role=updated["role"],
        content=updated["content"],
        createdAt=updated.get("createdAt"),
        attachments=updated.get("attachments", []),
    )



__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch", "merge_chat", "merge_chat_new",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary",
]

# ===== aimodel/file_read/store/default_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3‚Äì5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 1400,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative ‚Äî use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 300,
  "web_search_cache_superset_k": 10,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., ‚Äúwhat did I say first?‚Äù). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "‚èπ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 8.0,
  "web_fetch_max_chars": 3000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 3,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 2000,
  "web_orch_per_doc_char_budget": 1200,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2.0,
  "web_orch_overfetch_min_extra": 2,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4.0,
  "web_orch_js_retry_timeout_cap": 12.0,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 200,
  "web_orch_head_fraction": 0.67,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " ‚Ä¶ ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2.0,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 2000.0,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50.0,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20.0,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5.0,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5.0,
      "cap": 0.2
    }
  ],
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 32,
  "query_sum_temperature": 0.0,
  "query_sum_top_p": 1.0,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 96,
  "router_decide_temperature": 0.0,
  "router_decide_top_p": 1.0,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": false,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "‚Ä¶",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 1000,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2‚Äì5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_llm_examples": [
    {
      "role": "user",
      "content": "police station"
    },
    {
      "role": "assistant",
      "content": "Police Station"
    },
    {
      "role": "user",
      "content": "fire truck"
    },
    {
      "role": "assistant",
      "content": "Fire Truck"
    },
    {
      "role": "user",
      "content": "how do i install node on windows"
    },
    {
      "role": "assistant",
      "content": "Node Installation Windows"
    }
  ],
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1.0,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*‚Ä¢]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s]",
  "retitle_sanitize_replace_with": " ",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
    "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 4,
  "rag_max_chars_per_chunk": 1200,
  "rag_chunk_overlap_chars": 150,
  "rag_min_chars": 300,
  "rag_total_char_budget": 2200,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 64,
  "stream_retry_fraction": 0.5,
  "stream_stop_strings": [
    "\n‚èπ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2.0,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 50,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": true,
  "excel_header_normalize": true,
 "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0.0,
  "router_rag_decide_top_p": 1.0,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
"csv_quote_strings": true,
"csv_header_normalize": true,
"csv_infer_max_rows": 50,
"csv_infer_max_cols": 26
}

# ===== aimodel/file_read/store/effective_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3‚Äì5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 1400,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative ‚Äî use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 300,
  "web_search_cache_superset_k": 10,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., ‚Äúwhat did I say first?‚Äù). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "‚èπ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 8,
  "web_fetch_max_chars": 3000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 3,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 2000,
  "web_orch_per_doc_char_budget": 1200,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 2,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 200,
  "web_orch_head_fraction": 0.67,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " ‚Ä¶ ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 2000,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 32,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 96,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": false,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "‚Ä¶",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 1000,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2‚Äì5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_llm_examples": [
    {
      "role": "user",
      "content": "police station"
    },
    {
      "role": "assistant",
      "content": "Police Station"
    },
    {
      "role": "user",
      "content": "fire truck"
    },
    {
      "role": "assistant",
      "content": "Fire Truck"
    },
    {
      "role": "user",
      "content": "how do i install node on windows"
    },
    {
      "role": "assistant",
      "content": "Node Installation Windows"
    }
  ],
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*‚Ä¢]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s]",
  "retitle_sanitize_replace_with": " ",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": false,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 4,
  "rag_max_chars_per_chunk": 1200,
  "rag_chunk_overlap_chars": 150,
  "rag_min_chars": 300,
  "rag_total_char_budget": 2200,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 64,
  "stream_retry_fraction": 0.5,
  "stream_stop_strings": [
    "\n‚èπ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 50,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": true,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26
}

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/store/override_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3‚Äì5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 1400,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative ‚Äî use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 300,
  "web_search_cache_superset_k": 10,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., ‚Äúwhat did I say first?‚Äù). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "‚èπ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 8,
  "web_fetch_max_chars": 3000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 3,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 2000,
  "web_orch_per_doc_char_budget": 1200,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 2,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 200,
  "web_orch_head_fraction": 0.67,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " ‚Ä¶ ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 2000,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 32,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 96,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": false,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "‚Ä¶",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 1000,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2‚Äì5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_llm_examples": [
    {
      "role": "user",
      "content": "police station"
    },
    {
      "role": "assistant",
      "content": "Police Station"
    },
    {
      "role": "user",
      "content": "fire truck"
    },
    {
      "role": "assistant",
      "content": "Fire Truck"
    },
    {
      "role": "user",
      "content": "how do i install node on windows"
    },
    {
      "role": "assistant",
      "content": "Node Installation Windows"
    }
  ],
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*‚Ä¢]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s]",
  "retitle_sanitize_replace_with": " ",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": false,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 4,
  "rag_max_chars_per_chunk": 1200,
  "rag_chunk_overlap_chars": 150,
  "rag_min_chars": 300,
  "rag_total_char_budget": 2200,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 64,
  "stream_retry_fraction": 0.5,
  "stream_stop_strings": [
    "\n‚èπ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 50,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": true,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26
}

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..runtime.model_runtime import current_model_info, get_llm

# Markers MUST match the frontend parser
RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

# llama.cpp common stop strings
STOP_STRINGS = ["</s>", "User:", "\nUser:"]

# ---------- token + model helpers ----------

def strip_runjson(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    out, i = [], 0
    while True:
        start = s.find(RUNJSON_START, i)
        if start == -1:
            out.append(s[i:])
            break
        out.append(s[i:start])
        end = s.find(RUNJSON_END, start)
        if end == -1:
            break  # unmatched start ‚Üí drop tail
        i = end + len(RUNJSON_END)
    return "".join(out).strip()

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))  # type: ignore[arg-type]
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def build_run_json(
    *,
    request_cfg: Dict[str, object],   # temperature, top_p, max_tokens
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None

    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()

    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
        },
    }

# ---------- connection helper ----------

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/duckduckgo.py =====

# aimodel/file_read/web/duckduckgo.py
from __future__ import annotations
from typing import List, Optional, Tuple
import asyncio, time
from urllib.parse import urlparse

# Prefer new package; fallback for compatibility
try:
    from ddgs import DDGS  # type: ignore
except Exception:
    try:
        from duckduckgo_search import DDGS  # type: ignore
    except Exception:
        DDGS = None  # no provider

from ..core.settings import SETTINGS
from .provider import SearchHit

# -------- simple in-memory cache (superset caching) --------------------------
_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}

def _cache_key(query: str) -> str:
    return (query or "").strip().lower()

def _host(u: str) -> str:
    try:
        h = (urlparse(u).hostname or "").lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return ""

def _cache_get(query: str) -> Optional[List[SearchHit]]:
    eff = SETTINGS.effective()  # no fallbacks allowed
    ttl = int(eff["web_search_cache_ttl_sec"])
    key = _cache_key(query)
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > ttl:
        _CACHE.pop(key, None)
        return None
    return hits

def _cache_set(query: str, hits: List[SearchHit]) -> None:
    _CACHE[_cache_key(query)] = (time.time(), hits)

# -------- DDGS (official client) --------------------------------------------
def _ddg_sync_search(query: str, k: int, *, region: str, safesearch: str) -> List[SearchHit]:
    results: List[SearchHit] = []
    if DDGS is None:
        print(f"[{time.strftime('%X')}] ddg: PROVIDER MISSING (DDGS=None)")
        return results
    with DDGS() as ddg:
        for i, r in enumerate(ddg.text(query, max_results=max(1, k),
                                       safesearch=safesearch, region=region)):
            title = (r.get("title") or "").strip()
            url = (r.get("href") or "").strip()
            snippet: Optional[str] = (r.get("body") or "").strip() or None
            if not url:
                continue
            results.append(SearchHit(title=title or url, url=url, snippet=snippet, rank=i))
            if i + 1 >= k:
                break
    return results

# -------- public provider ----------------------------------------------------
class DuckDuckGoProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        eff = SETTINGS.effective()  # strict read; raise if missing
        q_norm = (query or "").strip()
        if not q_norm:
            return []

        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        region = str(eff["web_search_region"])
        safesearch = str(eff["web_search_safesearch"])
        verbose = bool(eff["web_search_debug_logging"])

        t0 = time.time()
        if verbose:
            print(f"[{time.strftime('%X')}] ddg: START q={q_norm!r} k={k}")

        # cache
        cached = _cache_get(q_norm)
        if cached is not None:
            out = cached[:k]
            if verbose:
                top_preview = [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]]
                print(f"[{time.strftime('%X')}] ddg: CACHE HIT dt={time.time()-t0:.2f}s hits={len(out)} top={top_preview}")
            return out

        # fetch superset once
        hits: List[SearchHit] = []
        if DDGS is not None:
            try:
                step = time.time()
                hits = await asyncio.to_thread(_ddg_sync_search, q_norm, superset_k,
                                               region=region, safesearch=safesearch)
                if verbose:
                    print(f"[{time.strftime('%X')}] ddg: HITS RECEIVED dt={time.time()-step:.2f}s count={len(hits)}")
                    for h in hits[:5]:
                        print(f"[{time.strftime('%X')}] ddg:   {h.rank:>2} | host={_host(h.url)} | "
                              f"title={(h.title or '')[:80]!r}")
            except Exception as e:
                print(f"[{time.strftime('%X')}] ddg: ERROR {e}")
        else:
            print(f"[{time.strftime('%X')}] ddg: SKIP (DDGS unavailable)")

        _cache_set(q_norm, hits)
        out = hits[:k]
        if verbose:
            print(f"[{time.strftime('%X')}] ddg: RETURN dt={time.time()-t0:.2f}s hits={len(out)}")
        return out

# ===== aimodel/file_read/web/fetch.py =====

# aimodel/file_read/web/fetch.py
from __future__ import annotations
import asyncio
from typing import Tuple, List, Optional
import httpx

# Optional extractors (use whatever is installed)
try:
    from readability import Document  # readability-lxml
except Exception:
    Document = None  # type: ignore

try:
    from bs4 import BeautifulSoup  # beautifulsoup4 + lxml
except Exception:
    BeautifulSoup = None  # type: ignore

try:
    from selectolax.parser import HTMLParser  # selectolax
except Exception:
    HTMLParser = None  # type: ignore

from ..core.settings import SETTINGS


def _ua() -> str:
    return str(SETTINGS.get("web_fetch_user_agent", "LocalAI/0.1 (+clean-fetch)"))

def _timeout() -> float:
    try:
        return float(SETTINGS.get("web_fetch_timeout_sec", 8.0))
    except Exception:
        return 8.0

def _max_chars() -> int:
    try:
        return int(SETTINGS.get("web_fetch_max_chars", 3000))
    except Exception:
        return 3000

def _max_bytes() -> int:
    try:
        return int(SETTINGS.get("web_fetch_max_bytes", 1_048_576))
    except Exception:
        return 1_048_576

def _max_parallel() -> int:
    try:
        return max(1, int(SETTINGS.get("web_fetch_max_parallel", 3)))
    except Exception:
        return 3


async def _read_capped_bytes(resp: httpx.Response, cap_bytes: int) -> bytes:
    # Stream and cap to avoid huge downloads
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    """
    Best-effort HTML ‚Üí text:
      1) readability-lxml (if available): main-article extraction
      2) selectolax (if available): fast DOM text
      3) BeautifulSoup (if available): generic text fallback
      4) Raw text as last resort
    """
    html = raw_html or ""

    # 1) readability main content
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
                # If bs4 missing, fall back to selectolax/plain below
        except Exception:
            pass

    # 2) selectolax full text
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            # Remove script/style/noscript quickly if present
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = tree.body.text(separator=" ", strip=True) if tree.body else tree.text(separator=" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass

    # 3) BeautifulSoup full text
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            # Strip scripts/styles
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass

    # 4) Raw (no HTML parsing available)
    return html


async def fetch_clean(
    url: str,
    timeout_s: Optional[float] = None,
    max_chars: Optional[int] = None,
    max_bytes: Optional[int] = None,
) -> Tuple[str, int, str]:
    """
    Returns: (final_url, status_code, cleaned_text)
    - cleaned_text is extracted via readability/selectolax/bs4 if available; else raw
    - caps by bytes first, then by chars
    """
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout, headers=headers) as client:
        r = await client.get(url)
        r.raise_for_status()

        # Stream with byte cap
        raw_bytes = await _read_capped_bytes(r, cap_bytes)
        # Text decode with fallback (httpx sets encoding heuristically)
        enc = r.encoding or "utf-8"
        raw_text = raw_bytes.decode(enc, errors="ignore")

        # Extract/clean HTML to text (no trafilatura)
        txt = _extract_text_from_html(raw_text, str(r.url))
        txt = (txt or "").strip().replace("\r", "")

        if len(txt) > cap_chars:
            txt = txt[:cap_chars]

        return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: List[str],
    per_timeout_s: Optional[float] = None,
    cap_chars: Optional[int] = None,
    cap_bytes: Optional[int] = None,
    max_parallel: Optional[int] = None,
):
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))

    async def _one(u: str):
        async with sem:
            try:
                return u, await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                )
            except Exception:
                return u, None

    tasks = [_one(u) for u in urls]
    return await asyncio.gather(*tasks)

# ===== aimodel/file_read/web/orchestrator.py =====

# aimodel/file_read/web/orchestrator.py
from __future__ import annotations
from typing import List, Tuple, Optional
from urllib.parse import urlparse
import time
import re

from ..core.settings import SETTINGS
from .duckduckgo import DuckDuckGoProvider
from .provider import SearchHit
from .fetch import fetch_many  # optional JS path resolved dynamically (see below)

# ===== helpers to read config (strict: no silent defaults) ====================

def _req(key: str):
    return SETTINGS[key]

def _as_int(key: str) -> int: return int(_req(key))
def _as_float(key: str) -> float: return float(_req(key))
def _as_bool(key: str) -> bool: return bool(_req(key))
def _as_str(key: str) -> str:
    v = _req(key)
    return "" if v is None else str(v)

# ===== small utils ============================================================

def _clean_ws(s: str) -> str:
    return " ".join((s or "").split())

def _host(url: str) -> str:
    h = (urlparse(url).hostname or "").lower()
    pref = _as_str("web_orch_www_prefix")
    return h[len(pref):] if pref and h.startswith(pref) else h

def _tokens(s: str) -> List[str]:
    return [t for t in re.findall(r"\w+", (s or "").lower()) if t]

def _head_tail(text: str, max_chars: int) -> str:
    """
    Trim long text to head/tail, using settings:
      - web_orch_head_fraction
      - web_orch_tail_min_chars
      - web_orch_ellipsis
    Mirrors the old behavior but configurable.
    """
    text = text or ""
    if max_chars <= 0 or len(text) <= max_chars:
        return _clean_ws(text)

    head_frac      = _as_float("web_orch_head_fraction")      # e.g., 0.67
    tail_min_chars = _as_int("web_orch_tail_min_chars")       # e.g., 200
    ellipsis       = _as_str("web_orch_ellipsis")             # e.g., " ‚Ä¶ "

    head = int(max_chars * head_frac)
    tail = max_chars - head
    if tail < tail_min_chars:
        head = max(1, max_chars - tail_min_chars)
        tail = tail_min_chars

    return _clean_ws(text[:head] + ellipsis + text[-tail:])

def condense_doc(title: str, url: str, text: str, *, max_chars: int) -> str:
    body = _head_tail(text or "", max_chars)
    safe_title = _clean_ws(title or url)
    bullet = _as_str("web_orch_bullet_prefix") or "- "
    indent = _as_str("web_orch_indent_prefix") or "  "
    return f"{bullet}{safe_title}\n{indent}{url}\n{indent}{body}"

# ===== scoring (generic; no domain/date heuristics) ===========================

def score_hit(hit: SearchHit, query: str) -> int:
    """
    Generic, content-based scoring.
      - exact phrase in title (+web_orch_score_w_exact)
      - substring in title (+web_orch_score_w_substr)
      - token coverage in title (+0..web_orch_score_w_title_full/title_part)
      - any token in snippet (+web_orch_score_w_snip_touch)
    """
    w_exact      = _as_int("web_orch_score_w_exact")
    w_substr     = _as_int("web_orch_score_w_substr")
    w_title_full = _as_int("web_orch_score_w_title_full")
    w_title_part = _as_int("web_orch_score_w_title_part")
    w_snip_touch = _as_int("web_orch_score_w_snip_touch")

    score = 0
    q = (query or "").strip().lower()
    title = (hit.title or "").strip()
    snippet = (hit.snippet or "").strip()
    title_l = title.lower()
    snip_l  = snippet.lower()

    if q:
        if title_l == q:
            score += w_exact
        elif q in title_l:
            score += w_substr

    qtoks = _tokens(q)
    if qtoks:
        cov_title = sum(1 for t in qtoks if t in title_l)
        if cov_title == len(qtoks) and len(qtoks) > 0:
            score += w_title_full
        elif cov_title > 0:
            score += w_title_part
        cov_snip = sum(1 for t in qtoks if t in snip_l)
        if cov_snip > 0:
            score += w_snip_touch

    return score

# ===== quality estimate (generic; configurable penalties) =====================

def _type_ratio(text: str, sub: str) -> float:
    if not text:
        return 1.0
    cnt = text.lower().count(sub)
    return float(cnt) / max(1, len(text))

def content_quality_score(text: str) -> float:
    if not text:
        return 0.0
    t = text.strip()
    n = len(t)

    len_div     = _as_float("web_orch_q_len_norm_divisor")
    w_len       = _as_float("web_orch_q_len_weight")
    w_div       = _as_float("web_orch_q_diversity_weight")

    length_score = min(1.0, n / len_div) if len_div > 0 else 0.0

    toks = _tokens(t)
    if not toks:
        return 0.1 * length_score  # tiny signal if no tokens

    uniq = len(set(toks))
    diversity = uniq / max(1.0, float(len(toks)))

    pen = 0.0
    # penalties: [{"token": str, "mult": float, "cap": float}, ...]
    for rule in _req("web_orch_q_penalties"):
        token = str(rule.get("token") or "")
        mult  = float(rule.get("mult") or 0.0)
        cap   = float(rule.get("cap") or 1.0)
        pen += min(cap, _type_ratio(t, token) * mult)

    raw = (w_len * length_score) + (w_div * diversity) - pen
    return max(0.0, min(1.0, raw))

def _dedupe_by_host(scored_hits: List[Tuple[int, SearchHit]], k: int) -> List[SearchHit]:
    picked: List[SearchHit] = []
    seen_hosts = set()
    for s, h in sorted(scored_hits, key=lambda x: x[0], reverse=True):
        u = (h.url or "").strip()
        if not u:
            continue
        host = _host(u)
        if host in seen_hosts:
            continue
        seen_hosts.add(host)
        picked.append(h)
        if len(picked) >= k:
            break
    return picked

# ===== fetch layer ============================================================

async def _fetch_round(
    urls: List[str],
    meta: List[Tuple[str, str]],
    per_url_timeout_s: float,
    max_parallel: int,
    use_js: bool = False,
) -> List[Tuple[str, Optional[Tuple[str, int, str]]]]:

    fetch_fn = fetch_many
    if use_js:
        try:
            from . import fetch as _fetch_mod  # type: ignore
            fetch_fn = getattr(_fetch_mod, "fetch_many_js", fetch_many)
        except Exception:
            fetch_fn = fetch_many

    # Cap per doc by multiplier, but never exceed global max bytes/chars
    cap_mult        = _as_float("web_orch_fetch_cap_multiplier")
    per_doc_budget  = _as_int("web_orch_per_doc_char_budget")
    fetch_max_chars = _as_int("web_fetch_max_chars")
    per_doc_cap     = min(int(per_doc_budget * cap_mult), fetch_max_chars)

    results = await fetch_fn(
        urls,
        per_timeout_s=per_url_timeout_s,
        cap_chars=per_doc_cap,
        max_parallel=max_parallel,
    )
    return results

# ===== main ===================================================================

async def build_web_block(query: str, k: Optional[int] = None, per_url_timeout_s: Optional[float] = None) -> str | None:
    # pull config each call (to honor hot-reloads)
    cfg_k               = (int(k) if k is not None else _as_int("web_orch_default_k"))
    total_char_budget   = _as_int("web_orch_total_char_budget")
    per_doc_budget      = _as_int("web_orch_per_doc_char_budget")
    max_parallel        = _as_int("web_orch_max_parallel_fetch")
    overfetch_factor    = _as_float("web_orch_overfetch_factor")
    overfetch_min_extra = _as_int("web_orch_overfetch_min_extra")

    enable_js_retry     = _as_bool("web_orch_enable_js_retry")
    js_avg_q_thresh     = _as_float("web_orch_js_retry_avg_q")
    js_low_q_thresh     = _as_float("web_orch_js_retry_low_q")
    js_lowish_ratio     = _as_float("web_orch_js_retry_lowish_ratio")
    js_timeout_add      = _as_float("web_orch_js_retry_timeout_add")
    js_timeout_cap      = _as_float("web_orch_js_retry_timeout_cap")
    js_parallel_delta   = _as_int("web_orch_js_retry_parallel_delta")
    js_min_parallel     = _as_int("web_orch_js_retry_min_parallel")

    header_tpl          = _as_str("web_block_header")
    sep_str             = _as_str("web_orch_block_separator")
    min_block_reserve   = _as_int("web_orch_min_block_reserve")
    min_chunk_after     = _as_int("web_orch_min_chunk_after_shrink")

    # timeouts
    per_timeout = (float(per_url_timeout_s)
                   if per_url_timeout_s is not None
                   else _as_float("web_fetch_timeout_sec"))

    start_time = time.time()
    print(f"[orchestrator] IN  @ {start_time:.3f}s | query={query!r}")

    provider = DuckDuckGoProvider()

    # --- SEARCH (configurable overfetch) ---
    overfetch = max(cfg_k + overfetch_min_extra, int(round(cfg_k * overfetch_factor)))
    print(f"[orchestrator] SEARCH start overfetch={overfetch} k={cfg_k}")

    t0 = time.perf_counter()
    try:
        hits: List[SearchHit] = await provider.search(query, k=overfetch)
    except Exception as e:
        print(f"[orchestrator] ERROR during search for {query!r}: {e}")
        return None
    print(f"[orchestrator] SEARCH done hits={len(hits)} dt={time.perf_counter() - t0:.3f}s")

    if not hits:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no hits | elapsed={time.time()-start_time:.3f}s")
        return None

    # --- SCORING / DEDUPE ---
    print(f"[orchestrator] SCORING generic (no hardcoded boosts)")
    seen_urls = set()
    scored: List[Tuple[int, SearchHit]] = []
    for idx, h in enumerate(hits):
        u = (h.url or "").strip()
        if not u:
            print(f"[orchestrator]   skip[{idx}] empty url")
            continue
        if u in seen_urls:
            print(f"[orchestrator]   dup [{idx}] host={_host(u)} title={(h.title or '')[:60]!r}")
            continue
        seen_urls.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
        print(f"[orchestrator]   meta[{idx}] score={s} host={_host(u)} title={(h.title or '')[:80]!r} url={u}")

    if not scored:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no unique hits | elapsed={time.time()-start_time:.3f}s")
        return None

    top_hits = _dedupe_by_host(scored, cfg_k)
    for i, h in enumerate(top_hits, 1):
        # reuse computed scores when printing
        s = next((sc for sc, hh in scored if hh is h), 0)
        print(f"[orchestrator] PICK {i}/{cfg_k} score={s} host={_host(h.url)} title={(h.title or '')[:80]!r}")

    #