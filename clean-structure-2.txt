  *list(recent),
        ]

    # Final safety trim to budget
    t0_trim = time.time()
    packed = _final_safety_trim(packed, input_budget)
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0_trim)

    end_tokens = count_prompt_tokens(packed)
    PACK_TELEMETRY["rollEndTokens"] = int(end_tokens)
    return packed, summary

# ===== aimodel/file_read/core/request_ctx.py =====

# aimodel/file_read/core/request_ctx.py
from contextvars import ContextVar

from ..core.logging import get_logger

log = get_logger(__name__)

x_id_ctx: ContextVar[str] = ContextVar("x_id_ctx", default="")
id_token_ctx: ContextVar[str] = ContextVar("id_token_ctx", default="")
user_email_ctx: ContextVar[str] = ContextVar("user_email_ctx", default="")


def get_x_id() -> str:
    return x_id_ctx.get()


def set_x_id(val: str):
    x_id_ctx.set((val or "").strip())


def get_id_token() -> str:
    return id_token_ctx.get()


def set_id_token(val: str):
    id_token_ctx.set((val or "").strip())


def get_user_email() -> str:
    return user_email_ctx.get()


def set_user_email(val: str):
    user_email_ctx.set((val or "").strip())

# ===== aimodel/file_read/core/schemas.py =====

# core/schemas.py
from __future__ import annotations

from typing import Literal

from pydantic import BaseModel

from ..core.logging import get_logger

log = get_logger(__name__)


class Attachment(BaseModel):
    name: str
    source: str | None = None
    sessionId: str | None = None


class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str
    attachments: list[Attachment] | None = None


class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: str | None = None
    createdAt: str
    updatedAt: str
    ownerUid: str | None = None
    ownerEmail: str | None = None


class PageResp(BaseModel):
    content: list[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool


class BatchMsgDeleteReq(BaseModel):
    messageIds: list[int]


class BatchDeleteReq(BaseModel):
    sessionIds: list[str]


class EditMessageReq(BaseModel):
    messageId: int
    content: str


class ChatBody(BaseModel):
    sessionId: str | None = None
    messages: list[ChatMessage] | None = None

    max_tokens: int | None = None
    temperature: float | None = None
    top_p: float | None = None

    autoWeb: bool | None = None
    webK: int | None = None
    autoRag: bool | None = None

# ===== aimodel/file_read/core/settings.py =====

from __future__ import annotations

import json
from threading import RLock
from typing import Any

from ..core.logging import get_logger

log = get_logger(__name__)
from .files import (DEFAULTS_SETTINGS_FILE, OVERRIDES_SETTINGS_FILE,
                    load_json_file, save_json_file)


def _deep_merge(dst: dict[str, Any], src: dict[str, Any]) -> dict[str, Any]:
    out = dict(dst)
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)
        else:
            out[k] = v
    return out


class _SettingsManager:
    """
    Sources of truth:
      - defaults:    read-only from DEFAULTS_SETTINGS_FILE
      - overrides:   persisted to OVERRIDES_SETTINGS_FILE
      - adaptive:    in-memory (per-session or global)
    Effective settings are computed on the fly: defaults <- adaptive <- overrides.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._defaults: dict[str, Any] = self._load_defaults()
        self._overrides: dict[str, Any] = self._load_overrides()
        self._adaptive_by_session: dict[str, dict[str, Any]] = {}

    # ----- I/O -----
    def _load_defaults(self) -> dict[str, Any]:
        return load_json_file(DEFAULTS_SETTINGS_FILE, default={})

    def _load_overrides(self) -> dict[str, Any]:
        return load_json_file(OVERRIDES_SETTINGS_FILE, default={})

    def _save_overrides_unlocked(self) -> None:
        save_json_file(OVERRIDES_SETTINGS_FILE, self._overrides)

    # ----- Effective -----
    def _effective_unlocked(self, session_id: str | None = None) -> dict[str, Any]:
        eff = _deep_merge(
            self._defaults, self._adaptive_by_session.get(session_id or "_global_", {})
        )
        eff = _deep_merge(eff, self._overrides)
        return eff

    def _get_unlocked(self, key: str, default: Any = None, *, session_id: str | None = None) -> Any:
        eff = self._effective_unlocked(session_id)
        if key in eff:
            return eff[key]
        if default is not None:
            return default
        raise AttributeError(f"_SettingsManager has no key '{key}'")

    # ----- Public API -----
    @property
    def defaults(self) -> dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._defaults))

    @property
    def overrides(self) -> dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._overrides))

    def adaptive(self, session_id: str | None = None) -> dict[str, Any]:
        key = session_id or "_global_"
        with self._lock:
            return json.loads(json.dumps(self._adaptive_by_session.get(key, {})))

    def effective(self, session_id: str | None = None) -> dict[str, Any]:
        with self._lock:
            return self._effective_unlocked(session_id)

    def __getattr__(self, name: str) -> Any:
        with self._lock:
            return self._get_unlocked(name)

    def __getitem__(self, key: str) -> Any:
        with self._lock:
            return self._get_unlocked(key)

    def get(self, key: str, default: Any = None, *, session_id: str | None = None) -> Any:
        with self._lock:
            try:
                return self._get_unlocked(key, default=default, session_id=session_id)
            except AttributeError:
                return default

    # ----- Mutations -----
    def patch_overrides(self, patch: dict[str, Any]) -> None:
        if not isinstance(patch, dict):
            return
        with self._lock:
            self._overrides = _deep_merge(self._overrides, patch)
            self._save_overrides_unlocked()

    def replace_overrides(self, new_overrides: dict[str, Any]) -> None:
        if not isinstance(new_overrides, dict):
            new_overrides = {}
        with self._lock:
            self._overrides = json.loads(json.dumps(new_overrides))
            self._save_overrides_unlocked()

    def reload_overrides(self) -> None:
        with self._lock:
            self._overrides = self._load_overrides()

    def set_adaptive_for_session(self, session_id: str | None, values: dict[str, Any]) -> None:
        key = session_id or "_global_"
        if not isinstance(values, dict):
            values = {}
        with self._lock:
            self._adaptive_by_session[key] = json.loads(json.dumps(values))

    def recompute_adaptive(self, session_id: str | None = None) -> None:
        # Kept for API compatibility; effective is always computed on demand.
        return None


SETTINGS = _SettingsManager()

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations

import re

from ..core.logging import get_logger
from .settings import SETTINGS

log = get_logger(__name__)


def get_style_sys() -> str:
    return SETTINGS.get("style_sys", "")


def extract_style_and_prefs(user_text: str) -> tuple[str | None, bool, bool]:
    S = SETTINGS.effective()
    pats = S.get("style_patterns", {})
    template = S.get("style_template", "You must talk like {style}.")

    compiled = []
    for key in ["talk_like", "respond_like", "from_now", "be"]:
        if key in pats:
            compiled.append(re.compile(pats[key], re.I))

    t = user_text.strip()
    style_match = None
    for pat in compiled:
        style_match = pat.search(t)
        if style_match:
            break

    style_inst: str | None = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = template.format(style=raw)

    return style_inst, False, False

# ===== aimodel/file_read/deps/admin_deps.py =====

# aimodel/file_read/deps/admin_deps.py
from __future__ import annotations

from fastapi import Depends, HTTPException

from ..core import admins as reg
from .auth_deps import require_auth


async def require_admin(user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    if uid and reg.is_admin(uid):
        return user
    raise HTTPException(403, "Admin required")

# ===== aimodel/file_read/deps/auth_deps.py =====

# aimodel/file_read/deps/auth_deps.py
from __future__ import annotations

import os
from typing import Any

from fastapi import Cookie, Header, HTTPException, Request, status

from ..core.logging import get_logger
from ..services.auth_service import verify_jwt_with_google

log = get_logger(__name__)

ID_COOKIE_NAME = "fb_id"
LEGACY_COOKIE_NAME = "fb_session"
AUTH_ORG_ID = (os.getenv("AUTH_ORG_ID") or "").strip() or None


async def require_auth(
    request: Request,
    authorization: str | None = Header(None),
    fb_id_cookie: str | None = Cookie(None, alias=ID_COOKIE_NAME),
    fb_session_cookie: str | None = Cookie(None, alias=LEGACY_COOKIE_NAME),
) -> dict[str, Any]:
    # ---- Dev bypass (local only) ----
    if os.getenv("DEV_AUTH_BYPASS", "").lower() in ("1", "true", "yes"):
        user = {"email": "dev@local", "user_id": "dev", "sub": "dev"}
        request.state.auth_error_reason = "BYPASS_DEV"
        return user

    reasons: list[str] = []

    async def _try(token: str, origin: str) -> dict[str, Any] | None:
        try:
            claims = await verify_jwt_with_google(token)
            if not isinstance(claims, dict) or not claims.get("email"):
                reasons.append(f"{origin}:NO_EMAIL")
                return None
            # Optional org gate
            if AUTH_ORG_ID and str(claims.get("org_id") or "").strip() != AUTH_ORG_ID:
                reasons.append(f"{origin}:ORG_MISMATCH")
                return None
            return claims
        except HTTPException as e:  # your verifier may raise HTTPException(401, "…")
            # Common: TOKEN_EXPIRED / INVALID_SIGNATURE / INVALID_AUD, etc.
            reasons.append(f"{origin}:{e.detail or 'HTTP_401'}")
            log.debug("[auth] verify error (%s): %s", origin, e.detail)
            return None
        except Exception as e:
            reasons.append(f"{origin}:INVALID:{type(e).__name__}")
            log.debug("[auth] verify exception (%s): %r", origin, e)
            return None

    # 1) First-choice: fb_id cookie
    if fb_id_cookie:
        claims = await _try(fb_id_cookie, f"cookie:{ID_COOKIE_NAME}")
        if claims:
            return claims

    # 2) Legacy fallback cookie
    if fb_session_cookie:
        claims = await _try(fb_session_cookie, f"cookie:{LEGACY_COOKIE_NAME}")
        if claims:
            return claims

    # 3) Authorization: Bearer <token>
    if authorization and authorization.lower().startswith("bearer "):
        token = authorization.split(None, 1)[1].strip()
        if token:
            claims = await _try(token, "authz:bearer")
            if claims:
                return claims
        else:
            reasons.append("authz:bearer:EMPTY")

    # No valid auth presented or all attempts failed
    request.state.auth_error_reason = ";".join(reasons) if reasons else "NO_CREDENTIALS"
    raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=request.state.auth_error_reason)

# ===== aimodel/file_read/deps/license_deps.py =====

from __future__ import annotations

import time
from fastapi import Depends, HTTPException

from ..core.request_ctx import get_user_email
from ..services.licensing_core import email_from_auth, license_status_local
from ..services.licensing_service import (
    get_activation_status,
    refresh_activation,
    remove_activation_file,
)
from .admin_deps import require_admin as _require_admin
from .auth_deps import require_auth

PRO_PLANS = {"pro", "enterprise"}


def _is_plan_pro(st: dict) -> bool:
    return bool(st.get("valid")) and (str(st.get("plan", "")).lower() in PRO_PLANS)


def has_personal_pro(email: str | None) -> bool:
    email = (email or "").strip().lower()
    st = license_status_local(expected_email=email if email else None)
    return _is_plan_pro(st)


async def _ensure_device_active() -> None:
    """
    Require local activation token. If close to expiry, try refreshing.
    If the licensing server rejects (401/403/404), delete the local token.
    """
    st = get_activation_status()
    if not st.get("present"):
        raise HTTPException(403, "Device not activated")

    now = int(time.time())
    exp = int(st.get("exp") or 0)
    needs_refresh = (not exp) or (exp - now < 7 * 24 * 3600)

    if needs_refresh:
        try:
            await refresh_activation()
        except HTTPException as e:
            if e.status_code in (401, 403, 404):
                remove_activation_file()
                raise HTTPException(403, "Device activation revoked")


# ---------- Exported FastAPI dependencies ----------

async def require_activation(user=Depends(require_auth)):
    """
    Device-scoped Pro: requires this device to be activated.
    Any authenticated user on this device passes.
    """
    await _ensure_device_active()
    return user


# Back-compat alias (some modules import require_pro)
require_pro = require_activation


async def require_personal_pro(user=Depends(require_auth)):
    """
    Personal Pro only (no device activation check).
    Use for actions tied to the user's own subscription.
    """
    email = email_from_auth(user)
    if not has_personal_pro(email):
        raise HTTPException(403, "Personal Pro required")
    return user


async def require_personal_pro_activated(user=Depends(require_auth)):
    """
    Personal Pro + Activated device (no admin). Use for RAG/Web endpoints.
    """
    email = email_from_auth(user)
    if not has_personal_pro(email):
        raise HTTPException(403, "Personal Pro required")
    await _ensure_device_active()
    return user


async def require_admin(user=Depends(_require_admin)):
    return user


async def require_admin_personal_pro(user=Depends(_require_admin)):
    """
    Admin + Personal Pro (NO device activation).
    Use when the action is admin-scoped but shouldn't force device activation.
    """
    email = email_from_auth(user)
    if not has_personal_pro(email):
        raise HTTPException(403, "Personal Pro required for admin action")
    return user


async def require_admin_pro(user=Depends(_require_admin)):
    """
    Admin + Personal Pro + Activated device.
    """
    email = email_from_auth(user)
    if not has_personal_pro(email):
        raise HTTPException(403, "Personal Pro required for admin action")
    await _ensure_device_active()
    return user


def is_request_pro_activated() -> bool:
    """
    Synchronous check for code paths that can't use FastAPI Depends.
    Requires BOTH: device activation AND personal Pro for the current request user.
    """
    try:
        # Device must be activated
        if not get_activation_status().get("present"):
            return False
        # Caller must be Pro (use request context email if available)
        email = (get_user_email() or "").strip().lower()
        return has_personal_pro(email)
    except Exception:
        return False

# ===== aimodel/file_read/deps/model_deps.py =====

# deps/model_deps.py
from fastapi import HTTPException
from ..runtime.model_runtime import ensure_ready

# NEW: import quick worker check
import httpx
from ..api.model_workers import get_active_worker_addr  # lightweight helper

def _worker_ready() -> bool:
    try:
        host, port = get_active_worker_addr()
    except Exception:
        return False
    try:
        # fast health probe (no auth needed for internal hop)
        url = f"http://{host}:{port}/api/worker/health"
        r = httpx.get(url, timeout=1.5)
        return r.status_code == 200 and r.json().get("ok") is True
    except Exception:
        return False

def require_model_ready():
    # If main runtime is ready, we're good.
    try:
        ensure_ready()
        return
    except Exception:
        pass

    # Otherwise, accept an active worker.
    if _worker_ready():
        return

    # Neither is ready → preserve your original 409 shape.
    raise HTTPException(
        status_code=409,
        detail={"code": "MODEL_NOT_LOADED", "message": "No model loaded (runtime) and no active worker."}
    )

# ===== aimodel/file_read/rag/__init__.py =====



# ===== aimodel/file_read/rag/ingest/__init__.py =====

# aimodel/file_read/rag/ingest/__init__.py

from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import csv
import hashlib
import io
import json
import re
from dataclasses import dataclass
from datetime import date, datetime, time
from typing import Dict, List, Optional, Tuple

import pypdfium2 as pdfium
import pytesseract
from PIL import Image

from ...core.settings import SETTINGS
from .common import Chunk, _strip_html, _utf8, build_metas, chunk_text
from .csv_ingest import extract_csv
from .doc_binary_ingest import extract_doc_binary
from .docx_ingest import extract_docx
from .excel_ingest import extract_excel
from .excel_ingest_core import (rightmost_nonempty_header,
                                scan_blocks_by_blank_rows, select_indices)
from .main import sniff_and_extract
from .ocr import is_bad_text, ocr_image_bytes, ocr_pdf
from .pdf_ingest import extract_pdf
from .ppt_ingest import extract_ppt, extract_pptx
from .xls_ingest import extract_xls

__all__ = [
    "SETTINGS",
    "Chunk",
    "Dict",
    "Image",
    "List",
    "Optional",
    "Tuple",
    "_strip_html",
    "_utf8",
    "build_metas",
    "chunk_text",
    "csv",
    "dataclass",
    "date",
    "datetime",
    "extract_csv",
    "extract_doc_binary",
    "extract_docx",
    "extract_excel",
    "extract_pdf",
    "extract_ppt",
    "extract_pptx",
    "extract_xls",
    "hashlib",
    "io",
    "is_bad_text",
    "json",
    "ocr_image_bytes",
    "ocr_pdf",
    "pdfium",
    "pytesseract",
    "re",
    "rightmost_nonempty_header",
    "scan_blocks_by_blank_rows",
    "select_indices",
    "sniff_and_extract",
    "time",
]

# ===== aimodel/file_read/rag/ingest/common.py =====

# ===== aimodel/file_read/rag/ingest/common.py =====
from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import re
from dataclasses import dataclass

from ...core.settings import SETTINGS


@dataclass
class Chunk:
    text: str
    meta: dict[str, str]


def _utf8(data: bytes) -> str:
    return (data or b"").decode("utf-8", errors="ignore")


def _strip_html(txt: str) -> str:
    if not txt:
        return ""

    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", txt)
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p>", "\n\n", txt)
    txt = re.sub(r"(?is)<.*?>", " ", txt)
    txt = re.sub(r"[ \t]+", " ", txt)
    return txt.strip()


_HDR_RE = re.compile(r"^(#{1,3})\s+.*$", flags=re.MULTILINE)
_PARA_SPLIT_RE = re.compile(r"\n\s*\n+")


def _split_sections(text: str) -> list[str]:
    text = (text or "").strip()
    if not text:
        return []
    starts = [m.start() for m in _HDR_RE.finditer(text)]
    if not starts:
        return [text]
    if 0 not in starts:
        starts = [0] + starts
    sections: list[str] = []
    for i, s in enumerate(starts):
        e = starts[i + 1] if i + 1 < len(starts) else len(text)
        block = text[s:e].strip()
        if block:
            sections.append(block)
    return sections


def _split_paragraphs(block: str) -> list[str]:
    paras = [p.strip() for p in _PARA_SPLIT_RE.split(block or "")]
    return [p for p in paras if p]


def _hard_split(text: str, max_len: int) -> list[str]:
    approx = re.split(r"(?<=[\.\!\?\;])\s+", text or "")
    out: list[str] = []
    buf = ""
    for s in approx:
        if not s:
            continue
        if len(buf) + (1 if buf else 0) + len(s) <= max_len:
            buf = s if not buf else (buf + " " + s)
        else:
            if buf:
                out.append(buf)
            if len(s) <= max_len:
                out.append(s)
            else:
                words = re.split(r"\s+", s)
                cur = ""
                for w in words:
                    if not w:
                        continue
                    if len(cur) + (1 if cur else 0) + len(w) <= max_len:
                        cur = w if not cur else (cur + " " + w)
                    else:
                        if cur:
                            out.append(cur)
                        cur = w
                if cur:
                    out.append(cur)
            buf = ""
    if buf:
        out.append(buf)
    return out


def _pack_with_budget(pieces: list[str], *, max_chars: int) -> list[str]:
    chunks: list[str] = []
    cur: list[str] = []
    cur_len = 0
    for p in pieces:
        plen = len(p)
        if plen > max_chars:
            chunks.extend(_hard_split(p, max_chars))
            continue
        if cur_len == 0:
            cur, cur_len = [p], plen
            continue
        if cur_len + 2 + plen <= max_chars:
            cur.append(p)
            cur_len += 2 + plen
        else:
            chunks.append("\n\n".join(cur).strip())
            cur, cur_len = [p], plen
    if cur_len:
        chunks.append("\n\n".join(cur).strip())
    return chunks


def chunk_text(
    text: str,
    meta: dict[str, str] | None = None,
    *,
    max_chars: int = int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    overlap: int = int(SETTINGS.get("rag_chunk_overlap_chars", 150)),
) -> list[Chunk]:
    base_meta = (meta or {}).copy()
    text = (text or "").strip()
    if not text:
        return []

    if len(text) <= max_chars:
        return [Chunk(text=text, meta=base_meta)]

    sections = _split_sections(text)
    if not sections:
        sections = [text]

    chunks: list[Chunk] = []
    last_tail: str | None = None

    for sec in sections:
        paras = _split_paragraphs(sec)
        if not paras:
            continue
        packed = _pack_with_budget(paras, max_chars=max_chars)
        for ch in packed:
            if last_tail and overlap > 0:
                tail = last_tail[-overlap:] if len(last_tail) > overlap else last_tail
                candidate = f"{tail}\n{ch}"
                chunks.append(
                    Chunk(text=candidate if len(candidate) <= max_chars else ch, meta=base_meta)
                )
            else:
                chunks.append(Chunk(text=ch, meta=base_meta))
            last_tail = ch

    return chunks


def build_metas(
    session_id: str | None, filename: str, chunks: list[Chunk], *, size: int = 0
) -> list[dict[str, str]]:
    out: list[dict[str, str]] = []
    for i, c in enumerate(chunks):
        out.append(
            {
                "id": f"{filename}:{i}",
                "sessionId": session_id or "",
                "source": filename,
                "title": filename,
                "mime": "text/plain",
                "size": str(size),
                "chunkIndex": str(i),
                "text": c.text,
            }
        )
    return out

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====
from __future__ import annotations

import csv
import io
import re

from ...core.logging import get_logger
from ...core.settings import SETTINGS

log = get_logger(__name__)

_WS_RE = re.compile(r"[ \t]+")
_PHANTOM_RX = re.compile(r"^\d+_\d+$")


def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()


def extract_csv(data: bytes) -> tuple[str, str]:
    S = SETTINGS.effective
    max_chars = int(S().get("csv_value_max_chars"))
    quote_strings = bool(S().get("csv_quote_strings"))
    header_normalize = bool(S().get("csv_header_normalize"))
    max_rows = int(S().get("csv_infer_max_rows"))
    max_cols = int(S().get("csv_infer_max_cols"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_val(v) -> str:
        if v is None:
            return ""
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f'"{s}"'
        return s

    def normalize_header(h: str) -> str:
        if not header_normalize:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def rightmost_nonempty_header(headers: list[str]) -> int:
        for i in range(len(headers) - 1, -1, -1):
            h = headers[i]
            if h and not h.isspace():
                return i
        return -1

    def keep_headers(headers: list[str]) -> list[int]:
        keep = []
        for i, h in enumerate(headers):
            hn = (h or "").strip().lower()
            if not hn:
                continue
            if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
                continue
            keep.append(i)
        return keep or list(range(len(headers)))

    def _row_blank_csv(row: list[str]) -> bool:
        if row is None:
            return True
        for c in row:
            if c is None:
                continue
            if str(c).strip():
                return False
        return True

    txt = io.StringIO(data.decode("utf-8", errors="ignore"))
    sample = txt.read(2048)
    txt.seek(0)
    try:
        dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
    except Exception:
        dialect = csv.excel
    reader = csv.reader(txt, dialect)
    rows = list(reader)
    if not rows:
        return "", "text/plain"

    n = len(rows)
    lines: list[str] = []
    lines.append("# Sheet: CSV")

    i = 0
    while i < n:
        while i < n and _row_blank_csv(rows[i]):
            i += 1
        if i >= n:
            break

        start = i
        while i < n and not _row_blank_csv(rows[i]):
            i += 1
        end = i - 1
        if start > end:
            continue

        headers_raw = (rows[start] if start < n else [])[:max_cols]
        norm_headers = [normalize_header(fmt_val(h)) for h in headers_raw]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[: rmax + 1]
        norm_headers = norm_headers[:max_cols]
        keep_idx = keep_headers(norm_headers)
        kept_headers = [norm_headers[j] for j in keep_idx]

        total_rows_block = end - start + 1
        use_rows = total_rows_block if max_rows <= 0 else min(total_rows_block, max_rows + 1)
        total_cols_block = len(kept_headers)
        if max_cols > 0:
            total_cols_block = min(total_cols_block, max_cols)

        lines.append(f"## Table: CSV!R{start + 1}-{start + use_rows},C1-{max(total_cols_block, 1)}")
        if any(kept_headers):
            lines.append("headers: " + ", ".join(h for h in kept_headers if h))
        lines.append("")

        data_start = start + 1
        data_end = min(end, start + use_rows - 1)
        usable_cols_for_slice = min(
            len(norm_headers), max_cols if max_cols > 0 else len(norm_headers)
        )
        for r in range(data_start, data_end + 1):
            row_vals_raw = rows[r][:usable_cols_for_slice] if r < n else []
            vals = [fmt_val(c) for c in row_vals_raw]
            vals = [vals[j] if j < len(vals) else "" for j in keep_idx]
            while vals and (vals[-1] == "" or vals[-1] is None):
                vals.pop()
            if not any(vals):
                continue

            pairs: list[str] = []
            for h, v in zip(kept_headers, vals, strict=False):
                if h and v:
                    pairs.append(f"{h}={v}")

            excel_row_num = r + 1
            lines.append(f"### Row {excel_row_num} — CSV")
            lines.append(", ".join(pairs) if pairs else ", ".join(vals))
            lines.append("")

    return "\n".join(lines).strip() + "\n", "text/plain"

# ===== aimodel/file_read/rag/ingest/doc_binary_ingest.py =====

from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import re

from ...core.settings import SETTINGS

_WS_RE = re.compile("[ \\t]+")


def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()


def _is_ole(b: bytes) -> bool:
    return len(b) >= 8 and b[:8] == b"\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1"


def _dbg(msg: str):
    try:
        S = SETTINGS.effective
        if bool(S().get("doc_debug", False)):
            log.info(f"[doc_ingest] {msg}")
    except Exception:
        pass


_RTF_CTRL_RE = re.compile("\\\\[a-zA-Z]+-?\\d* ?")
_RTF_GROUP_RE = re.compile("[{}]")
_RTF_UNICODE_RE = re.compile("\\\\u(-?\\d+)\\??")
_RTF_HEX_RE = re.compile("\\\\'[0-9a-fA-F]{2}")
_HEX_BLOCK_RE = re.compile("(?:\\s*[0-9A-Fa-f]{2}){120,}")


def _rtf_to_text_simple(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")

    def _hex_sub(m):
        try:
            return bytes.fromhex(m.group(0)[2:]).decode("latin-1", errors="ignore")
        except Exception:
            return ""

    s = _RTF_HEX_RE.sub(_hex_sub, s)

    def _uni_sub(m):
        try:
            cp = int(m.group(1))
            if cp < 0:
                cp = 65536 + cp
            return chr(cp)
        except Exception:
            return ""

    s = _RTF_UNICODE_RE.sub(_uni_sub, s)
    s = s.replace("\\par", "\n").replace("\\line", "\n")
    s = _RTF_CTRL_RE.sub("", s)
    s = _RTF_GROUP_RE.sub("", s)
    s = _HEX_BLOCK_RE.sub("", s)
    s = s.replace("\r", "\n")
    s = re.sub("\\n\\s*\\n\\s*\\n+", "\n\n", s)
    s = _squeeze_spaces(s)
    return s if keep_newlines else s.replace("\n", " ")


def _rtf_to_text_via_lib(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        from striprtf.striprtf import rtf_to_text
    except Exception:
        return _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    try:
        txt = rtf_to_text(s)
    except Exception:
        txt = _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    txt = _squeeze_spaces(txt)
    return txt if keep_newlines else txt.replace("\n", " ")


def _generic_ole_text(data: bytes) -> str:
    S = SETTINGS.effective
    MIN_RUN = int(S().get("doc_ole_min_run_chars", 8))
    MAX_LINE = int(S().get("doc_ole_max_line_chars", 600))
    MIN_ALPHA_RATIO = float(S().get("doc_ole_min_alpha_ratio", 0.25))
    DROP_XMLISH = bool(S().get("doc_ole_drop_xmlish", True))
    DROP_PATHISH = bool(S().get("doc_ole_drop_pathish", True))
    DROP_SYMBOL_LINES = bool(S().get("doc_ole_drop_symbol_lines", True))
    DEDUPE_SHORT_REPEATS = bool(S().get("doc_ole_dedupe_short_repeats", True))
    XMLISH = re.compile("^\\s*<[^>]+>", re.I)
    PATHISH = re.compile("[\\\\/].+\\.(?:xml|rels|png|jpg|jpeg|gif|bmp|bin|dat)\\b", re.I)
    SYMBOLLINE = re.compile("^[\\W_]{6,}$")
    s = data.replace(b"\x00", b"")
    runs = re.findall(b"[\\t\\r\\n\\x20-\\x7E]{%d,}" % MIN_RUN, s)
    if not runs:
        return ""

    def _dec(b: bytes) -> str:
        try:
            return b.decode("cp1252", errors="ignore")
        except Exception:
            return b.decode("latin-1", errors="ignore")

    kept: list[str] = []
    for raw in runs:
        chunk = _dec(raw).replace("\r", "\n")
        for ln in re.split("\\n+", chunk):
            t = ln.strip()
            if not t:
                continue
            if MAX_LINE > 0 and len(t) > MAX_LINE:
                t = t[:MAX_LINE].rstrip()
            t = _squeeze_spaces(t)
            letters = sum(1 for c in t if c.isalpha())
            if letters / max(1, len(t)) < MIN_ALPHA_RATIO:
                continue
            if DROP_XMLISH and XMLISH.search(t):
                continue
            if DROP_PATHISH and PATHISH.search(t):
                continue
            if DROP_SYMBOL_LINES and SYMBOLLINE.fullmatch(t):
                continue
            if DEDUPE_SHORT_REPEATS:
                t = re.sub("\\b(\\w{2,4})\\1{2,}\\b", "\\1\\1", t)
            kept.append(t)
    out = "\n".join(kept)
    out = re.sub("\\n\\s*\\n\\s*\\n+", "\n\n", out).strip()
    return out


def extract_doc_binary(data: bytes) -> tuple[str, str]:
    head = (data[:64] or b"").lstrip()
    is_rtf = head.startswith(b"{\\rtf") or head.startswith(b"{\\RTF}")
    is_ole = _is_ole(data)
    _dbg(f"extract_doc_binary: bytes={len(data)} is_rtf={is_rtf} is_ole={is_ole}")
    if is_rtf:
        txt = _rtf_to_text_via_lib(data, keep_newlines=True).strip()
        return (txt + "\n" if txt else "", "text/plain")
    if is_ole:
        txt = _generic_ole_text(data)
        return (txt + "\n" if txt else "", "text/plain")
    try:
        txt = data.decode("utf-8", errors="ignore").strip()
    except Exception:
        txt = data.decode("latin-1", errors="ignore").strip()
    return (txt + ("\n" if txt else ""), "text/plain")

# ===== aimodel/file_read/rag/ingest/docx_ingest.py =====

# DOCX-only extraction (no .doc/RTF here)
from __future__ import annotations

import io
import re

from ...core.logging import get_logger
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

log = get_logger(__name__)

_WS_RE = re.compile(r"[ \t]+")


def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()


def _is_heading(style_name: str) -> int | None:
    if not style_name:
        return None
    m = re.match(r"Heading\s+(\d+)", style_name, flags=re.IGNORECASE)
    if not m:
        return None
    try:
        return max(1, min(6, int(m.group(1))))
    except Exception:
        return None


def _is_list_style(style_name: str) -> bool:
    return bool(style_name) and any(k in style_name.lower() for k in ("list", "bullet", "number"))


def _extract_paragraph_text(p) -> str:
    return _squeeze_spaces(p.text)


def _docx_image_blobs(doc) -> list[bytes]:
    blobs: list[bytes] = []
    seen_rids = set()
    try:
        part = doc.part

        # inline images
        for ish in getattr(doc, "inline_shapes", []) or []:
            try:
                rId = ish._inline.graphic.graphicData.pic.blipFill.blip.embed
                if rId and rId not in seen_rids:
                    blob = part.related_parts[rId].blob
                    if blob:
                        blobs.append(blob)
                        seen_rids.add(rId)
            except Exception:
                pass

        for p in doc.paragraphs:
            for r in p.runs:
                for d in getattr(r._element, "xpath", lambda *_: [])(".//a:blip"):
                    try:
                        rId = d.get(
                            "{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed"
                        )
                        if rId and rId not in seen_rids:
                            blob = part.related_parts[rId].blob
                            if blob:
                                blobs.append(blob)
                                seen_rids.add(rId)
                    except Exception:
                        pass

    except Exception:
        pass
    return blobs


def extract_docx(data: bytes) -> tuple[str, str]:
    from docx import Document

    S = SETTINGS.effective
    HEADING_MAX_LEVEL = int(S().get("docx_heading_max_level", 3))
    USE_MARKDOWN_HEADINGS = bool(S().get("docx_use_markdown_headings", True))
    PRESERVE_BULLETS = bool(S().get("docx_preserve_bullets", True))
    INCLUDE_TABLES = bool(S().get("docx_include_tables", True))
    INCLUDE_HEADERS_FOOTERS = bool(S().get("docx_include_headers_footers", False))
    MAX_PARA_CHARS = int(S().get("docx_para_max_chars", 0))
    DROP_EMPTY_LINES = bool(S().get("docx_drop_empty_lines", True))

    doc = Document(io.BytesIO(data))
    lines: list[str] = []

    try:
        title = (getattr(doc, "core_properties", None) or {}).title
        if title:
            lines.append(f"# {title}")
            lines.append("")
    except Exception:
        pass

    def _clip(s: str) -> str:
        if MAX_PARA_CHARS > 0 and len(s) > MAX_PARA_CHARS:
            return s[:MAX_PARA_CHARS] + "…"
        return s

    if INCLUDE_HEADERS_FOOTERS:
        try:
            for i, sec in enumerate(getattr(doc, "sections", []) or []):
                if i > 0:
                    break
                try:
                    hdr_ps = getattr(sec.header, "paragraphs", []) or []
                    hdr_text = "\n".join(
                        _squeeze_spaces(p.text) for p in hdr_ps if _squeeze_spaces(p.text)
                    )
                    if hdr_text:
                        lines.append("## Header")
                        lines.append(_clip(hdr_text))
                        lines.append("")
                except Exception:
                    pass
                try:
                    ftr_ps = getattr(sec.footer, "paragraphs", []) or []
                    ftr_text = "\n".join(
                        _squeeze_spaces(p.text) for p in ftr_ps if _squeeze_spaces(p.text)
                    )
                    if ftr_text:
                        lines.append("## Footer")
                        lines.append(_clip(ftr_text))
                        lines.append("")
                except Exception:
                    pass
        except Exception:
            pass

    for p in doc.paragraphs:
        txt = _extract_paragraph_text(p)
        if not txt and DROP_EMPTY_LINES:
            continue
        style_name = getattr(p.style, "name", "") or ""
        lvl = _is_heading(style_name)
        if lvl and lvl <= HEADING_MAX_LEVEL and USE_MARKDOWN_HEADINGS:
            prefix = "#" * max(1, min(6, lvl))
            lines.append(f"{prefix} {txt}".strip())
            continue
        if PRESERVE_BULLETS and _is_list_style(style_name):
            if txt:
                lines.append(f"- {_clip(txt)}")
            continue
        if txt:
            lines.append(_clip(txt))
        elif not DROP_EMPTY_LINES:
            lines.append("")

    if INCLUDE_TABLES and getattr(doc, "tables", None):
        for t_idx, tbl in enumerate(doc.tables):
            try:
                non_empty = any(
                    _squeeze_spaces(cell.text) for row in tbl.rows for cell in row.cells
                )
            except Exception:
                non_empty = True
            if not non_empty:
                continue
            lines.append("")
            lines.append(f"## Table {t_idx + 1}")
            try:
                for row in tbl.rows:
                    cells = [_squeeze_spaces(c.text) for c in row.cells]
                    if any(cells):
                        lines.append(" | ".join(c for c in cells if c))
            except Exception:
                pass

    if bool(S().get("docx_ocr_images", False)):
        min_bytes = int(S().get("ocr_min_image_bytes", 16384))
        seen_ocr_text = set()
        for blob in _docx_image_blobs(doc):
            if len(blob) >= min_bytes:
                t = (ocr_image_bytes(blob) or "").strip()
                if t:
                    key = t.lower()
                    if key not in seen_ocr_text:
                        lines.append(t)
                        seen_ocr_text.add(key)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====
from __future__ import annotations

import hashlib
import io
import re
from datetime import date, datetime, time

from ...core.logging import get_logger
from ...core.settings import SETTINGS
from .excel_ingest_core import (rightmost_nonempty_header,
                                scan_blocks_by_blank_rows, select_indices)

log = get_logger(__name__)

_WS_RE = re.compile(r"[ \t]+")


def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()


def extract_excel(data: bytes) -> tuple[str, str]:
    from openpyxl import load_workbook
    from openpyxl.utils import range_boundaries
    from openpyxl.utils.datetime import from_excel as _from_excel
    from openpyxl.worksheet.worksheet import Worksheet

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f'"{s}"'
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def _sheet_used_range(ws: Worksheet):
        if callable(getattr(ws, "calculate_dimension", None)):
            dim_ref = ws.calculate_dimension()
            try:
                min_c, min_r, max_c, max_r = range_boundaries(dim_ref)
                return min_c, min_r, max_c, max_r
            except Exception:
                pass
        return 1, 1, ws.max_column or 1, ws.max_row or 1

    def _cap_rows(min_r: int, max_r: int) -> int:
        return max_r if INFER_MAX_ROWS <= 0 else min(max_r, min_r + INFER_MAX_ROWS - 1)

    def _keep_and_rename_phantom(headers: list[str]) -> tuple[list[int], list[str]]:
        if (
            len(headers) >= 2
            and re.fullmatch(r"\d+_\d+$", (headers[1] or ""))
            and (headers[0] or "")
        ):
            keep_idx = [0, 1]
            new_headers = headers[:]
            new_headers[1] = "value"
            return keep_idx, new_headers
        keep_idx = [i for i, h in enumerate(headers) if h and not re.fullmatch(r"\d+_\d+$", h)]
        new_headers = [headers[i] for i in keep_idx]
        return keep_idx, new_headers

    wb_vals = load_workbook(io.BytesIO(data), data_only=True, read_only=True)

    def _coerce_excel_datetime(cell, v):
        try:
            if getattr(cell, "is_date", False) and isinstance(v, (int, float)):
                return _from_excel(v, wb_vals.epoch)
        except Exception:
            pass
        return v

    lines: list[str] = []
    ingest_id = hashlib.sha1(data).hexdigest()[:16]
    lines.append(f"# Ingest-ID: {ingest_id}")

    def _emit_inferred_table(ws: Worksheet, sheet_name: str, min_c, min_r, max_c, max_r):
        lines.append(f"# Sheet: {sheet_name}")
        max_c_eff = min(max_c, min_c + INFER_MAX_COLS - 1)
        max_r_eff = _cap_rows(min_r, max_r)
        headers: list[str] = []
        header_fill = 0
        for c in range(min_c, max_c_eff + 1):
            val = ws.cell(row=min_r, column=c).value
            s = fmt_val("" if val is None else str(val).strip())
            if s:
                header_fill += 1
            headers.append(s)
        fill_ratio = header_fill / max(1, (max_c_eff - min_c + 1))
        if fill_ratio < INFER_MIN_HEADER_FILL and (min_r + 1) <= max_r_eff:
            headers = []
            hdr_r = min_r + 1
            for c in range(min_c, max_c_eff + 1):
                val = ws.cell(row=hdr_r, column=c).value
                s = fmt_val("" if val is None else str(val).strip())
                headers.append(s)
            min_r = hdr_r
        norm_headers = [normalize_header(h) for h in headers]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[: rmax + 1]
            max_c_eff = min(max_c_eff, min_c + rmax)
        keep_idx, norm_headers = _keep_and_rename_phantom(norm_headers)
        lines.append("## Table: " + f"{sheet_name}!R{min_r}-{max_r_eff},C{min_c}-{max_c_eff}")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))
        lines.append("")
        for r in range(min_r + 1, max_r_eff + 1):
            raw_vals: list[str] = []
            for c in range(min_c, max_c_eff + 1):
                cell = ws.cell(row=r, column=c)
                vv = _coerce_excel_datetime(cell, cell.value)
                raw_vals.append(fmt_val(vv))
            row_vals = select_indices(raw_vals, keep_idx)
            while row_vals and (row_vals[-1] == "" or row_vals[-1] is None):
                row_vals.pop()
            if not any(v for v in row_vals):
                continue
            pairs: list[str] = []
            for h, v in zip(norm_headers, row_vals, strict=False):
                if h and v:
                    pairs.append(f"{h}={v}")
            lines.append(f"### Row {r} — {sheet_name}")
            lines.append(", ".join(pairs) if pairs else ", ".join(row_vals))
            lines.append("")

    for sheet_name in wb_vals.sheetnames:
        ws_v: Worksheet = wb_vals[sheet_name]
        min_c, min_r, max_c, max_r = _sheet_used_range(ws_v)
        max_c = min(max_c, min_c + INFER_MAX_COLS - 1)
        for b_min_c, b_min_r, b_max_c, b_max_r in scan_blocks_by_blank_rows(
            ws_v, min_c, min_r, max_c, max_r
        ):
            if b_min_r > b_max_r:
                continue
            _emit_inferred_table(ws_v, sheet_name, b_min_c, b_min_r, b_max_c, b_max_r)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====
from __future__ import annotations

import re

from ...core.logging import get_logger

log = get_logger(__name__)

_PHANTOM_RX = re.compile(r"^\d+_\d+$")


def row_blank(ws, r: int, min_c: int, max_c: int) -> bool:
    for c in range(min_c, max_c + 1):
        v = ws.cell(row=r, column=c).value
        if v not in (None, "") and not (isinstance(v, str) and not v.strip()):
            return False
    return True


def scan_blocks_by_blank_rows(ws, min_c: int, min_r: int, max_c: int, max_r: int):
    r = min_r
    while r <= max_r:
        while r <= max_r and row_blank(ws, r, min_c, max_c):
            r += 1
        if r > max_r:
            break
        start = r
        while r <= max_r and not row_blank(ws, r, min_c, max_c):
            r += 1
        end = r - 1
        yield (min_c, start, max_c, end)


def rightmost_nonempty_header(headers: list[str]) -> int:
    for i in range(len(headers) - 1, -1, -1):
        h = headers[i]
        if h and not h.isspace():
            return i
    return -1


def drop_bad_columns(headers: list[str]) -> list[int]:
    keep = []
    for i, h in enumerate(headers):
        hn = (h or "").strip().lower()
        if not hn:
            continue
        if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
            continue
        keep.append(i)
    return keep or list(range(len(headers)))


def select_indices(seq: list[str], idxs: list[int]) -> list[str]:
    out = []
    for i in idxs:
        out.append(seq[i] if i < len(seq) else "")
    return out

# ===== aimodel/file_read/rag/ingest/main.py =====

from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import json

from ...core.settings import SETTINGS
from .common import Chunk, _strip_html, _utf8, build_metas, chunk_text
from .csv_ingest import extract_csv
from .doc_binary_ingest import extract_doc_binary
from .docx_ingest import extract_docx
from .excel_ingest import extract_excel
from .pdf_ingest import extract_pdf
from .ppt_ingest import extract_ppt, extract_pptx
from .xls_ingest import extract_xls

__all__ = ["Chunk", "build_metas", "chunk_text", "sniff_and_extract"]


def _ing_dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            log.debug("[ingest]", *args)
    except Exception:
        pass


def sniff_and_extract(filename: str, data: bytes) -> tuple[str, str]:
    name = (filename or "").lower()
    _ing_dbg("route:", name, "bytes=", len(data))
    if name.endswith((".pptx", ".pptm")):
        _ing_dbg("-> pptx/pptm")
        return extract_pptx(data)
    if name.endswith(".ppt"):
        _ing_dbg("-> ppt (ole)")
        return extract_ppt(data)
    if name.endswith((".xlsx", ".xlsm")):
        _ing_dbg("-> excel")
        return extract_excel(data)
    if name.endswith(".xls"):
        _ing_dbg("-> excel (xls via xlrd)")
        return extract_xls(data)
    if name.endswith((".csv", ".tsv")):
        _ing_dbg("-> csv/tsv")
        return extract_csv(data)
    if name.endswith(".docx"):
        _ing_dbg("-> docx")
        try:
            return extract_docx(data)
        except Exception as e:
            _ing_dbg("docx err:", repr(e))
            return (_utf8(data), "text/plain")
    if name.endswith(".doc"):
        _ing_dbg("-> doc (binary/rtf)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("doc err:", repr(e))
            return (_utf8(data), "text/plain")
    if name.endswith(".rtf"):
        _ing_dbg("-> rtf (via doc_binary)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("rtf err:", repr(e))
            return (_utf8(data), "text/plain")
    if name.endswith(".pdf"):
        _ing_dbg("-> pdf (delegating to extract_pdf)")
        return extract_pdf(data)
    if name.endswith(".json"):
        _ing_dbg("-> json")
        try:
            return (json.dumps(json.loads(_utf8(data)), ensure_ascii=False, indent=2), "text/plain")
        except Exception as e:
            _ing_dbg("json err:", repr(e))
            return (_utf8(data), "text/plain")
    if name.endswith((".jsonl", ".jsonlines")):
        _ing_dbg("-> jsonl")
        lines = _utf8(data).splitlines()
        out = []
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            try:
                out.append(json.dumps(json.loads(ln), ensure_ascii=False, indent=2))
            except Exception:
                out.append(ln)
        return ("\n".join(out).strip(), "text/plain")
    if name.endswith((".yaml", ".yml")):
        _ing_dbg("-> yaml")
        try:
            import yaml

            obj = yaml.safe_load(_utf8(data))
            return (json.dumps(obj, ensure_ascii=False, indent=2), "text/plain")
        except Exception as e:
            _ing_dbg("yaml err:", repr(e))
            return (_utf8(data), "text/plain")
    if name.endswith(".toml"):
        _ing_dbg("-> toml")
        try:
            try:
                import tomllib

                obj = tomllib.loads(_utf8(data))
            except Exception:
                import toml

                obj = toml.loads(_utf8(data))
            return (json.dumps(obj, ensure_ascii=False, indent=2), "text/plain")
        except Exception as e:
            _ing_dbg("toml err:", repr(e))
            return (_utf8(data), "text/plain")
    if name.endswith((".htm", ".html", ".xml")):
        _ing_dbg("-> html/xml")
        return (_strip_html(_utf8(data)), "text/plain")
    if name.endswith(
        (
            ".txt",
            ".log",
            ".md",
            ".c",
            ".cpp",
            ".h",
            ".hpp",
            ".py",
            ".js",
            ".ts",
            ".jsx",
            ".tsx",
            ".sh",
            ".ps1",
            ".rs",
            ".java",
            ".go",
            ".rb",
            ".php",
            ".swift",
            ".kt",
            ".scala",
            ".lua",
            ".perl",
        )
    ):
        _ing_dbg("-> plaintext/code")
        return (_utf8(data), "text/plain")
    _ing_dbg("-> default fallback")
    return (_utf8(data), "text/plain")

# ===== aimodel/file_read/rag/ingest/ocr.py =====

from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import io
import re

import pypdfium2 as pdfium
import pytesseract
from PIL import Image

from ...core.settings import SETTINGS

_cmd = str(SETTINGS.effective().get("tesseract_cmd", "")).strip()
if _cmd:
    pytesseract.pytesseract.tesseract_cmd = _cmd
_ALNUM = re.compile("[A-Za-z0-9]")


def _alnum_ratio(s: str) -> float:
    if not s:
        return 0.0
    a = len(_ALNUM.findall(s))
    return a / max(1, len(s))


def is_bad_text(s: str) -> bool:
    S = SETTINGS.effective
    min_len = int(S().get("ocr_min_chars_for_ok", 32))
    min_ratio = float(S().get("ocr_min_alnum_ratio_for_ok", 0.15))
    s = (s or "").strip()
    return len(s) < min_len or _alnum_ratio(s) < min_ratio


def ocr_image_bytes(img_bytes: bytes) -> str:
    S = SETTINGS.effective
    lang = str(S().get("ocr_lang", "eng"))
    psm = str(S().get("ocr_psm", "3"))
    oem = str(S().get("ocr_oem", "3"))
    cfg = f"--oem {oem} --psm {psm}"
    with Image.open(io.BytesIO(img_bytes)) as im:
        im = im.convert("L")
        return pytesseract.image_to_string(im, lang=lang, config=cfg) or ""


def ocr_pdf(data: bytes) -> str:
    S = SETTINGS.effective
    dpi = int(S().get("pdf_ocr_dpi", 300))
    max_pages = int(S().get("pdf_ocr_max_pages", 0))
    lang = str(S().get("ocr_lang", "eng"))
    oem = str(S().get("ocr_oem", "3"))
    psm_default = str(S().get("ocr_psm", "6"))
    try_psm = [psm_default, "4", "7", "3"]
    min_side = 1200

    def _dbg(*args):
        try:
            if bool(S().get("ingest_debug", False)):
                log.debug("[ocr_pdf]", *args)
        except Exception:
            pass

    try:
        doc = pdfium.PdfDocument(io.BytesIO(data))
    except Exception as e:
        _dbg("PdfDocument ERROR:", repr(e))
        return ""
    n = len(doc)
    limit = n if max_pages <= 0 else min(n, max_pages)
    _dbg(f"pages={n}", f"limit={limit}", f"dpi={dpi}", f"lang={lang}", f"psm_default={psm_default}")
    out: list[str] = []
    for i in range(limit):
        try:
            page = doc[i]
            pil = page.render(scale=dpi / 72, rotation=0).to_pil().convert("L")
            base_w, base_h = (pil.width, pil.height)
            variants = []
            img1 = pil
            if min(base_w, base_h) < min_side:
                f = max(1.0, min_side / float(min(base_w, base_h)))
                img1 = pil.resize((int(base_w * f), int(base_h * f)))
            variants.append(("gray", img1))
            img2 = img1.point(lambda x: 255 if x > 180 else 0)
            variants.append(("bin180", img2))
            img3 = img1.point(lambda x: 255 - x)
            variants.append(("inv", img3))
            got = ""
            for tag, imgv in variants:
                for psm in try_psm:
                    cfg = f"--oem {oem} --psm {psm}"
                    txt = pytesseract.image_to_string(imgv, lang=lang, config=cfg) or ""
                    txt = txt.strip()
                    _dbg(
                        f"page={i + 1}/{limit}",
                        f"{tag} {imgv.width}x{imgv.height}",
                        f"psm={psm}",
                        f"len={len(txt)}",
                        f"prev={txt[:80]!r}",
                    )
                    if txt:
                        got = txt
                        break
                if got:
                    break
            if got:
                out.append(got)
        except Exception as e:
            _dbg(f"page={i + 1} ERROR:", repr(e))
    final = "\n\n".join(out).strip()
    _dbg("final_len=", len(final))
    return final

# ===== aimodel/file_read/rag/ingest/pdf_ingest.py =====

from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import io

from ...core.settings import SETTINGS
from .common import _utf8
from .ocr import is_bad_text, ocr_pdf


def _dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            log.debug("[pdf_ingest]", *args)
    except Exception:
        pass


def extract_pdf(data: bytes) -> tuple[str, str]:
    log.debug("[pdf_ingest] ENTER extract_pdf")
    S = SETTINGS.effective
    OCR_ENABLED = bool(S().get("pdf_ocr_enable", False))
    OCR_MODE = str(S().get("pdf_ocr_mode", "auto")).lower()
    WHEN_BAD = bool(S().get("pdf_ocr_when_bad", True))
    DPI = int(S().get("pdf_ocr_dpi", 300))
    MAX_PAGES = int(S().get("pdf_ocr_max_pages", 0))
    log.debug(
        f"[pdf_ingest] cfg ocr_enabled={OCR_ENABLED} mode={OCR_MODE} when_bad={WHEN_BAD} dpi={DPI} max_pages={MAX_PAGES}"
    )

    def _do_ocr() -> str:
        log.debug("[pdf_ingest] OCR_CALL begin")
        txt = (ocr_pdf(data) or "").strip()
        log.debug(f"[pdf_ingest] OCR_CALL end text_len={len(txt)} preview={txt[:120]!r}")
        return txt

    if OCR_ENABLED and OCR_MODE == "force":
        _dbg("mode=force -> OCR first")
        ocr_txt = _do_ocr()
        if ocr_txt:
            log.debug("[pdf_ingest] EXIT (force OCR success)")
            return (ocr_txt, "text/plain")
        _dbg("mode=force -> OCR empty, trying text extract")
    txt = ""
    try:
        from pdfminer.high_level import extract_text

        txt = (extract_text(io.BytesIO(data)) or "").strip()
        log.debug(f"[pdf_ingest] pdfminer text_len={len(txt)} preview={txt[:120]!r}")
    except Exception as e:
        log.error(f"[pdf_ingest] pdfminer ERROR {e!r}")
        txt = ""
    if OCR_ENABLED and OCR_MODE != "never":
        try_ocr = not txt or (WHEN_BAD and is_bad_text(txt))
        log.debug(
            f"[pdf_ingest] auto-eval try_ocr={try_ocr} has_text={bool(txt)} is_bad={(is_bad_text(txt) if txt else 'n/a')}"
        )
        if try_ocr:
            ocr_txt = _do_ocr()
            if ocr_txt:
                log.debug("[pdf_ingest] EXIT (auto OCR success)")
                return (ocr_txt, "text/plain")
    if not txt:
        try:
            from PyPDF2 import PdfReader

            r = PdfReader(io.BytesIO(data))
            pages = [(p.extract_text() or "").strip() for p in r.pages]
            txt = "\n\n".join([p for p in pages if p]).strip()
            log.debug(f"[pdf_ingest] pypdf2 text_len={len(txt)} preview={(txt or '')[:120]!r}")
        except Exception as e2:
            log.error(f"[pdf_ingest] pypdf2 ERROR {e2!r}")
            txt = _utf8(data)
            log.debug(f"[pdf_ingest] bytes-fallback text_len={len(txt)}")
    final = txt.strip() if txt else ""
    log.debug(f"[pdf_ingest] EXIT (returned_len={len(final)})")
    return (final, "text/plain")

# ===== aimodel/file_read/rag/ingest/ppt_ingest.py =====

from __future__ import annotations

from ...core.logging import get_logger

log = get_logger(__name__)
import io
import re

from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile("[ \\t]+")


def _squeeze(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()


def _clip(s: str, limit: int) -> str:
    if limit > 0 and len(s) > limit:
        return s[:limit] + "…"
    return s


def _shape_text(shape) -> list[str]:
    out: list[str] = []
    if getattr(shape, "has_text_frame", False):
        for p in shape.text_frame.paragraphs:
            txt = _squeeze("".join(r.text for r in p.runs))
            if txt:
                out.append(txt)
    if getattr(shape, "has_table", False):
        tbl = shape.table
        for r in tbl.rows:
            cells = [_squeeze(c.text) for c in r.cells]
            if any(cells):
                out.append(" | ".join(c for c in cells if c))
    if getattr(shape, "shape_type", None) and str(shape.shape_type) == "GROUP":
        try:
            for sh in getattr(shape, "shapes", []):
                out.extend(_shape_text(sh))
        except Exception:
            pass
    try:
        S = SETTINGS.effective
        if bool(S().get("pptx_ocr_images", False)):
            is_pic = getattr(shape, "shape_type", None)
            if is_pic and "PICTURE" in str(is_pic):
                img = getattr(shape, "image", None)
                blob = getattr(img, "blob", None) if img is not None else None
                if blob and len(blob) >= int(S().get("ocr_min_image_bytes", 16384)):
                    log.info("[OCR] candidate image size:", len(blob))
                    t = (ocr_image_bytes(blob) or "").strip()
                    log.info("[OCR] result:", repr(t[:200]))
                    if t:
                        out.append(t)
    except Exception as e:
        log.error("[OCR] error:", e)
    return out


def extract_pptx(data: bytes) -> tuple[str, str]:
    from pptx import Presentation

    S = SETTINGS.effective
    USE_MD = bool(S().get("pptx_use_markdown_headings", True))
    INCLUDE_NOTES = bool(S().get("pptx_include_notes", True))
    INCLUDE_TABLES = bool(S().get("pptx_include_tables", True))
    DROP_EMPTY = bool(S().get("pptx_drop_empty_lines", True))
    MAX_PARA = int(S().get("pptx_para_max_chars", 0))
    NUMBER_SLIDES = bool(S().get("pptx_number_slides", True))
    prs = Presentation(io.BytesIO(data))
    lines: list[str] = []
    for i, slide in enumerate(prs.slides, start=1):
        title = ""
        try:
            if getattr(slide, "shapes", None):
                for sh in slide.shapes:
                    if getattr(sh, "is_placeholder", False) and str(
                        getattr(sh, "placeholder_format", "").type
                    ).lower().endswith("title"):
                        title = _squeeze(getattr(sh, "text", "") or "")
                        break
        except Exception:
            pass
        head = f"Slide {i}" + (f": {title}" if title else "")
        if USE_MD:
            lines.append(("## " if NUMBER_SLIDES else "## ") + head)
        else:
            lines.append(head)
        body: list[str] = []
        for sh in getattr(slide, "shapes", []):
            if getattr(sh, "has_table", False) and (not INCLUDE_TABLES):
                continue
            body.extend(_shape_text(sh))
        for t in body:
            t = _clip(t, MAX_PARA)
            if t or not DROP_EMPTY:
                lines.append(t)
        if INCLUDE_NOTES:
            try:
                notes = slide.notes_slide
                if notes and getattr(notes, "notes_text_frame", None):
                    note_txt = _squeeze(notes.notes_text_frame.text)
                    if note_txt:
                        lines.append("")
                        lines.append("### Notes")
                        for ln in note_txt.splitlines():
                            ln = _squeeze(ln)
                            if ln or not DROP_EMPTY:
                                lines.append(_clip(ln, MAX_PARA))
            except Exception:
                pass
        lines.append("")
    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else "", "text/plain")


def extract_ppt(data: bytes) -> tuple[str, str]:
    from .doc_binary_ingest import _generic_ole_text

    S = SETTINGS.effective
    DROP_EMPTY = bool(S().get("ppt_drop_empty_lines", True))
    DEDUPE = bool(S().get("ppt_dedupe_lines", True))
    MAX_PARA = int(S().get("ppt_max_line_chars", 600))
    MIN_ALPHA = float(S().get("ppt_min_alpha_ratio", 0.4))
    MAX_PUNCT = float(S().get("ppt_max_punct_ratio", 0.5))
    TOKEN_MAX = int(S().get("ppt_token_max_chars", 40))
    raw = _generic_ole_text(data)
    if not raw:
        try:
            raw = data.decode("utf-8", errors="ignore")
        except Exception:
            raw = ""
    out: list[str] = []
    seen = set()
    for ln in raw.splitlines() if raw else []:
        s = _squeeze(ln)
        if not s and DROP_EMPTY:
            continue
        if MAX_PARA > 0 and len(s) > MAX_PARA:
            s = s[:MAX_PARA] + "…"
        if s:
            letters = sum(1 for c in s if c.isalpha())
            alen = max(1, len(s))
            if letters / alen < MIN_ALPHA:
                continue
            punct = sum(1 for c in s if not c.isalnum() and (not c.isspace()))
            if punct / alen > MAX_PUNCT:
                continue
            if " " not in s and len(s) <= TOKEN_MAX and re.fullmatch("[\\w.\\-]+", s):
                continue
        if DEDUPE:
            if s in seen:
                continue
            seen.add(s)
        if s or not DROP_EMPTY:
            out.append(s)
    text = "\n".join(out).strip()
    return (text + "\n" if text else "", "text/plain")

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====
from __future__ import annotations

import re
from datetime import date, datetime, time

from ...core.logging import get_logger
from ...core.settings import SETTINGS

log = get_logger(__name__)

_WS_RE = re.compile(r"[ \t]+")


def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()


def extract_xls(data: bytes) -> tuple[str, str]:
    try:
        import xlrd
    except Exception:
        return (data.decode("utf-8", errors="replace"), "text/plain")

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    EMIT_KEYVALUES = bool(S().get("excel_emit_key_values"))
    EMIT_CELL_ADDR = bool(S().get("excel_emit_cell_addresses"))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and isinstance(dt, datetime) and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f'"{s}"'
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    try:
        book = xlrd.open_workbook(file_contents=data)
    except Exception:
        return (data.decode("utf-8", errors="replace"), "text/plain")

    datemode = book.datemode

    def xlrd_cell_to_py(cell):
        ctype, value = cell.ctype, cell.value
        if ctype == xlrd.XL_CELL_DATE:
            try:
                return xlrd.xldate_as_datetime(value, datemode)
            except Exception:
                return value
        if ctype == xlrd.XL_CELL_NUMBER:
            return float(value)
        if ctype == xlrd.XL_CELL_BOOLEAN:
            return bool(value)
        return value

    lines: list[str] = []

    for sheet in book.sheets():
        nrows = min(sheet.nrows or 0, INFER_MAX_ROWS)
        ncols = min(sheet.ncols or 0, INFER_MAX_COLS)
        if nrows == 0 or ncols == 0:
            continue

        headers_raw = []
        header_fill = 0
        for c in range(ncols):
            v = xlrd_cell_to_py(sheet.cell(0, c))
            s = "" if v is None else str(v).strip()
            if s:
                header_fill += 1
            headers_raw.append(s)

        fill_ratio = header_fill / max(1, ncols)
        start_row = 1
        if fill_ratio < INFER_MIN_HEADER_FILL and nrows >= 2:
            headers_raw = []
            for c in range(ncols):
                v = xlrd_cell_to_py(sheet.cell(1, c))
                s = "" if v is None else str(v).strip()
                headers_raw.append(s)
            start_row = 2

        norm_headers = [normalize_header(h) for h in headers_raw]

        lines.append(f"# Sheet: {sheet.name}")

        if EMIT_KEYVALUES and ncols == 2:
            textish = valueish = rows = 0
            for r in range(start_row, nrows):
                a = xlrd_cell_to_py(sheet.cell(r, 0))
                b = xlrd_cell_to_py(sheet.cell(r, 1))
                if a is None and b is None:
                    continue
                rows += 1
                if isinstance(a, str):
                    textish += 1
                if isinstance(b, (int, float, datetime, date, time)):
                    valueish += 1
            if rows >= 3 and textish / max(1, rows) >= 0.6 and valueish / max(1, rows) >= 0.6:
                lines.append("## Key/Values")
                for r in range(start_row, nrows):
                    k = fmt_val(xlrd_cell_to_py(sheet.cell(r, 0)))
                    v = fmt_val(xlrd_cell_to_py(sheet.cell(r, 1)))
                    if not k and not v:
                        continue
                    lines.append(f"- {k}: {v}" if k else f"- : {v}")
                lines.append("")
                continue

        lines.append("## Inferred Table")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))

        for r in range(start_row, nrows):
            row_vals: list[str] = []
            for c in range(ncols):
                val = fmt_val(xlrd_cell_to_py(sheet.cell(r, c)))
                if val:
                    row_vals.append(val if not EMIT_CELL_ADDR else f"{val}")
            if row_vals:
                lines.append("row: " + ", ".join(row_vals))
        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/rerank.py =====

from __future__ import annotations

import logging

log = logging.getLogger(__name__)

from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)
_RERANKER = None
_RERANKER_NAME = None


def _load_reranker():
    global _RERANKER, _RERANKER_NAME
    model_name = SETTINGS.get("rag_rerank_model")
    if not model_name:
        return None
    if _RERANKER is not None and _RERANKER_NAME == model_name:
        return _RERANKER
    try:
        from sentence_transformers import CrossEncoder

        _RERANKER = CrossEncoder(model_name)
        _RERANKER_NAME = model_name
        return _RERANKER
    except Exception as e:
        log.error(f"[RAG RERANK] failed to load reranker {model_name}: {e}")
        _RERANKER = None
        _RERANKER_NAME = None
        return None


def rerank_hits(query: str, hits: list[dict], *, top_m: int | None = None) -> list[dict]:
    if not hits:
        return hits
    model = _load_reranker()
    if model is None:
        return hits
    pairs = [(query, h.get("text") or "") for h in hits]
    try:
        scores = model.predict(pairs)
    except Exception as e:
        log.error(f"[RAG RERANK] predict failed: {e}")
        return hits
    out: list[dict] = []
    for h, s in zip(hits, scores, strict=False):
        hh = dict(h)
        hh["rerankScore"] = float(s)
        out.append(hh)
    out.sort(key=lambda x: x.get("rerankScore", 0.0), reverse=True)
    if isinstance(top_m, int) and top_m > 0:
        out = out[:top_m]
    return out


def cap_per_source(hits: list[dict], per_source_cap: int) -> list[dict]:
    if per_source_cap is None or per_source_cap <= 0:
        return hits
    bucket: dict[str, int] = {}
    out: list[dict] = []
    for h in hits:
        src = str(h.get("source") or "")
        seen = bucket.get(src, 0)
        if seen < per_source_cap:
            out.append(h)
            bucket[src] = seen + 1
    return out


def min_score_fraction(hits: list[dict], key: str, frac: float) -> list[dict]:
    if not hits:
        return hits
    vals = []
    for h in hits:
        try:
            v = float(h.get(key) or 0.0)
        except Exception:
            v = 0.0
        vals.append(v)
    s_min = min(vals)
    s_max = max(vals)
    if s_max == s_min:
        return hits
    kept = []
    for h, v in zip(hits, vals, strict=False):
        norm = (v - s_min) / (s_max - s_min)
        if norm >= float(frac):
            kept.append(h)
    return kept

# ===== aimodel/file_read/rag/retrieve_common.py =====

from __future__ import annotations

import logging

log = logging.getLogger(__name__)
from dataclasses import dataclass
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from .retrieve_tabular import make_rag_block_tabular

log = get_logger(__name__)

_EMBEDDER = None
_EMBEDDER_NAME = None
PRINT_MAX = 10


def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        log.info(f"[RAG] sentence_transformers unavailable: {e}")
        return (None, None)
    model_name = SETTINGS.get("rag_embedding_model")
    if not model_name:
        log.info("[RAG] no rag_embedding_model configured")
        return (None, None)
    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            log.error(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return (_EMBEDDER, _EMBEDDER_NAME)


def _embed_query(q: str) -> list[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        log.error(f"[RAG] embedding encode failed: {e}")
        return []


def _primary_score(h: dict[str, Any]) -> float:
    s = h.get("rerankScore")
    if s is not None:
        try:
            return float(s)
        except Exception:
            pass
    try:
        return float(h.get("score") or 0.0)
    except Exception:
        return 0.0


def _dedupe_and_sort(hits: list[dict], *, k: int) -> list[dict]:
    hits_sorted = sorted(hits, key=_primary_score, reverse=True)
    seen: set[tuple[str, str]] = set()
    out: list[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out


def _default_header(h: dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"


def _render_header(h: dict[str, Any]) -> str:
    return _default_header(h)


def make_rag_block_generic(hits: list[dict], *, max_chars: int) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"
    total_budget = int(SETTINGS.get("rag_total_char_budget"))
    lines = [preamble]
    used = len(lines[0]) + 1
    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()
        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost
        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost
    return "\n".join(lines)


def build_block_for_hits(hits_top: list[dict], preferred_sources: list[str] | None = None) -> str:
    block = make_rag_block_tabular(hits_top, preferred_sources=preferred_sources)
    if block is None:
        block = make_rag_block_generic(
            hits_top, max_chars=int(SETTINGS.get("rag_max_chars_per_chunk"))
        )
    return block or ""


def _rescore_for_preferred_sources(
    hits: list[dict], preferred_sources: list[str] | None = None
) -> list[dict]:
    if not preferred_sources:
        return hits
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))
    pref = set(s.strip().lower() for s in preferred_sources if s)
    out = []
    for h in hits:
        sc = float(h.get("score") or 0.0)
        src = str(h.get("source") or "").strip().lower()
        if src in pref:
            sc *= 1.0 + boost
        hh = dict(h)
        hh["score"] = sc
        out.append(hh)
    return out


def _fmt_hit(h: dict[str, Any]) -> str:
    return f"id={h.get('id')!s} src={h.get('source')!s} chunk={h.get('chunkIndex')!s} row={h.get('row')!s} score={h.get('score')!s} rerank={h.get('rerankScore')!s}"


def _print_hits(label: str, hits: list[dict[str, Any]], limit: int = PRINT_MAX) -> None:
    log.debug(f"[RAG DEBUG] {label}: count={len(hits)}")
    for i, h in enumerate(hits[:limit]):
        log.debug(f"[RAG DEBUG]   {i + 1:02d}: {_fmt_hit(h)}")
    if len(hits) > limit:
        log.debug(f"[RAG DEBUG]   … (+{len(hits) - limit} more)")


def _nohit_block(q: str) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "Local knowledge")
    preamble = preamble if preamble.endswith(":") else preamble + ":"
    msg = str(
        SETTINGS.get("rag_nohit_message")
        or "⛔ No relevant local entries found for this query. Do not guess."
    )
    if q:
        return f"{preamble}\n- {msg}\n- query={q!r}"
    return f"{preamble}\n- {msg}"


@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    rerankSec: float = 0.0
    usedReranker: bool = False
    keptAfterRerank: int = 0
    capPerSource: int = 0
    keptAfterCap: int = 0
    minScoreFrac: float = 0.0
    keptAfterMinFrac: int = 0
    fallbackUsed: bool = False
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"

# ===== aimodel/file_read/rag/retrieve_core.py =====

from __future__ import annotations

import logging

log = logging.getLogger(__name__)
import time
from dataclasses import asdict, dataclass
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from .rerank import cap_per_source, min_score_fraction, rerank_hits
from .retrieve_tabular import make_rag_block_tabular
from .store import search_vectors

log = get_logger(__name__)

_EMBEDDER = None
_EMBEDDER_NAME = None
PRINT_MAX = 10


def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        log.info(f"[RAG] sentence_transformers unavailable: {e}")
        return (None, None)
    model_name = SETTINGS.get("rag_embedding_model")
    if not model_name:
        log.info("[RAG] no rag_embedding_model configured")
        return (None, None)
    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            log.error(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return (_EMBEDDER, _EMBEDDER_NAME)


def _embed_query(q: str) -> list[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        log.error(f"[RAG] embedding encode failed: {e}")
        return []


def _primary_score(h: dict[str, Any]) -> float:
    s = h.get("rerankScore")
    if s is not None:
        try:
            return float(s)
        except Exception:
            pass
    try:
        return float(h.get("score") or 0.0)
    except Exception:
        return 0.0


def _dedupe_and_sort(hits: list[dict], *, k: int) -> list[dict]:
    hits_sorted = sorted(hits, key=_primary_score, reverse=True)
    seen: set[tuple[str, str]] = set()
    out: list[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out


def _default_header(h: dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"


def _render_header(h: dict[str, Any]) -> str:
    return _default_header(h)


def make_rag_block_generic(hits: list[dict], *, max_chars: int) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"
    total_budget = int(SETTINGS.get("rag_total_char_budget"))
    lines = [preamble]
    used = len(lines[0]) + 1
    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()
        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost
        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost
    return "\n".join(lines)


@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    rerankSec: float = 0.0
    usedReranker: bool = False
    keptAfterRerank: int = 0
    capPerSource: int = 0
    keptAfterCap: int = 0
    minScoreFrac: float = 0.0
    keptAfterMinFrac: int = 0
    fallbackUsed: bool = False
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"


def _rescore_for_preferred_sources(
    hits: list[dict], preferred_sources: list[str] | None = None
) -> list[dict]:
    if not preferred_sources:
        return hits
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))
    pref = set(s.strip().lower() for s in preferred_sources if s)
    out = []
    for h in hits:
        sc = float(h.get("score") or 0.0)
        src = str(h.get("source") or "").strip().lower()
        if src in pref:
            sc *= 1.0 + boost
        hh = dict(h)
        hh["score"] = sc
        out.append(hh)
    return out


def build_block_for_hits(hits_top: list[dict], preferred_sources: list[str] | None = None) -> str:
    block = make_rag_block_tabular(hits_top, preferred_sources=preferred_sources)
    if block is None:
        block = make_rag_block_generic(
            hits_top, max_chars=int(SETTINGS.get("rag_max_chars_per_chunk"))
        )
    return block or ""


def _fmt_hit(h: dict[str, Any]) -> str:
    return f"id={h.get('id')!s} src={h.get('source')!s} chunk={h.get('chunkIndex')!s} row={h.get('row')!s} score={h.get('score')!s} rerank={h.get('rerankScore')!s}"


def _print_hits(label: str, hits: list[dict[str, Any]], limit: int = PRINT_MAX) -> None:
    log.debug(f"[RAG DEBUG] {label}: count={len(hits)}")
    for i, h in enumerate(hits[:limit]):
        log.debug(f"[RAG DEBUG]   {i + 1:02d}: {_fmt_hit(h)}")
    if len(hits) > limit:
        log.debug(f"[RAG DEBUG]   … (+{len(hits) - limit} more)")


def _build_rag_block_core(
    query: str,
    *,
    session_id: str | None,
    k: int,
    session_only: bool,
    preferred_sources: list[str] | None = None,
) -> tuple[str | None, RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode="session-only" if session_only else "global")
    q = (query or "").strip()
    log.debug(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")
    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)
    if not qvec:
        log.debug("[RAG SEARCH] no qvec")
        return (None, tel)
    d = len(qvec)
    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)
    hits_glob: list[dict] = []
    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)
    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)
    _print_hits("ANN hits (chat)", hits_chat)
    if not session_only:
        _print_hits("ANN hits (global)", hits_glob)
    all_hits = hits_chat + ([] if session_only else hits_glob)
    if not all_hits:
        log.debug("[RAG SEARCH] no hits")
        return (None, tel)
    all_hits = _rescore_for_preferred_sources(all_hits, preferred_sources=preferred_sources)
    _print_hits("After preference boost", all_hits)
    pre_prune_hits = list(all_hits)
    t_rr = time.perf_counter()
    top_m_cfg = SETTINGS.get("rag_rerank_top_m")
    try:
        top_m = int(top_m_cfg) if top_m_cfg not in (None, "", False) else None
    except Exception:
        top_m = None
    if isinstance(top_m, int):
        top_m = max(1, min(top_m, len(all_hits)))
    try:
        all_hits = rerank_hits(q, all_hits, top_m=top_m)
    except Exception as e:
        log.error(f"[RAG] rerank error: {e}; skipping rerank")
        pass
    tel.rerankSec = round(time.perf_counter() - t_rr, 6)
    tel.usedReranker = any("rerankScore" in h for h in all_hits)
    tel.keptAfterRerank = len(all_hits)
    _print_hits(f"After rerank (top_m={top_m})", all_hits)
    frac_cfg = SETTINGS.get("rag_min_score_frac")
    try:
        frac = float(frac_cfg) if frac_cfg not in (None, "", False) else None
    except Exception:
        frac = None
    tel.minScoreFrac = float(frac) if isinstance(frac, (int, float)) else 0.0
    if isinstance(frac, (int, float)):
        key = "rerankScore" if any("rerankScore" in h for h in all_hits) else "score"
        before = list(all_hits)
        all_hits = min_score_fraction(all_hits, key, float(frac))
        _print_hits(f"After minScoreFrac={frac} on key={key}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by minScoreFrac", dropped)
    tel.keptAfterMinFrac = len(all_hits)
    cap_cfg = SETTINGS.get("rag_per_source_cap")
    try:
        cap_val = int(cap_cfg) if cap_cfg not in (None, "", False) else 0
    except Exception:
        cap_val = 0
    tel.capPerSource = cap_val
    if cap_val and cap_val > 0:
        before = list(all_hits)
        all_hits = cap_per_source(all_hits, cap_val)
        _print_hits(f"After per-source cap={cap_val}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by per-source cap", dropped)
    tel.keptAfterCap = len(all_hits)
    if not all_hits:
        best = sorted(pre_prune_hits, key=_primary_score, reverse=True)[:1]
        all_hits = best
        tel.fallbackUsed = True
        log.info("[RAG] pruning yielded 0 hits; falling back to best pre-prune hit")
        _print_hits("Fallback best pre-prune", all_hits)
    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)
    _print_hits(f"Final top-k (k={k})", hits_top)
    t4 = time.perf_counter()
    block = build_block_for_hits(hits_top, preferred_sources=preferred_sources)
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    preview = (block or "")[:400].replace("\n", "\\n")
    log.debug(f'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
    log.debug(
        f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}"
    )
    return (block, tel)


def build_rag_block(
    query: str, session_id: str | None = None, *, preferred_sources: list[str] | None = None
) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    k = int(SETTINGS.get("rag_top_k"))
    block, _ = _build_rag_block_core(
        query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources
    )
    return block


def build_rag_block_with_telemetry(
    query: str, session_id: str | None = None, *, preferred_sources: list[str] | None = None
) -> tuple[str | None, dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return (None, {})
    k = int(SETTINGS.get("rag_top_k"))
    block, tel = _build_rag_block_core(
        query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources
    )
    return (block, asdict(tel))


def build_rag_block_session_only(
    query: str,
    session_id: str | None,
    *,
    k: int | None = None,
    preferred_sources: list[str] | None = None,
) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, _ = _build_rag_block_core(
        query,
        session_id=session_id,
        k=int(k),
        session_only=True,
        preferred_sources=preferred_sources,
    )
    return block


def build_rag_block_session_only_with_telemetry(
    query: str,
    session_id: str | None,
    *,
    k: int | None = None,
    preferred_sources: list[str] | None = None,
) -> tuple[str | None, dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return (None, {})
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, tel = _build_rag_block_core(
        query,
        session_id=session_id,
        k=int(k),
        session_only=True,
        preferred_sources=preferred_sources,
    )
    return (block, asdict(tel))

# ===== aimodel/file_read/rag/retrieve_pipeline.py =====

from __future__ import annotations

import logging

log = logging.getLogger(__name__)
import time
from dataclasses import asdict
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from .rerank import cap_per_source, min_score_fraction, rerank_hits
from .retrieve_common import (RagTelemetry, _dedupe_and_sort, _embed_query,
                              _nohit_block, _primary_score, _print_hits,
                              _rescore_for_preferred_sources,
                              build_block_for_hits)
from .store import search_vectors

log = get_logger(__name__)


def _mk_preview(text: str | None, limit: int = 400) -> str:
    t = (text or "")[:limit]
    return t.replace("\n", "\\n")


def _build_rag_block_core(
    query: str,
    *,
    session_id: str | None,
    k: int,
    session_only: bool,
    preferred_sources: list[str] | None = None,
) -> tuple[str | None, RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode="session-only" if session_only else "global")
    q = (query or "").strip()
    log.debug(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")
    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)
    if not qvec:
        log.debug("[RAG SEARCH] no qvec")
        return (None, tel)
    d = len(qvec)
    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)
    hits_glob: list[dict] = []
    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)
    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)
    _print_hits("ANN hits (chat)", hits_chat)
    if not session_only:
        _print_hits("ANN hits (global)", hits_glob)
    all_hits = hits_chat + ([] if session_only else hits_glob)
    if not all_hits:
        log.debug("[RAG SEARCH] no hits")
        return (None, tel)
    all_hits = _rescore_for_preferred_sources(all_hits, preferred_sources=preferred_sources)
    _print_hits("After preference boost", all_hits)
    pre_prune_hits = list(all_hits)
    t_rr = time.perf_counter()
    top_m_cfg = SETTINGS.get("rag_rerank_top_m")
    try:
        top_m = int(top_m_cfg) if top_m_cfg not in (None, "", False) else None
    except Exception:
        top_m = None
    if isinstance(top_m, int):
        top_m = max(1, min(top_m, len(all_hits)))
    try:
        all_hits = rerank_hits(q, all_hits, top_m=top_m)
    except Exception as e:
        log.error(f"[RAG] rerank error: {e}; skipping rerank")
        pass
    tel.rerankSec = round(time.perf_counter() - t_rr, 6)
    tel.usedReranker = any("rerankScore" in h for h in all_hits)
    tel.keptAfterRerank = len(all_hits)
    _print_hits(f"After rerank (top_m={top_m})", all_hits)
    min_abs = SETTINGS.get("rag_min_abs_rerank")
    try:
        min_abs = float(min_abs) if min_abs not in (None, "", False) else None
    except Exception:
        min_abs = None
    if isinstance(min_abs, float):
        before = list(all_hits)
        all_hits = [
            h for h in all_hits if float(h.get("rerankScore") or -1000000000000.0) >= min_abs
        ]
        if len(all_hits) != len(before):
            _print_hits(f"After abs rerank cutoff >= {min_abs}", all_hits)
    frac_cfg = SETTINGS.get("rag_min_score_frac")
    try:
        frac = float(frac_cfg) if frac_cfg not in (None, "", False) else None
    except Exception:
        frac = None
    tel.minScoreFrac = float(frac) if isinstance(frac, (int, float)) else 0.0
    if isinstance(frac, (int, float)):
        key = "rerankScore" if any("rerankScore" in h for h in all_hits) else "score"
        before = list(all_hits)
        all_hits = min_score_fraction(all_hits, key, float(frac))
        _print_hits(f"After minScoreFrac={frac} on key={key}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by minScoreFrac", dropped)
    tel.keptAfterMinFrac = len(all_hits)
    cap_cfg = SETTINGS.get("rag_per_source_cap")
    try:
        cap_val = int(cap_cfg) if cap_cfg not in (None, "", False) else 0
    except Exception:
        cap_val = 0
    tel.capPerSource = cap_val
    if cap_val and cap_val > 0:
        before = list(all_hits)
        all_hits = cap_per_source(all_hits, cap_val)
        _print_hits(f"After per-source cap={cap_val}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by per-source cap", dropped)
    tel.keptAfterCap = len(all_hits)
    if not all_hits:
        if pre_prune_hits:
            all_hits = sorted(pre_prune_hits, key=_primary_score, reverse=True)[:1]
            tel.fallbackUsed = True
            log.info("[RAG] pruning yielded 0; keeping best pre-prune (low-confidence fallback)")
            _print_hits("Fallback best pre-prune", all_hits)
        else:
            tel.fallbackUsed = False
            log.info("[RAG] no ANN hits; returning no-hit block")
            block = _nohit_block(q)
            tel.blockChars = len(block or "")
            preview = _mk_preview(block)
            log.debug(f