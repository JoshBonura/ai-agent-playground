s:
            ln = ln.strip()
            if not ln:
                continue
            try:
                out.append(json.dumps(json.loads(ln), ensure_ascii=False, indent=2))
            except Exception:
                out.append(ln)
        return "\n".join(out).strip(), "text/plain"

    if name.endswith((".yaml", ".yml")):
        _ing_dbg("-> yaml")
        try:
            import yaml
            obj = yaml.safe_load(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("yaml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".toml"):
        _ing_dbg("-> toml")
        try:
            try:
                import tomllib
                obj = tomllib.loads(_utf8(data))
            except Exception:
                import toml
                obj = toml.loads(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("toml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".htm", ".html", ".xml")):
        _ing_dbg("-> html/xml")
        return _strip_html(_utf8(data)), "text/plain"

    if name.endswith((
        ".txt", ".log", ".md",
        ".c", ".cpp", ".h", ".hpp",
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".sh", ".ps1",
        ".rs", ".java", ".go", ".rb", ".php",
        ".swift", ".kt", ".scala", ".lua", ".perl",
    )):
        _ing_dbg("-> plaintext/code")
        return _utf8(data), "text/plain"

    _ing_dbg("-> default fallback")
    return _utf8(data), "text/plain"

# ===== aimodel/file_read/rag/ingest/ocr.py =====

# aimodel/file_read/rag/ingest/ocr.py
from __future__ import annotations
from typing import List
import io, re
import pytesseract
from PIL import Image
import pypdfium2 as pdfium
from ...core.settings import SETTINGS

_cmd = str(SETTINGS.effective().get("tesseract_cmd", "")).strip()
if _cmd:
    pytesseract.pytesseract.tesseract_cmd = _cmd

_ALNUM = re.compile(r"[A-Za-z0-9]")

def _alnum_ratio(s: str) -> float:
    if not s:
        return 0.0
    a = len(_ALNUM.findall(s))
    return a / max(1, len(s))

def is_bad_text(s: str) -> bool:
    S = SETTINGS.effective
    min_len = int(S().get("ocr_min_chars_for_ok", 32))
    min_ratio = float(S().get("ocr_min_alnum_ratio_for_ok", 0.15))
    s = (s or "").strip()
    return (len(s) < min_len) or (_alnum_ratio(s) < min_ratio)

def ocr_image_bytes(img_bytes: bytes) -> str:
    S = SETTINGS.effective
    lang = str(S().get("ocr_lang", "eng"))
    psm = str(S().get("ocr_psm", "3"))
    oem = str(S().get("ocr_oem", "3"))
    cfg = f"--oem {oem} --psm {psm}"
    with Image.open(io.BytesIO(img_bytes)) as im:
        im = im.convert("L")
        return pytesseract.image_to_string(im, lang=lang, config=cfg) or ""

def ocr_pdf(data: bytes) -> str:
    S = SETTINGS.effective
    dpi = int(S().get("pdf_ocr_dpi", 300))
    max_pages = int(S().get("pdf_ocr_max_pages", 0))
    lang = str(S().get("ocr_lang", "eng"))
    oem = str(S().get("ocr_oem", "3"))
    psm_default = str(S().get("ocr_psm", "6"))
    try_psm = [psm_default, "4", "7", "3"]  
    min_side = 1200  

    def _dbg(*args):
        try:
            if bool(S().get("ingest_debug", False)):
                print("[ocr_pdf]", *args, flush=True)
        except Exception:
            pass

    try:
        doc = pdfium.PdfDocument(io.BytesIO(data))
    except Exception as e:
        _dbg("PdfDocument ERROR:", repr(e))
        return ""

    n = len(doc)
    limit = n if max_pages <= 0 else min(n, max_pages)
    _dbg(f"pages={n}", f"limit={limit}", f"dpi={dpi}", f"lang={lang}", f"psm_default={psm_default}")

    out: List[str] = []
    for i in range(limit):
        try:
            page = doc[i]
            pil = page.render(scale=dpi/72, rotation=0).to_pil().convert("L")
            base_w, base_h = pil.width, pil.height

            variants = []

            img1 = pil
            if min(base_w, base_h) < min_side:
                f = max(1.0, min_side / float(min(base_w, base_h)))
                img1 = pil.resize((int(base_w * f), int(base_h * f)))
            variants.append(("gray", img1))

            img2 = img1.point(lambda x: 255 if x > 180 else 0)
            variants.append(("bin180", img2))

            img3 = img1.point(lambda x: 255 - x)
            variants.append(("inv", img3))

            got = ""
            for tag, imgv in variants:
                for psm in try_psm:
                    cfg = f"--oem {oem} --psm {psm}"   
                    txt = pytesseract.image_to_string(imgv, lang=lang, config=cfg) or ""
                    txt = txt.strip()
                    _dbg(f"page={i+1}/{limit}", f"{tag} {imgv.width}x{imgv.height}", f"psm={psm}", f"len={len(txt)}", f"prev={repr(txt[:80])}")
                    if txt:
                        got = txt
                        break
                if got:
                    break

            if got:
                out.append(got)
        except Exception as e:
            _dbg(f"page={i+1} ERROR:", repr(e))

    final = "\n\n".join(out).strip()
    _dbg("final_len=", len(final))
    return final

# ===== aimodel/file_read/rag/ingest/pdf_ingest.py =====

# aimodel/file_read/rag/ingest/pdf_ingest.py
from __future__ import annotations
from typing import Tuple
import io
from ...core.settings import SETTINGS
from .ocr import is_bad_text, ocr_pdf
from .common import _utf8

def _dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[pdf_ingest]", *args, flush=True)
    except Exception:
        pass

def extract_pdf(data: bytes) -> Tuple[str, str]:
    print("[pdf_ingest] ENTER extract_pdf", flush=True)

    S = SETTINGS.effective
    OCR_ENABLED = bool(S().get("pdf_ocr_enable", False))
    OCR_MODE = str(S().get("pdf_ocr_mode", "auto")).lower()   
    WHEN_BAD  = bool(S().get("pdf_ocr_when_bad", True))
    DPI       = int(S().get("pdf_ocr_dpi", 300))
    MAX_PAGES = int(S().get("pdf_ocr_max_pages", 0))

    print(f"[pdf_ingest] cfg ocr_enabled={OCR_ENABLED} mode={OCR_MODE} when_bad={WHEN_BAD} dpi={DPI} max_pages={MAX_PAGES}", flush=True)

    def _do_ocr() -> str:
        print("[pdf_ingest] OCR_CALL begin", flush=True)  
        txt = (ocr_pdf(data) or "").strip()
        print(f"[pdf_ingest] OCR_CALL end text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
        return txt

    if OCR_ENABLED and OCR_MODE == "force":
        _dbg("mode=force -> OCR first")
        ocr_txt = _do_ocr()
        if ocr_txt:
            print("[pdf_ingest] EXIT (force OCR success)", flush=True)
            return ocr_txt, "text/plain"
        _dbg("mode=force -> OCR empty, trying text extract")

    txt = ""
    try:
        from pdfminer.high_level import extract_text
        txt = (extract_text(io.BytesIO(data)) or "").strip()
        print(f"[pdf_ingest] pdfminer text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
    except Exception as e:
        print(f"[pdf_ingest] pdfminer ERROR {repr(e)}", flush=True)
        txt = ""

    if OCR_ENABLED and OCR_MODE != "never":
        try_ocr = (not txt) or (WHEN_BAD and is_bad_text(txt))
        print(f"[pdf_ingest] auto-eval try_ocr={try_ocr} has_text={bool(txt)} is_bad={(is_bad_text(txt) if txt else 'n/a')}", flush=True)
        if try_ocr:
            ocr_txt = _do_ocr()
            if ocr_txt:
                print("[pdf_ingest] EXIT (auto OCR success)", flush=True)
                return ocr_txt, "text/plain"

    if not txt:
        try:
            from PyPDF2 import PdfReader
            r = PdfReader(io.BytesIO(data))
            pages = [(p.extract_text() or "").strip() for p in r.pages]
            txt = "\n\n".join([p for p in pages if p]).strip()
            print(f"[pdf_ingest] pypdf2 text_len={len(txt)} preview={repr((txt or '')[:120])}", flush=True)
        except Exception as e2:
            print(f"[pdf_ingest] pypdf2 ERROR {repr(e2)}", flush=True)
            txt = _utf8(data)
            print(f"[pdf_ingest] bytes-fallback text_len={len(txt)}", flush=True)

    final = (txt.strip() if txt else "")
    print(f"[pdf_ingest] EXIT (returned_len={len(final)})", flush=True)
    return final, "text/plain"

# ===== aimodel/file_read/rag/ingest/ppt_ingest.py =====

# OCR-enabled PPT ingest (minimal changes per request)
from __future__ import annotations
from typing import Tuple, List
import io, re
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile(r"[ \t]+")
def _squeeze(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _clip(s: str, limit: int) -> str:
    if limit > 0 and len(s) > limit:
        return s[:limit] + "…"
    return s

def _shape_text(shape) -> List[str]:
    out: List[str] = []
    if getattr(shape, "has_text_frame", False):
        for p in shape.text_frame.paragraphs:
            txt = _squeeze("".join(r.text for r in p.runs))
            if txt:
                out.append(txt)
    if getattr(shape, "has_table", False):
        tbl = shape.table
        for r in tbl.rows:
            cells = [_squeeze(c.text) for c in r.cells]
            if any(cells):
                out.append(" | ".join(c for c in cells if c))
    if getattr(shape, "shape_type", None) and str(shape.shape_type) == "GROUP":
        try:
            for sh in getattr(shape, "shapes", []):
                out.extend(_shape_text(sh))
        except Exception:
            pass
    try:
        S = SETTINGS.effective
        if bool(S().get("pptx_ocr_images", False)):
            is_pic = getattr(shape, "shape_type", None)
            if is_pic and "PICTURE" in str(is_pic):
                img = getattr(shape, "image", None)
                blob = getattr(img, "blob", None) if img is not None else None
                if blob and len(blob) >= int(S().get("ocr_min_image_bytes", 16384)):
                    print("[OCR] candidate image size:", len(blob))
                    t = (ocr_image_bytes(blob) or "").strip()
                    print("[OCR] result:", repr(t[:200]))
                    if t:
                        out.append(t)
    except Exception as e:
        print("[OCR] error:", e)
    return out

def extract_pptx(data: bytes) -> Tuple[str, str]:
    from pptx import Presentation
    S = SETTINGS.effective
    USE_MD = bool(S().get("pptx_use_markdown_headings", True))
    INCLUDE_NOTES = bool(S().get("pptx_include_notes", True))
    INCLUDE_TABLES = bool(S().get("pptx_include_tables", True))
    DROP_EMPTY = bool(S().get("pptx_drop_empty_lines", True))
    MAX_PARA = int(S().get("pptx_para_max_chars", 0))
    NUMBER_SLIDES = bool(S().get("pptx_number_slides", True))

    prs = Presentation(io.BytesIO(data))
    lines: List[str] = []

    for i, slide in enumerate(prs.slides, start=1):
        title = ""
        try:
            if getattr(slide, "shapes", None):
                for sh in slide.shapes:
                    if getattr(sh, "is_placeholder", False) and str(getattr(sh, "placeholder_format", "").type).lower().endswith("title"):
                        title = _squeeze(getattr(sh, "text", "") or "")
                        break
        except Exception:
            pass

        head = f"Slide {i}" + (f": {title}" if title else "")
        if USE_MD:
            lines.append(("## " if NUMBER_SLIDES else "## ") + head)
        else:
            lines.append(head)

        body: List[str] = []
        for sh in getattr(slide, "shapes", []):
            if getattr(sh, "has_table", False) and not INCLUDE_TABLES:
                continue
            body.extend(_shape_text(sh))

        for t in body:
            t = _clip(t, MAX_PARA)
            if t or not DROP_EMPTY:
                lines.append(t)

        if INCLUDE_NOTES:
            try:
                notes = slide.notes_slide
                if notes and getattr(notes, "notes_text_frame", None):
                    note_txt = _squeeze(notes.notes_text_frame.text)
                    if note_txt:
                        lines.append("")
                        lines.append("### Notes")
                        for ln in note_txt.splitlines():
                            ln = _squeeze(ln)
                            if ln or not DROP_EMPTY:
                                lines.append(_clip(ln, MAX_PARA))
            except Exception:
                pass

        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

def extract_ppt(data: bytes) -> Tuple[str, str]:
    from .doc_binary_ingest import _generic_ole_text
    S = SETTINGS.effective
    DROP_EMPTY = bool(S().get("ppt_drop_empty_lines", True))
    DEDUPE = bool(S().get("ppt_dedupe_lines", True))
    MAX_PARA = int(S().get("ppt_max_line_chars", 600))
    MIN_ALPHA = float(S().get("ppt_min_alpha_ratio", 0.4))
    MAX_PUNCT = float(S().get("ppt_max_punct_ratio", 0.5))
    TOKEN_MAX = int(S().get("ppt_token_max_chars", 40))

    raw = _generic_ole_text(data)
    if not raw:
        try:
            raw = data.decode("utf-8", errors="ignore")
        except Exception:
            raw = ""

    out: List[str] = []
    seen = set()
    for ln in (raw.splitlines() if raw else []):
        s = _squeeze(ln)
        if not s and DROP_EMPTY:
            continue
        if MAX_PARA > 0 and len(s) > MAX_PARA:
            s = s[:MAX_PARA] + "…"
        if s:
            letters = sum(1 for c in s if c.isalpha())
            alen = max(1, len(s))
            if letters / alen < MIN_ALPHA:
                continue
            punct = sum(1 for c in s if not c.isalnum() and not c.isspace())
            if punct / alen > MAX_PUNCT:
                continue
            if " " not in s and len(s) <= TOKEN_MAX and re.fullmatch(r"[\w.\-]+", s):
                continue
        if DEDUPE:
            if s in seen:
                continue
            seen.add(s)
        if s or not DROP_EMPTY:
            out.append(s)

    text = "\n".join(out).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
from datetime import datetime, date, time
from ...core.settings import SETTINGS
import re

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_xls(data: bytes) -> Tuple[str, str]:
    try:
        import xlrd  
    except Exception:
        return (data.decode("utf-8", errors="replace"), "text/plain")

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    EMIT_KEYVALUES = bool(S().get("excel_emit_key_values"))
    EMIT_CELL_ADDR = bool(S().get("excel_emit_cell_addresses"))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and isinstance(dt, datetime) and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    try:
        book = xlrd.open_workbook(file_contents=data)
    except Exception:
        return (data.decode("utf-8", errors="replace"), "text/plain")

    datemode = book.datemode

    def xlrd_cell_to_py(cell):
        ctype, value = cell.ctype, cell.value
        if ctype == xlrd.XL_CELL_DATE:
            try:
                return xlrd.xldate_as_datetime(value, datemode)
            except Exception:
                return value
        if ctype == xlrd.XL_CELL_NUMBER:
            return float(value)
        if ctype == xlrd.XL_CELL_BOOLEAN:
            return bool(value)
        return value

    lines: List[str] = []

    for sheet in book.sheets():
        nrows = min(sheet.nrows or 0, INFER_MAX_ROWS)
        ncols = min(sheet.ncols or 0, INFER_MAX_COLS)
        if nrows == 0 or ncols == 0:
            continue

        headers_raw = []
        header_fill = 0
        for c in range(ncols):
            v = xlrd_cell_to_py(sheet.cell(0, c))
            s = "" if v is None else str(v).strip()
            if s:
                header_fill += 1
            headers_raw.append(s)

        fill_ratio = header_fill / max(1, ncols)
        start_row = 1
        if fill_ratio < INFER_MIN_HEADER_FILL and nrows >= 2:
            headers_raw = []
            for c in range(ncols):
                v = xlrd_cell_to_py(sheet.cell(1, c))
                s = "" if v is None else str(v).strip()
                headers_raw.append(s)
            start_row = 2

        norm_headers = [normalize_header(h) for h in headers_raw]

        lines.append(f"# Sheet: {sheet.name}")

        if EMIT_KEYVALUES and ncols == 2:
            textish = valueish = rows = 0
            for r in range(start_row, nrows):
                a = xlrd_cell_to_py(sheet.cell(r, 0))
                b = xlrd_cell_to_py(sheet.cell(r, 1))
                if a is None and b is None:
                    continue
                rows += 1
                if isinstance(a, str):
                    textish += 1
                if isinstance(b, (int, float, datetime, date, time)):
                    valueish += 1
            if rows >= 3 and textish / max(1, rows) >= 0.6 and valueish / max(1, rows) >= 0.6:
                lines.append("## Key/Values")
                for r in range(start_row, nrows):
                    k = fmt_val(xlrd_cell_to_py(sheet.cell(r, 0)))
                    v = fmt_val(xlrd_cell_to_py(sheet.cell(r, 1)))
                    if not k and not v:
                        continue
                    lines.append(f"- {k}: {v}" if k else f"- : {v}")
                lines.append("")
                continue 
            
        lines.append("## Inferred Table")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))

        for r in range(start_row, nrows):
            row_vals: List[str] = []
            for c in range(ncols):
                val = fmt_val(xlrd_cell_to_py(sheet.cell(r, c)))
                if val:
                    row_vals.append(val if not EMIT_CELL_ADDR else f"{val}")
            if row_vals:
                lines.append("row: " + ", ".join(row_vals))
        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/rerank.py =====

from __future__ import annotations
from typing import List, Dict, Optional
from ..core.settings import SETTINGS

_RERANKER = None
_RERANKER_NAME = None

def _load_reranker():
    global _RERANKER, _RERANKER_NAME
    model_name = SETTINGS.get("rag_rerank_model")
    if not model_name:
        return None
    if _RERANKER is not None and _RERANKER_NAME == model_name:
        return _RERANKER
    try:
        from sentence_transformers import CrossEncoder
        _RERANKER = CrossEncoder(model_name)
        _RERANKER_NAME = model_name
        return _RERANKER
    except Exception as e:
        print(f"[RAG RERANK] failed to load reranker {model_name}: {e}")
        _RERANKER = None
        _RERANKER_NAME = None
        return None

def rerank_hits(query: str, hits: List[dict], *, top_m: Optional[int] = None) -> List[dict]:
    if not hits:
        return hits
    model = _load_reranker()
    if model is None:
        return hits

    pairs = [(query, (h.get("text") or "")) for h in hits]
    try:
        scores = model.predict(pairs)
    except Exception as e:
        print(f"[RAG RERANK] predict failed: {e}")
        return hits

    out: List[dict] = []
    for h, s in zip(hits, scores):
        hh = dict(h)
        hh["rerankScore"] = float(s)
        out.append(hh)

    out.sort(key=lambda x: x.get("rerankScore", 0.0), reverse=True)
    if isinstance(top_m, int) and top_m > 0:
        out = out[:top_m]
    return out

def cap_per_source(hits: List[dict], per_source_cap: int) -> List[dict]:
    if per_source_cap is None or per_source_cap <= 0:
        return hits
    bucket: dict[str, int] = {}
    out: List[dict] = []
    for h in hits:
        src = str(h.get("source") or "")
        seen = bucket.get(src, 0)
        if seen < per_source_cap:
            out.append(h)
            bucket[src] = seen + 1
    return out

def min_score_fraction(hits: List[Dict], key: str, frac: float) -> List[Dict]:
    if not hits:
        return hits
    vals = []
    for h in hits:
        try:
            v = float(h.get(key) or 0.0)
        except Exception:
            v = 0.0
        vals.append(v)

    s_min = min(vals)
    s_max = max(vals)
    if s_max == s_min:
        return hits

    kept = []
    for h, v in zip(hits, vals):
        norm = (v - s_min) / (s_max - s_min)
        if norm >= float(frac):
            kept.append(h)
    return kept

# ===== aimodel/file_read/rag/retrieve_common.py =====

# aimodel/file_read/rag/retrieve_common.py
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any, Optional
from ..core.settings import SETTINGS
from .retrieve_tabular import make_rag_block_tabular

_EMBEDDER = None
_EMBEDDER_NAME = None
PRINT_MAX = 10 

def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        print(f"[RAG] sentence_transformers unavailable: {e}")
        return None, None

    model_name = SETTINGS.get("rag_embedding_model")
    if not model_name:
        print("[RAG] no rag_embedding_model configured")
        return None, None

    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            print(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return _EMBEDDER, _EMBEDDER_NAME


def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding encode failed: {e}")
        return []

def _primary_score(h: Dict[str, Any]) -> float:
    s = h.get("rerankScore")
    if s is not None:
        try:
            return float(s)
        except Exception:
            pass
    try:
        return float(h.get("score") or 0.0)
    except Exception:
        return 0.0

def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    hits_sorted = sorted(hits, key=_primary_score, reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out

def _default_header(h: Dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"


def _render_header(h: Dict[str, Any]) -> str:
    return _default_header(h)


def make_rag_block_generic(hits: List[dict], *, max_chars: int) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"
    total_budget = int(SETTINGS.get("rag_total_char_budget"))

    lines = [preamble]
    used = len(lines[0]) + 1

    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()

        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost

    return "\n".join(lines)


def build_block_for_hits(hits_top: List[dict], preferred_sources: Optional[List[str]] = None) -> str:
    block = make_rag_block_tabular(hits_top, preferred_sources=preferred_sources)
    if block is None:
        block = make_rag_block_generic(
            hits_top,
            max_chars=int(SETTINGS.get("rag_max_chars_per_chunk")),
        )
    return block or ""

def _rescore_for_preferred_sources(hits: List[dict], preferred_sources: Optional[List[str]] = None) -> List[dict]:
    if not preferred_sources:
        return hits
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))
    pref = set(s.strip().lower() for s in preferred_sources if s)
    out = []
    for h in hits:
        sc = float(h.get("score") or 0.0)
        src = str(h.get("source") or "").strip().lower()
        if src in pref:
            sc *= (1.0 + boost)
        hh = dict(h)
        hh["score"] = sc
        out.append(hh)
    return out


def _fmt_hit(h: Dict[str, Any]) -> str:
    return (
        f"id={h.get('id')!s} src={h.get('source')!s} "
        f"chunk={h.get('chunkIndex')!s} row={h.get('row')!s} "
        f"score={h.get('score')!s} rerank={h.get('rerankScore')!s}"
    )


def _print_hits(label: str, hits: List[Dict[str, Any]], limit: int = PRINT_MAX) -> None:
    print(f"[RAG DEBUG] {label}: count={len(hits)}")
    for i, h in enumerate(hits[:limit]):
        print(f"[RAG DEBUG]   {i+1:02d}: {_fmt_hit(h)}")
    if len(hits) > limit:
        print(f"[RAG DEBUG]   … (+{len(hits)-limit} more)")


def _nohit_block(q: str) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "Local knowledge")
    preamble = preamble if preamble.endswith(":") else preamble + ":"
    msg = str(SETTINGS.get("rag_nohit_message") or "⛔ No relevant local entries found for this query. Do not guess.")
    if q:
        return f"{preamble}\n- {msg}\n- query={q!r}"
    return f"{preamble}\n- {msg}"

@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    rerankSec: float = 0.0
    usedReranker: bool = False
    keptAfterRerank: int = 0
    capPerSource: int = 0
    keptAfterCap: int = 0
    minScoreFrac: float = 0.0
    keptAfterMinFrac: int = 0
    fallbackUsed: bool = False
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"

# ===== aimodel/file_read/rag/retrieve_core.py =====

# aimodel/file_read/rag/retrieve_core.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Any, Optional
import time
from ..core.settings import SETTINGS
from .store import search_vectors
from .retrieve_tabular import make_rag_block_tabular
from .rerank import rerank_hits, cap_per_source, min_score_fraction

_EMBEDDER = None
_EMBEDDER_NAME = None
PRINT_MAX = 10  


def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        print(f"[RAG] sentence_transformers unavailable: {e}")
        return None, None

    model_name = SETTINGS.get("rag_embedding_model")
    if not model_name:
        print("[RAG] no rag_embedding_model configured")
        return None, None

    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            print(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return _EMBEDDER, _EMBEDDER_NAME


def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding encode failed: {e}")
        return []


def _primary_score(h: Dict[str, Any]) -> float:
    s = h.get("rerankScore")
    if s is not None:
        try:
            return float(s)
        except Exception:
            pass
    try:
        return float(h.get("score") or 0.0)
    except Exception:
        return 0.0


def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    hits_sorted = sorted(hits, key=_primary_score, reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out


def _default_header(h: Dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"


def _render_header(h: Dict[str, Any]) -> str:
    return _default_header(h)


def make_rag_block_generic(hits: List[dict], *, max_chars: int) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"
    total_budget = int(SETTINGS.get("rag_total_char_budget"))

    lines = [preamble]
    used = len(lines[0]) + 1

    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()

        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost

    return "\n".join(lines)


@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    rerankSec: float = 0.0
    usedReranker: bool = False
    keptAfterRerank: int = 0
    capPerSource: int = 0
    keptAfterCap: int = 0
    minScoreFrac: float = 0.0
    keptAfterMinFrac: int = 0
    fallbackUsed: bool = False
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"


def _rescore_for_preferred_sources(hits: List[dict], preferred_sources: Optional[List[str]] = None) -> List[dict]:
    if not preferred_sources:
        return hits
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))
    pref = set(s.strip().lower() for s in preferred_sources if s)
    out = []
    for h in hits:
        sc = float(h.get("score") or 0.0)
        src = str(h.get("source") or "").strip().lower()
        if src in pref:
            sc *= (1.0 + boost)
        hh = dict(h)
        hh["score"] = sc
        out.append(hh)
    return out


def build_block_for_hits(hits_top: List[dict], preferred_sources: Optional[List[str]] = None) -> str:
    block = make_rag_block_tabular(hits_top, preferred_sources=preferred_sources)
    if block is None:
        block = make_rag_block_generic(
            hits_top,
            max_chars=int(SETTINGS.get("rag_max_chars_per_chunk")),
        )
    return block or ""


def _fmt_hit(h: Dict[str, Any]) -> str:
    return (
        f"id={h.get('id')!s} src={h.get('source')!s} "
        f"chunk={h.get('chunkIndex')!s} row={h.get('row')!s} "
        f"score={h.get('score')!s} rerank={h.get('rerankScore')!s}"
    )


def _print_hits(label: str, hits: List[Dict[str, Any]], limit: int = PRINT_MAX) -> None:
    print(f"[RAG DEBUG] {label}: count={len(hits)}")
    for i, h in enumerate(hits[:limit]):
        print(f"[RAG DEBUG]   {i+1:02d}: {_fmt_hit(h)}")
    if len(hits) > limit:
        print(f"[RAG DEBUG]   … (+{len(hits)-limit} more)")


def _build_rag_block_core(
    query: str,
    *,
    session_id: str | None,
    k: int,
    session_only: bool,
    preferred_sources: Optional[List[str]] = None,
) -> Tuple[Optional[str], RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode=("session-only" if session_only else "global"))
    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")

    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)

    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None, tel

    d = len(qvec)

    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)

    hits_glob: List[dict] = []
    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)

    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)

    _print_hits("ANN hits (chat)", hits_chat)
    if not session_only:
        _print_hits("ANN hits (global)", hits_glob)

    all_hits = hits_chat + ([] if session_only else hits_glob)
    if not all_hits:
        print("[RAG SEARCH] no hits")
        return None, tel

    all_hits = _rescore_for_preferred_sources(all_hits, preferred_sources=preferred_sources)
    _print_hits("After preference boost", all_hits)

    pre_prune_hits = list(all_hits)

    t_rr = time.perf_counter()
    top_m_cfg = SETTINGS.get("rag_rerank_top_m")
    try:
        top_m = int(top_m_cfg) if top_m_cfg not in (None, "", False) else None
    except Exception:
        top_m = None
    if isinstance(top_m, int):
        top_m = max(1, min(top_m, len(all_hits)))

    try:
        all_hits = rerank_hits(q, all_hits, top_m=top_m)
    except Exception as e:
        print(f"[RAG] rerank error: {e}; skipping rerank")
        pass

    tel.rerankSec = round(time.perf_counter() - t_rr, 6)
    tel.usedReranker = any("rerankScore" in h for h in all_hits)
    tel.keptAfterRerank = len(all_hits)
    _print_hits(f"After rerank (top_m={top_m})", all_hits)

    frac_cfg = SETTINGS.get("rag_min_score_frac")
    try:
        frac = float(frac_cfg) if frac_cfg not in (None, "", False) else None
    except Exception:
        frac = None

    tel.minScoreFrac = float(frac) if isinstance(frac, (int, float)) else 0.0
    if isinstance(frac, (int, float)):
        key = "rerankScore" if any("rerankScore" in h for h in all_hits) else "score"
        before = list(all_hits)
        all_hits = min_score_fraction(all_hits, key, float(frac))
        _print_hits(f"After minScoreFrac={frac} on key={key}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by minScoreFrac", dropped)
    tel.keptAfterMinFrac = len(all_hits)

    cap_cfg = SETTINGS.get("rag_per_source_cap")
    try:
        cap_val = int(cap_cfg) if cap_cfg not in (None, "", False) else 0
    except Exception:
        cap_val = 0
    tel.capPerSource = cap_val
    if cap_val and cap_val > 0:
        before = list(all_hits)
        all_hits = cap_per_source(all_hits, cap_val)
        _print_hits(f"After per-source cap={cap_val}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by per-source cap", dropped)
    tel.keptAfterCap = len(all_hits)

    if not all_hits:
        best = sorted(pre_prune_hits, key=_primary_score, reverse=True)[:1]
        all_hits = best
        tel.fallbackUsed = True
        print("[RAG] pruning yielded 0 hits; falling back to best pre-prune hit")
        _print_hits("Fallback best pre-prune", all_hits)

    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)
    _print_hits(f"Final top-k (k={k})", hits_top)

    t4 = time.perf_counter()
    block = build_block_for_hits(hits_top, preferred_sources=preferred_sources)
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    preview = (block or "")[:400].replace("\n", "\\n")
    print(f"[RAG BLOCK] chars={tel.blockChars} preview=\"{preview}\"")
    print(f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}")

    return block, tel


def build_rag_block(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    k = int(SETTINGS.get("rag_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block


def build_rag_block_with_telemetry(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    k = int(SETTINGS.get("rag_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block, asdict(tel)


def build_rag_block_session_only(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Optional[str]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block


def build_rag_block_session_only_with_telemetry(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block, asdict(tel)

# ===== aimodel/file_read/rag/retrieve_pipeline.py =====

# ===== aimodel/file_read/rag/retrieve_pipeline.py =====
from __future__ import annotations
from dataclasses import asdict
from typing import List, Tuple, Dict, Any, Optional
import time
from ..core.settings import SETTINGS
from .store import search_vectors
from .rerank import rerank_hits, cap_per_source, min_score_fraction
from .retrieve_common import (
    RagTelemetry,
    _embed_query,
    _primary_score,
    _dedupe_and_sort,
    _rescore_for_preferred_sources,
    _print_hits,
    build_block_for_hits,
    _nohit_block,
)

def _mk_preview(text: Optional[str], limit: int = 400) -> str:
    t = (text or "")[:limit]
    return t.replace("\n", "\\n")

def _build_rag_block_core(
    query: str,
    *,
    session_id: str | None,
    k: int,
    session_only: bool,
    preferred_sources: Optional[List[str]] = None,
) -> Tuple[Optional[str], RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode=("session-only" if session_only else "global"))
    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")

    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)
    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None, tel

    d = len(qvec)

    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)

    hits_glob: List[dict] = []
    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)

    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)

    _print_hits("ANN hits (chat)", hits_chat)
    if not session_only:
        _print_hits("ANN hits (global)", hits_glob)

    all_hits = hits_chat + ([] if session_only else hits_glob)
    if not all_hits:
        print("[RAG SEARCH] no hits")
        return None, tel

    all_hits = _rescore_for_preferred_sources(all_hits, preferred_sources=preferred_sources)
    _print_hits("After preference boost", all_hits)

    pre_prune_hits = list(all_hits)

    t_rr = time.perf_counter()
    top_m_cfg = SETTINGS.get("rag_rerank_top_m")
    try:
        top_m = int(top_m_cfg) if top_m_cfg not in (None, "", False) else None
    except Exception:
        top_m = None
    if isinstance(top_m, int):
        top_m = max(1, min(top_m, len(all_hits)))

    try:
        all_hits = rerank_hits(q, all_hits, top_m=top_m)
    except Exception as e:
        print(f"[RAG] rerank error: {e}; skipping rerank")
        pass

    tel.rerankSec = round(time.perf_counter() - t_rr, 6)
    tel.usedReranker = any("rerankScore" in h for h in all_hits)
    tel.keptAfterRerank = len(all_hits)
    _print_hits(f"After rerank (top_m={top_m})", all_hits)

    min_abs = SETTINGS.get("rag_min_abs_rerank")
    try:
        min_abs = float(min_abs) if min_abs not in (None, "", False) else None
    except Exception:
        min_abs = None
    if isinstance(min_abs, float):
        before = list(all_hits)
        all_hits = [h for h in all_hits if float(h.get("rerankScore") or -1e12) >= min_abs]
        if len(all_hits) != len(before):
            _print_hits(f"After abs rerank cutoff >= {min_abs}", all_hits)

    frac_cfg = SETTINGS.get("rag_min_score_frac")
    try:
        frac = float(frac_cfg) if frac_cfg not in (None, "", False) else None
    except Exception:
        frac = None
    tel.minScoreFrac = float(frac) if isinstance(frac, (int, float)) else 0.0
    if isinstance(frac, (int, float)):
        key = "rerankScore" if any("rerankScore" in h for h in all_hits) else "score"
        before = list(all_hits)
        all_hits = min_score_fraction(all_hits, key, float(frac))
        _print_hits(f"After minScoreFrac={frac} on key={key}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by minScoreFrac", dropped)
    tel.keptAfterMinFrac = len(all_hits)

    cap_cfg = SETTINGS.get("rag_per_source_cap")
    try:
        cap_val = int(cap_cfg) if cap_cfg not in (None, "", False) else 0
    except Exception:
        cap_val = 0
    tel.capPerSource = cap_val
    if cap_val and cap_val > 0:
        before = list(all_hits)
        all_hits = cap_per_source(all_hits, cap_val)
        _print_hits(f"After per-source cap={cap_val}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by per-source cap", dropped)
    tel.keptAfterCap = len(all_hits)

    if not all_hits:
        if pre_prune_hits:
            all_hits = sorted(pre_prune_hits, key=_primary_score, reverse=True)[:1]
            tel.fallbackUsed = True
            print("[RAG] pruning yielded 0; keeping best pre-prune (low-confidence fallback)")
            _print_hits("Fallback best pre-prune", all_hits)
        else:
            tel.fallbackUsed = False
            print("[RAG] no ANN hits; returning no-hit block")
            block = _nohit_block(q)
            tel.blockChars = len(block or "")
            preview = _mk_preview(block)
            print(f'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
            print(f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}")
            return block, tel

    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)
    _print_hits(f"Final top-k (k={k})", hits_top)

    t4 = time.perf_counter()
    block = build_block_for_hits(hits_top, preferred_sources=preferred_sources)
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    preview = _mk_preview(block)
    print(f'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
    print(f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}")

    return block, tel

def build_rag_block(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    k = int(SETTINGS.get("rag_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block

def build_rag_block_with_telemetry(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    k = int(SETTINGS.get("rag_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block, asdict(tel)

def build_rag_block_session_only(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Optional[str]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block

def build_rag_block_session_only_with_telemetry(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block, asdict(tel)

# ===== aimodel/file_read/rag/retrieve_tabular.py =====

from __future__ import annotations
from typing import List, Dict, Any, Optional, Tuple
import re, os
from ..core.settings import SETTINGS

_TABLE_RE = re.compile(r"^##\s*Table:\s*(?P<sheet>[^!]+)!\s*R(?P<r1>\d+)-(?P<r2>\d+),C(?P<c1>\d+)-(?P<c2>\d+)", re.MULTILINE)
_ROW_RE = re.compile(r"^#{0,3}\s*Row\s+(?P<row>\d+)\s+—\s+(?P<sheet>[^\r\n]+)", re.MULTILINE)


def is_csv_source(src: str) -> bool:
    try:
        _, ext = os.path.splitext(src.lower())
        return ext in {".csv", ".tsv"}
    except Exception:
        return False


def is_xlsx_source(src: str) -> bool:
    try:
        _, ext = os.path.splitext(src.lower())
        return ext in {".xlsx", ".xlsm", ".xls"}
    except Exception:
        return False


def _capture_table_block(text: str) -> Optional[str]:
    m = _TABLE_RE.search(text or "")
    if not m:
        return None
    start = m.start()
    end = len(text)
    nxt = re.search(r"^\s*$", text[m.end():], re.MULTILINE)
    if nxt:
        end = m.end() + nxt.start()
    return text[start:end].strip()


def _capture_row_block(text: str, row_num: int, sheet: str) -> Optional[str]:
    if not text:
        return None
    pat = re.compile(rf"^#{0,3}\s*Row\s+{row_num}\s+—\s+{re.escape(sheet)}[^\n]*\n(?P<body>.*?)(?:\n\s*\n|$)", re.MULTILINE | re.DOTALL)
    m = pat.search(text)
    if not m:
        pat2 = re.compile(rf"^\s*{row_num}\s+—\s+{re.escape(sheet)}[^\n]*\n(?P<body>.*?)(?:\n\s*\n|$)", re.MULTILINE | re.DOTALL)
        m = pat2.search(text)
        if not m:
            return None
    head = f"### Row {row_num} — {sheet}"
    body = (m.group("body") or "").strip()
    if not body:
        return head
    return f"{head}\n{body}"


def _collect_tabular_hits(hits: List[dict]) -> Dict[str, Any]:
    headers: Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]] = {}
    rows: List[Dict[str, Any]] = []
    for h in hits:
        src = str(h.get("source") or "")
        body = (h.get("text") or "").strip()

        for mt in _TABLE_RE.finditer(body):
            sheet = mt.group("sheet").strip()
            r1 = int(mt.group("r1")); r2 = int(mt.group("r2"))
            c1 = int(mt.group("c1")); c2 = int(mt.group("c2"))
            key = (src, sheet, r1, r2, c1, c2)
            if key not in headers:
                tb = _capture_table_block(body)
                headers[key] = {
                    "source": src, "sheet": sheet,
                    "r1": r1, "r2": r2, "c1": c1, "c2": c2,
                    "text": tb or "", "score": float(h.get("score") or 0.0)
                }
            else:
                headers[key]["score"] = max(headers[key]["score"], float(h.get("score") or 0.0))

        for mr in _ROW_RE.finditer(body):
            rn = int(mr.group("row"))
            sheet = mr.group("sheet").strip()
            rows.append({"source": src, "sheet": sheet, "row": rn, "hit": h, "score": float(h.get("score") or 0.0)})
    return {"headers": headers, "rows": rows}


def _pair_rows_with_headers(collected: Dict[str, Any]) -> Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]]:
    headers = collected["headers"]
    rows = collected["rows"]
    groups: Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]] = {}
    for r in rows:
        src = r["source"]; sheet = r["sheet"]; rown = r["row"]
        match_key = None
        for key in headers.keys():
            s, sh, r1, r2, c1, c2 = key
            if s == src and sh == sheet and r1 <= rown <= r2:
                match_key = key
                break
        if not match_key:
            continue
        g = groups.setdefault(match_key, {"header": headers[match_key], "rows": []})
        g["rows"].append(r)
    return groups


def _render_tabular_groups(
    groups: Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]],
    preferred_sources: Optional[List[str]] = None
) -> List[str]:
    total_budget = int(SETTINGS.get("rag_total_char_budget"))
    max_row_snippets = int(SETTINGS.get("rag_tabular_rows_per_table"))
    per_row_max = int(SETTINGS.get("rag_max_chars_per_chunk"))
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"

    pref = set(s.strip().lower() for s in (preferred_sources or []) if s)
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))

    lines: List[str] = [preamble]
    used = len(lines[0]) + 1

    def _group_score(key):
        base = groups[key]["header"]["score"]
        src = str(groups[key]["header"]["source"] or "").strip().lower()
        return base * (1.0 + boost) if src in pref else base

    keys_sorted = sorted(groups.keys(), key=_group_score, reverse=True)

    for key in keys_sorted:
        hdr = groups[key]["header"]
        hdr_text = (hdr.get("text") or "").strip()
        if not hdr_text:
            hdr_text = f"## Table: {hdr['sheet']}!R{hdr['r1']}-{hdr['r2']},C{hdr['c1']}-{hdr['c2']}"
        hdr_cost = len(hdr_text) + 1
        if used + hdr_cost > total_budget:
            break
        lines.append(hdr_text)
        used += hdr_cost

        row_list = sorted(groups[key]["rows"], key=lambda r: r["score"], reverse=True)[:max_row_snippets]
        for r in row_list:
            body = (r["hit"].get("text") or "")
            row_block = _capture_row_block(body, r["row"], r["sheet"]) or ""
            if not row_block:
                continue
            row_snip = row_block[:per_row_max].strip()
            row_cost = len(row_snip) + 1
            if used + row_cost > total_budget:
                break
            lines.append(row_snip)
            used += row_cost

        if used >= total_budget:
            break

    return lines


def make_rag_block_tabular(hits: List[dict], preferred_sources: Optional[List[str]] = None) -> Optional[str]:
    if not hits:
        return None
    # Only keep sources that look like CSV/Excel to avoid mixing generic text
    tabular_hits = [
        h for h in hits
        if is_xlsx_source(str(h.get("source") or "")) or is_csv_source(str(h.get("source") or ""))
    ]
    if not tabular_hits:
        return None
    collected = _collect_tabular_hits(tabular_hits)
    groups = _pair_rows_with_headers(collected)
    if not groups:
        return None
    lines = _render_tabular_groups(groups, preferred_sources=preferred_sources)
    return "\n".join(lines)

# ===== aimodel/file_read/rag/router_ai.py =====

# ===== aimodel/file_read/rag/router_ai.py =====
from __future__ import annotations
from typing import Tuple, Optional, Any
import json, re, traceback
from ..core.settings import SETTINGS

def _dbg(msg: str):
    print(f"[RAG ROUTER] {msg}")

def _force_json_strict(s: str) -> dict:
    if not s:
        return {}
    try:
        v = json.loads(s)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass
    rgx = SETTINGS.get("router_rag_json_extract_regex")
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
                v = json.loads(cand)
                return v if isinstance(v, dict) else {}
        except Exception:
            pass
    return {}

def _strip_wrappers(text: str) -> str:
    t = text or ""
    if SETTINGS.get("router_rag_trim_whitespace") is True:
        t = t.strip()
    if SETTINGS.get("router_rag_strip_wrappers_enabled") is not True:
        return t
    head = t
    if SETTINGS.get("router_rag_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]
    pat = SETTINGS.get("router_rag_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

def _normalize_keys(d: dict) -> dict:
    return {str(k).strip().strip('"').strip("'").strip().lower(): v for k, v in d.items()}

def _as_bool(v) -> Optional[bool]:
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        s = v.strip().strip('"').strip("'").lower()
        if s in ("true", "yes", "y", "1"):  return True
        if s in ("false", "no", "n", "0"):  return False
    return None

def decide_rag(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    try:
        if not user_text or not user_text.strip():
            return (False, None)

        core_text = _strip_wrappers(user_text.strip())

        prompt_tpl = SETTINGS.get("router_rag_decide_prompt")
        if not isinstance(prompt_tpl, str) or ("$text" not in prompt_tpl and "{text}" not in prompt_tpl):
            _dbg("router_rag_decide_prompt missing/invalid")
            return (False, None)

        from string import Template
        if "$text" in prompt_tpl:
            the_prompt = Template(prompt_tpl).safe_substitute(text=core_text)
        else:
            the_prompt = prompt_tpl.format(text=core_text)

        params = {
            "max_tokens": SETTINGS.get("router_rag_decide_max_tokens"),
            "temperature": SETTINGS.get("router_rag_decide_temperature"),
            "top_p": SETTINGS.get("router_rag_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_rag_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}

        raw = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw.get("choices", [{}])[0].get("message", {}).get("content") or "").strip()

        data = _force_json_strict(text_out)
        if not isinstance(data, dict):
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)
        data = _normalize_keys(data)

        need_raw = data.get("need")
        need_bool = _as_bool(need_raw) if not isinstance(need_raw, bool) else need_raw
        if need_bool is None:
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)

        need = bool(need_bool)
        if not need:
            return (False, None)

        query_field = data.get("query", "")
        query_clean = _strip_wrappers(str(query_field or "").strip())
        if not query_clean:
            query_clean = core_text[:512]

        return (True, query_clean)

    except Exception as e:
        _dbg(f"FATAL {type(e).__name__}: {e}")
        traceback.print_exc()
        need_default = SETTINGS.get("router_rag_default_need_when_invalid")
        return (bool(need_default) if isinstance(need_default, bool) else False, None)

# ===== aimodel/file_read/rag/schemas.py =====

from pydantic import BaseModel, Field
from typing import Optional, Dict

class SearchReq(BaseModel):
    query: str
    sessionId: Optional[str] = None
    kChat: int = 6
    kGlobal: int = 4
    hybrid_alpha: float = 0.5  

class ItemRow(BaseModel):
    id: str
    sessionId: Optional[str]
    source: str
    title: Optional[str]
    mime: Optional[str]
    size: Optional[int]
    createdAt: str
    meta: Dict[str, str] = Field(default_factory=dict)

class SearchHit(BaseModel):
    id: str
    text: str
    score: float
    source: Optional[str] = None   
    title: Optional[str] = None
    sessionId: Optional[str] = None
    url: Optional[str] = None

# ===== aimodel/file_read/rag/search.py =====

from __future__ import annotations
from typing import List, Dict

def reciprocal_rank_fusion(results: List[List[Dict]], k: int = 60) -> List[Dict]:
    scores: Dict[str, float] = {}
    lookup: Dict[str, Dict] = {}
    for lst in results:
        for rank, r in enumerate(lst, start=1):
            rid = r["id"]
            scores[rid] = scores.get(rid, 0.0) + 1.0 / (k + rank)
            lookup[rid] = r
    fused = [{"score": s, **lookup[rid]} for rid, s in scores.items()]
    fused.sort(key=lambda x: x["score"], reverse=True)
    return fused

# ===== aimodel/file_read/rag/store.py =====

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import faiss, json
import numpy as np
from ..store.base import APP_DIR   
import shutil

BASE = APP_DIR / "rag"

def _ns_dir(session_id: Optional[str]) -> Path:
    if session_id:
        return BASE / "by_session" / session_id
    return BASE / "global"

def _paths(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / "index.faiss", d / "meta.jsonl"

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def _load_index(dim: int, p: Path) -> faiss.Index:
    if p.exists():
        return faiss.read_index(str(p))
    return faiss.IndexFlatIP(dim)

def _save_index(idx: faiss.Index, p: Path) -> None:
    faiss.write_index(idx, str(p))

def add_vectors(session_id: Optional[str], embeds: np.ndarray, metas: List[Dict], dim: int):
    idx_path, meta_path = _paths(session_id)
    idx = _load_index(dim, idx_path)

    if not isinstance(idx, faiss.IndexFlatIP):
        idx = faiss.IndexFlatIP(dim) if idx.ntotal == 0 else idx

    embeds = _norm(embeds)

    existing_ids = set()
    if meta_path.exists():
        with meta_path.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    existing_ids.add(j["id"])
                except:
                    pass

    start = idx.ntotal
    new_embeds = []
    new_metas = []

    for i, m in enumerate(metas):
        if m["id"] in existing_ids:
            continue
        m["row"] = start + len(new_embeds)
        new_embeds.append(embeds[i])
        new_metas.append(m)

    if new_embeds:
        idx.add(np.vstack(new_embeds))
        _save_index(idx, idx_path)
        with meta_path.open("a", encoding="utf-8") as f:
            for m in new_metas:
                f.write(json.dumps(m, ensure_ascii=False) + "\n")

def search_vectors(session_id: Optional[str], query_vec: np.ndarray, topk: int, dim: int) -> List[Dict]:
    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return []

    idx = _load_index(dim, idx_path)

    query_vec = np.asarray(query_vec, dtype="float32")
    q = _norm(query_vec.reshape(1, -1))

    D, I = idx.search(q, topk)
    out: List[Dict] = []
    rows: Dict[int, Dict] = {}
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                j = json.loads(line)
                rows[int(j["row"])] = j
            except:
                pass
    for score, row in zip(D[0].tolist(), I[0].tolist()):
        if row < 0:
            continue
        m = rows.get(row)
        if not m:
            continue
        m = dict(m)
        m["score"] = float(score)
        out.append(m)
    return out

def search_similar(qvec: List[float] | np.ndarray, *, k: int = 5, session_id: Optional[str] = None) -> List[Dict]:
    """
    Compatibility wrapper used by retrieve.py.
    qvec: a single embedding vector (list or np.ndarray)
    """
    arr = np.asarray(qvec, dtype="float32")
    dim = int(arr.shape[-1])
    return search_vectors(session_id, arr, k, dim)

def add_texts(
    texts: List[str],
    metas: List[Dict],
    *,
    session_id: Optional[str],
    embed_fn,  
) -> int:
    if not texts:
        return 0
    vecs = embed_fn(texts) 
    if not isinstance(vecs, np.ndarray):
        vecs = np.asarray(vecs, dtype="float32")
    dim = int(vecs.shape[-1])
    add_vectors(session_id, vecs, metas, dim)
    return len(texts)

def delete_namespace(session_id: str) -> bool:
    d = _ns_dir(session_id)
    try:
        if d.exists():
            shutil.rmtree(d, ignore_errors=True)
            return True
        return False
    except Exception:
        return False

def session_has_any_vectors(session_id: Optional[str]) -> bool:
    if not session_id:
        return False

    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return False

    try:
        idx = _load_index(dim=1, p=idx_path)  
        if getattr(idx, "ntotal", 0) <= 0:
            return False
    except Exception as e:
        print(f"[RAG STORE] failed to read index for {session_id}: {e}")
        return False

    try:
        with meta_path.open("r", encoding="utf-8") as f:
            for _ in f:
                return True
        return False
    except Exception as e:
        print(f"[RAG STORE] failed to read meta for {session_id}: {e}")
        return False

# ===== aimodel/file_read/rag/uploads.py =====

from __future__ import annotations
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import json
import numpy as np
import faiss

from .store import _ns_dir  

_META_FN = "meta.jsonl"
_INDEX_FN = "index.faiss"

def _meta_path_ro(session_id: Optional[str]) -> Path:
    return _ns_dir(session_id) / _META_FN

def _index_path_ro(session_id: Optional[str]) -> Path:
    return _ns_dir(session_id) / _INDEX_FN

def _paths_mut(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / _INDEX_FN, d / _META_FN

def _read_meta(meta_path: Path) -> List[dict]:
    if not meta_path.exists():
        return []
    out: List[dict] = []
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                j = json.loads(line)
                if isinstance(j, dict):
                    out.append(j)
            except:
                pass
    return out

def _write_meta(meta_path: Path, rows: List[dict]) -> None:
    tmp = meta_path.with_suffix(".jsonl.tmp")
    with tmp.open("w", encoding="utf-8") as f:
        for j in rows:
            f.write(json.dumps(j, ensure_ascii=False) + "\n")
    tmp.replace(meta_path)

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def list_sources(session_id: Optional[str], include_global: bool = True) -> List[dict]:
    def _agg(ns: Optional[str]) -> Dict[str, int]:
        mp = _meta_path_ro(ns)  
        agg: Dict[str, int] = {}
        for j in _read_meta(mp):
            src = (j.get("source") or "").strip()
            if not src:
                continue
            agg[src] = agg.get(src, 0) + 1
        return agg

    rows: List[dict] = []
    # session first
    if session_id is not None:
        for src, n in _agg(session_id).items():
            rows.append({"source": src, "sessionId": session_id, "chunks": n})
    if include_global:
        for src, n in _agg(None).items():
            rows.append({"source": src, "sessionId": None, "chunks": n})
    return rows

def hard_delete_source(source: str, *, session_id: Optional[str], embedder) -> dict:
    idx_path, meta_path = _paths_mut(session_id) 
    rows = _read_meta(meta_path)
    if not rows:
        return {"ok": True, "removed": 0, "remaining": 0}

    keep: List[dict] = []
    removed = 0
    for j in rows:
        if str(j.get("source") or "").strip() == source:
            removed += 1
        else:
            keep.append(j)

    if removed == 0:
        return {"ok": True, "removed": 0, "remaining": len(keep)}

    for i, j in enumerate(keep):
        j["row"] = i

    if len(keep) == 0:
        if idx_path.exists():
            try:
                idx_path.unlink()
            except:
                pass
        _write_meta(meta_path, [])
        return {"ok": True, "removed": removed, "remaining": 0}

    texts = [str(j.get("text") or "") for j in keep]
    B = 128  # batch size
    parts: List[np.ndarray] = []
    for i in range(0, len(texts), B):
        vec = embedder(texts[i:i + B])
        if not isinstance(vec, np.ndarray):
            vec = np.asarray(vec, dtype="float32")
        parts.append(vec.astype("float32"))
    embeds = np.vstack(parts)
    embeds = _norm(embeds)

    dim = int(embeds.shape[-1])
    new_index = faiss.IndexFlatIP(dim)
    new_index.add(embeds)

    faiss.write_index(new_index, str(idx_path))
    _write_meta(meta_path, keep)

    return {"ok": True, "removed": removed, "remaining": len(keep)}

# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs==9.5.4
httpx==0.27.0
# HTML parsing / readability (permissive)
lxml==6.0.1
readability-lxml==0.8.1
selectolax==0.3.21
beautifulsoup4==4.12.3

# RAG
faiss-cpu==1.8.0.post1
numpy==1.26.4
sentence-transformers==3.0.1
tzlocal==5.2
openpyxl==3.1.5 
python-multipart==0.0.20

python-docx==1.1.2
PyYAML==6.0.2
toml==0.10.2
pdfminer.six==20240706

PyPDF2==3.0.1
pandas==2.2.2

striprtf==0.0.26

python-pptx==0.6.23

pytesseract==0.3.13
pypdfium2==4.30.0
Pillow==10.4.

pydantic[email]==2.11.7
python-jose==3.3.0
passlib[bcrypt]==1.7.4


pynacl==1.5.0

# ===== aimodel/file_read/runtime/__init__.py =====



# ===== aimodel/file_read/runtime/model_runtime.py =====

# aimodel/file_read/model_runtime.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Optional, List
from ..adaptive.config.paths import read_settings, write_settings

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath","")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None,"") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None,"") else None),
        )

_runtime_lock = RLock()
_llm: Optional[Llama] = None
_cfg: Optional[ModelConfig] = None

def _build_kwargs(cfg: ModelConfig) -> Dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _attach_introspection(llm: Llama) -> None:
    def get_last_timings():
        try:
            t = getattr(llm, "get_timings", None)
            if callable(t):
                v = t()
                if isinstance(v, dict):
                    return v
        except Exception:
            pass
        try:
            v = getattr(llm, "timings", None)
            if isinstance(v, dict):
                return v
        except Exception:
            pass
        try:
            v = getattr(llm, "perf", None)
            if isinstance(v, dict):
                return v
        except Exception:
            pass
        return None
    try:
        setattr(llm, "get_last_timings", get_last_timings)
    except Exception:
        pass

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            _llm = None
    except Exception:
        _llm = None

def current_model_info() -> Dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
        }

def ensure_ready() -> None:
    global _llm, _cfg
    with _runtime_lock:
        if _llm is not None:
            return
        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected. Load one via /models/load or set LOCALAI_MODEL_PATH.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")
        _llm = Llama(**_build_kwargs(cfg))
        _attach_introspection(_llm)
        _cfg = cfg

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: Dict[str, Any]) -> Dict[str, Any]:
    global _llm, _cfg
    with _runtime_lock:
        s = read_settings()
        s.update({k:v for k,v in config_patch.items() if v is not None})
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")
        _close_llm()
        _llm = Llama(**_build_kwargs(cfg))
        _attach_introspection(_llm)
        _cfg = cfg
        write_settings(asdict(cfg))
        return current_model_info()

def unload_model() -> None:
    global _llm
    with _runtime_lock:
        _close_llm()

def list_local_models() -> List[Dict[str, Any]]:
    s = read_settings()
    root = Path(s.get("modelsDir") or "")
    root.mkdir(parents=True, exist_ok=True)
    out: List[Dict[str, Any]] = []
    for p in root.rglob("*.gguf"):
        try:
            out.append({
                "path": str(p.resolve()),
                "sizeBytes": p.stat().st_size,
                "name": p.name,
                "rel": str(p.relative_to(root)),
            })
        except Exception:
            pass
    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/services/__init__.py =====



# ===== aimodel/file_read/services/attachments.py =====

# aimodel/file_read/services/attachments.py
from __future__ import annotations
from typing import Any, Iterable, Optional, List


def att_get(att: Any, key: str, default=None):
    try:
        return att.get(key, default)          
    except AttributeError:
        return getattr(att, key, default)    


def join_attachment_names(attachments: Optional[Iterable[Any]]) -> str:
    if not attachments:
        return ""
    names: List[str] = [att_get(a, "name") for a in attachments]  
    names = [n for n in names if n]
    return ", ".join(names)

# ===== aimodel/file_read/services/budget.py =====

# aimodel/file_read/services/budget.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
from .context_window import current_n_ctx, estimate_tokens

@dataclass
class TurnBudget:
    n_ctx: int
    input_tokens_est: Optional[int]
    requested_out_tokens: int
    clamped_out_tokens: int
    clamp_margin: int
    reserved_system_tokens: Optional[int] = None
    available_for_out_tokens: Optional[int] = None
    headroom_tokens: Optional[int] = None
    overage_tokens: Optional[int] = None
    reason: str = "ok"

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

def analyze_budget(
    llm: Any,
    messages: List[Dict[str, str]],
    *,
    requested_out_tokens: int,
    clamp_margin: int,
    reserved_system_tokens: Optional[int] = None,
) -> TurnBudget:
    n_ctx = current_n_ctx()
    try:
        inp = estimate_tokens(llm, messages)
    except Exception:
        inp = None

    rst = int(reserved_system_tokens or 0)
    min_out = 16

    if inp is None:
        available = None
        clamped = requested_out_tokens
        headroom = None
        overage = None
        reason = "input_tokens_unknown"
    else:
        available_raw = n_ctx - inp - clamp_margin - rst
        available = max(min_out, available_raw)
        clamped = max(min_out, min(requested_out_tokens, available))
        headroom = max(0, available - clamped)
        overage = max(0, requested_out_tokens - available)
        reason = "ok" if overage == 0 else "requested_exceeds_available"

    return TurnBudget(
        n_ctx=n_ctx,
        input_tokens_est=inp,
        requested_out_tokens=requested_out_tokens,
        clamped_out_tokens=clamped,
        clamp_margin=clamp_margin,
        reserved_system_tokens=reserved_system_tokens,
        available_for_out_tokens=available,
        headroom_tokens=headroom,
        overage_tokens=overage,
        reason=reason,
    )

# ===== aimodel/file_read/services/cancel.py =====

from __future__ import annotations
import asyncio
from threading import Event
from typing import Dict
from ..core.settings import SETTINGS

eff = SETTINGS.effective()
GEN_SEMAPHORE = asyncio.Semaphore(int(eff["gen_semaphore_permits"]))
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

# aimodel/file_read/services/context_window.py
from __future__ import annotations
from typing import List, Dict, Optional, Tuple, Any
from ..utils.streaming import safe_token_count_messages
from ..runtime.model_runtime import current_model_info
from ..core.settings import SETTINGS

def estimate_tokens(llm, messages: List[Dict[str, str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    eff = SETTINGS.effective()
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or eff["nctx_fallback"])
    except Exception:
        return int(eff["nctx_fallback"])

def clamp_out_budget(
    *, llm, messages: List[Dict[str, str]], requested_out: int, margin: int = 32, reserved_system_tokens: Optional[int] = None
) -> Tuple[int, Optional[int]]:
    eff = SETTINGS.effective()
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = int(eff["token_estimate_fallback"])
    n_ctx = current_n_ctx()
    rst = int(reserved_system_tokens or 0)
    min_out = int(eff.get("min_out_tokens", 16))
    available = max(min_out, n_ctx - prompt_est - margin - rst)
    safe_out = max(min_out, min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

def compute_budget_view(
    llm,
    messages: List[Dict[str, str]],
    requested_out: Optional[int] = None,
    clamp_margin: Optional[int] = None,
    reserved_system_tokens: Optional[int] = None,
) -> Dict[str, Any]:
    eff = SETTINGS.effective()
    n_ctx = current_n_ctx()
    margin = int(clamp_margin if clamp_margin is not None else eff.get("clamp_margin", 32))
    rst = int(reserved_system_tokens if reserved_system_tokens is not None else eff.get("reserved_system_tokens", 0))
    min_out = int(eff.get("min_out_tokens", 16))
    default_out = int(eff.get("out_budget", 512))
    req_out = int(requested_out if requested_out is not None else default_out)

    inp_opt = estimate_tokens(llm, messages)
    if inp_opt is None:
        try:
            prompt_est = safe_token_count_messages(llm, messages)
        except Exception:
            prompt_est = int(eff["token_estimate_fallback"])
    else:
        prompt_est = int(inp_opt)

    available = max(min_out, n_ctx - prompt_est - margin - rst)
    out_budget_chosen = max(min_out, min(req_out, available))
    over_by_tokens = max(0, (prompt_est + req_out + margin + rst) - n_ctx)
    usable_ctx = max(0, n_ctx - margin - rst)

    return {
        "modelCtx": n_ctx,
        "clampMargin": margin,
        "usableCtx": usable_ctx,
        "reservedSystemTokens": rst,
        "inputTokensEst": prompt_est,
        "outBudgetChosen": out_budget_chosen,
        "outBudgetDefault": default_out,
        "outBudgetRequested": req_out,
        "outBudgetMaxAllowed": available,
        "overByTokens": over_by_tokens,
        "minOutTokens": min_out,
        "queueWaitSec": None,
    }

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations
from typing import AsyncGenerator, AsyncIterator, Dict
from dataclasses import asdict
from fastapi.responses import StreamingResponse
import time
from ..core.settings import SETTINGS
from ..utils.streaming import RUNJSON_START, RUNJSON_END
from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .streaming_worker import run_stream as _run_stream
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream

from .generate_pipeline import prepare_generation_with_telemetry

async def generate_stream_flow(data, request) -> StreamingResponse:
    prep = await prepare_generation_with_telemetry(data)
    eff = SETTINGS.effective(session_id=prep.session_id)
    stop_ev = cancel_event(prep.session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        q_start = time.perf_counter()
        async with GEN_SEMAPHORE:
            try:
                q_wait = time.perf_counter() - q_start
                if isinstance(prep.budget_view, dict):
                    prep.budget_view["queueWaitSec"] = round(q_wait, 3)
            except Exception:
                pass

            mark_active(prep.session_id, +1)
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
                s = chunk_bytes.decode("utf-8", errors="ignore")
                if RUNJSON_START in s and RUNJSON_END in s:
                    return
                if s.strip() == eff["stopped_line_marker"]:
                    return
                out_buf.extend(chunk_bytes)

            try:
                async for chunk in run_stream(
                    llm=prep.llm,
                    messages=prep.packed,
                    out_budget=prep.out_budget,
                    stop_ev=stop_ev,
                    request=request,
                    temperature=prep.temperature,
                    top_p=prep.top_p,
                    input_tokens_est=prep.input_tokens_est,
                    t0_request=prep.t_request_start,
                    budget_view=prep.budget_view,
                ):
                    if isinstance(chunk, (bytes, bytearray)):
                        _accum_visible(chunk)
                    else:
                        _accum_visible(chunk.encode("utf-8"))
                    yield chunk
            finally:
                try:
                    full_text = out_buf.decode("utf-8", errors="ignore").strip()
                    start = full_text.find(RUNJSON_START)
                    if start != -1:
                        end = full_text.find(RUNJSON_END, start)
                        if end != -1:
                            full_text = (full_text[:start] + full_text[end + len(RUNJSON_END):]).strip()
                    if full_text:
                        prep.st["recent"].append({"role": "assistant", "content": full_text})
                except Exception:
                    pass

                try:
                    from ..store import apply_pending_for
                    apply_pending_for(prep.session_id)
                except Exception:
                    pass

                try:
                    from ..store import list_messages as store_list_messages
                    from ..workers.retitle_worker import enqueue as enqueue_retitle
                    msgs = store_list_messages(prep.session_id)
                    last_seq = max((int(m.id) for m in msgs), default=0)
                    enqueue_retitle(prep.session_id, [asdict(m) for m in msgs], job_seq=last_seq)
                except Exception:
                    pass

                mark_active(prep.session_id, -1)

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "Connection": "keep-alive"},
    )

async def cancel_session(session_id: str) -> Dict[str, bool]:
    from .cancel import cancel_event
    cancel_event(session_id).set()
    return {"ok": True}

async def cancel_session_alias(session_id: str) -> Dict[str, bool]:
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/generate_pipeline.py =====

# aimodel/file_read/services/generate_pipeline.py
from __future__ import annotations
import time
from typing import Any, Dict, List, Optional
from ..core.settings import SETTINGS
from ..runtime.model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody
from .session_io import handle_incoming
from .packing import build_system_text, pack_with_rollup
from .prompt_utils import chars_len
from .router_text import compose_router_text
from .attachments import att_get
from ..web.router_ai import decide_web_and_fetch
from ..rag.retrieve_pipeline import build_rag_block_session_only_with_telemetry
from .generate_pipeline_support import (
    Prep,
    _bool,
    _approx_block_tokens,
)
from ..core.packing_memory_core import PACK_TELEMETRY
from .generate_pipeline_part2 import _finish_prepare_generation_with_telemetry

async def prepare_generation_with_telemetry(data: ChatBody) -> Prep:
    ensure_ready()
    llm = get_llm()
    t_request_start = time.perf_counter()
    eff0 = SETTINGS.effective()
    session_id = data.sessionId or eff0["default_session_id"]
    eff = SETTINGS.effective(session_id=session_id)

    rag_global_enabled = bool(eff.get("rag_global_enabled", True))
    rag_session_enabled = bool(eff.get("rag_session_enabled", True))
    force_session_only = bool(eff.get("rag_force_session_only")) or (not rag_global_enabled)

    temperature = float(eff["default_temperature"] if getattr(data, "temperature") is None else data.temperature)
    top_p = float(eff["default_top_p"] if getattr(data, "top_p") is None else data.top_p)
    out_budget_req = int(eff["default_max_tokens"] if getattr(data, "max_tokens") is None else data.max_tokens)
    auto_web = _bool(eff["default_auto_web"])
    if getattr(data, "autoWeb") is not None:
        auto_web = _bool(data.autoWeb)
    web_k = int(eff["default_web_k"] if getattr(data, "webK") is None else data.webK)
    web_k = max(int(eff["web_k_min"]), min(web_k, int(eff["web_k_max"])))
    auto_rag = _bool(eff["default_auto_rag"])
    if getattr(data, "autoRag") is not None:
        auto_rag = _bool(data.autoRag)
    model_ctx = int(eff["model_ctx"])

    incoming = [
        {
            "role": m.role,
            "content": m.content,
            "attachments": getattr(m, "attachments", None),
        }
        for m in (data.messages or [])
    ]
    print(f"[PIPE] incoming_msgs={len(incoming)}")
    latest_user = next((m for m in reversed(incoming) if m["role"] == "user"), {})
    latest_user_text = (latest_user.get("content") or "").strip()
    atts = (latest_user.get("attachments") or [])
    has_atts = bool(atts)
    print(f"[PIPE] latest_user_text_len={len(latest_user_text)} has_atts={has_atts} att_count={len(atts)}")

    if not latest_user_text and has_atts:
        names = [att_get(a, "name") for a in atts]
        names = [n for n in names if n]
        latest_user_text = "User uploaded: " + (", ".join(names) if names else "files")

    st = handle_incoming(session_id, incoming)

    base_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    router_text = compose_router_text(
        st.get("recent", []),
        str(base_user_text or ""),
        st.get("summary", "") or "",
        tail_turns=int(eff["router_tail_turns"]),
        summary_chars=int(eff["router_summary_chars"]),
        max_chars=int(eff["router_max_chars"]),
    )

    telemetry: Dict[str, Any] = {"web": {}, "rag": {}, "pack": {}, "prepSec": round(time.perf_counter() - t_request_start, 6)}
    ephemeral_once: List[Dict[str, str]] = []
    telemetry["web"]["injectElapsedSec"] = 0.0
    telemetry["web"]["ephemeralBlocks"] = 0

    try:
        web_block: Optional[str] = None
        web_tel: Dict[str, Any] = {}
        if auto_web and not (has_atts and bool(eff.get("disable_global_rag_on_attachments"))):
            res = await decide_web_and_fetch(llm, router_text, k=web_k)
            if isinstance(res, tuple):
                web_block = res[0] if len(res) > 0 else None
                tel_candidate = res[1] if len(res) > 1 else None
                if isinstance(tel_candidate, dict):
                    web_tel = tel_candidate
            elif isinstance(res, str):
                web_block = res
            else:
                web_block = None
        telemetry["web"].update(web_tel or {})
        need_flag = (web_tel or {}).get("needed")
        injected_candidate = isinstance(web_block, str) and bool(web_block.strip())
        if (need_flag is True) and (not injected_candidate):
            try:
                from ..web.orchestrator import build_web_block
                fb, fb_tel = await build_web_block(router_text, k=web_k)
                print(f"[PIPE][WEB] orchestrator block preview: {fb[:200]!r}" if fb else "[PIPE][WEB] orchestrator returned no block")
                print(f"[PIPE][WEB] orchestrator telemetry: {fb_tel}")
                if fb and fb.strip():
                    web_block = fb
                    injected_candidate = True
            except Exception as e:
                print(f"[PIPE][WEB] orchestrator fallback error: {e}")
        if injected_candidate:
            t0_inject = time.perf_counter()
            web_text = str(eff["web_block_preamble"]) + "\n\n" + web_block.strip()

            max_chars = int(eff.get("web_inject_max_chars") or 0)
            if max_chars > 0 and len(web_text) > max_chars:
                web_text = web_text[:max_chars]

            telemetry["web"]["blockChars"] = len(web_text)
            tok = _approx_block_tokens(llm, "assistant", web_text)
            if tok is not None:
                telemetry["web"]["blockTokensApprox"] = tok
            telemetry["web"]["injected"] = True

            ephemeral_only = bool(eff.get("web_ephemeral_only", True))
            telemetry["web"]["ephemeral"] = ephemeral_only
            telemetry["web"]["droppedFromSummary"] = ephemeral_only

            PACK_TELEMETRY["ignore_ephemeral_in_summary"] = ephemeral_only

            ephemeral_once.append({
                "role": "assistant",
                "content": web_text,
                "_ephemeral": True if ephemeral_only else False,
                "_source": "web"
            })

            telemetry["web"]["injectElapsedSec"] = round(time.perf_counter() - t0_inject, 6)
        else:
            telemetry["web"]["injected"] = False
            telemetry["web"]["injectElapsedSec"] = 0.0
    except Exception as e:
        print(f"[PIPE][WEB] error: {e}")
        telemetry["web"].setdefault("injected", False)
        telemetry["web"].setdefault("injectElapsedSec", 0.0)

    telemetry["web"]["ephemeralBlocks"] = len(ephemeral_once)
    print(f"[PIPE] has_atts={has_atts} disable_global_rag_on_attachments={bool(eff.get('disable_global_rag_on_attachments'))}")

    if has_atts and bool(eff.get("disable_global_rag_on_attachments")):
        att_names = [att_get(a, "name") for a in atts if att_get(a, "name")]
        query_for_atts = (base_user_text or "").strip() or " ".join(att_names) or "document"
        print(f"[PIPE] session-only RAG path query_for_atts={query_for_atts!r} att_names={att_names}")
        t0_att = time.perf_counter()
        try:
            att_block, att_tel = build_rag_block_session_only_with_telemetry(query_for_atts, session_id)
            print(f"[PIPE][RAG] session-only query: {query_for_atts!r}")
            print(f"[PIPE][RAG] session-only block preview: {att_block[:200]!r}" if att_block else "[PIPE][RAG] no session-only block")
        except Exception:
            att_block, att_tel = (None, {})
        if att_tel:
            telemetry["rag"].update(att_tel)
        print(f"[PIPE] session-only RAG built={bool(att_block)} block_chars={len(att_block or '')}")
        if att_block:
            rag_text = str(eff["rag_block_preamble"]) + "\n\n" + att_block
            telemetry["rag"]["sessionOnly"] = True
            telemetry["rag"]["mode"] = "session-only"
            telemetry["rag"]["blockChars"] = len(rag_text)
            tok = _approx_block_tokens(llm, "assistant", rag_text)
            if tok is not None:
                telemetry["rag"]["sessionOnlyTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            ephemeral_once.append({"role": "assistant", "content": rag_text, "_ephemeral": True})
        else:
            telemetry["rag"]["sessionOnly"] = False
            telemetry["rag"].setdefault("injected", False)
        telemetry["rag"]["sessionOnlyBuildSec"] = round(time.perf_counter() - t0_att, 6)

    system_text = build_system_text()
    t_pack0 = time.perf_counter()
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )
    telemetry["pack"]["packSec"] = round(time.perf_counter() - t_pack0, 6)

    return await _finish_prepare_generation_with_telemetry(
        llm, eff, data, st, router_text, latest_user_text, base_user_text, has_atts,
        force_session_only, rag_session_enabled, rag_global_enabled, auto_rag,
        telemetry, packed, out_budget_req, temperature, top_p, t_request_start, session_id
    )

# ===== aimodel/file_read/services/generate_pipeline_part2.py =====

# aimodel/file_read/services/generate_pipeline_part2.py
from __future__ import annotations
import time
from typing import Any, Dict, Optional, List
from .prompt_utils import chars_len
from .generate_pipeline_support import (
    Prep, _tok_count, _approx_block_tokens, _diff_find_inserted_block,
    _web_breakdown, _web_unattributed, _enforce_fit
)
from ..rag.router_ai import decide_rag
from .packing import maybe_inject_rag_block
from .session_io import persist_summary
from .budget import analyze_budget
from ..core.packing_memory_core import PACK_TELEMETRY
from .context_window import clamp_out_budget

async def _finish_prepare_generation_with_telemetry(
    llm, eff, data, st, router_text, latest_user_text, base_user_text, has_atts,
    force_session_only, rag_session_enabled, rag_global_enabled, auto_rag,
    telemetry, packed, out_budget_req, temperature, top_p, t_request_start, session_id
) -> Prep:
    must_inject_session = bool(
        force_session_only and rag_session_enabled and not has_atts and not telemetry.get("web", {}).get("ephemeralBlocks")
    )

    rag_router_allowed = ((rag_session_enabled or rag_global_enabled) and not (
        has_atts and bool(eff["disable_global_rag_on_attachments"])
    )) or must_inject_session

    # NEW: if web router said it's needed or we already injected web, skip RAG router
    web_needed = bool((telemetry.get("web") or {}).get("needed"))
    web_injected = bool((telemetry.get("web") or {}).get("injected"))
    if web_needed or web_injected:
        rag_router_allowed = False
        telemetry.setdefault("rag", {})
        telemetry["rag"]["routerSkipped"] = True
        telemetry["rag"]["routerSkippedReason"] = "web_needed" if web_needed else "web_block_present"

    ephemeral_once: List[Dict[str, str]] = []
    if rag_router_allowed and bool(eff["rag_enabled"]) and not ephemeral_once:
        rag_need = False
        rag_query: Optional[str] = None

        if must_inject_session:
            rag_need = True
            rag_query = (latest_user_text or base_user_text or "").strip()
            telemetry["rag"]["routerDecideSec"] = 0.0
            telemetry["rag"]["routerNeeded"] = True
            telemetry["rag"]["routerForcedSession"] = True
            telemetry["rag"]["routerQuery"] = rag_query
        else:
            t_router0 = time.perf_counter()
            if auto_rag:
                try:
                    rag_need, rag_query = decide_rag(llm, router_text)
                except Exception:
                    rag_need, rag_query = (False, None)
            telemetry["rag"]["routerDecideSec"] = round(time.perf_counter() - t_router0, 6)
            telemetry["rag"]["routerNeeded"] = bool(rag_need)
            if rag_query is not None:
                telemetry["rag"]["routerQuery"] = rag_query

        skip_rag = bool(ephemeral_once) or (not rag_need)
        tokens_before = _tok_count(llm, packed)
        t_inject0 = time.perf_counter()

        print(f"[PIPE][RAG] router query: {rag_query!r} skip_rag={skip_rag}")
        res = maybe_inject_rag_block(
            packed,
            session_id=session_id,
            skip_rag=skip_rag,
            rag_query=rag_query,
            force_session_only=force_session_only,
        )

        telemetry["rag"]["injectBuildSec"] = round(time.perf_counter() - t_inject0, 6)

        if isinstance(res, tuple):
            packed2 = res[0]
            tel = res[1] if len(res) > 1 and isinstance(res[1], dict) else {}
            block_text = res[2] if len(res) > 2 and isinstance(res[2], str) else None
        else:
            packed2 = res
            tel = {}
            block_text = None

        if tel:
            telemetry["rag"].update(tel)
        if block_text:
            print(f"[PIPE][RAG] injected block preview: {block_text[:200]!r}")
            telemetry["rag"]["blockChars"] = len(block_text)
            tok = _approx_block_tokens(llm, "use