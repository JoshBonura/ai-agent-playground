
# ===== aimodel/file_read/__init__.py =====

from .paths import app_data_dir, read_settings, write_settings
from .model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/chats.py =====

# ===== aimodel/file_read/api/chats.py =====

from __future__ import annotations
from dataclasses import asdict
from typing import List, Optional, Dict

from fastapi import APIRouter
from pydantic import BaseModel
from ..services.cancel import GEN_SEMAPHORE
from ..retitle_worker import enqueue as enqueue_retitle  # ✅ import the enqueuer

from ..core.schemas import (
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    MergeChatReq,
    EditMessageReq,
)

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    merge_chat as store_merge_chat,
    merge_chat_new as store_merge_chat_new,
    edit_message as edit_message,
)

router = APIRouter()

# ---------- Routes ----------
@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)

@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]

# ✅ Single append route; triggers retitle only after assistant turn is persisted
@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, body: Dict[str, str]):
    role = (body.get("role") or "user").strip()
    content = (body.get("content") or "").rstrip()
    row = store_append(session_id, role, content)

    if role == "assistant":
        try:
            msgs = store_list_messages(session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            enqueue_retitle(session_id, [asdict(m) for m in msgs], job_seq=last_seq)
            # print(f"[retitle] ENQUEUE session={session_id} job_seq={last_seq} msgs={len(msgs)}")
        except Exception:
            pass

    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}

@router.post("/api/chats/merge")
async def api_merge_chat(req: MergeChatReq):
    if req.newChat:
        new_id, merged = store_merge_chat_new(req.sourceId, req.targetId)
        return {"newChatId": new_id, "mergedCount": len(merged)}
    else:
        merged = store_merge_chat(req.sourceId, req.targetId)
        return {"mergedCount": len(merged)}

@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

# aimodel/file_read/api/generate_router.py
from __future__ import annotations
from fastapi import APIRouter, Body, Request
from ..core.schemas import ChatBody
from ..services.generate_flow import generate_stream_flow, cancel_session, cancel_session_alias
from ..services.cancel import is_active  # re-export for back-compat

router = APIRouter()

@router.post("/generate/stream")
async def generate_stream(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

# legacy alias (kept identical)
@router.post("/api/ai/generate/stream")
async def generate_stream_alias(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

@router.post("/cancel/{session_id}")
async def _cancel_session(session_id: str):
    return await cancel_session(session_id)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/models.py =====

from __future__ import annotations
from typing import Optional, Dict
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..paths import read_settings, write_settings
from ..model_runtime import (
    list_local_models, current_model_info,
    load_model, unload_model
)

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/app.py =====

# aimodel/file_read/app.py
from __future__ import annotations
import os, asyncio, atexit
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .paths import bootstrap
from .retitle_worker import start_worker

from .api.models import router as models_router
from .api.chats import router as chats_router
from .model_runtime import load_model
from .api.generate_router import router as generate_router
from .services.cancel import is_active

bootstrap()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("APP_CORS_ORIGIN", "http://localhost:5173")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(models_router)
app.include_router(chats_router)
app.include_router(generate_router)


@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")

    asyncio.create_task(start_worker(), name="retitle_worker")

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/memory.py =====

# aimodel/file_read/memory.py
from __future__ import annotations
import math
from typing import Dict, List
from collections import deque
from ..model_runtime import get_llm     # <-- use runtime, not a global llm
from .style import STYLE_SYS
from ..store import get_summary as store_get_summary

SESSIONS: Dict[str, Dict] = {}

def get_session(session_id: str):
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=50),
        "style": STYLE_SYS,
        "short": False,
        "bullets": False,
    })
    # seed once from storage if empty
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
        except Exception:
            pass
    return st

def approx_tokens(text: str) -> int:
    return max(1, math.ceil(len(text) / 4))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    return sum(approx_tokens(m["content"]) + 4 for m in msgs)

def summarize_chunks(chunks: List[Dict[str,str]]) -> str:
    text = "\n".join(f'{m["role"]}: {m["content"]}' for m in chunks)
    prompt = f"Summarize crisply the key points and decisions:\n\n{text}\n\nSummary:"
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[{"role":"system","content":"Be concise."},
                  {"role":"user","content":prompt}],
        max_tokens=240, temperature=0.2, top_p=0.9, stream=False
    )
    return out["choices"][0]["message"]["content"].strip()

def build_system(style: str, short: bool, bullets: bool) -> str:
    parts = [STYLE_SYS]
    if style and style != STYLE_SYS:
        parts.append(style)
    if short:
        parts.append("Keep answers extremely brief: max 2 sentences OR 5 short bullets.")
    if bullets:
        parts.append("Use bullet points when possible; each bullet under 15 words.")
    parts.append("Always follow the user's most recent style instructions.")
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    input_budget = max_ctx - out_budget - 256
    sys_text = build_system(style, short, bullets)

    # Put “system” guidance into a *user* prologue for Mistral-Instruct
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": f"Conversation summary so far:\n{summary}"})

    packed = prologue + list(recent)
    return packed, input_budget

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    while count_prompt_tokens(packed) > input_budget and len(recent) > 6:
        peel = []
        for _ in range(max(4, len(recent) // 5)):
            peel.append(recent.popleft())
        new_sum = summarize_chunks(peel)
        summary = (summary + "\n" + new_sum).strip() if summary else new_sum

        # Rebuild using *user* role, matching pack_messages
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": f"Conversation summary so far:\n{summary}"},
            *list(recent),
        ]
    return packed, summary

# ===== aimodel/file_read/core/schemas.py =====

from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class MergeChatReq(BaseModel):
    sourceId: str
    targetId: Optional[str] = None
    newChat: bool = False

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str    

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None
    max_tokens: Optional[int] = 512
    temperature: float = 0.7
    top_p: float = 0.95
    # NEW:
    autoWeb: bool = True
    webK: int = 3

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple

STYLE_SYS = (
    "You are a helpful assistant. "
    "Always follow the user's explicit instructions carefully and exactly. "
    "Do not repeat yourself. Stay coherent and complete."
)

PAT_TALK_LIKE = re.compile(r"\btalk\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_RESPOND_LIKE = re.compile(r"\brespond\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_BE = re.compile(r"\bbe\s+(?P<style>[^.;\n]+)", re.I)
PAT_FROM_NOW = re.compile(r"\bfrom\s+now\s+on[, ]+\s*(?P<style>[^.;\n]+)", re.I)

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    t = user_text.strip()
    style_match = (
        PAT_TALK_LIKE.search(t)
        or PAT_RESPOND_LIKE.search(t)
        or PAT_FROM_NOW.search(t)
        or PAT_BE.search(t)
    )
    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = (
            f"You must talk like {raw}. "
            f"Stay in character but remain helpful and accurate. "
            f"Follow the user's latest style instructions."
        )
    return style_inst, False, False

# ===== aimodel/file_read/model_runtime.py =====

# aimodel/file_read/model_runtime.py
from __future__ import annotations
import os
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Optional, List

from .paths import read_settings, write_settings

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath","")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None,"") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None,"") else None),
        )

_runtime_lock = RLock()
_llm: Optional[Llama] = None
_cfg: Optional[ModelConfig] = None

def _build_kwargs(cfg: ModelConfig) -> Dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    # advanced optional tuning
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            # llama_cpp doesn't expose explicit close; allow GC
            _llm = None
    except Exception:
        _llm = None

def current_model_info() -> Dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
        }

def ensure_ready() -> None:
    """
    Lazily load a model based on current settings if nothing is loaded.
    """
    global _llm, _cfg
    with _runtime_lock:
        if _llm is not None:
            return
        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected. Load one via /models/load or set LOCALAI_MODEL_PATH.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")
        _llm = Llama(**_build_kwargs(cfg))
        _cfg = cfg

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: Dict[str, Any]) -> Dict[str, Any]:
    """
    Load/swap to a new model with given config fields (any subset).
    Persists to settings.json so the choice survives restarts.
    """
    global _llm, _cfg
    with _runtime_lock:
        s = read_settings()
        s.update({k:v for k,v in config_patch.items() if v is not None})
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")

        # swap
        _close_llm()
        _llm = Llama(**_build_kwargs(cfg))
        _cfg = cfg
        write_settings(asdict(cfg))
        return current_model_info()

def unload_model() -> None:
    """
    Unload current model, keep settings as-is.
    """
    global _llm
    with _runtime_lock:
        _close_llm()

def list_local_models() -> List[Dict[str, Any]]:
    """
    Scan modelsDir for .gguf files and return a lightweight listing.
    """
    s = read_settings()
    root = Path(s.get("modelsDir") or "")
    root.mkdir(parents=True, exist_ok=True)
    out: List[Dict[str, Any]] = []
    for p in root.rglob("*.gguf"):
        try:
            out.append({
                "path": str(p.resolve()),
                "sizeBytes": p.stat().st_size,
                "name": p.name,
                "rel": str(p.relative_to(root)),
            })
        except Exception:
            pass
    # sort largest first (usually higher quant or full precision)
    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/policy.py/capability_probe.py =====

# aimodel/file_read/services/capability_probe.py
from __future__ import annotations
import asyncio
import json
import os
import platform
import re
import shutil
import subprocess
import threading
import time
from dataclasses import dataclass, asdict, field
from typing import Any, Dict, List, Optional, Tuple

# Optional imports – all guarded
try:
    import psutil  # type: ignore
except Exception:  # pragma: no cover
    psutil = None  # type: ignore

# Local runtime hooks
try:
    from ..model_runtime import current_model_info
except Exception:  # pragma: no cover
    def current_model_info() -> Dict[str, Any]:  # type: ignore
        return {"loaded": False, "config": None}

# --------------------------------------------------------------------------------------
# Data models
# --------------------------------------------------------------------------------------

@dataclass
class GPUInfo:
    index: int
    name: str
    vram_total_mb: int
    vram_free_mb: int
    driver: Optional[str] = None
    temperature_c: Optional[float] = None

@dataclass
class CapabilityReport:
    # Hardware/OS
    os: str
    arch: str
    backend: str  # "cuda"|"rocm"|"mps"|"cpu"
    cpu_cores: int
    load_1m: Optional[float]
    ram_total_mb: int
    ram_free_mb: int

    # GPUs
    gpu_count: int
    gpus: List[GPUInfo] = field(default_factory=list)

    # Model/runtime config snapshot
    model_ctx_tokens: int
    n_threads: int
    n_gpu_layers: int
    n_batch: int

    # App/runtime signals (optional, filled by integrators)
    active_sessions: int = 0
    p95_latency_ms_decode: Optional[float] = None
    p95_latency_ms_tokenize: Optional[float] = None
    tokenizer_tps: Optional[float] = None

    ts_unix: float = field(default_factory=lambda: time.time())

    def to_dict(self) -> Dict[str, Any]:
        d = asdict(self)
        # dataclasses of GPUs are already dicts because of asdict recursion
        return d

# --------------------------------------------------------------------------------------
# Helpers – CPU/RAM
# --------------------------------------------------------------------------------------

def _cpu_cores() -> int:
    try:
        return os.cpu_count() or 1
    except Exception:
        return 1


def _load_1m() -> Optional[float]:
    try:
        if hasattr(os, "getloadavg"):
            return float(os.getloadavg()[0])
    except Exception:
        pass
    return None


def _ram_tot_free_mb() -> Tuple[int, int]:
    # Prefer psutil when present
    if psutil is not None:
        try:
            vm = psutil.virtual_memory()  # type: ignore[attr-defined]
            return int(vm.total // (1024 * 1024)), int(vm.available // (1024 * 1024))
        except Exception:
            pass

    # Fallbacks by OS
    try:
        if platform.system() == "Linux":
            total = free = 0
            with open("/proc/meminfo", "r", encoding="utf-8") as f:
                for line in f:
                    if line.startswith("MemTotal:"):
                        total = int(re.findall(r"\d+", line)[0]) // 1024
                    elif line.startswith("MemAvailable:"):
                        free = int(re.findall(r"\d+", line)[0]) // 1024
            if total and free:
                return total, free
        elif platform.system() == "Darwin":
            # macOS: use vm_stat (pages) as a rough estimate
            out = subprocess.check_output(["vm_stat"], text=True)
            m = re.findall(r"page size of (\d+) bytes", out)
            page = int(m[0]) if m else 4096
            pages_free = 0
            for line in out.splitlines():
                if line.startswith("Pages free") or line.startswith("Pages speculative"):
                    pages_free += int(re.findall(r"\d+", line)[0])
                if line.startswith("Pages active") or line.startswith("Pages inactive") or line.startswith("Pages wired down"):
                    # not used directly; we'll compute total via sysctl
                    pass
            total_bytes = int(subprocess.check_output(["sysctl", "-n", "hw.memsize"]).strip())
            free_bytes = pages_free * page
            return total_bytes // (1024 * 1024), free_bytes // (1024 * 1024)
        elif platform.system() == "Windows":
            out = subprocess.check_output(["wmic", "OS", "get", "TotalVisibleMemorySize,FreePhysicalMemory", "/Value"], text=True)
            kv = dict(
                (k.strip(), v.strip())
                for k, v in (line.split("=", 1) for line in out.splitlines() if "=" in line)
            )
            total_kib = int(kv.get("TotalVisibleMemorySize", "0") or 0)
            free_kib = int(kv.get("FreePhysicalMemory", "0") or 0)
            return total_kib // 1024, free_kib // 1024
    except Exception:
        pass

    # Ultimate fallback
    return 0, 0

# --------------------------------------------------------------------------------------
# Helpers – GPU detection
# --------------------------------------------------------------------------------------

_DEF_TIMEOUT = 2.5  # seconds for small probe calls


def _run(cmd: List[str]) -> Optional[str]:
    try:
        return subprocess.check_output(cmd, text=True, stderr=subprocess.DEVNULL, timeout=_DEF_TIMEOUT)
    except Exception:
        return None


def _probe_nvidia() -> Tuple[str, List[GPUInfo]]:
    """Return (driver, gpus[]) if nvidia-smi is available, else ("", [])."""
    if not shutil.which("nvidia-smi"):
        return "", []

    q = _run([
        "nvidia-smi",
        "--query-gpu=index,name,memory.total,memory.free,driver_version,temperature.gpu",
        "--format=csv,noheader,nounits",
    ])
    if not q:
        return "", []

    gpus: List[GPUInfo] = []
    driver = None
    for line in q.splitlines():
        parts = [p.strip() for p in line.split(",")]
        if len(parts) < 5:
            continue
        try:
            idx = int(parts[0])
            name = parts[1]
            total_mb = int(parts[2])
            free_mb = int(parts[3])
            driver = parts[4]
            temp_c = float(parts[5]) if len(parts) > 5 else None
            gpus.append(GPUInfo(index=idx, name=name, vram_total_mb=total_mb, vram_free_mb=free_mb, driver=driver, temperature_c=temp_c))
        except Exception:
            continue
    return (driver or ""), gpus


def _probe_rocm() -> Tuple[str, List[GPUInfo]]:
    # Try rocm-smi JSON first; fall back to text parse
    exe = shutil.which("rocm-smi") or shutil.which("amd-smi")
    if not exe:
        return "", []

    out = _run([exe, "--showproductname", "--showid", "--showmeminfo", "vram", "--showtemp"])
    if not out:
        return "", []

    # Very loose parsing (rocm-smi text varies)
    gpus: List[GPUInfo] = []
    total_mb = free_mb = None
    name = None
    idx = -1
    for line in out.splitlines():
        if re.search(r"GPU\s*\[?\d+\]?", line):
            idx += 1
            total_mb = free_mb = None
            name = None
        m_name = re.search(r"Product\s*Name\s*:\s*(.*)$", line)
        if m_name:
            name = m_name.group(1).strip()
        m_total = re.search(r"VRAM Total:\s*(\d+)\s*MiB", line)
        if m_total:
            total_mb = int(m_total.group(1))
        m_free = re.search(r"VRAM Free:\s*(\d+)\s*MiB", line)
        if m_free:
            free_mb = int(m_free.group(1))
        if name and total_mb is not None and free_mb is not None:
            gpus.append(GPUInfo(index=max(idx, len(gpus)), name=name, vram_total_mb=total_mb, vram_free_mb=free_mb))
            name = None
            total_mb = free_mb = None
    return ("rocm" if gpus else ""), gpus


def _probe_mps() -> Tuple[str, List[GPUInfo]]:
    if platform.system() != "Darwin":
        return "", []
    # Quick check for Apple GPU via system_profiler
    if not shutil.which("system_profiler"):
        return "", []
    out = _run(["system_profiler", "SPDisplaysDataType"])
    if not out:
        return "", []
    # We can't get VRAM free reliably here; report total per adapter when available
    gpus: List[GPUInfo] = []
    idx = -1
    for block in out.split("\n\n"):
        if "Chipset Model" in block or "Apple" in block:
            idx += 1
            name_m = re.search(r"Chipset Model:\s*(.*)", block)
            vram_m = re.search(r"VRAM.*:\s*(\d+(?:\.\d+)?)\s*GB", block)
            name = name_m.group(1).strip() if name_m else "Apple GPU"
            total_mb = int(float(vram_m.group(1)) * 1024) if vram_m else 0
            gpus.append(GPUInfo(index=idx, name=name, vram_total_mb=total_mb, vram_free_mb=0))
    return ("mps" if gpus else ""), gpus


# --------------------------------------------------------------------------------------
# Probe – aggregate
# --------------------------------------------------------------------------------------

def _detect_backend_and_gpus() -> Tuple[str, List[GPUInfo]]:
    # Priority: CUDA > ROCm > MPS > CPU
    drv, gpus = _probe_nvidia()
    if gpus:
        return "cuda", gpus

    drv, gpus = _probe_rocm()
    if gpus:
        return "rocm", gpus

    drv, gpus = _probe_mps()
    if gpus:
        return "mps", gpus

    return "cpu", []


def _model_cfg_snapshot() -> Tuple[int, int, int, int]:
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return (
            int(cfg.get("nCtx") or 4096),
            int(cfg.get("nThreads") or 0),
            int(cfg.get("nGpuLayers") or 0),
            int(cfg.get("nBatch") or 0),
        )
    except Exception:
        return (4096, 0, 0, 0)


def probe_once() -> CapabilityReport:
    os_name = platform.system()
    arch = platform.machine()

    backend, gpus = _detect_backend_and_gpus()
    cpu_cores = _cpu_cores()
    load_1m = _load_1m()
    ram_total_mb, ram_free_mb = _ram_tot_free_mb()

    nctx, nthreads, ngpu_layers, nbatch = _model_cfg_snapshot()

    return CapabilityReport(
        os=os_name,
        arch=arch,
        backend=backend,
        cpu_cores=cpu_cores,
        load_1m=load_1m,
        ram_total_mb=ram_total_mb,
        ram_free_mb=ram_free_mb,
        gpu_count=len(gpus),
        gpus=gpus,
        model_ctx_tokens=nctx,
        n_threads=nthreads,
        n_gpu_layers=ngpu_layers,
        n_batch=nbatch,
    )

# --------------------------------------------------------------------------------------
# Public API – cached access + periodic refresh
# --------------------------------------------------------------------------------------

_refresh_interval_s = 12.0
_cache_ttl_s = 9.0
_latest_lock = threading.RLock()
_latest: Optional[CapabilityReport] = None
_task: Optional[asyncio.Task] = None


def collect_capabilities(force: bool = False) -> CapabilityReport:
    global _latest
    with _latest_lock:
        if not force and _latest and (time.time() - _latest.ts_unix) < _cache_ttl_s:
            return _latest
        rep = probe_once()
        _latest = rep
        return rep


async def _periodic_loop():  # pragma: no cover
    global _latest
    while True:
        try:
            rep = probe_once()
            with _latest_lock:
                _latest = rep
        except Exception:
            # Keep going on probe errors; next loop may succeed
            pass
        await asyncio.sleep(_refresh_interval_s)


def start_periodic_refresh(interval_s: float = 12.0, cache_ttl_s: float = 9.0) -> None:
    """Start a background task that keeps the capability snapshot fresh.

    Safe to call multiple times; it will start once.
    """
    global _task, _refresh_interval_s, _cache_ttl_s
    _refresh_interval_s = max(3.0, float(interval_s))
    _cache_ttl_s = max(1.0, float(cache_ttl_s))

    if _task and not _task.done():
        return

    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # No loop running (e.g., during unit tests); create a temporary loop thread
        def _runner():
            asyncio.run(_periodic_loop())
        t = threading.Thread(target=_runner, name="cap_probe", daemon=True)
        t.start()
        return

    _task = loop.create_task(_periodic_loop(), name="capability_probe")


# --------------------------------------------------------------------------------------
# Debug helpers
# --------------------------------------------------------------------------------------

def debug_dump() -> str:
    rep = collect_capabilities()
    return json.dumps(rep.to_dict(), indent=2)


# ===== aimodel/file_read/policy.py/policy_config.py =====

"""Adaptive policy configuration for routing, summarization, and budgets.

This module converts a CapabilityReport (from capability_probe.py) into a concrete
set of runtime policies used by the app: context budgeting, summarization
aggressiveness, web-search breadth, and streaming parameters.

Design goals
------------
- Deterministic, side‑effect free mapping from capabilities → knobs
- Safe minimums on low-spec systems; scale up gracefully on high-spec
- Central place for all heuristics and guardrails

How to use
----------
from capability_probe import collect_capabilities
from policy_config import compute_policy

report = collect_capabilities()               # snapshot of host + llama runtime
policy = compute_policy(report)               # PolicyBundle

# Example: sizing a generation
safe_out, prompt_est = policy.inference.clamp_for_ctx(
    prompt_tokens_est, requested_out_tokens
)

# Example: how many sources to fetch
k = policy.web.search_k

You can pass user/admin overrides to compute_policy(overrides=...).
"""
from __future__ import annotations

from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

# Optional: tolerate absence if capability_probe isn't wired yet
try:
    from capability_probe import CapabilityReport
except Exception:  # pragma: no cover
    @dataclass
    class CapabilityReport:  # minimal shim
        cpu_logical: int = 4
        cpu_score: float = 1.0
        ram_total_gb: float = 8.0
        vram_total_gb: Optional[float] = None
        gpu_backend: str = "none"
        llama: Dict[str, Any] | None = None


# -------------------------------
# Policy data classes
# -------------------------------

@dataclass
class InferencePolicy:
    # Context/budgeting
    n_ctx: int
    margin_tokens: int
    max_out_tokens_default: int

    def clamp_for_ctx(self, prompt_tokens_est: int, requested_out: int) -> tuple[int, int]:
        """Return (safe_out, prompt_tokens_est) with margin enforcement.
        Matches services/context_window.clamp_out_budget, but centralized.
        """
        n_ctx = max(512, int(self.n_ctx))
        margin = max(16, int(self.margin_tokens))
        prompt = max(0, int(prompt_tokens_est or 0))
        available = max(16, n_ctx - prompt - margin)
        safe_out = max(16, min(int(requested_out or self.max_out_tokens_default), available))
        return safe_out, prompt


@dataclass
class SummarizationPolicy:
    # How aggressively we roll older turns into the summary
    # Higher = more aggressive summarization (peel more turns per rollup)
    aggressiveness: int  # 1..5
    # Max number of tail turns kept verbatim before summarization kicks in
    tail_turns: int
    # Router text budgets
    router_summary_chars: int
    router_max_chars: int


@dataclass
class WebPolicy:
    # Search breadth (number of sources to fetch)
    search_k: int
    # Fetch parallelism & timeouts (seconds)
    fetch_parallel: int
    per_url_timeout_s: float
    # Total assembled web block character budget
    total_char_budget: int
    per_doc_char_budget: int


@dataclass
class RouterPolicy:
    # Whether to even attempt live web
    enable_auto_web: bool


@dataclass
class PolicyBundle:
    inference: InferencePolicy
    summarization: SummarizationPolicy
    web: WebPolicy
    router: RouterPolicy

    def to_dict(self) -> Dict[str, Any]:
        return {
            "inference": asdict(self.inference),
            "summarization": asdict(self.summarization),
            "web": asdict(self.web),
            "router": asdict(self.router),
        }


# -------------------------------
# Tiering helpers
# -------------------------------

def _vram_tier(vram_gb: Optional[float]) -> str:
    if not vram_gb or vram_gb <= 0.0:
        return "none"
    if vram_gb < 4:
        return "tiny"
    if vram_gb < 8:
        return "small"
    if vram_gb < 16:
        return "mid"
    if vram_gb < 24:
        return "large"
    return "xl"


def _cpu_tier(threads: int, cpu_score: float) -> str:
    t = max(1, threads)
    # cpu_score is an abstracted 0.0..N measure; we blend both
    eff = (t * max(0.5, cpu_score))
    if eff < 6:
        return "low"
    if eff < 16:
        return "mid"
    if eff < 32:
        return "high"
    return "ultra"


# -------------------------------
# Main mapping logic
# -------------------------------

def compute_policy(report: CapabilityReport, overrides: Optional[Dict[str, Any]] = None) -> PolicyBundle:
    """Map host+runtime capability → policies.

    The mapping is intentionally conservative and monotonic: higher VRAM/CPU
    never reduces budgets, only raises them within safe bounds.
    """
    overrides = overrides or {}

    # Derive tiers
    llama_cfg = (report.llama or {}).get("config", {}) if isinstance(report.llama, dict) else {}
    n_ctx = int(llama_cfg.get("nCtx" , 4096))
    n_threads = int(llama_cfg.get("nThreads" , report.cpu_logical or 4))

    vram_gb = report.vram_total_gb
    vram = _vram_tier(vram_gb)
    cpu = _cpu_tier(n_threads, float(getattr(report, "cpu_score", 1.0) or 1.0))

    # ---------------- Inference policy ----------------
    # Margin scales slightly with ctx to reduce overflow retries
    if n_ctx <= 2048:
        margin = 64
        default_out = 256
    elif n_ctx <= 4096:
        margin = 64
        default_out = 384
    elif n_ctx <= 8192:
        margin = 96
        default_out = 512
    else:
        margin = 128
        default_out = 768

    inference = InferencePolicy(
        n_ctx=n_ctx,
        margin_tokens=overrides.get("margin_tokens", margin),
        max_out_tokens_default=overrides.get("max_out_tokens_default", default_out),
    )

    # ---------------- Summarization policy ----------------
    # Base values
    tail_turns = 6
    router_summary_chars = 600
    router_max_chars = 1400
    aggr = 2

    # Scale with capabilities: bigger ctx/VRAM → keep more tail, less aggressive
    if n_ctx >= 8192 or vram in {"mid", "large", "xl"}:
        tail_turns = 8
        aggr = 2
        router_summary_chars = 900
        router_max_chars = 1800
    if n_ctx >= 12288 or vram in {"large", "xl"}:
        tail_turns = 10
        aggr = 1
        router_summary_chars = 1200
        router_max_chars = 2200

    # But clamp if CPU is very weak (router/summarizer are model calls)
    if cpu == "low":
        aggr = max(aggr, 3)
        router_summary_chars = min(router_summary_chars, 800)
        router_max_chars = min(router_max_chars, 1600)

    summarization = SummarizationPolicy(
        aggressiveness=overrides.get("summarize_aggressiveness", aggr),
        tail_turns=overrides.get("tail_turns", tail_turns),
        router_summary_chars=overrides.get("router_summary_chars", router_summary_chars),
        router_max_chars=overrides.get("router_max_chars", router_max_chars),
    )

    # ---------------- Web policy ----------------
    # Start conservative to avoid prompt bloat; scale with tiers
    search_k = 2
    fetch_parallel = 2
    per_url_timeout_s = 8.0
    total_char_budget = 1600
    per_doc_char_budget = 900

    if vram in {"mid", "large", "xl"} or cpu in {"high", "ultra"}:
        search_k = 3
        fetch_parallel = 3
        total_char_budget = 2000
        per_doc_char_budget = 1100
    if vram in {"large", "xl"} and cpu in {"high", "ultra"}:
        search_k = 4
        fetch_parallel = 4
        per_url_timeout_s = 10.0
        total_char_budget = 2400
        per_doc_char_budget = 1200

    web = WebPolicy(
        search_k=int(overrides.get("search_k", search_k)),
        fetch_parallel=int(overrides.get("fetch_parallel", fetch_parallel)),
        per_url_timeout_s=float(overrides.get("per_url_timeout_s", per_url_timeout_s)),
        total_char_budget=int(overrides.get("total_char_budget", total_char_budget)),
        per_doc_char_budget=int(overrides.get("per_doc_char_budget", per_doc_char_budget)),
    )

    # ---------------- Router policy ----------------
    enable_auto_web = True
    # On extremely weak devices, you might want to default off
    if cpu == "low" and vram in {"none", "tiny"}:
        enable_auto_web = True  # keep on but rely on small k/timeouts

    router = RouterPolicy(enable_auto_web=overrides.get("enable_auto_web", enable_auto_web))

    return PolicyBundle(
        inference=inference,
        summarization=summarization,
        web=web,
        router=router,
    )


# -------------------------------
# Small utility: pretty debug line
# -------------------------------

def debug_line(policy: PolicyBundle) -> str:
    p = policy.to_dict()
    inf = p["inference"]; summ = p["summarization"]; web = p["web"]; r = p["router"]
    return (
        f"INF[n_ctx={inf['n_ctx']}, margin={inf['margin_tokens']}, out_def={inf['max_out_tokens_default']}] "
        f"SUMM[aggr={summ['aggressiveness']}, tail={summ['tail_turns']}, rs={summ['router_summary_chars']}, rmax={summ['router_max_chars']}] "
        f"WEB[k={web['search_k']}, par={web['fetch_parallel']}, t={web['per_url_timeout_s']}s, T={web['total_char_budget']}, per={web['per_doc_char_budget']}] "
        f"ROUTER[auto={r['enable_auto_web']}]"
    )

# ===== aimodel/file_read/policy.py/policy_engine.py =====

# policy_engine.py
from __future__ import annotations

import asyncio
import json
import os
from dataclasses import asdict
from typing import Any, Callable, Dict, Optional

from .policy_config import Policy, PolicyOverrides, compute_policy
from .capability_probe import CapabilityReport, probe_capabilities
from .model_runtime import current_model_info


class PolicyEngine:
    """
    Central adaptive policy manager.

    Responsibilities
    - Run a hardware/model capability probe and compute an execution Policy
    - Expose the current Policy to callers (router, packer, web)
    - Periodically refresh in the background (configurable interval)
    - Allow runtime overrides (e.g., admin knob, user preference)
    - Notify subscribers when the policy changes
    """

    def __init__(self, *, refresh_interval_sec: int = 600) -> None:
        self._lock = asyncio.Lock()
        self._policy: Optional[Policy] = None
        self._report: Optional[CapabilityReport] = None
        self._overrides: PolicyOverrides = {}
        self._refresh_interval = max(10, int(refresh_interval_sec))
        self._bg_task: Optional[asyncio.Task] = None
        self._subscribers: set[Callable[[Policy], None]] = set()

    # ------------------------------- lifecycle -------------------------------
    async def start(self) -> None:
        """Start background refresh loop (idempotent)."""
        async with self._lock:
            if self._bg_task and not self._bg_task.done():
                return
            # initial compute immediately
            await self._refresh_locked(reason="startup")
            # spawn periodic refresher
            self._bg_task = asyncio.create_task(self._run_loop(), name="policy_engine_loop")

    async def stop(self) -> None:
        task = self._bg_task
        if task:
            task.cancel()
            try:
                await task
            except Exception:
                pass
            self._bg_task = None

    async def _run_loop(self) -> None:
        while True:
            try:
                await asyncio.sleep(self._refresh_interval)
                async with self._lock:
                    await self._refresh_locked(reason="periodic")
            except asyncio.CancelledError:
                break
            except Exception:
                # keep the loop alive; next tick will retry
                pass

    # ------------------------------ core updates -----------------------------
    async def refresh_now(self, *, reason: str = "manual") -> Policy:
        async with self._lock:
            await self._refresh_locked(reason=reason)
            assert self._policy is not None
            return self._policy

    async def _refresh_locked(self, *, reason: str) -> None:
        # 1) Probe system/model
        report = await probe_capabilities()

        # 2) Pull current model context length if available
        try:
            info = current_model_info() or {}
            cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
            if cfg.get("nCtx"):
                report.model_ctx = int(cfg.get("nCtx"))
        except Exception:
            pass

        # 3) Merge env overrides (lowest precedence of overrides)
        env_overrides = _read_env_overrides()
        merged_overrides = {**env_overrides, **self._overrides}

        # 4) Compute policy
        new_policy = compute_policy(report, overrides=merged_overrides)

        # 5) Set & notify if changed
        changed = (self._policy is None) or (asdict(self._policy) != asdict(new_policy))
        self._policy = new_policy
        self._report = report
        if changed:
            for fn in list(self._subscribers):
                try:
                    fn(new_policy)
                except Exception:
                    pass

    # ------------------------------ subscriptions ----------------------------
    def subscribe(self, fn: Callable[[Policy], None]) -> None:
        self._subscribers.add(fn)

    def unsubscribe(self, fn: Callable[[Policy], None]) -> None:
        self._subscribers.discard(fn)

    # -------------------------------- getters --------------------------------
    def get_policy(self) -> Policy:
        if self._policy is None:
            # best-effort synchronous bootstrap (blocking probe via loop.run_until?)
            # In async servers, call start() at app startup to avoid this path.
            raise RuntimeError("PolicyEngine not started. Call await policy_engine.start().")
        return self._policy

    def get_report(self) -> CapabilityReport:
        if self._report is None:
            raise RuntimeError("PolicyEngine not started. Call await policy_engine.start().")
        return self._report

    def snapshot(self) -> Dict[str, Any]:
        p = self.get_policy()
        r = self.get_report()
        return {
            "policy": asdict(p),
            "report": asdict(r),
            "refreshIntervalSec": self._refresh_interval,
        }

    # -------------------------------- overrides ------------------------------
    async def set_overrides(self, overrides: PolicyOverrides) -> Policy:
        async with self._lock:
            self._overrides = dict(overrides or {})
            await self._refresh_locked(reason="overrides")
            return self._policy  # type: ignore[return-value]

    async def clear_overrides(self) -> Policy:
        return await self.set_overrides({})


# --------------------------- module-level singleton ---------------------------
_policy_engine: Optional[PolicyEngine] = None


def get_engine() -> PolicyEngine:
    global _policy_engine
    if _policy_engine is None:
        # Default interval can be nudged via env
        refresh = int(os.getenv("LOCALAI_POLICY_REFRESH_SEC", "600") or "600")
        _policy_engine = PolicyEngine(refresh_interval_sec=refresh)
    return _policy_engine


async def start_engine() -> None:
    await get_engine().start()


# ------------------------------ env utilities ------------------------------

def _read_env_overrides() -> PolicyOverrides:
    """Read optional JSON from LOCALAI_POLICY_OVERRIDES.

    Example:
      export LOCALAI_POLICY_OVERRIDES='{"inference.maxTokens":768,"web.k":4}'
    """
    raw = os.getenv("LOCALAI_POLICY_OVERRIDES", "").strip()
    if not raw:
        return {}
    try:
        data = json.loads(raw)
    except Exception:
        return {}
    # Accept either flat keys or nested dicts
    # (compute_policy can handle both due to dot-walking in its merge)
    if not isinstance(data, dict):
        return {}
    return data  # type: ignore[return-value]


# ----------------------------- handy convenience -----------------------------

async def current_policy() -> Policy:
    eng = get_engine()
    if eng._policy is None:
        await eng.start()
    return eng.get_policy()


async def current_snapshot() -> Dict[str, Any]:
    eng = get_engine()
    if eng._policy is None:
        await eng.start()
    return eng.snapshot()

# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs==9.5.4
httpx==0.27.0
trafilatura==1.9.0
lxml[html_clean]>=5.3

# ===== aimodel/file_read/retitle_worker.py =====

# aimodel/file_read/retitle_worker.py
from __future__ import annotations
import asyncio, logging, re
from typing import Dict, List, Optional, Tuple

from .model_runtime import get_llm
from .store.index import load_index, save_index
from .store.base import now_iso
from .services.cancel import is_active, GEN_SEMAPHORE  # serialize with main generation
from .store.chats import _load_chat  # for current seq watermark

# -----------------------------------------------------------------------------
# Coalesced per-session queue (last-write-wins)
# -----------------------------------------------------------------------------
# We store only the latest snapshot + watermark per session. The worker consumes
# session IDs; when it handles a session, it reads the latest snapshot once.
_PENDING: Dict[str, dict] = {}
_ENQUEUED: set[str] = set()
_queue: asyncio.Queue[str] = asyncio.Queue()
_lock = asyncio.Lock()  # guards _PENDING/_ENQUEUED

# ---- Timings (intervals) -----------------------------------------------------
_GRACE_MS = 1000  # debounce after stream (was 500)
_ACTIVE_BACKOFF_START_MS = 75     # initial wait while session is active (was fixed 100ms)
_ACTIVE_BACKOFF_MAX_MS = 600      # cap per-iteration sleep
_ACTIVE_BACKOFF_TOTAL_MS = 20000  # max total wait while active (~20s)

def _preview(s: str, n: int = 60) -> str:
    s = s or ""
    return (s[:n] + "…") if len(s) > n else s

def _is_substantial(text: str) -> bool:
    t = (text or "").strip()
    return len(t) >= 12 and re.search(r"[A-Za-z]", t) is not None

def _pick_source(messages: List[dict]) -> Optional[str]:
    """Choose best text to title: first substantial user message; fallback to latest substantial user."""
    if not messages:
        return None
    # first substantial user
    for m in messages:
        if (m.get("role") == "user") and _is_substantial(m.get("content", "")):
            return m.get("content", "")
    # fallback: latest substantial user
    for m in reversed(messages):
        if (m.get("role") == "user") and _is_substantial(m.get("content", "")):
            return m.get("content", "")
    # final fallback: first user at all
    for m in messages:
        if m.get("role") == "user":
            return m.get("content", "")
    return None

def _sanitize_title(s: str) -> str:
    """Single line, 3–5 words, no quotes/numbering/punct clutter."""
    if not s:
        return ""
    s = s.strip()
    # drop common prefixes like "Expanded explanation:", bullets, quotes
    s = re.sub(r'^\s*("[^"]*"|\'[^\']*\'|[-*•]+|\d+\.)\s*', "", s)
    s = s.strip().strip('"\'').strip()
    # keep letters, digits, spaces; collapse spaces
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    # limit to 5 words / ~40 chars
    words = s.split()
    s = " ".join(words[:5])
    if len(s) > 40:
        s = s[:40].rstrip()
    return s

def _make_title(llm, src: str) -> str:
    sys = (
        "You generate ultra-concise chat titles.\n"
        "Rules: 2–5 words, Title Case, nouns/adjectives only.\n"
        "No articles (a, an, the). No verbs. No punctuation. One line.\n"
        "Output only the title."
    )

    examples = [
        {"role": "user", "content": "police station"},
        {"role": "assistant", "content": "Police Station"},
        {"role": "user", "content": "fire truck"},
        {"role": "assistant", "content": "Fire Truck"},
        {"role": "user", "content": "how do i install node on windows"},
        {"role": "assistant", "content": "Node Installation Windows"},
    ]

    out = llm.create_chat_completion(
        messages=[{"role": "system", "content": sys}, *examples, {"role": "user", "content": src}],
        max_tokens=12,
        temperature=0.1,
        top_p=1.0,
        stream=False,
        stop=["\n", "."],  # cut off if it tries to continue a sentence
    )
    # trust the model to format; just trim whitespace/quotes
    raw = (out["choices"][0]["message"]["content"] or "").strip().strip('"').strip("'")
    return raw

async def start_worker():
    """Background loop to process retitle jobs one by one."""
    while True:
        sid = await _queue.get()
        try:
            await _process_session(sid)
        except Exception:
            logging.exception("Retitle worker failed")
        finally:
            _queue.task_done()

def _extract_job(snapshot: dict) -> Tuple[List[dict], int]:
    msgs = snapshot.get("messages") or []
    job_seq = int(snapshot.get("job_seq") or 0)
    return msgs, job_seq

async def _process_session(session_id: str):
    # allow quick follow-up enqueues to coalesce
    await asyncio.sleep(_GRACE_MS / 1000.0)

    # lower priority than generation: wait while this session is active
    waited = 0
    backoff = _ACTIVE_BACKOFF_START_MS
    while is_active(session_id) and waited < _ACTIVE_BACKOFF_TOTAL_MS:
        await asyncio.sleep(backoff / 1000.0)
        waited += backoff
        # exponential backoff with cap
        backoff = min(int(backoff * 1.5), _ACTIVE_BACKOFF_MAX_MS)

    # fetch latest coalesced snapshot
    async with _lock:
        snapshot = _PENDING.pop(session_id, None)
        _ENQUEUED.discard(session_id)

    if not snapshot:
        return

    messages, job_seq = _extract_job(snapshot)

    # stale guard: if chat seq has advanced, skip (a newer enqueue will run)
    try:
        cur_seq = int((_load_chat(session_id) or {}).get("seq") or 0)
    except Exception:
        cur_seq = job_seq
    if cur_seq > job_seq:
        print(f"[retitle] SKIP (stale) session={session_id} job_seq={job_seq} current_seq={cur_seq}")
        return

    # pick source text
    src = _pick_source(messages) or ""
    if not src.strip():
        return

    print(f"[retitle] START session={session_id} job_seq={job_seq} src={_preview(src)!r}")

    # Serialize with main generation semaphore; run LLM call in a worker thread
    async with GEN_SEMAPHORE:
        llm = get_llm()
        try:
            title = await asyncio.to_thread(_make_title, llm, src)
        except Exception as e:
            logging.exception("retitle: LLM error: %s", e)
            return
        finally:
            try:
                llm.reset()
            except Exception:
                pass

    print(f"[retitle] FINISH session={session_id} -> {title!r}")

    if not title:
        return

    # write only if changed
    idx = load_index()
    row = next((r for r in idx if r.get("sessionId") == session_id), None)
    if not row:
        return
    if (row.get("title") or "").strip() == title:
        return

    row["title"] = title
    row["updatedAt"] = now_iso()
    save_index(idx)

def enqueue(session_id: str, messages: List[dict], *, job_seq: Optional[int] = None):
    """Coalesced enqueue; last-write-wins per session, with a seq watermark."""
    if not session_id:
        return
    if not isinstance(messages, list):
        messages = []
    # infer watermark if not provided (max message id)
    if job_seq is None:
        try:
            job_seq = max(int(m.get("id") or 0) for m in messages) if messages else 0
        except Exception:
            job_seq = 0

    snap = {"messages": messages, "job_seq": int(job_seq)}

    async def _put():
        async with _lock:
            _PENDING[session_id] = snap  # overwrite older snapshot
            if session_id not in _ENQUEUED:
                _ENQUEUED.add(session_id)
                try:
                    _queue.put_nowait(session_id)
                except Exception as e:
                    logging.warning(f"Failed to enqueue retitle: {e}")

    # enqueue can be called from sync contexts; schedule safely
    try:
        loop = asyncio.get_running_loop()
        loop.create_task(_put())
    except RuntimeError:
        # no running loop (unlikely here); fall back to a temp loop
        asyncio.run(_put())

# ===== aimodel/file_read/services/cancel.py =====

# aimodel/file_read/services/cancel.py
from __future__ import annotations
import asyncio
from threading import Event
from typing import Dict

GEN_SEMAPHORE = asyncio.Semaphore(1)
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

# aimodel/file_read/services/context_window.py
from __future__ import annotations
from typing import List, Dict, Optional, Tuple
from ..utils.streaming import safe_token_count_messages
from ..model_runtime import current_model_info

def estimate_tokens(llm, messages: List[Dict[str,str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or 4096)
    except Exception:
        return 4096

def clamp_out_budget(
    *, llm, messages: List[Dict[str,str]], requested_out: int, margin: int = 32
) -> Tuple[int, int]:
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = 1024
    n_ctx = current_n_ctx()
    available = max(16, n_ctx - prompt_est - margin)
    safe_out = max(16, min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations
import asyncio, time, json
from typing import AsyncGenerator, Dict, List
from fastapi.responses import StreamingResponse
from datetime import datetime
from dataclasses import asdict  # for serializing message rows

from ..model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody

from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .session_io import handle_incoming, persist_summary
from .packing import build_system_text, pack_with_rollup
from .context_window import clamp_out_budget

# Router (router will decide & fetch if needed)
from ..web.router_ai import decide_web_and_fetch

# Stream meta markers to filter out from the buffered assistant text
from ..utils.streaming import RUNJSON_START, RUNJSON_END

# Tell the type checker run_stream is an async iterator of bytes (stops yellow underline)
from .streaming_worker import run_stream as _run_stream
from typing import AsyncIterator
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream  # type: ignore[assignment]

# ---- helpers for instrumentation --------------------------------------------
def _now() -> str:
    return datetime.now().isoformat(timespec="milliseconds")

def _chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total

def _dump_full_prompt(messages: List[Dict[str, object]], *, params: Dict[str, object], session_id: str) -> None:
    try:
        print(f"[{_now()}] PROMPT DUMP BEGIN session={session_id} msgs={len(messages)}")
        print(json.dumps({"messages": messages, "params": params}, ensure_ascii=False, indent=2))
        print(f"[{_now()}] PROMPT DUMP END   session={session_id}")
    except Exception as e:
        print(f"[{_now()}] PROMPT DUMP ERROR session={session_id} err={type(e).__name__}: {e}")

# ---- tiny helper: last user + recent tail (user & assistant) + summary -------
def _compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: int = 6,
    summary_chars: int = 600,
    max_chars: int = 1400,
) -> str:
    """
    Priority input for the router:
      1) Latest user message (verbatim)
      2) Short recent tail (user + assistant) so URLs/snippets are visible
      3) Trimmed slice of the persisted conversation summary

    Hard-caps the final text to max_chars to avoid overfeeding the router.
    """
    parts: List[str] = []

    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    # Convert to list for safe slicing; include both roles
    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tail_turns:]
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append("Context:\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if len(s) > summary_chars:
            s = s[-summary_chars:]  # take the most recent slice of the summary
        parts.append("Summary:\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > max_chars:
        out = out[:max_chars].rstrip()
    return out

# -----------------------------------------------------------------------------

async def generate_stream_flow(data: ChatBody, request) -> StreamingResponse:
    ensure_ready()
    llm = get_llm()

    session_id = data.sessionId or "default"
    if not data.messages:
        return StreamingResponse(iter([b"No messages provided."]), media_type="text/plain")

    incoming = [{"role": m.role, "content": m.content} for m in data.messages]
    print(f"[{_now()}] GEN request START session={session_id} msgs_in={len(incoming)}")

    st = handle_incoming(session_id, incoming)

    # latest user text from THIS request only
    latest_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    lut_chars = len(latest_user_text) if isinstance(latest_user_text, str) else len(str(latest_user_text) or "")

    # --- ROUTER (one-hop) → maybe WEB BLOCK -----------------------------------
    t0 = time.perf_counter()
    print(f"[{_now()}] GEN web_inject START session={session_id} latest_user_text_chars={lut_chars}")
    try:
        k = int(getattr(data, "webK", 3) or 3)

        # Minimal change: give the router a compact view (last user + tail + summary)
        router_text = _compose_router_text(
            st.get("recent", []),
            str(latest_user_text or ""),
            st.get("summary", "") or "",
        )
        block = await decide_web_and_fetch(llm, router_text, k=k)

        print(f"[{_now()}] ORCH build done has_block={bool(block)} block_len={(len(block) if block else 0)}")

        if block:
            st["_ephemeral_web"] = (st.get("_ephemeral_web") or []) + [
                {
                    "role": "assistant",
                    "content": "Web findings (authoritative — use these to answer accurately; override older knowledge):\n\n" + block,
                }
            ]
            types_preview = [type(x).__name__ for x in (st.get("_ephemeral_web") or [])]
            print(f"[{_now()}] EPHEMERAL attached count={len(st['_ephemeral_web'])} types={types_preview}")

        dt = time.perf_counter() - t0
        eph_cnt = len(st.get("_ephemeral_web") or [])
        print(f"[{_now()}] GEN web_inject END   session={session_id} elapsed={dt:.3f}s ephemeral_blocks={eph_cnt}")
    except Exception as e:
        dt = time.perf_counter() - t0
        print(f"[{_now()}] GEN web_inject ERROR session={session_id} elapsed={dt:.3f}s err={type(e).__name__}: {e}")

    out_budget_req = data.max_tokens or 512
    system_text = build_system_text()

    # consume ephemeral web block so it doesn't stick across turns
    ephemeral_once = st.pop("_ephemeral_web", [])
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=4096,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )

    packed_chars = _chars_len(packed)
    print(f"[{_now()}] GEN pack READY       session={session_id} msgs={len(packed)} chars={packed_chars} out_budget_req={out_budget_req}")

    _dump_full_prompt(
        packed,
        params={"requested_out": out_budget_req, "temperature": (data.temperature or 0.6), "top_p": (data.top_p or 0.9)},
        session_id=session_id,
    )

    persist_summary(session_id, st["summary"])

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_req, margin=32
    )
    print(f"[{_now()}] GEN clamp_out_budget  session={session_id} out_budget={out_budget} input_tokens_est={input_tokens_est}")

    stop_ev = cancel_event(session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        async with GEN_SEMAPHORE:
            mark_active(session_id, +1)
            # Buffer the assistant text we stream so we can add it to st["recent"]
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
                s = chunk_bytes.decode("utf-8", errors="ignore")
                # Skip embedded run-json envelopes entirely
                if RUNJSON_START in s and RUNJSON_END in s:
                    return
                # Skip the “stopped” line if present
                if s.strip() == "⏹ stopped":
                    return
                out_buf.extend(chunk_bytes)

            try:
                print(f"[{_now()}] GEN run_stream START session={session_id} msgs={len(packed)} chars={packed_chars} out_budget={out_budget} tokens_in~={input_tokens_est}")
                async for chunk in run_stream(
                    llm=llm,
                    messages=packed,
                    out_budget=out_budget,
                    stop_ev=stop_ev,
                    request=request,
                    temperature=(data.temperature or 0.6),
                    top_p=(data.top_p or 0.9),
                    input_tokens_est=input_tokens_est,
                ):
                    # accumulate for router/model future context
                    if isinstance(chunk, (bytes, bytearray)):
                        _accum_visible(chunk)
                    else:
                        _accum_visible(chunk.encode("utf-8"))
                    yield chunk
            finally:
                # Append the assistant’s full response to recent so both router and model
                # will see it on the next turn.
                try:
                    full_text = out_buf.decode("utf-8", errors="ignore").strip()
                    # hard-strip any trailing RUNJSON block that slipped through
                    start = full_text.find(RUNJSON_START)
                    if start != -1:
                        end = full_text.find(RUNJSON_END, start)
                        if end != -1:
                            full_text = (full_text[:start] + full_text[end + len(RUNJSON_END):]).strip()
                    if full_text:
                        st["recent"].append({"role": "assistant", "content": full_text})
                        print(f"[{_now()}] RECENT append assistant chars={len(full_text)}")
                except Exception:
                    pass

                # 1) flush pending ops
                try:
                    from ..store import apply_pending_for
                    apply_pending_for(session_id)
                except Exception:
                    pass

                # 2) enqueue retitle AFTER the stream has fully finished (coalesced, with watermark)
                try:
                    from ..store import list_messages as store_list_messages
                    from ..retitle_worker import enqueue as enqueue_retitle
                    msgs = store_list_messages(session_id)
                    last_seq = max((int(m.id) for m in msgs), default=0)
                    enqueue_retitle(session_id, [asdict(m) for m in msgs], job_seq=last_seq)
                except Exception:
                    pass

                print(f"[{_now()}] GEN run_stream END   session={session_id}")
                mark_active(session_id, -1)

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "Connection": "keep-alive"},
    )

async def cancel_session(session_id: str):
    from .cancel import cancel_event
    cancel_event(session_id).set()
    return {"ok": True}

async def cancel_session_alias(session_id: str):
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/packing.py =====

# aimodel/file_read/services/packing.py
from __future__ import annotations
from typing import Tuple, List, Dict, Optional
from ..core.memory import build_system, pack_messages, roll_summary_if_needed

def build_system_text() -> str:
    base = build_system(style="", short=False, bullets=False)
    guidance = (
        "\nYou may consult the prior messages to answer questions about the conversation itself "
        "(e.g., “what did I say first?”). When web context is present, consider it as evidence, "
        "prefer newer info if it conflicts with older memory, and respond in your own words."
    )
    return (base + guidance)

def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    packed, input_budget = pack_messages(
        style="", short=False, bullets=False,
        summary=summary, recent=recent, max_ctx=max_ctx, out_budget=out_budget
    )
    packed, new_summary = roll_summary_if_needed(
        packed=packed, recent=recent, summary=summary,
        input_budget=input_budget, system_text=system_text
    )

    # Inject ephemeral (web findings) BEFORE the last user message, so the final turn is still user.
    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.memory import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    # ensure ephemeral web bucket exists
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass  # preserve original non-fatal behavior

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END, STOP_STRINGS,
    build_run_json, watch_disconnect,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int]
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=64)
    SENTINEL = object()

    def produce():
        t_start = time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []

        try:
            try:
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=40,
                    repeat_penalty=1.25,
                    stop=STOP_STRINGS,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(64, out_budget // 2)
                    log.warning("generate: context overflow, retrying with max_tokens=%d", retry_tokens)
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=40,
                        repeat_penalty=1.25,
                        stop=STOP_STRINGS,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(0.005)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                llm.reset()
            except Exception:
                pass

            try:
                out_text = "".join(out_parts)
                run_json = build_run_json(
                    request_cfg={"temperature": temperature, "top_p": top_p, "max_tokens": out_budget},
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                )
                q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass

            try:
                q.put_nowait(SENTINEL)
            except Exception:
                pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            if stop_ev.is_set():
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set():
            yield b"\n\u23F9 stopped\n"
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=2.0)
        except Exception:
            pass

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message, update_last, append_message,
    delete_message, delete_messages_batch, list_messages,
    list_paged, delete_batch,
    merge_chat, merge_chat_new, edit_message, set_summary, get_summary,  # ← add these
)
from .index import ChatMeta

__all__ = [
    # chats
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "merge_chat", "merge_chat_new", "edit_message",  # ← add these
    # index
    "ChatMeta",
    # pending
    "set_summary", "get_summary"
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile, threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..paths import app_data_dir

# -------- Directories & Paths --------
APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"
PENDING_PATH = APP_DIR / "pending.json"              # NEW
OLD_PENDING_DELETES = APP_DIR / "pending_deletes.json"  # NEW

# -------- Lock for safe writes --------
_lock = threading.RLock()

# -------- Helpers --------
def now_iso() -> str:
    """UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    """Safely write JSON to a temp file then move into place."""
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    """Ensure app/chats directories exist and index.json is initialized."""
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    """Return path to chat file for a session ID."""
    return CHATS_DIR / f"{session_id}.json"

# -------- Exports --------
__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "PENDING_PATH",          # NEW
    "OLD_PENDING_DELETES",   # NEW
    "_lock",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta

def _load_chat(session_id: str) -> Dict:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}  # add summary
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""  # backfill older files
        return data
    
@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str

def upsert_on_first_message(session_id: str, title: str) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or "New Chat"),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row); save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)

def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)

def append_message(session_id: str, role: str, content: str) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq, "sessionId": session_id, "role": role,
        "content": content, "createdAt": now_iso(),
    }
    data["messages"].append(msg); data["seq"] = seq
    _save_chat(session_id, data)

    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if row:
        row["updatedAt"] = msg["createdAt"]
        if role == "assistant":
            row["lastMessage"] = content
        save_index(idx)

    # pending ops are applied by pending.apply_pending_for() from the router after appends
    return ChatMessageRow(**msg)

def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1

def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted

def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    return [ChatMessageRow(**m) for m in data.get("messages", [])]

def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    size = max(1, min(100, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag

def delete_batch(session_ids: List[str]) -> List[str]:
    for sid in session_ids:
        try: chat_path(sid).unlink(missing_ok=True)
        except Exception: pass
    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids

def merge_chat(source_id: str, target_id: str):
    source_msgs = list_messages(source_id)
    target_msgs = list_messages(target_id)

    # Insert source first, then re-add target to preserve order
    merged = []
    for m in source_msgs:
        row = append_message(target_id, m.role, m.content)
        merged.append(row)

    for m in target_msgs:
        row = append_message(target_id, m.role, m.content)
        merged.append(row)

    return merged

def _save_chat(session_id: str, data: Dict):
    atomic_write(chat_path(session_id), data)

def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)

def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")

def merge_chat_new(source_id: str, target_id: Optional[str] = None):
    from uuid import uuid4
    new_id = str(uuid4())
    upsert_on_first_message(new_id, "Merged Chat")

    merged = []

    # source first
    for m in list_messages(source_id):
        row = append_message(new_id, m.role, m.content)
        merged.append(row)

    # then target (if exists)
    if target_id:
        for m in list_messages(target_id):
            row = append_message(new_id, m.role, m.content)
            merged.append(row)

    return new_id, merged

def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)

    # refresh index if last message changed
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(**updated)


# expose internals for pending ops
__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch", "merge_chat", "merge_chat_new",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary", 
]

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..model_runtime import current_model_info, get_llm

# Markers MUST match the frontend parser
RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

# llama.cpp common stop strings
STOP_STRINGS = ["</s>", "User:", "\nUser:"]

# ---------- token + model helpers ----------

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))  # type: ignore[arg-type]
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def build_run_json(
    *,
    request_cfg: Dict[str, object],   # temperature, top_p, max_tokens
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None

    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()

    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
        },
    }

# ---------- connection helper ----------

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/duckduckgo.py =====

# aimodel/file_read/web/duckduckgo.py
from __future__ import annotations
from typing import List, Optional, Tuple
import asyncio, time
from urllib.parse import urlparse

# Prefer new package; fallback for compatibility
try:
    from ddgs import DDGS  # type: ignore
except Exception:
    try:
        from duckduckgo_search import DDGS  # type: ignore
    except Exception:
        DDGS = None  # no provider

from .provider import SearchHit

# -------- simple in-memory cache (store superset = 10) -----------------------
_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}
CACHE_TTL_SEC = 300  # 5 minutes
CACHE_SUPERSET_K = 10

def _cache_key(query: str) -> str:
    return (query or "").strip().lower()

def _cache_get(query: str) -> Optional[List[SearchHit]]:
    key = _cache_key(query)
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > CACHE_TTL_SEC:
        _CACHE.pop(key, None)
        return None
    return hits

def _cache_set(query: str, hits: List[SearchHit]) -> None:
    _CACHE[_cache_key(query)] = (time.time(), hits)

def _host(u: str) -> str:
    try:
        h = (urlparse(u).hostname or "").lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return ""

# -------- DDGS (official client) ---------------------------------------------
def _ddg_sync_search(query: str, k: int) -> List[SearchHit]:
    results: List[SearchHit] = []
    if DDGS is None:
        print(f"[{time.strftime('%X')}] ddg: PROVIDER MISSING (DDGS=None)")
        return results
    with DDGS() as ddg:
        for i, r in enumerate(ddg.text(query, max_results=max(1, k),
                                       safesearch="moderate", region="us-en")):
            title = (r.get("title") or "").strip()
            url = (r.get("href") or "").strip()
            snippet: Optional[str] = (r.get("body") or "").strip() or None
            if not url:
                continue
            results.append(SearchHit(title=title or url, url=url, snippet=snippet, rank=i))
            if i + 1 >= k:
                break
    return results

# -------- public provider ----------------------------------------------------
class DuckDuckGoProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        """
        Returns top-k SearchHit results via DDGS.

        Added prints:
          - START with normalized query, k
          - CACHE HIT / MISS with timing
          - For fetched results: total count, top items (idx, host, title, url)
          - RETURN with timing and top titles
        """
        t0 = time.time()
        q_norm = (query or "").strip()
        if not q_norm:
            print("ddg: empty query")
            return []

        print(f"[{time.strftime('%X')}] ddg: START q={q_norm!r} k={k}")

        # superset cache
        cached = _cache_get(q_norm)
        if cached is not None:
            dt = time.time() - t0
            out = cached[:k]
            top_preview = [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]]
            print(f"[{time.strftime('%X')}] ddg: CACHE HIT dt={dt:.2f}s hits={len(out)} top={top_preview}")
            return out

        # fetch superset once
        sup_k = max(k, CACHE_SUPERSET_K)
        hits: List[SearchHit] = []
        if DDGS is not None:
            try:
                step = time.time()
                hits = await asyncio.to_thread(_ddg_sync_search, q_norm, sup_k)
                dt_fetch = time.time() - step
                print(f"[{time.strftime('%X')}] ddg: HITS RECEIVED dt={dt_fetch:.2f}s count={len(hits)}")
                # verbose preview of first few results
                for h in hits[:5]:
                    print(f"[{time.strftime('%X')}] ddg:   {h.rank:>2} | host={_host(h.url)} | title={(h.title or '')[:80]!r} | url={h.url}")
            except Exception as e:
                print(f"[{time.strftime('%X')}] ddg: ERROR {e}")
        else:
            print(f"[{time.strftime('%X')}] ddg: SKIP (DDGS unavailable)")

        # store superset and return slice
        _cache_set(q_norm, hits)
        dt = time.time() - t0
        out = hits[:k]
        top_titles = [h.title for h in out[:3]]
        print(f"[{time.strftime('%X')}] ddg: RETURN dt={dt:.2f}s hits={len(out)} top={top_titles}")
        return out

# ===== aimodel/file_read/web/fetch.py =====

from __future__ import annotations
import asyncio
from typing import Tuple, List
import httpx
import trafilatura

HEADERS = {"User-Agent": "LocalAI/0.1 (+clean-fetch)"}

async def fetch_clean(url: str, timeout_s: float = 8.0, max_chars: int = 3000) -> Tuple[str, int, str]:
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout_s) as client:
        r = await client.get(url, headers=HEADERS)
        r.raise_for_status()
        # trafilatura extracts readable article text (Apache-2.0)
        txt = trafilatura.extract(r.text, url=str(r.url)) or ""
        if not txt:
            # fallback: raw text up to cap
            txt = r.text
        txt = txt.strip().replace("\r", "")
        if len(txt) > max_chars:
            txt = txt[:max_chars]
        return (str(r.url), r.s