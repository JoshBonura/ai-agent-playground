
# ===== aimodel/file_read/__init__.py =====

from .adaptive.config.paths import app_data_dir, read_settings, write_settings
from .core.logging import get_logger
from .runtime.model_runtime import current_model_info, ensure_ready, get_llm

log = get_logger(__name__)

__all__ = [
    "app_data_dir",
    "current_model_info",
    "ensure_ready",
    "get_llm",
    "read_settings",
    "write_settings",
]

# ===== aimodel/file_read/adaptive/config/adaptive_config.py =====

# aimodel/file_read/runtime/adaptive_config.py
from __future__ import annotations

import os
import platform
import shutil
import subprocess
from dataclasses import asdict, dataclass
from typing import Any

from ...core.logging import get_logger

log = get_logger(__name__)

try:
    import psutil
except Exception:
    psutil = None
try:
    import torch
except Exception:
    torch = None

from .paths import read_settings


def _env_bool(k: str, default: bool) -> bool:
    v = os.getenv(k)
    if v is None:
        return default
    return v.strip().lower() in ("1", "true", "yes", "on")


def _cpu_count() -> int:
    try:
        import multiprocessing as mp

        return max(1, mp.cpu_count() or os.cpu_count() or 1)
    except Exception:
        return os.cpu_count() or 1


def _avail_ram() -> int | None:
    if not psutil:
        return None
    try:
        return int(psutil.virtual_memory().available)
    except Exception:
        return None


def _cuda_vram() -> int | None:
    if torch and torch.cuda.is_available():
        try:
            dev = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(dev)
            return int(props.total_memory)
        except Exception:
            pass
    if shutil.which("nvidia-smi"):
        try:
            out = subprocess.check_output(
                ["nvidia-smi", "--query-gpu=memory.total", "--format=csv,noheader,nounits"],
                text=True,
                stderr=subprocess.DEVNULL,
                timeout=2.0,
            )
            mb = max(int(x.strip()) for x in out.strip().splitlines() if x.strip())
            return mb * 1024 * 1024
        except Exception:
            return None
    return None


def _gpu_kind() -> str:
    if _cuda_vram():
        return "cuda"
    if torch and getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def _safe_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default


def _pick_dtype_quant(
    device: str, a: dict[str, Any], vram_bytes: int | None
) -> tuple[str | None, str | None]:
    dq = a.get("dtype_quant", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = dq.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype"), best.get("quant")
        return dq.get("cuda_default_dtype"), dq.get("cuda_default_quant")
    if device == "mps":
        return dq.get("mps_default_dtype"), None
    return dq.get("cpu_default_dtype"), dq.get("cpu_default_quant")


def _pick_kv(device: str, a: dict[str, Any], vram_bytes: int | None) -> str | None:
    kv = a.get("kv_cache", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = kv.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype")
        return kv.get("cuda_default")
    if device == "mps":
        return kv.get("mps_default")
    return kv.get("cpu_default")


def _pick_capacity(
    device: str, a: dict[str, Any], vram_bytes: int | None, threads: int
) -> tuple[int, int, int | None]:
    cap = a.get("capacity", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = cap.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return (
                int(best.get("seq_len") or 0),
                int(best.get("batch") or 1),
                int(best.get("n_gpu_layers") or 0),
            )
        return 0, 1, 0
    if device == "mps":
        m = cap.get("mps", {})
        return int(m.get("seq_len") or 0), int(m.get("batch") or 1), 0
    cpu = cap.get("cpu", {})
    seq_len = int(cpu.get("seq_len") or 0)
    batch = 1
    by = cpu.get("batch_by_threads") or []
    best = None
    for t in sorted(by, key=lambda x: int(x.get("min_threads", 0)), reverse=True):
        if threads >= int(t.get("min_threads") or 0):
            best = t
            break
    if best:
        batch = int(best.get("batch") or 1)
    return seq_len, batch, 0


def _gpu_mem_fraction(device: str, a: dict[str, Any]) -> float:
    table = a.get("gpu_fraction", {}) if isinstance(a, dict) else {}
    v = table.get(device)
    return _safe_float(v, 0.0)


def _torch_flags(device: str, a: dict[str, Any]) -> tuple[bool, bool]:
    flags = a.get("flags", {}) if isinstance(a, dict) else {}
    flash = bool(flags.get("enable_flash_attn_cuda")) if device == "cuda" else False
    tc = (
        bool(flags.get("use_torch_compile_on_cuda_linux"))
        if (device == "cuda" and platform.system().lower() == "linux")
        else False
    )
    return flash, tc


def _threads(a: dict[str, Any]) -> tuple[int, int, int, int]:
    policy = a.get("cpu_threads_policy", {}) if isinstance(a, dict) else {}
    mode = str(policy.get("mode") or "").lower()
    ncpu = _cpu_count()
    if mode == "fixed":
        v = int(policy.get("value") or max(1, ncpu - 1))
        t = max(1, min(v, ncpu))
    elif mode == "percent":
        pct = _safe_float(policy.get("value"), 0.0)
        t = max(1, min(ncpu, int(round(ncpu * pct / 100.0))))
        if t < 1:
            t = 1
    else:
        t = max(1, ncpu - 1)
    intra = t
    inter = max(1, ncpu // 2)
    return ncpu, t, intra, inter


@dataclass
class AdaptiveConfig:
    device: str
    dtype: str | None
    quant: str | None
    kv_cache_dtype: str | None
    max_seq_len: int
    max_batch_size: int
    gpu_memory_fraction: float
    cpu_threads: int
    torch_intraop_threads: int
    torch_interop_threads: int
    enable_flash_attn: bool
    use_torch_compile: bool
    total_vram_bytes: int | None
    avail_ram_bytes: int | None
    cpu_count: int

    def as_dict(self) -> dict[str, Any]:
        return asdict(self)


def compute_adaptive_config() -> AdaptiveConfig:
    settings = read_settings()
    a = settings.get("adaptive", {}) if isinstance(settings, dict) else {}
    device = _gpu_kind()
    vram = _cuda_vram() if device == "cuda" else None
    ram = _avail_ram()
    ncpu, threads, intra, inter = _threads(a)
    dtype, quant = _pick_dtype_quant(device, a, vram)
    kv = _pick_kv(device, a, vram)
    seq_len, batch, n_gpu_layers = _pick_capacity(device, a, vram, threads)
    frac = _gpu_mem_fraction(device, a)
    flash, tcompile = _torch_flags(device, a)
    return AdaptiveConfig(
        device=device,
        dtype=dtype,
        quant=quant,
        kv_cache_dtype=kv,
        max_seq_len=int(seq_len or 0),
        max_batch_size=int(batch or 1),
        gpu_memory_fraction=frac,
        cpu_threads=threads,
        torch_intraop_threads=intra,
        torch_interop_threads=inter,
        enable_flash_attn=flash,
        use_torch_compile=tcompile,
        total_vram_bytes=vram,
        avail_ram_bytes=ram,
        cpu_count=ncpu,
    )

# ===== aimodel/file_read/adaptive/config/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations

import json
import os
import sys
from pathlib import Path
from typing import Any

from ...core.logging import get_logger

log = get_logger(__name__)


# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"


SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",  # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,  # advanced (optional)
    "ropeFreqScale": None,  # advanced (optional)
}


def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")


def _read_json(path: Path) -> dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def read_settings() -> dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = (
                    int(v)
                    if key in {"nCtx", "nThreads", "nGpuLayers", "nBatch"}
                    else float(v)
                    if key in {"ropeFreqBase", "ropeFreqScale"}
                    else v
                )
            except Exception:
                cfg[key] = v

    return cfg


def write_settings(patch: dict[str, Any]) -> dict[str, Any]:
    cfg = read_settings()
    cfg.update({k: v for k, v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/adaptive/config/settings.json =====

{
  "adaptiveEnabled": false,
  "adaptive": {
    "enabled": true,
    "cpu_threads_policy": {
      "mode": "leave_one"
    },
    "dtype_quant": {
      "cuda_default_dtype": "float16",
      "cuda_default_quant": null,
      "mps_default_dtype": "float16",
      "cpu_default_dtype": "int8",
      "cpu_default_quant": "q4_K_M",
      "cuda_tiers": [
        { "min_vram_gb": 24, "dtype": "bfloat16", "quant": null },
        { "min_vram_gb": 12, "dtype": "float16", "quant": null },
        { "min_vram_gb": 6, "dtype": "float16", "quant": "bnb-int8" },
        { "min_vram_gb": 4, "dtype": "float16", "quant": "bnb-int8" }
      ]
    },
    "kv_cache": {
      "cuda_default": "fp8",
      "mps_default": "fp16",
      "cpu_default": "fp32",
      "cuda_tiers": [
        { "min_vram_gb": 16, "dtype": "fp16" },
        { "min_vram_gb": 0, "dtype": "fp8" }
      ]
    },
    "capacity": {
      "cuda_tiers": [
        {
          "min_vram_gb": 24,
          "seq_len": 8192,
          "batch": 8,
          "n_gpu_layers": 9999
        },
        { "min_vram_gb": 12, "seq_len": 4096, "batch": 4, "n_gpu_layers": 48 },
        { "min_vram_gb": 8, "seq_len": 3072, "batch": 2, "n_gpu_layers": 40 },
        { "min_vram_gb": 6, "seq_len": 2048, "batch": 1, "n_gpu_layers": 32 },
        { "min_vram_gb": 4, "seq_len": 2048, "batch": 1, "n_gpu_layers": 28 }
      ],
      "mps": { "seq_len": 2048, "batch": 1 },
      "cpu": {
        "seq_len": 2048,
        "batch_by_threads": [
          { "min_threads": 16, "batch": 8 },
          { "min_threads": 8, "batch": 4 },
          { "min_threads": 1, "batch": 2 }
        ]
      }
    },
    "gpu_fraction": {
      "cuda": 0.8,
      "mps": 0.7,
      "cpu": 0.0
    },
    "flags": {
      "enable_flash_attn_cuda": true,
      "use_torch_compile_on_cuda_linux": true
    },
    "batchTokenMap": [
      { "minConcurrency": 8, "n_batch": 512 },
      { "minConcurrency": 4, "n_batch": 384 },
      { "minConcurrency": 2, "n_batch": 256 },
      { "minConcurrency": 1, "n_batch": 192 }
    ]
  },
  "modelPath": "",
  "nCtx": null,
  "nThreads": null,
  "nGpuLayers": null,
  "nBatch": null
}

# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/admin_chats.py =====

from __future__ import annotations

from dataclasses import asdict
from pathlib import Path
from typing import Any

from fastapi import APIRouter, Depends, HTTPException

# Strict gate: Admin + Personal Pro + Activated device
from ..deps.license_deps import require_admin_pro, require_admin_personal_pro
# Looser gate: Admin only (no Pro/activation requirement)
from ..deps.admin_deps import require_admin

from ..store import chats as store
from ..store.base import APP_DIR, user_root

router = APIRouter(prefix="/api/admins/chats", tags=["admins"])


def _users_dir() -> Path:
    # Assumes per-user data lives under APP_DIR/users/<uid> (same convention as user_root)
    return APP_DIR / "users"


def _list_all_uids() -> list[str]:
    try:
        d = _users_dir()
        if not d.exists():
            return []
        return sorted([p.name for p in d.iterdir() if p.is_dir()])
    except Exception:
        return []


# ---------- Admin: only MY chats (Admin-only; no Pro/activation needed) ----------
@router.get("/mine/paged")
def admin_list_mine_paged(
    page: int = 0,
    size: int = 30,
    ceiling: str | None = None,
    user=Depends(require_admin_personal_pro),  # <- Admin + Pro, no activation
):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)
    rows, total, total_pages, last_flag = store.list_paged(root, uid, page, size, ceiling)
    content = [
        {
            **asdict(r),
            "ownerUid": uid,  # include owner for frontend clarity
            "ownerEmail": (user.get("email") or "").lower(),
        }
        for r in rows
    ]
    return {
        "content": content,
        "totalElements": total,
        "totalPages": total_pages,
        "size": size,
        "number": page,
        "first": (page == 0),
        "last": last_flag,
        "empty": len(content) == 0,
    }


# ---------- Admin: ALL users’ chats (requires Admin + Pro + Activated) ----------
@router.get("/all/paged")
def admin_list_all_paged(
    page: int = 0,
    size: int = 30,
    ceiling: str | None = None,
    _user=Depends(require_admin_pro),  # <- strict gate
):
    # Aggregate per-user indexes, sort by updatedAt desc, then paginate globally
    aggregate: list[dict[str, Any]] = []

    for uid in _list_all_uids():
        try:
            root = user_root(uid)
            rows, _total, _tp, _last = store.list_paged(
                root, uid, 0, 10_000, ceiling
            )  # big page to collect all
            for r in rows:
                d = asdict(r)
                # enrich with owner for cross-user listing
                d["ownerUid"] = uid
                aggregate.append(d)
        except Exception:
            # best-effort; skip bad/empty users
            continue

    # Sort globally
    aggregate.sort(key=lambda r: r.get("updatedAt") or "", reverse=True)

    # Global pagination
    page = max(0, int(page))
    size = max(1, int(size))
    start = page * size
    end = start + size
    page_items = aggregate[start:end]
    total = len(aggregate)
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    return {
        "content": page_items,
        "totalElements": total,
        "totalPages": total_pages,
        "size": size,
        "number": page,
        "first": (page == 0),
        "last": last_flag,
        "empty": len(page_items) == 0,
    }


# ---------- Admin: read messages of a specific user’s chat (strict) ----------
@router.get("/{target_uid}/{session_id}/messages")
def admin_list_messages(target_uid: str, session_id: str, _user=Depends(require_admin_pro)):  # strict gate
    root = user_root(target_uid)
    try:
        rows = store.list_messages(root, target_uid, session_id)
    except Exception as e:
        raise HTTPException(404, f"Chat not found: {e}") from e
    return [asdict(r) for r in rows]

# ===== aimodel/file_read/api/admins.py =====

from __future__ import annotations

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel

from ..core import admins as reg
from ..deps.auth_deps import require_auth
from ..deps.license_deps import has_personal_pro, require_personal_pro, require_admin_pro

router = APIRouter(prefix="/api/admins", tags=["admins"])


class GuestToggleReq(BaseModel):
    enabled: bool


@router.get("/state")
def state(user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    email = (user.get("email") or "").lower()

    personal_pro = has_personal_pro(email)
    admin_rec = reg.get_admin()
    is_listed_admin = bool(admin_rec and admin_rec.get("uid") == uid)

    # Admin is independent of personal Pro
    is_admin = is_listed_admin

    return {
        # single-admin state
        "hasAdmin": bool(admin_rec),
        "isAdmin": is_admin,            # UI: show admin controls when True
        "isAdminRaw": is_listed_admin,  # listed as admin, regardless of Pro
        "ownerUid": (admin_rec or {}).get("uid"),
        "ownerEmail": (admin_rec or {}).get("email"),
        # guest toggle
        "guestEnabled": reg.get_guest_enabled(),
        # first Pro user can self-promote if no admin exists yet
        "canSelfPromote": (admin_rec is None) and personal_pro,
        # convenience flags for UI
        "pro": personal_pro,
        "me": {"uid": uid, "email": email, "pro": personal_pro},
    }


@router.post("/self-promote")
def self_promote(user=Depends(require_personal_pro)):
    """
    First Pro user can claim the single admin slot (no device activation required).
    """
    if reg.has_admin():
        raise HTTPException(403, "Admin already set")
    uid = user.get("user_id") or user.get("sub")
    email = (user.get("email") or "").lower()
    reg.set_admin(uid, email)
    return {"ok": True}


@router.post("/guest")
def set_guest(req: GuestToggleReq, user=Depends(require_admin_pro)):
    """
    Requires: Admin + Personal Pro + Activated device.
    """
    reg.set_guest_enabled(bool(req.enabled))
    return {"ok": True, "enabled": reg.get_guest_enabled()}

# ===== aimodel/file_read/api/auth_router.py =====

# aimodel/file_read/api/auth_router.py
from __future__ import annotations

from ..core.logging import get_logger

log = get_logger(__name__)
import os

from fastapi import APIRouter, Depends, HTTPException, Response

from ..deps.auth_deps import require_auth
from ..services.auth_service import (firebase_sign_in_with_password,
                                     firebase_sign_up_with_password,
                                     verify_jwt_with_google)
from ..services.licensing_core import license_status_local, recover_by_email

router = APIRouter(prefix="/api")

AUTH_REQUIRE_VERIFIED = os.getenv("AUTH_REQUIRE_VERIFIED", "false").lower() == "true"
ID_COOKIE_NAME = os.getenv("AUTH_IDTOKEN_COOKIE", "fb_id")
LEGACY_COOKIE_NAME = os.getenv("AUTH_SESSION_COOKIE", "fb_session")
SESSION_DAYS = int(os.getenv("AUTH_SESSION_DAYS", "7"))
COOKIE_SECURE = os.getenv("AUTH_COOKIE_SECURE", "false").lower() == "true"
COOKIE_SAMESITE = (os.getenv("AUTH_COOKIE_SAMESITE", "lax") or "lax").lower()
COOKIE_DOMAIN = os.getenv("AUTH_COOKIE_DOMAIN", "").strip() or None
COOKIE_PATH = "/"

if COOKIE_SAMESITE == "none" and (not COOKIE_SECURE):
    raise RuntimeError("AUTH_COOKIE_SAMESITE=none requires AUTH_COOKIE_SECURE=true")


def _set_cookie(resp: Response, name: str, value: str, max_age_s: int):
    resp.set_cookie(
        key=name,
        value=value,
        max_age=max_age_s,
        httponly=True,
        secure=COOKIE_SECURE,
        samesite=COOKIE_SAMESITE,
        domain=COOKIE_DOMAIN,
        path=COOKIE_PATH,
    )


def _clear_cookie(resp: Response, name: str):
    resp.delete_cookie(key=name, domain=COOKIE_DOMAIN, path=COOKIE_PATH)


@router.post("/auth/login")
async def login(body: dict[str, str], response: Response):
    email = (body.get("email") or "").strip().lower()
    password = body.get("password") or ""
    if not email or not password:
        raise HTTPException(400, "Email and password required")

    # Firebase sign-in
    data = await firebase_sign_in_with_password(email, password)
    id_token = data.get("idToken")
    if not id_token:
        raise HTTPException(401, "Login failed")

    # ✅ FIX: await the async verifier
    try:
        claims = await verify_jwt_with_google(id_token)
    except Exception as e:
        log.warning("[auth] verify_jwt_with_google failed", extra={"err": str(e)})
        raise HTTPException(401, "Invalid ID token")

    if not isinstance(claims, dict):
        log.error("[auth] verifier did not return a dict")
        raise HTTPException(401, "Invalid ID token")

    if AUTH_REQUIRE_VERIFIED and (not bool(claims.get("email_verified"))):
        raise HTTPException(401, "Email not verified")

    max_age = SESSION_DAYS * 86400
    _set_cookie(response, ID_COOKIE_NAME, id_token, max_age)
    _set_cookie(response, LEGACY_COOKIE_NAME, id_token, max_age)

    # Try to hydrate local license state (best effort)
    lic_snapshot = {"plan": "free", "valid": False, "exp": None}
    try:
        await recover_by_email(email)
        # pass expected_email to ensure we show the right license
        lic_snapshot = license_status_local(expected_email=email)
    except Exception as e:
        log.error(f"[auth] license recover after login failed: {e!r}")

    return {
        "ok": True,
        "email": email,
        "uid": claims.get("user_id") or claims.get("sub"),
        "emailVerified": bool(claims.get("email_verified")),
        "expiresInDays": SESSION_DAYS,
        "license": lic_snapshot,
    }


@router.post("/auth/logout")
def logout(response: Response):
    _clear_cookie(response, ID_COOKIE_NAME)
    _clear_cookie(response, LEGACY_COOKIE_NAME)
    return {"ok": True}


@router.get("/auth/me")
def me(user=Depends(require_auth)):
    try:
        lic = license_status_local(expected_email=(user.get("email") or "").lower())
    except Exception:
        lic = {"plan": "free", "valid": False, "exp": None}

    return {
        "email": (user.get("email") or "").lower(),
        "uid": user.get("user_id") or user.get("sub"),
        "emailVerified": bool(user.get("email_verified")),
        "name": user.get("name"),
        "picture": user.get("picture"),
        "iat": user.get("iat"),
        "exp": user.get("exp"),
        "license": lic,
    }


@router.post("/auth/register")
async def register(body: dict[str, str]):
    email = (body.get("email") or "").strip().lower()
    password = body.get("password") or ""
    if not email or not password:
        raise HTTPException(400, "Email and password required")
    await firebase_sign_up_with_password(email, password)
    return {"ok": True}

# ===== aimodel/file_read/api/billing.py =====

# aimodel/file_read/api/billing.py
from __future__ import annotations

import os

from fastapi import APIRouter, Depends, HTTPException

from ..core.http import ExternalServiceError, arequest_json
from ..core.logging import get_logger
from ..deps.auth_deps import require_auth as decode_bearer
from ..services.licensing_core import license_status_local

log = get_logger(__name__)
router = APIRouter(prefix="/api", tags=["billing"])

LIC_SERVER = (os.getenv("LIC_SERVER_BASE") or "").rstrip("/")
if not LIC_SERVER:
    raise RuntimeError(
        "LIC_SERVER_BASE env var is required (e.g. https://lic-server.localmind.workers.dev)"
    )

SERVICE = "licensing"
ACCEPT_JSON = {"Accept": "application/json"}


@router.get("/billing/status")
async def billing_status(auth=Depends(decode_bearer)):
    email = (auth.get("email") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    try:
        lic = license_status_local(expected_email=email)
        active = bool(lic.get("valid"))
        return {
            "status": "active" if active else "inactive",
            "current_period_end": int(lic.get("exp") or 0),
        }
    except Exception as e:
        # Local-only path: treat any error as inactive but log it.
        log.warning("billing_status_local_error", extra={"detail": str(e)})
        return {"status": "inactive", "current_period_end": 0}


@router.post("/billing/checkout")
async def start_checkout(auth=Depends(decode_bearer)):
    email = (auth.get("email") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    url = f"{LIC_SERVER}/api/checkout/session"
    try:
        data = await arequest_json(
            method="POST",
            url=url,
            service=SERVICE,
            headers=ACCEPT_JSON,
            json_body={"email": email},
        )
    except ExternalServiceError as e:
        # Map to a sensible HTTP status. If upstream gave a status, reuse it;
        # otherwise use 504 for timeouts / gateway issues.
        status = e.status or 504
        log.warning(
            "billing_checkout_error", extra={"status": status, "url": e.url, "detail": e.detail}
        )
        raise HTTPException(status, e.detail or "Checkout failed")

    if not isinstance(data, dict) or "url" not in data:
        log.warning("billing_checkout_bad_response", extra={"url": url})
        raise HTTPException(502, "Bad response from licensing server")

    return {"url": data["url"]}


@router.post("/billing/portal")
async def open_portal(auth=Depends(decode_bearer)):
    email = (auth.get("email") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    url = f"{LIC_SERVER}/api/portal/session"
    try:
        data = await arequest_json(
            method="POST",
            url=url,
            service=SERVICE,
            headers=ACCEPT_JSON,
            json_body={"email": email},
        )
    except ExternalServiceError as e:
        status = e.status or 504
        log.warning(
            "billing_portal_error", extra={"status": status, "url": e.url, "detail": e.detail}
        )
        raise HTTPException(status, e.detail or "Portal session failed")

    if not isinstance(data, dict) or "url" not in data:
        log.warning("billing_portal_bad_response", extra={"url": url})
        raise HTTPException(502, "Bad response from licensing server")

    return {"url": data["url"]}


@router.get("/license/by-session")
async def license_by_session(session_id: str):
    if not session_id:
        raise HTTPException(400, "Missing session_id")

    url = f"{LIC_SERVER}/api/license/by-session"
    try:
        data = await arequest_json(
            method="GET",
            url=url,
            service=SERVICE,
            headers=ACCEPT_JSON,
            params={"session_id": session_id},
        )
    except ExternalServiceError as e:
        status = e.status or 504
        log.warning(
            "license_by_session_error", extra={"status": status, "url": e.url, "detail": e.detail}
        )
        raise HTTPException(status, e.detail or "License lookup failed")

    return data

# ===== aimodel/file_read/api/chats.py =====

# ===== aimodel/file_read/api/chats.py =====
from __future__ import annotations

from dataclasses import asdict

from fastapi import APIRouter, Depends

from ..core.logging import get_logger
from ..core.schemas import (BatchDeleteReq, BatchMsgDeleteReq, ChatMessage,
                            ChatMetaModel, EditMessageReq, PageResp)
from ..deps.auth_deps import require_auth
from ..store import chats as store
from ..store.base import user_root
from ..utils.streaming import strip_runjson
from ..deps.model_deps import require_model_ready  
# retitle worker is optional
try:
    from ..workers.retitle_worker import \
        enqueue as enqueue_retitle  # type: ignore
except Exception:  # pragma: no cover
    enqueue_retitle = None  # type: ignore

log = get_logger(__name__)
router = APIRouter()


def _is_admin(user) -> bool:
    import os

    admins = {e.strip().lower() for e in (os.getenv("ADMIN_EMAILS", "").split(","))}
    return (user.get("email") or "").lower() in admins


@router.post("/api/chats")
async def api_create_chat(body: dict[str, str], user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    email = (user.get("email") or "").lower()
    root = user_root(uid)

    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip() or "New Chat"

    row = store.upsert_on_first_message(root, uid, email, session_id, title)
    return asdict(row)


@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: dict[str, str], user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)

    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store.update_last(root, uid, session_id, last_message, title)
    return asdict(row)


@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(
    page: int = 0,
    size: int = 30,
    ceiling: str | None = None,
    user=Depends(require_auth),
):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)

    rows, total, total_pages, last_flag = store.list_paged(root, uid, page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )


@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str, user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)
    rows = store.list_messages(root, uid, session_id)
    return [asdict(r) for r in rows]


@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, msg: ChatMessage, user=Depends(require_auth), _=Depends(require_model_ready)):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)

    role = msg.role
    content = (msg.content or "").rstrip()
    attachments = msg.attachments or []

    row = store.append_message(root, uid, session_id, role, content, attachments=attachments)

    # queue retitle opportunistically
    if role == "assistant" and enqueue_retitle:
        try:
            msgs = store.list_messages(root, uid, session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            msgs_clean = []
            for m in msgs:
                dm = asdict(m)
                dm["content"] = strip_runjson(dm.get("content") or "")
                msgs_clean.append(dm)
                enqueue_retitle(root, uid, session_id, msgs_clean, job_seq=last_seq)
        except Exception as e:  # best-effort
            log.debug(f"[retitle] enqueue failed for {session_id}: {e!r}")

    return asdict(row)


@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int, user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)
    deleted = store.delete_message(root, uid, session_id, int(message_id))
    return {"deleted": deleted}


@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(
    session_id: str, req: BatchMsgDeleteReq, user=Depends(require_auth)
):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)
    deleted = store.delete_messages_batch(root, uid, session_id, req.messageIds or [])
    return {"deleted": deleted}


@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq, user=Depends(require_auth)):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)
    deleted = store.delete_batch(root, uid, req.sessionIds or [])
    return {"deleted": deleted}


@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(
    session_id: str, message_id: int, req: EditMessageReq, user=Depends(require_auth)
):
    uid = user.get("user_id") or user.get("sub")
    root = user_root(uid)
    row = store.edit_message(root, uid, session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/devices_router.py =====

from __future__ import annotations

from fastapi import APIRouter, Depends, HTTPException

from ..deps.admin_deps import require_admin
from ..deps.license_deps import require_personal_pro
from ..services.licensing_service import _lic_base, _lic_get_json, _lic_post_json, current_license_string
from ..services.licensing_service import redeem_activation, current_device_info, device_id
from ..core.logging import get_logger

log = get_logger(__name__)
router = APIRouter(prefix="/api/devices", tags=["devices"])


def _require_license() -> str:
    lic = (current_license_string() or "").strip()
    if not lic:
        raise HTTPException(404, "license_not_present")
    return lic


@router.get("")
async def list_devices(_=Depends(require_admin)):
    base = _lic_base()
    lic = _require_license()
    data = await _lic_get_json(f"{base}/api/devices", params={"license": lic}) or []
    cur = device_id()
    for d in data:
        d["isCurrent"] = (d.get("id") == cur)
    return [
        {
            "id": d.get("id"),
            "name": d.get("name"),
            "platform": d.get("platform"),
            "appVersion": d.get("appVersion"),
            "lastSeen": d.get("lastSeen"),
            "isCurrent": bool(d.get("isCurrent")),
            "exp": d.get("exp"),
        }
        for d in data
    ]


@router.get("/current")
async def current_device(_=Depends(require_admin)):
    """
    Return local device info and whether the licensing server currently lists it.
    """
    info = current_device_info()
    res = {"local": info, "onServer": False, "serverRecord": None}

    try:
        base = _lic_base()
        lic = _require_license()
        arr = await _lic_get_json(f"{base}/api/devices", params={"license": lic}) or []
        for d in arr:
            if d.get("id") == info["id"]:
                res["onServer"] = True
                res["serverRecord"] = d
                break
    except Exception as e:
        log.warning(f"[devices/current] server lookup failed: {e!r}")

    return res


@router.post("/rename")
async def rename_device(body: dict, _=Depends(require_admin)):
    name = (body or {}).get("name")
    device_id_val = (body or {}).get("deviceId")
    lic = _require_license()
    if not device_id_val:
        raise HTTPException(400, "missing_fields")
    await _lic_post_json(
        f"{_lic_base()}/api/devices/rename",
        body={"license": lic, "deviceId": device_id_val, "name": name},
    )
    return {"ok": True}


@router.delete("/{device_id_val}")
async def revoke_device(device_id_val: str, _=Depends(require_admin)):
    lic = _require_license()
    await _lic_post_json(
        f"{_lic_base()}/api/devices/revoke",
        body={"license": lic, "deviceId": device_id_val},
    )
    return {"ok": True}


@router.post("/activate-here")
async def activate_here(
    _admin=Depends(require_admin),
    _pro=Depends(require_personal_pro),
):
    """
    Admin convenience to activate THIS device.
    Requires: admin AND the caller personally has Pro.
    """
    lic = _require_license()
    res = await redeem_activation(lic, device_name="this device")
    return {"ok": True, "exp": res.get("exp")}

# ===== aimodel/file_read/api/generate_router.py =====

# aimodel/file_read/api/generate_router.py
from __future__ import annotations

import httpx
from fastapi import APIRouter, Body, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse

from ..api.auth_router import require_auth
from ..core.logging import get_logger
from ..core.schemas import ChatBody
from ..deps.model_deps import require_model_ready
from .model_workers import get_active_worker_addr

log = get_logger(__name__)

router = APIRouter(
    dependencies=[Depends(require_auth), Depends(require_model_ready)]
)

@router.get("/api/aiw/health")
async def aiw_health():
    try:
        host, port = get_active_worker_addr()
    except Exception:
        return {"ok": False, "error": "no_active_worker"}

    url = f"http://{host}:{port}/api/worker/health"
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            r = await client.get(url)
            r.raise_for_status()
            return r.json()
    except Exception as e:
        log.error("aiw_health proxy failed: %s", e)
        return {"ok": False, "error": "worker_unreachable"}


@router.post("/api/ai/generate/stream")
async def generate_stream_alias(request: Request, data: ChatBody = Body(...)):
    """
    Single public entrypoint:
      - If a worker is active, forward ChatBody to /api/worker/generate/stream and relay bytes.
      - Else, run the in-process generate_stream_flow.
    """
    host = port = None
    try:
        host, port = get_active_worker_addr()
    except Exception:
        pass

    if host and port:
        url = f"http://{host}:{port}/api/worker/generate/stream"
        raw = await request.body()

        async def _proxy():
            # Immediate kick so the browser starts processing the stream
            yield b": proxy-open\n\n"

            # Keep upstream stream open for the whole response
            async with httpx.AsyncClient(timeout=None) as client:
                async with client.stream(
                    "POST",
                    url,
                    content=raw,
                    headers={
                        "content-type": "application/json",
                        "accept": "text/event-stream",
                        "accept-encoding": "identity",
                    },
                ) as r:
                    if r.status_code >= 400:
                        # pull body to give a real message; raise to let FastAPI format it
                        detail = await r.aread()
                        msg = (detail.decode("utf-8", "ignore") or "worker error")
                        raise HTTPException(status_code=r.status_code, detail=msg)

                    async for chunk in r.aiter_bytes():
                        # Relay exactly what the worker emits (SSE frames/comments)
                        if chunk:
                            yield chunk

        return StreamingResponse(
            _proxy(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
                "Connection": "keep-alive",
            },
        )

    # Fallback to in-process pipeline if no worker.
    from ..services.generate_flow import generate_stream_flow
    return await generate_stream_flow(data, request)


# ---- cancel passthrough (unchanged) ----
from ..services.generate_flow import cancel_session_alias
cancel_router = APIRouter(dependencies=[Depends(require_auth)])

@cancel_router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/licensing_router.py =====

# aimodel/file_read/api/licensing_router.py
from __future__ import annotations

from fastapi import APIRouter, Depends, Header, Query, HTTPException
from pydantic import BaseModel

from ..deps.auth_deps import require_auth
from ..services.licensing_core import (
    apply_license_string,
    email_from_auth,
    license_status_local,
    refresh_license,
    current_license_string,
)
from ..services.licensing_service import (
    get_activation_status,
    redeem_activation,
    refresh_activation,
    remove_activation_file,
)
from ..core.logging import get_logger

log = get_logger(__name__)
router = APIRouter(prefix="/api/license", tags=["license"])


class ApplyReq(BaseModel):
    license: str


@router.post("/apply")
async def apply_license(body: ApplyReq, user_agent: str | None = Header(default=None)):
    """
    1) Verify and persist the LM1 license locally.
    2) Redeem a device-scoped activation token (best-effort).
    """
    log.info("[license] POST /apply")
    res = apply_license_string(body.license)

    # Best-effort device activation
    try:
        device_name = (user_agent or "device").split("(")[0].strip()[:64]
        act = await redeem_activation(body.license, device_name=device_name)
        res.update({"activation": {"ok": True, "exp": act.get("exp")}})
    except Exception as e:
        log.warning(f"[license] activation redeem failed: {e!r}")
        res.update({"activation": {"ok": False}})

    return res


@router.get("/apply")
async def apply_license_get(
    license: str = Query(..., min_length=10),
    user_agent: str | None = Header(default=None),
):
    log.info("[license] GET /apply (discouraged; use POST)")
    return await apply_license(ApplyReq(license=license), user_agent=user_agent)


@router.post("/refresh")
async def refresh(
    auth=Depends(require_auth),
    force: bool = Query(False),
    user_agent: str | None = Header(default=None),
):
    """
    Refresh the license (pull from server) and the activation.
    If activation refresh is rejected by the licensing server (401/403/404),
    delete the local activation so UI flips to 'needs activation'.
    """
    log.info(f"[license] POST /refresh force={force}")
    email = email_from_auth(auth)
    lr = await refresh_license(email, force)

    try:
        ar = await refresh_activation()
        lr.update({"activation": {"ok": True, "exp": ar.get("exp")}})
    except HTTPException as e:
        if e.status_code == 404:
            # No activation yet: seed once
            try:
                lic = current_license_string()
                if lic:
                    device_name = (user_agent or "device").split("(")[0].strip()[:64]
                    ar2 = await redeem_activation(lic, device_name=device_name)
                    lr.update({"activation": {"ok": True, "exp": ar2.get("exp")}})
                else:
                    lr.setdefault("activation", {"ok": False})
            except Exception as e2:
                log.warning(f"[license] activation redeem-on-refresh failed: {e2!r}")
                lr.setdefault("activation", {"ok": False})
        elif e.status_code in (401, 403):
            # Explicit server rejection (e.g., device_limit_reached / revoked)
            remove_activation_file()
            log.warning(f"[license] activation revoked on refresh: {e.status_code} {e.detail}")
            lr.setdefault("activation", {"ok": False, "revoked": True})
        else:
            log.warning(f"[license] activation refresh failed: {e!r}")
            lr.setdefault("activation", {"ok": False})
    except Exception as e:
        log.warning(f"[license] activation refresh unexpected error: {e!r}")
        lr.setdefault("activation", {"ok": False})

    return lr


@router.get("/status")
def status(user=Depends(require_auth)):
    email = (user.get("email") or "").strip().lower()
    st = license_status_local(expected_email=email) or {"plan": "free", "valid": False}
    act = get_activation_status()
    st["activation"] = act
    log.info(
        "[license/status] plan=%s valid=%s activation_present=%s exp=%s",
        st.get("plan"),
        st.get("valid"),
        act.get("present"),
        act.get("exp"),
    )
    return st

# ===== aimodel/file_read/api/metrics.py =====

# aimodel/file_read/api/metrics.py
from __future__ import annotations

from fastapi import APIRouter, Query, Depends, HTTPException
from ..deps.license_deps import is_request_pro_activated

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..runtime.model_runtime import get_llm
from ..services.budget import analyze_budget
from ..services.packing import build_system_text, pack_with_rollup
from ..store import get_summary, list_messages, set_summary

log = get_logger(__name__)


def _require_pro_activation():
    if not is_request_pro_activated():
        raise HTTPException(status_code=403, detail="Pro + Activation required")


router = APIRouter(
    prefix="/metrics",
    tags=["metrics"],
    dependencies=[Depends(_require_pro_activation)],
)

@router.get("/budget")
def get_budget(sessionId: str | None = Query(default=None), maxTokens: int | None = None):
    eff0 = SETTINGS.effective()
    sid = sessionId or eff0["default_session_id"]
    llm = get_llm()
    eff = SETTINGS.effective(session_id=sid)

    msgs = [{"role": m.role, "content": m.content} for m in list_messages(sid)]
    summary = get_summary(sid)
    system_text = build_system_text()
    packed, new_summary, _ = pack_with_rollup(
        system_text=system_text,
        summary=summary,
        recent=msgs,
        max_ctx=int(eff["model_ctx"]),
        out_budget=int(eff["default_max_tokens"]),
    )
    if new_summary != summary:
        set_summary(sid, new_summary)

    requested_out = int(maxTokens or eff["default_max_tokens"])
    budget = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=requested_out,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()
    return {"sessionId": sid, "budget": budget}

# ===== aimodel/file_read/api/model_workers.py =====

# aimodel/file_read/api/model_workers.py
from __future__ import annotations
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
import httpx
from ..workers.model_worker import supervisor
from ..core.logging import get_logger
from ..services.system_snapshot import get_system_snapshot

router = APIRouter(prefix="/api/model-workers", tags=["model-workers"])
log = get_logger(__name__)

_ACTIVE_WORKER_ID: str | None = None

class SpawnReq(BaseModel):
    modelPath: str = Field(..., description="Absolute path to the GGUF model")

def _list_workers() -> list[dict]:
    """
    Be robust to different supervisor APIs across branches.
    Normalizes to a list[dict].
    """
    # Try most likely method names first
    for name in ("list", "list_workers", "snapshot", "status", "info"):
        if hasattr(supervisor, name):
            fn = getattr(supervisor, name)
            try:
                res = fn()
            except TypeError:
                # Some variants accept a flag; try harmless truthy
                try:
                    res = fn(True)
                except Exception:
                    continue

            # Normalize common shapes
            if isinstance(res, dict):
                if "workers" in res and isinstance(res["workers"], list):
                    return res["workers"]
                if "items" in res and isinstance(res["items"], list):
                    return res["items"]
            if isinstance(res, list):
                return res
            # Last-ditch: single worker dict
            if isinstance(res, dict):
                return [res]

    # Peek at common attributes as a final fallback
    if hasattr(supervisor, "workers"):
        w = supervisor.workers
        if isinstance(w, dict):
            return list(w.values())
        if isinstance(w, list):
            return w

    return []

def _require_active_worker_id() -> str:
    if not _ACTIVE_WORKER_ID:
        raise RuntimeError("No active worker")
    return _ACTIVE_WORKER_ID

def get_active_worker_port() -> int:
    wid = _require_active_worker_id()
    port = supervisor.get_port(wid)
    if not port:
        raise RuntimeError("Active worker not found")
    return port

def get_active_worker_addr() -> tuple[str, int]:
    return "127.0.0.1", get_active_worker_port()

@router.get("")
async def list_workers():
    return {"ok": True, "workers": _list_workers(), "active": _ACTIVE_WORKER_ID}

@router.get("/active")
async def get_active():
    if not _ACTIVE_WORKER_ID:
        return {"ok": True, "active": None}
    workers = _list_workers()
    info = next((w for w in workers if w.get("id") == _ACTIVE_WORKER_ID), None)
    return {"ok": True, "active": info}

@router.get("/inspect")
async def inspect_workers():
    sys_res = await get_system_snapshot()

    try:
        workers = supervisor.list()
    except Exception:
        workers = []

    # if registry is empty but an active id exists, try to synthesize one
    synth = None
    if (not workers) and _ACTIVE_WORKER_ID:
        try:
            host, port = get_active_worker_addr()
            async with httpx.AsyncClient(timeout=5.0) as client:
                r = await client.get(f"http://{host}:{port}/api/worker/health")
                h = r.json()
            synth = {
                "id": _ACTIVE_WORKER_ID,
                "port": port,
                "model_path": h.get("path"),
                "status": "ready" if h.get("ok") else "unknown",
            }
        except Exception:
            pass

    return {
        "ok": True,
        "workers": workers or ([synth] if synth else []),
        "active": _ACTIVE_WORKER_ID,
        "system": sys_res,
    }

# ✅ Make sure these POST routes are present
@router.post("/spawn")
async def spawn_worker(req: SpawnReq):
    info = await supervisor.spawn_worker(req.modelPath)
    global _ACTIVE_WORKER_ID
    if _ACTIVE_WORKER_ID is None:
        _ACTIVE_WORKER_ID = info.id
    return {
        "ok": True,
        "worker": {"id": info.id, "port": info.port, "modelPath": info.model_path, "status": info.status},
        "active": _ACTIVE_WORKER_ID,
    }

@router.post("/activate/{worker_id}")
async def activate_worker(worker_id: str):
    workers = _list_workers()
    if not any(w.get("id") == worker_id for w in workers):
        raise HTTPException(status_code=404, detail="Worker not found")
    global _ACTIVE_WORKER_ID
    _ACTIVE_WORKER_ID = worker_id
    return {"ok": True, "active": _ACTIVE_WORKER_ID}

@router.post("/kill/{worker_id}")
async def kill_worker(worker_id: str):
    ok = await supervisor.stop_worker(worker_id)
    if not ok:
        raise HTTPException(status_code=404, detail="Worker not found")
    global _ACTIVE_WORKER_ID
    if _ACTIVE_WORKER_ID == worker_id:
        _ACTIVE_WORKER_ID = None
    return {"ok": True, "killed": worker_id, "active": _ACTIVE_WORKER_ID}

@router.post("/kill-all")
async def kill_all_workers():
    n = await supervisor.stop_all()
    global _ACTIVE_WORKER_ID
    _ACTIVE_WORKER_ID = None
    return {"ok": True, "stopped": n, "active": _ACTIVE_WORKER_ID}

# ===== aimodel/file_read/api/models.py =====

# aimodel/file_read/api/models.py
from __future__ import annotations

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..adaptive.config.paths import read_settings, write_settings
from ..runtime.model_runtime import (
    current_model_info,
    list_local_models,
    load_model,
    unload_model,
    request_cancel_load,
)

router = APIRouter(prefix="/api", tags=["models"])

class LoadReq(BaseModel):
    modelPath: str
    nCtx: int | None = None
    nThreads: int | None = None
    nGpuLayers: int | None = None
    nBatch: int | None = None
    ropeFreqBase: float | None = None
    ropeFreqScale: float | None = None
    resetDefaults: bool | None = None   # ← NEW

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.get("/models/health")
def models_health():
    info = current_model_info()
    return {
        "ok": True,
        "loaded": info["loaded"],
        "config": info["config"],
        "loading": info.get("loading"),
        "loadingPath": info.get("loadingPath"),
    }

@router.post("/models/load")
def api_load_model(req: LoadReq):  # sync def
    try:
        payload = req.model_dump(exclude_none=True)
        info = load_model(payload)
        return info
    except Exception as e:
        msg = str(e).strip()
        # Distinguish user cancel vs error
        if msg.upper() == "CANCELLED":
            return JSONResponse({"cancelled": True}, status_code=499)
        return JSONResponse({"error": msg}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/models/settings")
async def api_update_model_settings(patch: dict[str, object]):
    s = write_settings(patch)
    return s

@router.post("/models/cancel-load")
async def api_cancel_model_load():
    did = request_cancel_load()
    return {"ok": True, "cancelled": did}

# ===== aimodel/file_read/api/proxy_generate.py =====

# aimodel/file_read/api/proxy_generate.py
from __future__ import annotations

import httpx
from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import StreamingResponse

from .model_workers import get_active_worker_addr  # <-- use addr, not port
from ..api.auth_router import require_auth  # keep auth consistent

router = APIRouter(dependencies=[Depends(require_auth)])

@router.get("/api/aiw/health")
async def proxy_health():
    try:
        host, port = get_active_worker_addr()
    except Exception as e:
        raise HTTPException(status_code=409, detail=str(e))
    url = f"http://{host}:{port}/api/worker/health"
    async with httpx.AsyncClient(timeout=5.0) as client:
        r = await client.get(url)
        return r.json()

@router.post("/api/aiw/generate")
async def proxy_generate(req: Request):
    try:
        host, port = get_active_worker_addr()
    except Exception as e:
        raise HTTPException(status_code=409, detail=str(e))
    body = await req.json()
    url = f"http://{host}:{port}/api/worker/generate"
    async with httpx.AsyncClient(timeout=120.0) as client:
        r = await client.post(url, json=body)
        if r.status_code >= 400:
            raise HTTPException(status_code=r.status_code, detail=r.text)
        return r.json()

@router.post("/api/aiw/generate/stream")
async def proxy_generate_stream(req: Request):
    """
    Streaming shim. The worker doesn't stream; we stream the raw bytes to fit a streaming UI.
    """
    try:
      host, port = get_active_worker_addr()
    except Exception as e:
      raise HTTPException(status_code=409, detail=str(e))
    url = f"http://{host}:{port}/api/worker/generate"
    raw = await req.body()
    async with httpx.AsyncClient(timeout=None) as client:
        r = await client.post(url, content=raw, headers={"content-type": "application/json"})
        if r.status_code >= 400:
            # return body as detail so caller can show a useful error
            detail = await r.aread()
            raise HTTPException(status_code=r.status_code, detail=detail)
        return StreamingResponse(
            r.aiter_raw(),
            media_type=r.headers.get("content-type", "application/json"),
        )

# ===== aimodel/file_read/api/rag.py =====

from __future__ import annotations

from ..core.logging import get_logger
log = get_logger(__name__)

from threading import RLock
import numpy as np
from fastapi import APIRouter, File, Form, HTTPException, UploadFile, Depends, Request
from sentence_transformers import SentenceTransformer

from ..deps.license_deps import require_personal_pro_activated
from ..rag.ingest import build_metas, chunk_text, sniff_and_extract
from ..rag.schemas import SearchHit, SearchReq
from ..rag.store import add_vectors, search_vectors
from ..rag.uploads import hard_delete_source
from ..rag.uploads import list_sources as rag_list_sources

router = APIRouter(
    prefix="/api/rag",
    tags=["rag"],
    dependencies=[Depends(require_personal_pro_activated)],  # Pro + Activation, no admin
)

_st_model: SentenceTransformer | None = None
_st_lock = RLock()


def _get_st_model() -> SentenceTransformer:
    global _st_model
    if _st_model is None:
        with _st_lock:
            if _st_model is None:
                log.info("[RAG EMBED] loading e5-small-v2… (one-time)")
                _st_model = SentenceTransformer("intfloat/e5-small-v2")
                log.info("[RAG EMBED] model ready")
    return _st_model


def _embed(texts: list[str]) -> np.ndarray:
    model = _get_st_model()
    arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
    return arr.astype("float32")


# --------------------------------------------------------------------
# Upload
# --------------------------------------------------------------------
@router.post("/upload")
async def upload_doc(
    request: Request,
    sessionId: str | None = Form(default=None),
    file: UploadFile = File(...),
):
    # Entry log (helps prove the request actually reached this handler)
    log.info(
        "[RAG UPLOAD] entered handler method=%s path=%s ct=%s",
        request.method,
        request.url.path,
        request.headers.get("content-type"),
    )

    log.info(
        "[RAG UPLOAD] sessionId=%s, filename=%s, content_type=%s",
        sessionId,
        getattr(file, "filename", None),
        getattr(file, "content_type", None),
    )
    data = await file.read()
    log.info("[RAG UPLOAD] file size=%d bytes", len(data))
    text, mime = sniff_and_extract(file.filename, data)
    log.info("[RAG UPLOAD] extracted mime=%s, text_len=%d", mime, len(text))
    if not text.strip():
        raise HTTPException(status_code=400, detail="Empty/unsupported file")
    chunks = chunk_text(text, {"mime": mime})
    log.info("[RAG UPLOAD] chunk_count=%d", len(chunks))
    metas = build_metas(sessionId, file.filename, chunks, size=len(data))
    embeds = _embed([c.text for c in chunks])
    log.info("[RAG UPLOAD] embed_shape=%s", getattr(embeds, "shape", None))
    add_vectors(sessionId, embeds, metas, dim=embeds.shape[1])
    log.info("[RAG UPLOAD] done added=%d", len(chunks))
    return {"ok": True, "added": len(chunks)}


# Optional: respond to OPTIONS explicitly (useful if CORS preflight is involved)
@router.options("/upload")
async def options_upload():
    return {}


# --------------------------------------------------------------------
# Search
# --------------------------------------------------------------------
@router.post("/search")
async def search(req: SearchReq, request: Request):
    log.info(
        "[RAG SEARCH] entered handler method=%s path=%s",
        request.method,
        request.url.path,
    )
    q = (req.query or "").strip()
    if not q:
        return {"hits": []}
    qv = _embed([q])[0]
    chat_hits = search_vectors(req.sessionId, qv, req.kChat, dim=qv.shape[0]) if req.kChat else []
    global_hits = search_vectors(None, qv, req.kGlobal, dim=qv.shape[0]) if req.kGlobal else []
    fused = sorted(chat_hits + global_hits, key=lambda r: r["score"], reverse=True)
    out: list[SearchHit] = []
    for r in fused:
        out.append(
            SearchHit(
                id=r["id"],
                text=r["text"],
                score=float(r["score"]),
                source=r.get("source"),
                title=r.get("title"),
                sessionId=r.get("sessionId"),
            )
        )
    return {"hits": [h.model_dump() for h in out]}


# --------------------------------------------------------------------
# List / Dump
# --------------------------------------------------------------------
@router.get("/list")
async def list_items(sessionId: str | None = None, k: int = 20, request: Request = None):
    if request is not None:
        log.info(
            "[RAG LIST] entered handler method=%s path=%s sessionId=%s k=%d",
            request.method,
            request.url.path,
            sessionId,
            k,
        )
    qv = _embed(["list"])[0]
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])
    items = []
    for h in hits:
        txt = h.get("text") or ""
        items.append(
            {
                "id": h.get("id"),
                "sessionId": h.get("sessionId"),
                "source": h.get("source"),
                "title": h.get("title"),
                "score": float(h.get("score", 0.0)),
                "text": txt,
            }
        )
    log.info("[RAG LIST] sessionId=%s k=%d -> %d items", sessionId, k, len(items))
    return {"items": items}


@router.get("/dump")
async def dump_items(sessionId: str | None = None, k: int = 50, request: Request = None):
    if request is not None:
        log.info(
            "[RAG DUMP] entered handler method=%s path=%s sessionId=%s k=%d",
            request.method,
            request.url.path,
            sessionId,
            k,
        )
    qv = _embed(["dump"])[0]
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])
    chunks = []
    for h in hits:
        chunks.append(
            {
                "id": h.get("id"),
                "sessionId": h.get("sessionId"),
                "source": h.get("source"),
                "title": h.get("title"),
                "score": float(h.get("score", 0.0)),
                "text": h.get("text") or "",
            }
        )
    log.info("[RAG DUMP] sessionId=%s k=%d -> %d items", sessionId, k, len(chunks))
    return {"chunks": chunks}


# --------------------------------------------------------------------
# Uploads index / delete
# --------------------------------------------------------------------
@router.get("/uploads")
async def api_list_uploads(sessionId: str | None = None, scope: str = "all", request: Request = None):
    if request is not None:
        log.info(
            "[RAG UPLOADS LIST] entered handler method=%s path=%s sessionId=%s scope=%s",
            request.method,
            request.url.path,
            sessionId,
            scope,
        )
    include_global = scope != "session"
    return {"uploads": rag_list_sources(sessionId, include_global=include_global)}


@router.options("/uploads")
async def options_uploads():
    return {}


@router.post("/uploads/delete-hard")
async def api_delete_upload_hard(body: dict[str, str], request: Request):
    log.info(
        "[RAG UPLOADS DELETE] entered handler method=%s path=%s",
        request.method,
        request.url.path,
    )
    source = (body.get("source") or "").strip()
    session_id = body.get("sessionId") or None
    if not source:
        return {"error": "source required"}
    out = hard_delete_source(source, session_id=session_id, embedder=_embed)
    return out


# --------------------------------------------------------------------
# Debug: list the routes this router actually registered
# --------------------------------------------------------------------
try:
    for r in router.routes:
        methods = sorted(list(getattr(r, "methods", []) or []))
        path = getattr(r, "path", "")
        log.info("[RAG ROUTER] registered %s %s", methods, path)
except Exception as e:
    log.error("[RAG ROUTER] failed to list router routes: %r", e)

# ===== aimodel/file_read/api/settings.py =====

from typing import Any

from fastapi import APIRouter, Body, Query

from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)

router = APIRouter(prefix="/api/settings", tags=["settings"])


@router.get("/defaults")
def get_defaults():
    return SETTINGS.defaults


@router.get("/overrides")
def get_overrides():
    return SETTINGS.overrides


@router.patch("/overrides")
def patch_overrides(payload: dict[str, Any] = Body(...)):
    SETTINGS.patch_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}


@router.put("/overrides")
def put_overrides(payload: dict[str, Any] = Body(...)):
    SETTINGS.replace_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}


@router.get("/adaptive")
def get_adaptive(session_id: str | None = Query(default=None, alias="sessionId")):
    return SETTINGS.adaptive(session_id=session_id)


@router.post("/adaptive/recompute")
def recompute_adaptive(session_id: str | None = Query(default=None, alias="sessionId")):
    SETTINGS.recompute_adaptive(session_id=session_id)
    return {"ok": True, "adaptive": SETTINGS.adaptive(session_id=session_id)}


@router.get("/effective")
def get_effective(session_id: str | None = Query(default=None, alias="sessionId")):
    return SETTINGS.effective(session_id=session_id)

# ===== aimodel/file_read/api/system.py =====

from __future__ import annotations

from fastapi import APIRouter
from ..core.logging import get_logger
from ..services.system_snapshot import get_system_snapshot

router = APIRouter(prefix="/api/system", tags=["system"])
log = get_logger(__name__)

@router.get("/resources")
async def system_resources():
    # Instant return of the last cached snapshot (updated by poller)
    return await get_system_snapshot()

# Optional quick debug endpoint to see NVML status
@router.get("/nvml-debug")
def nvml_debug():
    import os, ctypes, platform, json
    info = {
        "platform": platform.platform(),
        "env_NVML_DLL_PATH": os.getenv("NVML_DLL_PATH"),
        "env_NVML_DLL": os.getenv("NVML_DLL"),
        "preload_ok": False,
        "preload_error": None,
        "pynvml_init_ok": False,
        "pynvml_error": None,
        "driver_version": None,
    }

    path = info["env_NVML_DLL_PATH"] or info["env_NVML_DLL"]
    if path:
        try:
            ctypes.WinDLL(path)
            info["preload_ok"] = True
        except OSError as e:
            info["preload_error"] = str(e)

    try:
        import pynvml  # type: ignore
        pynvml.nvmlInit()
        info["pynvml_init_ok"] = True
        try:
            info["driver_version"] = pynvml.nvmlSystemGetDriverVersion().decode("utf-8", "ignore")
        except Exception:
            pass
        finally:
            try:
                pynvml.nvmlShutdown()
            except Exception:
                pass
    except Exception as e:
        info["pynvml_error"] = str(e)

    return json.loads(json.dumps(info, default=str))

# ===== aimodel/file_read/app.py =====

# aimodel/file_read/app.py
from __future__ import annotations

import asyncio
import os
from pathlib import Path

from fastapi import Depends, FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse
from starlette.staticfiles import StaticFiles
from starlette.routing import Mount

from .api.system import router as system_router
from .api.proxy_generate import router as worker_proxy_router
from .api import admins as admins_api
from .core.logging import get_logger, setup_logging
from .services.system_snapshot import poll_system_snapshot

setup_logging()
log = get_logger(__name__)

# 2) Now it’s safe to log during .env load
try:
    from dotenv import load_dotenv

    _ENV_PATH = Path(__file__).resolve().parent / ".env"
    load_dotenv(dotenv_path=_ENV_PATH, override=False)
except Exception as _e:
    log.info(f"[env] NOTE: could not load .env: {_e}")

from .adaptive.config.paths import bootstrap
from .api import settings as settings_router
from .api.admin_chats import router as admin_chats_router
from .api.auth_router import require_auth
from .api.auth_router import router as auth_router
from .api.billing import router as billing_router
from .api.chats import router as chats_router
from .api.generate_router import router as generate_router, cancel_router
from .api.licensing_router import router as licensing_router
from .api.metrics import router as metrics_router
from .api.models import router as models_router
from .api.rag import router as rag_router
from .api.model_workers import router as model_workers_router  # ✅ single import
from .workers.model_worker import supervisor  # ✅ single supervisor import

from .core import request_ctx
from .workers.retitle_worker import start_worker

KEYS = (
    "LIC_ED25519_PUB_HEX",
    "LIC_SERVER_BASE",
    "FIREBASE_WEB_API_KEY",
    "FIREBASE_PROJECT_ID",
)

log.info(f"[env] .env path={_ENV_PATH} exists={_ENV_PATH.exists()}")
for k in KEYS:
    v = os.getenv(k)
    log.info(f"[env] {k}: set={bool(v)} len={len(v or '')} head={(v or '')[:6]}")
bootstrap()

app = FastAPI()
app.state.bg_tasks = []  # keep handles to background tasks so we can cancel/await on shutdown

origins = [
    o.strip() for o in os.getenv("APP_CORS_ORIGIN", "http://localhost:5173").split(",") if o.strip()
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

deps = [Depends(require_auth)]

# Core APIs
app.include_router(models_router)
app.include_router(chats_router, dependencies=deps)
app.include_router(generate_router, dependencies=deps)
app.include_router(settings_router.router, dependencies=deps)
app.include_router(rag_router, dependencies=deps)
app.include_router(metrics_router, dependencies=deps)
app.include_router(billing_router, dependencies=deps)
app.include_router(licensing_router, dependencies=deps)
app.include_router(admins_api.router, dependencies=deps)
app.include_router(auth_router)
app.include_router(admin_chats_router, dependencies=deps)
app.include_router(cancel_router, dependencies=deps)
app.include_router(worker_proxy_router, dependencies=deps)
app.include_router(system_router, dependencies=deps)
app.include_router(model_workers_router, dependencies=deps)

# --- Route dump (after all includes/mounts) ---
try:
    def _dump_routes():
        log.info("==== ROUTES (app) ====")
        for r in app.routes:
            methods = sorted(list(getattr(r, "methods", []) or []))
            path = getattr(r, "path", "")
            typ = type(r).__name__
            if isinstance(r, Mount) and hasattr(r.app, "routes"):
                log.info("[APP] %-12s %s type=%s (mounted app)", ",".join(methods), path, typ)
                for sr in r.app.routes:
                    sm = sorted(list(getattr(sr, "methods", []) or []))
                    sp = getattr(sr, "path", "")
                    log.info("       -> %-12s %s", ",".join(sm), sp)
            else:
                log.info("[APP] %-12s %s type=%s", ",".join(methods), path, typ)
        log.info("==== ROUTES END ====")
    _dump_routes()
except Exception as e:
    log.error("Failed to dump routes: %r", e)

# ---------- Serve built frontend (Vite 'dist') ----------
FRONTEND_DIR = Path(__file__).resolve().parents[2] / "frontend" / "dist"
ASSETS_DIR = FRONTEND_DIR / "assets"
INDEX_FILE = FRONTEND_DIR / "index.html"

if ASSETS_DIR.exists():
    app.mount("/assets", StaticFiles(directory=str(ASSETS_DIR), html=False), name="assets")

@app.get("/", response_class=HTMLResponse)
async def _index():
    if INDEX_FILE.exists():
        return FileResponse(str(INDEX_FILE))
    return HTMLResponse("<h1>Frontend not built</h1>", status_code=500)

# SPA catch-all that WON'T swallow /api/* or /assets/*
@app.get("/{full_path:path}", response_class=HTMLResponse)
async def _spa(full_path: str):
    if full_path.startswith(("api/", "assets/")):
        return HTMLResponse(status_code=404)
    if INDEX_FILE.exists():
        return FileResponse(str(INDEX_FILE))
    return HTMLResponse("<h1>Frontend not built</h1>", status_code=500)

# --------------------------------------------------------

# singleton
@app.on_event("startup")
async def _startup():
    # Do NOT auto-load a model at startup; load on demand via /api/models/load
    asyncio.create_task(start_worker(), name="retitle_worker")

    # Start non-blocking system poller and keep a handle so we can cancel/await it
    t = asyncio.create_task(poll_system_snapshot(1.0), name="system_snapshot_poller")
    app.state.bg_tasks.append(t)

    log.info("🟡 Model will be loaded on-demand (via /api/models/load)")

@app.on_event("shutdown")
async def _shutdown():
    # 1) Cancel and await background tasks so they don't touch a torn-down executor
    tasks = getattr(app.state, "bg_tasks", [])
    for t in tasks:
        try:
            t.cancel()
        except Exception:
            pass
    if tasks:
        try:
            await asyncio.gather(*tasks, return_exceptions=True)
        except Exception:
            pass

    # 2) Ensure any spawned model workers are terminated to free VRAM/ports
    try:
        n = await supervisor.stop_all()
        log.info("[workers] shutdown: stopped %s worker(s)", n)
    except Exception as e:
        log.warning("[workers] shutdown stop_all error: %r", e)

@app.middleware("http")
async def _capture_auth_headers(request: Request, call_next):
    auth = (request.headers.get("authorization") or "").strip()
    if auth.lower().startswith("bearer "):
        request_ctx.set_id_token(auth.split(None, 1)[1])
    else:
        request_ctx.set_id_token("")
    request_ctx.set_x_id((request.headers.get("x-id") or "").strip())
    return await call_next(request)

# ===== aimodel/file_read/backend/.runtime/ports.json =====

{ "api_port": 8001 }

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/admins.py =====

# aimodel/file_read/core/admins.py
from __future__ import annotations

from datetime import UTC, datetime

from ..store.base import (APP_DIR, read_json_encrypted_org,
                          write_json_encrypted_org)
from .logging import get_logger

log = get_logger(__name__)

SEC_DIR = APP_DIR / "security"
SEC_DIR.mkdir(parents=True, exist_ok=True)


# Single-admin record (not a list)
ADMIN_PATH = SEC_DIR / "admin.json"


def _now() -> str:
    return datetime.now(UTC).isoformat()


# ---------- Single admin (encrypted, org-wide) ----------


def _load_admin() -> dict | None:
    if not ADMIN_PATH.exists():
        return None
    try:
        return read_json_encrypted_org(APP_DIR, ADMIN_PATH)
    except Exception as e:
        log.error(f"[admin] failed to read admin file: {e!r}")
        return None


def _save_admin(obj: dict):
    write_json_encrypted_org(APP_DIR, ADMIN_PATH, obj)


def has_admin() -> bool:
    return _load_admin() is not None


def get_admin() -> dict | None:
    """
    Returns: {"uid": str, "email": str, "setAt": str, "guestEnabled": bool} or None
    """
    return _load_admin()


def set_admin(uid: str, email: str):
    """
    Set the single admin if (and only if) none exists yet.
    """
    if has_admin():
        # do nothing if admin already exists
        return
    rec = {
        "v": 1,
        "uid": (uid or "").strip(),
        "email": (email or "").strip().lower(),
        "setAt": _now(),
        "guestEnabled": False,
    }
    _save_admin(rec)


def is_admin(uid: str | None) -> bool:
    if not uid:
        return False
    adm = _load_admin()
    return bool(adm and adm.get("uid") == uid)


# ---------- Guest mode toggle ----------


def get_guest_enabled() -> bool:
    adm = _load_admin()
    return bool(adm and adm.get("guestEnabled"))


def set_guest_enabled(enabled: bool):
    adm = _load_admin() or {}
    if not adm:
        # no admin set → ignore
        return
    adm["guestEnabled"] = bool(enabled)
    adm.setdefault("v", 1)
    _save_admin(adm)


__all__ = [
    "get_admin",
    "get_guest_enabled",
    "has_admin",
    "is_admin",
    "set_admin",
    "set_guest_enabled",
]

# ===== aimodel/file_read/core/crypto_keys.py =====

# core/crypto_keys.py  (new file; minimal)
import base64
import os
import secrets

SERVICE = "LocalMindChats"
KEY_ENV = "LOCALMIND_DEK_BASE64"
ORG_SERVICE = "LocalMindOrg"
ORG_ENV = "LOCALMIND_ORG_DEK_BASE64"  # emergency override


def _from_env():
    v = os.getenv(KEY_ENV, "").strip()
    if v:
        raw = base64.b64decode(v)
        if len(raw) == 32:
            return raw
    return None


def _from_keyring(uid: str):
    try:
        import keyring

        b64 = keyring.get_password(SERVICE, uid)
        if b64:
            raw = base64.b64decode(b64)
            if len(raw) == 32:
                return raw
        # create and save
        raw = secrets.token_bytes(32)
        keyring.set_password(SERVICE, uid, base64.b64encode(raw).decode())
        return raw
    except Exception:
        return None


# Start simple: env → keyring → crash (you can add Argon2 fallback later)
_cache: dict[str, bytes] = {}


def get_user_dek(uid: str) -> bytes:
    if uid in _cache:
        return _cache[uid]
    env = _from_env()
    if env:
        _cache[uid] = env
        return env
    kr = _from_keyring(uid)
    if kr:
        _cache[uid] = kr
        return kr
    raise RuntimeError("No keyring available and no env DEK provided")


def get_org_dek() -> bytes:
    v = os.getenv(ORG_ENV, "").strip()
    if v:
        raw = base64.b64decode(v)
        if len(raw) == 32:
            return raw
    try:
        import keyring

        b64 = keyring.get_password(ORG_SERVICE, "org")
        if b64:
            raw = base64.b64decode(b64)
            if len(raw) == 32:
                return raw
        raw = secrets.token_bytes(32)
        keyring.set_password(ORG_SERVICE, "org", base64.b64encode(raw).decode())
        return raw
    except Exception:
        raise RuntimeError("No org key: set LOCALMIND_ORG_DEK_BASE64 or install keyring")

# ===== aimodel/file_read/core/files.py =====

# aimodel/file_read/core/files.py
from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any

from ..core.logging import get_logger

log = get_logger(__name__)

CORE_DIR = Path(__file__).resolve().parent
STORE_DIR = CORE_DIR.parent / "store"

EFFECTIVE_SETTINGS_FILE = Path(
    os.getenv("EFFECTIVE_SETTINGS_PATH", str(STORE_DIR / "effective_settings.json"))
)
OVERRIDES_SETTINGS_FILE = Path(
    os.getenv("OVERRIDES_SETTINGS_PATH", str(STORE_DIR / "override_settings.json"))
)
DEFAULTS_SETTINGS_FILE = Path(
    os.getenv("DEFAULT_SETTINGS_PATH", str(STORE_DIR / "default_settings.json"))
)


def load_json_file(path: Path, default: Any = None) -> Any:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {} if default is None else default


def save_json_file(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ===== aimodel/file_read/core/http.py =====

# aimodel/file_read/web/http.py
from __future__ import annotations

import json
from typing import Any

import httpx

from ..core.logging import get_logger

log = get_logger(__name__)

DEFAULT_TIMEOUT = httpx.Timeout(connect=2.0, read=10.0, write=2.0, pool=2.0)
DEFAULT_LIMITS = httpx.Limits(max_keepalive_connections=20, max_connections=40)


class ExternalServiceError(RuntimeError):
    def __init__(
        self,
        *,
        service: str,
        url: str,
        status: int | None = None,
        body_preview: str | None = None,
        detail: str | None = None,
    ) -> None:
        msg = f"{service} request failed status={status} url={url}"
        if detail:
            msg += f" detail={detail}"
        super().__init__(msg)
        self.service = service
        self.url = url
        self.status = status
        self.body_preview = body_preview
        self.detail = detail


# a single shared async client (uvicorn lifespan keeps it around)
_async_client: httpx.AsyncClient | None = None


async def get_client() -> httpx.AsyncClient:
    global _async_client
    if _async_client is None:
        _async_client = httpx.AsyncClient(
            timeout=DEFAULT_TIMEOUT,
            limits=DEFAULT_LIMITS,
            follow_redirects=False,
        )
    return _async_client


async def arequest_json(
    *,
    method: str,
    url: str,
    service: str,
    headers: dict[str, str] | None = None,
    params: dict[str, Any] | None = None,
    json_body: dict[str, Any] | None = None,
    timeout: httpx.Timeout | None = None,
) -> dict[str, Any]:
    """Make an async request that:
    - sets sane timeouts,
    - raises for non-2xx,
    - returns parsed JSON,
    - logs structured events,
    - throws ExternalServiceError on failure.
    """
    client = await get_client()
    try:
        resp = await client.request(
            method.upper(),
            url,
            headers=headers,
            params=params,
            json=json_body,
            timeout=timeout or DEFAULT_TIMEOUT,
        )
        resp.raise_for_status()

        ctype = (resp.headers.get("content-type") or "").lower()
        if "application/json" in ctype:
            return resp.json()
        # best-effort JSON parse, else wrap as {"text": "..."}
        try:
            return json.loads(resp.text)
        except Exception:
            return {"text": resp.text}

    except httpx.HTTPStatusError as e:
        body_preview = (e.response.text or "")[:400] if e.response is not None else None
        log.warning(
            "http_status_error",
            extra={
                "service": service,
                "url": url,
                "status": e.response.status_code if e.response else None,
                "body_preview": body_preview,
            },
        )
        raise ExternalServiceError(
            service=service,
            url=url,
            status=e.response.status_code if e.response else None,
            body_preview=body_preview,
            detail=str(e),
        ) from e

    except httpx.TimeoutException as e:
        log.warning("http_timeout", extra={"service": service, "url": url})
        raise ExternalServiceError(service=service, url=url, detail="timeout") from e

    except httpx.RequestError as e:
        log.error("http_request_error", extra={"service": service, "url": url, "detail": str(e)})
        raise ExternalServiceError(service=service, url=url, detail=str(e)) from e

# ===== aimodel/file_read/core/logging.py =====

# aimodel/core/logging.py
from __future__ import annotations

import logging
import sys

# request_ctx lives in the same package
try:
    from . import request_ctx  # aimodel/core/request_ctx.py
except Exception:
    request_ctx = None


class ContextFilter(logging.Filter):
    def filter(self, record: logging.LogRecord) -> bool:
        xid = ""
        sid = ""
        try:
            if request_ctx is not None:
                xid = (request_ctx.get_x_id() or "").strip()
                # If you add session id to request_ctx later, wire it here.
                # sid = (request_ctx.get_session_id() or "").strip()
        except Exception:
            pass
        record.xid = xid
        record.sid = sid
        return True


def _default_formatter() -> logging.Formatter:
    fmt = "%(asctime)s %(levelname)s %(name)s xid=%(xid)s sid=%(sid)s: %(message)s"
    return logging.Formatter(fmt)


def setup_logging(level: int = logging.INFO, *, json: bool = False) -> None:
    root = logging.getLogger()
    root.setLevel(level)

    # ensure a single stdout handler exists (uvicorn may add one)
    stream = None
    for h in root.handlers:
        if isinstance(h, logging.StreamHandler):
            stream = h
            break
    if stream is None:
        stream = logging.StreamHandler(sys.stdout)
        root.addHandler(stream)

    # always apply our formatter + context filter (don’t bail early)
    stream.setFormatter(_default_formatter())
    if not any(isinstance(f, ContextFilter) for f in stream.filters):
        stream.addFilter(ContextFilter())

    # keep uvicorn loggers at same level so our logs aren’t hidden
    logging.getLogger("uvicorn").setLevel(level)
    logging.getLogger("uvicorn.error").setLevel(level)
    logging.getLogger("uvicorn.access").setLevel(level)


def get_logger(name: str | None = None) -> logging.Logger:
    lg = logging.getLogger(name or __name__)
    lg.propagate = True
    lg.setLevel(logging.NOTSET)
    return lg

# ===== aimodel/file_read/core/packing_memory_core.py =====

from __future__ import annotations

import math
import time
from collections import deque

from ..core.logging import get_logger

log = get_logger(__name__)

from ..core.settings import SETTINGS
from ..runtime.model_runtime import get_llm
from ..store import get_summary as store_get_summary
from ..store import list_messages as store_list_messages
from ..telemetry.models import PackTel
from ..utils.streaming import strip_runjson
from .style import get_style_sys

SESSIONS: dict[str, dict] = {}
PACK_TELEMETRY = PackTel()
SUMMARY_TEL = PACK_TELEMETRY


def _S() -> dict:
    """Convenience accessor for effective settings."""
    return SETTINGS.effective()


def approx_tokens(text: str) -> int:
    cfg = _S()
    chars_per_token = int(cfg.get("chars_per_token", 4))
    return max(1, math.ceil(len(text or "") / chars_per_token))


def count_prompt_tokens(msgs: list[dict[str, str]]) -> int:
    cfg = _S()
    overhead = int(cfg.get("prompt_per_message_overhead", 4))
    return sum(approx_tokens(m.get("content", "")) + overhead for m in msgs)


def get_session(session_id: str):
    cfg = _S()
    recent_maxlen = int(cfg.get("recent_maxlen", 50))
    st = SESSIONS.setdefault(
        session_id,
        {
            "summary": "",
            "recent": deque(maxlen=recent_maxlen),
            "style": get_style_sys(),
            "short": False,
            "bullets": False,
        },
    )
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
        except Exception:
            pass
    if not st["recent"]:
        try:
            rows = store_list_messages(session_id)
            tail = rows[-st["recent"].maxlen :]
            for m in tail:
                st["recent"].append({"role": m.role, "content": strip_runjson(m.content)})
        except Exception:
            pass
    return st


def _heuristic_bullets(chunks: list[dict[str, str]], cfg: dict) -> str:
    max_bullets = int(cfg.get("heuristic_max_bullets", 8))
    max_words = int(cfg.get("heuristic_max_words", 40))
    prefix = cfg.get("bullet_prefix", "• ")

    bullets: list[str] = []
    for m in chunks:
        txt = " ".join((m.get("content") or "").split())
        if not txt:
            continue
        words = txt.replace("\n", " ").split()
        snippet = " ".join(words[:max_words]) if words else ""
        bullets.append(f"{prefix}{snippet}" if snippet else prefix.strip())
        if len(bullets) >= max_bullets:
            break
    return "\n".join(bullets) if bullets else prefix.strip()


def summarize_chunks(chunks: list[dict[str, str]]) -> tuple[str, bool]:
    cfg = _S()
    t0 = time.time()
    PACK_TELEMETRY["summarySec"] = 0.0
    PACK_TELEMETRY["summaryTokensApprox"] = 0
    PACK_TELEMETRY["summaryUsedLLM"] = False
    PACK_TELEMETRY["summaryBullets"] = 0
    PACK_TELEMETRY["summaryAddedChars"] = 0
    PACK_TELEMETRY["summaryOutTokensApprox"] = 0

    if bool(cfg.get("use_fast_summary", True)):
        txt = _heuristic_bullets(chunks, cfg)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = False
        PACK_TELEMETRY["summaryBullets"] = len([l for l in txt.splitlines() if l.strip()])
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        return txt, False

    # LLM summary path
    text = "\n".join(f"{m.get('role', '')}: {m.get('content', '')}" for m in chunks)
    sys_inst = cfg.get("summary_sys_inst", "")
    user_prefix = cfg.get("summary_user_prefix", "")
    user_suffix = cfg.get("summary_user_suffix", "")
    user_prompt = user_prefix + text + user_suffix

    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": sys_inst},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=int(cfg.get("llm_summary_max_tokens", 256)),
        temperature=float(cfg.get("llm_summary_temperature", 0.2)),
        top_p=float(cfg.get("llm_summary_top_p", 1.0)),
        stream=False,
        stop=list(cfg.get("llm_summary_stop", [])),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip()
    lines = [ln.strip() for ln in raw.splitlines()]
    bullets: list[str] = []
    seen = set()
    max_words = int(cfg.get("heuristic_max_words", 40))
    max_bullets = int(cfg.get("heuristic_max_bullets", 8))
    bullet_prefix = cfg.get("bullet_prefix", "• ")

    for ln in lines:
        if not ln.startswith(bullet_prefix):
            continue
        norm = " ".join(ln[len(bullet_prefix) :].lower().split())
        if not norm or norm in seen:
            continue
        seen.add(norm)
        words = ln[len(bullet_prefix) :].split()
        if len(words) > max_words:
            ln = bullet_prefix + " ".join(words[:max_words])
        bullets.append(ln)
        if len(bullets) >= max_bullets:
            break

    if bullets:
        txt = "\n".join(bullets)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(
            approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(txt)
        )
        PACK_TELEMETRY["summaryUsedLLM"] = True
        PACK_TELEMETRY["summaryBullets"] = len(bullets)
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        return txt, True

    # Fallback: single bullet
    s = " ".join(raw.split())[:160]
    fallback = (bullet_prefix + s) if s else bullet_prefix.strip()
    dt = time.time() - t0
    PACK_TELEMETRY["summarySec"] = float(dt)
    PACK_TELEMETRY["summaryTokensApprox"] = int(
        approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(fallback)
    )
    PACK_TELEMETRY["summaryUsedLLM"] = True
    PACK_TELEMETRY["summaryBullets"] = len([l for l in fallback.splitlines() if l.strip()])
    PACK_TELEMETRY["summaryAddedChars"] = len(fallback)
    PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(fallback))
    return fallback, True


def _compress_summary_block(s: str) -> str:
    cfg = _S()
    max_chars = int(cfg.get("summary_max_chars", 1200))
    prefix = cfg.get("bullet_prefix", "• ")
    lines = [ln.strip() for ln in (s or "").splitlines()]

    out, seen = [], set()
    for ln in lines:
        if not ln.startswith(prefix):
            continue
        norm = " ".join(ln[len(prefix) :].lower().split())
        if norm in seen:
            continue
        seen.add(norm)
        out.append(ln)

    text = "\n".join(out)
    PACK_TELEMETRY["summaryCompressedFromChars"] = len(s or "")

    if len(text) > max_chars:
        last, total = [], 0
        for ln in reversed(out):
            if total + len(ln) + 1 > max_chars:
                break
            last.append(ln)
            total += len(ln) + 1
        text = "\n".join(reversed(last))

    PACK_TELEMETRY["summaryCompressedToChars"] = len(text)
    PACK_TELEMETRY["summaryCompressedDroppedChars"] = int(
        max(
            0,
            int(PACK_TELEMETRY["summaryCompressedFromChars"])
            - int(PACK_TELEMETRY["summaryCompressedToChars"]),
        )
    )
    return text

# ===== aimodel/file_read/core/packing_ops.py =====

from __future__ import annotations

import time

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from .packing_memory_core import (PACK_TELEMETRY, _compress_summary_block,
                                  approx_tokens, count_prompt_tokens,
                                  summarize_chunks)
from .style import get_style_sys

log = get_logger(__name__)


def _S() -> dict:
    """Effective settings snapshot."""
    return SETTINGS.effective()


def build_system(style: str, short: bool, bullets: bool) -> str:
    cfg = _S()
    base = get_style_sys()

    parts: list[str] = [base]
    if style and style != base:
        parts.append(style)

    if short:
        parts.append(cfg.get("system_brief_directive", ""))

    if bullets:
        parts.append(cfg.get("system_bullets_directive", ""))

    follow = cfg.get("system_follow_user_style_directive", "")
    if follow:
        parts.append(follow)

    # Avoid stray spaces if some directives are blank
    return " ".join(p for p in parts if p)


def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    t0_pack = time.time()
    cfg = _S()

    model_ctx = int(max_ctx or cfg.get("model_ctx", 8192))
    gen_budget = int(out_budget or cfg.get("out_budget", 1024))
    reserved = int(cfg.get("reserved_system_tokens", 256))
    min_input_budget = int(cfg.get("min_input_budget", 512))

    input_budget = model_ctx - gen_budget - reserved
    if input_budget < min_input_budget:
        input_budget = min_input_budget

    sys_text = build_system(style, short, bullets)
    prologue = [{"role": "user", "content": sys_text}]

    summary_header_prefix = cfg.get("summary_header_prefix", "## Summary\n")
    if summary:
        prologue.append({"role": "user", "content": summary_header_prefix + summary})

    packed = prologue + list(recent)

    try:
        PACK_TELEMETRY["packInputTokensApprox"] = int(count_prompt_tokens(packed))
        PACK_TELEMETRY["packMsgs"] = len(packed)
    except Exception:
        pass

    PACK_TELEMETRY["packSec"] += float(time.time() - t0_pack)
    return packed, input_budget


def _final_safety_trim(packed: list[dict[str, str]], input_budget: int) -> list[dict[str, str]]:
    t0 = time.time()
    cfg = _S()

    keep_ratio = float(cfg.get("final_shrink_summary_keep_ratio", 0.5))
    min_keep = int(cfg.get("final_shrink_summary_min_chars", 200))
    summary_header_prefix = cfg.get("summary_header_prefix", "## Summary")

    def toks() -> int:
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999_999

    t_before = toks()
    PACK_TELEMETRY["finalTrimTokensBefore"] = int(t_before)

    dropped_msgs = 0
    dropped_tokens = 0

    keep_head = (
        2
        if len(packed) >= 2
        and isinstance(packed[1].get("content"), str)
        and packed[1]["content"].startswith(summary_header_prefix)
        else 1
    )

    # Drop older user/assistant messages first, keep system + summary head
    while toks() > input_budget and len(packed) > keep_head + 1:
        dropped = packed.pop(keep_head)
        try:
            dropped_tokens += int(approx_tokens(dropped.get("content", "")))
            dropped_msgs += 1
        except Exception:
            pass

    # If still over budget, shrink the summary body (tail keep to retain latest context)
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        summary_msg = packed[1]
        txt = summary_msg.get("content", "")
        n = max(min_keep, int(len(txt) * keep_ratio))
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"] = len(txt)
        except Exception:
            pass

        summary_msg["content"] = txt[-n:]

        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkToChars"] = len(summary_msg["content"])
            PACK_TELEMETRY["finalTrimSummaryDroppedChars"] = int(
                max(
                    0,
                    int(PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"])
                    - int(PACK_TELEMETRY["finalTrimSummaryShrunkToChars"]),
                )
            )
        except Exception:
            pass

    # As a last resort, remove the summary message entirely
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        removed = packed.pop(1)
        try:
            dropped_tokens += int(approx_tokens(removed.get("content", "")))
            dropped_msgs += 1
        except Exception:
            pass

    # Still over? Start trimming recent tail
    while toks() > input_budget and len(packed) > 2:
        removed = packed.pop(2 if len(packed) > 3 else 1)
        try:
            dropped_tokens += int(approx_tokens(removed.get("content", "")))
            dropped_msgs += 1
        except Exception:
            pass

    t_after = toks()
    PACK_TELEMETRY["finalTrimTokensAfter"] = int(t_after)
    PACK_TELEMETRY["finalTrimDroppedMsgs"] = int(dropped_msgs)
    PACK_TELEMETRY["finalTrimDroppedApproxTokens"] = int(max(0, dropped_tokens))
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0)
    return packed


def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    cfg = _S()

    def _tok():
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999_999

    start_tokens = _tok()
    overage = start_tokens - input_budget
    PACK_TELEMETRY["rollStartTokens"] = int(start_tokens)
    PACK_TELEMETRY["rollOverageTokens"] = int(overage)

    if overage <= int(cfg.get("skip_overage_lt", 128)):
        packed = _final_safety_trim(packed, input_budget)
        PACK_TELEMETRY["rollEndTokens"] = int(count_prompt_tokens(packed))
        return packed, summary

    peels_done = 0
    peeled_n = 0

    max_peel_per_turn = int(cfg.get("max_peel_per_turn", 1))
    if len(recent) > 6 and peels_done < max_peel_per_turn:
        peel_min = int(cfg.get("peel_min", 3))
        peel_frac = float(cfg.get("peel_frac", 0.2))
        peel_max = int(cfg.get("peel_max", 12))

        target = max(peel_min, min(peel_max, int(len(recent) * peel_frac)))
        peel = []
        for _ in range(min(target, len(recent))):
            peel.append(recent.popleft())
        peeled_n = len(peel)

        # Summarize peeled messages
        t0_sum = time.time()
        new_sum, _used_llm = summarize_chunks(peel)
        PACK_TELEMETRY["summarySec"] += float(time.time() - t0_sum)

        bullet_prefix = cfg.get("bullet_prefix", "- ")
        if new_sum.startswith(bullet_prefix):
            summary = (summary + "\n" + new_sum).strip() if summary else new_sum
        else:
            summary = new_sum

        # Compress summary
        t0_comp = time.time()
        summary = _compress_summary_block(summary)
        PACK_TELEMETRY["compressSec"] += float(time.time() - t0_comp)

        try:
            PACK_TELEMETRY["rollPeeledMsgs"] = int(peeled_n)
            PACK_TELEMETRY["rollNewSummaryChars"] = len(summary)
            PACK_TELEMETRY["rollNewSummaryTokensApprox"] = int(approx_tokens(summary))
        except Exception:
            pass

        summary_header_prefix = cfg.get("summary_header_prefix", "## Summary\n")
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": summary_header_prefix + summary},
          