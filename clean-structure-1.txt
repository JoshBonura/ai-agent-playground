
# ===== aimodel/file_read/__init__.py =====

from .adaptive.config.paths import app_data_dir, read_settings, write_settings
from .runtime.model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/adaptive/config/adaptive_config.py =====

# aimodel/file_read/runtime/adaptive_config.py
from __future__ import annotations
import os, shutil, subprocess, platform
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

try:
    import psutil
except Exception:
    psutil = None
try:
    import torch
except Exception:
    torch = None

from .paths import read_settings

def _env_bool(k:str, default:bool)->bool:
    v = os.getenv(k)
    if v is None: return default
    return v.strip().lower() in ("1","true","yes","on")

def _cpu_count()->int:
    try:
        import multiprocessing as mp
        return max(1, mp.cpu_count() or os.cpu_count() or 1)
    except Exception:
        return os.cpu_count() or 1

def _avail_ram()->Optional[int]:
    if not psutil: return None
    try: return int(psutil.virtual_memory().available)
    except Exception: return None

def _cuda_vram()->Optional[int]:
    if torch and torch.cuda.is_available():
        try:
            dev = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(dev)
            return int(props.total_memory)
        except Exception:
            pass
    if shutil.which("nvidia-smi"):
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-gpu=memory.total","--format=csv,noheader,nounits"],
                text=True, stderr=subprocess.DEVNULL, timeout=2.0
            )
            mb = max(int(x.strip()) for x in out.strip().splitlines() if x.strip())
            return mb * 1024 * 1024
        except Exception:
            return None
    return None

def _gpu_kind()->str:
    if _cuda_vram(): return "cuda"
    if torch and getattr(torch.backends,"mps",None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

def _safe_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default

def _pick_dtype_quant(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> tuple[Optional[str], Optional[str]]:
    dq = a.get("dtype_quant", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = dq.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype"), best.get("quant")
        return dq.get("cuda_default_dtype"), dq.get("cuda_default_quant")
    if device == "mps":
        return dq.get("mps_default_dtype"), None
    return dq.get("cpu_default_dtype"), dq.get("cpu_default_quant")

def _pick_kv(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> Optional[str]:
    kv = a.get("kv_cache", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = kv.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype")
        return kv.get("cuda_default")
    if device == "mps":
        return kv.get("mps_default")
    return kv.get("cpu_default")

def _pick_capacity(device: str, a: Dict[str, Any], vram_bytes: Optional[int], threads:int) -> tuple[int,int,Optional[int]]:
    cap = a.get("capacity", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = cap.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return int(best.get("seq_len") or 0), int(best.get("batch") or 1), int(best.get("n_gpu_layers") or 0)
        return 0, 1, 0
    if device == "mps":
        m = cap.get("mps", {})
        return int(m.get("seq_len") or 0), int(m.get("batch") or 1), 0
    cpu = cap.get("cpu", {})
    seq_len = int(cpu.get("seq_len") or 0)
    batch = 1
    by = cpu.get("batch_by_threads") or []
    best = None
    for t in sorted(by, key=lambda x: int(x.get("min_threads", 0)), reverse=True):
        if threads >= int(t.get("min_threads") or 0):
            best = t
            break
    if best:
        batch = int(best.get("batch") or 1)
    return seq_len, batch, 0

def _gpu_mem_fraction(device:str, a: Dict[str, Any]) -> float:
    table = a.get("gpu_fraction", {}) if isinstance(a, dict) else {}
    v = table.get(device)
    return _safe_float(v, 0.0)

def _torch_flags(device:str, a: Dict[str, Any]) -> tuple[bool,bool]:
    flags = a.get("flags", {}) if isinstance(a, dict) else {}
    flash = bool(flags.get("enable_flash_attn_cuda")) if device == "cuda" else False
    tc = bool(flags.get("use_torch_compile_on_cuda_linux")) if (device == "cuda" and platform.system().lower()=="linux") else False
    return flash, tc

def _threads(a: Dict[str, Any]) -> tuple[int,int,int,int]:
    policy = a.get("cpu_threads_policy", {}) if isinstance(a, dict) else {}
    mode = str(policy.get("mode") or "").lower()
    ncpu = _cpu_count()
    if mode == "fixed":
        v = int(policy.get("value") or max(1, ncpu-1))
        t = max(1, min(v, ncpu))
    elif mode == "percent":
        pct = _safe_float(policy.get("value"), 0.0)
        t = max(1, min(ncpu, int(round(ncpu*pct/100.0))))
        if t < 1: t = 1
    else:
        t = max(1, ncpu-1)
    intra = t
    inter = max(1, ncpu//2)
    return ncpu, t, intra, inter

@dataclass
class AdaptiveConfig:
    device: str
    dtype: Optional[str]
    quant: Optional[str]
    kv_cache_dtype: Optional[str]
    max_seq_len: int
    max_batch_size: int
    gpu_memory_fraction: float
    cpu_threads: int
    torch_intraop_threads: int
    torch_interop_threads: int
    enable_flash_attn: bool
    use_torch_compile: bool
    total_vram_bytes: Optional[int]
    avail_ram_bytes: Optional[int]
    cpu_count: int
    def as_dict(self)->Dict[str,Any]:
        return asdict(self)

def compute_adaptive_config()->AdaptiveConfig:
    settings = read_settings()
    a = settings.get("adaptive", {}) if isinstance(settings, dict) else {}
    device = _gpu_kind()
    vram = _cuda_vram() if device=="cuda" else None
    ram = _avail_ram()
    ncpu, threads, intra, inter = _threads(a)
    dtype, quant = _pick_dtype_quant(device, a, vram)
    kv = _pick_kv(device, a, vram)
    seq_len, batch, n_gpu_layers = _pick_capacity(device, a, vram, threads)
    frac = _gpu_mem_fraction(device, a)
    flash, tcompile = _torch_flags(device, a)
    return AdaptiveConfig(
        device=device,
        dtype=dtype,
        quant=quant,
        kv_cache_dtype=kv,
        max_seq_len=int(seq_len or 0),
        max_batch_size=int(batch or 1),
        gpu_memory_fraction=frac,
        cpu_threads=threads,
        torch_intraop_threads=intra,
        torch_interop_threads=inter,
        enable_flash_attn=flash,
        use_torch_compile=tcompile,
        total_vram_bytes=vram,
        avail_ram_bytes=ram,
        cpu_count=ncpu
    )

# ===== aimodel/file_read/adaptive/config/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/adaptive/config/settings.json =====

{
  "adaptiveEnabled": false,
  "adaptive": {
    "enabled": true,
    "cpu_threads_policy": {
      "mode": "leave_one"
    },
    "dtype_quant": {
      "cuda_default_dtype": "float16",
      "cuda_default_quant": null,
      "mps_default_dtype": "float16",
      "cpu_default_dtype": "int8",
      "cpu_default_quant": "q4_K_M",
      "cuda_tiers": [
        { "min_vram_gb": 24, "dtype": "bfloat16", "quant": null },
        { "min_vram_gb": 12, "dtype": "float16", "quant": null },
        { "min_vram_gb": 6,  "dtype": "float16", "quant": "bnb-int8" },
        { "min_vram_gb": 4,  "dtype": "float16", "quant": "bnb-int8" }
      ]
    },
    "kv_cache": {
      "cuda_default": "fp8",
      "mps_default": "fp16",
      "cpu_default": "fp32",
      "cuda_tiers": [
        { "min_vram_gb": 16, "dtype": "fp16" },
        { "min_vram_gb": 0,  "dtype": "fp8" }
      ]
    },
    "capacity": {
      "cuda_tiers": [
        { "min_vram_gb": 24, "seq_len": 8192, "batch": 8, "n_gpu_layers": 9999 },
        { "min_vram_gb": 12, "seq_len": 4096, "batch": 4, "n_gpu_layers": 48 },
        { "min_vram_gb": 8,  "seq_len": 3072, "batch": 2, "n_gpu_layers": 40 },
        { "min_vram_gb": 6,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 32 },
        { "min_vram_gb": 4,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 28 }
      ],
      "mps": { "seq_len": 2048, "batch": 1 },
      "cpu": {
        "seq_len": 2048,
        "batch_by_threads": [
          { "min_threads": 16, "batch": 8 },
          { "min_threads": 8,  "batch": 4 },
          { "min_threads": 1,  "batch": 2 }
        ]
      }
    },
    "gpu_fraction": {
      "cuda": 0.8,
      "mps": 0.7,
      "cpu": 0.0
    },
    "flags": {
      "enable_flash_attn_cuda": true,
      "use_torch_compile_on_cuda_linux": true
    },
    "batchTokenMap": [
      { "minConcurrency": 8, "n_batch": 512 },
      { "minConcurrency": 4, "n_batch": 384 },
      { "minConcurrency": 2, "n_batch": 256 },
      { "minConcurrency": 1, "n_batch": 192 }
    ]
  },
  "modelPath": "",
  "nCtx": null,
  "nThreads": null,
  "nGpuLayers": null,
  "nBatch": null
}

# ===== aimodel/file_read/adaptive/controller.py =====



# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/chats.py =====

# ===== aimodel/file_read/api/chats.py =====
from __future__ import annotations

from ..core.schemas import ChatMessage
from dataclasses import asdict
from typing import List, Optional, Dict
from ..utils.streaming import strip_runjson
from fastapi import APIRouter
from pydantic import BaseModel
from ..services.cancel import GEN_SEMAPHORE
from ..workers.retitle_worker import enqueue as enqueue_retitle  # ✅ import the enqueuer

from ..core.schemas import (
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    MergeChatReq,
    EditMessageReq,
)

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    merge_chat as store_merge_chat,
    merge_chat_new as store_merge_chat_new,
    edit_message as edit_message,
)

router = APIRouter()

# ---------- Routes ----------
@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)

@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]


@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, msg: ChatMessage):
    role = msg.role
    content = (msg.content or "").rstrip()
    attachments = msg.attachments or []

    row = store_append(session_id, role, content, attachments=attachments)

    if role == "assistant":
        try:
            msgs = store_list_messages(session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            msgs_clean = []
            for m in msgs:
                dm = asdict(m)
                dm["content"] = strip_runjson(dm.get("content") or "")
                msgs_clean.append(dm)
            enqueue_retitle(session_id, msgs_clean, job_seq=last_seq)
        except Exception:
            pass

    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}

@router.post("/api/chats/merge")
async def api_merge_chat(req: MergeChatReq):
    if req.newChat:
        new_id, merged = store_merge_chat_new(req.sourceId, req.targetId)
        return {"newChatId": new_id, "mergedCount": len(merged)}
    else:
        merged = store_merge_chat(req.sourceId, req.targetId)
        return {"mergedCount": len(merged)}

@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

# aimodel/file_read/api/generate_router.py
from __future__ import annotations
from fastapi import APIRouter, Body, Request
from ..core.schemas import ChatBody
from ..services.generate_flow import generate_stream_flow, cancel_session, cancel_session_alias
from ..services.cancel import is_active  # re-export for back-compat

router = APIRouter()

@router.post("/generate/stream")
async def generate_stream(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

# legacy alias (kept identical)
@router.post("/api/ai/generate/stream")
async def generate_stream_alias(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

@router.post("/cancel/{session_id}")
async def _cancel_session(session_id: str):
    return await cancel_session(session_id)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/models.py =====

from __future__ import annotations
from typing import Optional, Dict
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..adaptive.config.paths import read_settings, write_settings
from ..runtime.model_runtime import (
    list_local_models, current_model_info,
    load_model, unload_model
)

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/api/rag.py =====

# aimodel/file_read/api/rag.py
from __future__ import annotations
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from typing import Optional, List, Dict
import numpy as np
from sentence_transformers import SentenceTransformer
from threading import RLock
from ..rag.uploads import list_sources as rag_list_sources, hard_delete_source
from ..rag.schemas import SearchReq, SearchHit
from ..rag.ingest import sniff_and_extract, chunk_text, build_metas
from ..rag.store import add_vectors, search_vectors

router = APIRouter(prefix="/api/rag", tags=["rag"])

# ---- embedding model (sync, cached) ----
_st_model: SentenceTransformer | None = None
_st_lock = RLock()

def _get_st_model() -> SentenceTransformer:
    global _st_model
    if _st_model is None:
        with _st_lock:
            if _st_model is None:
                print("[RAG EMBED] loading e5-small-v2… (one-time)")
                _st_model = SentenceTransformer("intfloat/e5-small-v2")
                print("[RAG EMBED] model ready")
    return _st_model

def _embed(texts: List[str]) -> np.ndarray:
    model = _get_st_model()
    arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
    return arr.astype("float32")

@router.post("/upload")
async def upload_doc(sessionId: Optional[str] = Form(default=None), file: UploadFile = File(...)):
    print(f"[RAG UPLOAD] sessionId={sessionId}, filename={file.filename}, content_type={file.content_type}")

    data = await file.read()
    print(f"[RAG UPLOAD] file size={len(data)} bytes")

    text, mime = sniff_and_extract(file.filename, data)
    print(f"[RAG UPLOAD] extracted mime={mime}, text_len={len(text)}")

    if not text.strip():
        raise HTTPException(status_code=400, detail="Empty/unsupported file")

    chunks = chunk_text(text, {"mime": mime})
    print(f"[RAG UPLOAD] chunk_count={len(chunks)}")

    metas = build_metas(sessionId, file.filename, chunks, size=len(data))
    embeds = _embed([c.text for c in chunks])
    print(f"[RAG UPLOAD] embed_shape={embeds.shape}")

    add_vectors(sessionId, embeds, metas, dim=embeds.shape[1])
    return {"ok": True, "added": len(chunks)}

@router.post("/search")
async def search(req: SearchReq):
    q = req.query.strip()
    if not q:
        return {"hits": []}
    qv = np.array(_embed([q])[0], dtype="float32")
    chat_hits = search_vectors(req.sessionId, qv, req.kChat, dim=qv.shape[0])
    global_hits = search_vectors(None, qv, req.kGlobal, dim=qv.shape[0])
    from ..rag.search import merge_chat_first
    fused = merge_chat_first(chat_hits, global_hits, alpha=req.hybrid_alpha)
    out: List[SearchHit] = []
    for r in fused:
        out.append(SearchHit(
            id=r["id"], text=r["text"], score=float(r["score"]),
            source=r.get("source"), title=r.get("title"), sessionId=r.get("sessionId")
        ))
    return {"hits": [h.model_dump() for h in out]}

@router.get("/list")
async def list_items(sessionId: Optional[str] = None, k: int = 20):

    qv = np.array(_embed(["list"])[0], dtype="float32")
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    items = []
    for h in hits:
        txt = (h.get("text") or "")
        items.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": txt,
        })
    print(f"[RAG LIST] sessionId={sessionId} k={k} -> {len(items)} items")
    return {"items": items}

@router.get("/dump")
async def dump_items(sessionId: Optional[str] = None, k: int = 50):
    """
    Debug: return full texts of top-k chunks (be careful: can be large).
    Useful to confirm exact strings that were indexed.
    """
    qv = np.array(_embed(["dump"])[0], dtype="float32")
    hits = search_vectors(sessionId, qv, k=k, dim=qv.shape[0])

    chunks = []
    for h in hits:
        chunks.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": h.get("text") or "",
        })
    print(f"[RAG DUMP] sessionId={sessionId} k={k} -> {len(chunks)} items")
    return {"chunks": chunks}

@router.get("/uploads")
async def api_list_uploads(sessionId: Optional[str] = None, scope: str = "all"):
    include_global = scope != "session"
    return {"uploads": rag_list_sources(sessionId, include_global=include_global)}

@router.post("/uploads/delete-hard")
async def api_delete_upload_hard(body: dict[str, str]):
    source = (body.get("source") or "").strip()
    session_id = (body.get("sessionId") or None)
    if not source:
        return {"error": "source required"}

    # use the same model as ingest/search
    model = _get_st_model()  # you already have this in rag.py
    def _embed(texts: List[str]):
        arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
        return arr.astype("float32")

    out = hard_delete_source(source, session_id=session_id, embedder=_embed)
    return out

# ===== aimodel/file_read/api/settings.py =====

from fastapi import APIRouter, Query, Body
from typing import Dict, Any, Optional

from ..core.settings import SETTINGS

router = APIRouter(prefix="/api/settings", tags=["settings"])

@router.get("/defaults")
def get_defaults():
    return SETTINGS.defaults

@router.get("/overrides")
def get_overrides():
    return SETTINGS.overrides

@router.patch("/overrides")
def patch_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.patch_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.put("/overrides")
def put_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.replace_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.get("/adaptive")
def get_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.adaptive(session_id=session_id)

@router.post("/adaptive/recompute")
def recompute_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    SETTINGS.recompute_adaptive(session_id=session_id)
    return {"ok": True, "adaptive": SETTINGS.adaptive(session_id=session_id)}

@router.get("/effective")
def get_effective(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.effective(session_id=session_id)

# ===== aimodel/file_read/app.py =====

# aimodel/file_read/app.py
from __future__ import annotations
import os, asyncio, atexit
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .adaptive.config.paths import bootstrap
from .workers.retitle_worker import start_worker
from .api.models import router as models_router
from .api.chats import router as chats_router
from .runtime.model_runtime import load_model
from .api.generate_router import router as generate_router
from .api.rag import router as rag_router
from .services.cancel import is_active
from .api import settings as settings_router
bootstrap()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("APP_CORS_ORIGIN", "http://localhost:5173")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(models_router)
app.include_router(chats_router)
app.include_router(generate_router)
app.include_router(settings_router.router)
app.include_router(rag_router)

@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")

    asyncio.create_task(start_worker(), name="retitle_worker")

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/files.py =====

# aimodel/file_read/core/files.py
from __future__ import annotations
from pathlib import Path
import json, os
from typing import Any

# Base dirs
CORE_DIR = Path(__file__).resolve().parent
STORE_DIR = CORE_DIR.parent / "store"

# Paths (overridable via env)
EFFECTIVE_SETTINGS_FILE = Path(
    os.getenv("EFFECTIVE_SETTINGS_PATH", str(STORE_DIR / "effective_settings.json"))
)
OVERRIDES_SETTINGS_FILE = Path(
    os.getenv("OVERRIDES_SETTINGS_PATH", str(STORE_DIR / "override_settings.json"))
)
DEFAULTS_SETTINGS_FILE = Path(
    os.getenv("DEFAULT_SETTINGS_PATH", str(STORE_DIR / "default_settings.json"))
)

def load_json_file(path: Path, default: Any = None) -> Any:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {} if default is None else default

def save_json_file(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ===== aimodel/file_read/core/memory.py =====

from __future__ import annotations
import math, time, json
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque

from ..runtime.model_runtime import get_llm
from .style import STYLE_SYS
from ..store import get_summary as store_get_summary
from ..store import list_messages as store_list_messages
from ..utils.streaming import strip_runjson
from ..core.files import EFFECTIVE_SETTINGS_FILE, load_json_file

SESSIONS: Dict[str, Dict] = {}

def _now() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

def _log(msg: str) -> None:
    print(f"[{_now()}] {msg}")

def _snapshot(cfg: Dict) -> str:
    """Short human-readable snapshot of the most important knobs."""
    keys = [
        "model_ctx", "out_budget", "reserved_system_tokens", "min_input_budget",
        "chars_per_token", "prompt_per_message_overhead",
        "summary_max_chars", "use_fast_summary",
    ]
    parts = []
    for k in keys:
        if k in cfg:
            parts.append(f"{k}={cfg[k]}")
    return ", ".join(parts)

class _SettingsCache:
    def __init__(self) -> None:
        self.path: Path = EFFECTIVE_SETTINGS_FILE  # central path
        self._mtime: float | None = None
        self._data: Dict = {}
        _log(f"settings path = {self.path}")

    def get(self) -> Dict:
        """Load EFFECTIVE settings (defaults ⟶ adaptive ⟶ overrides)."""
        try:
            m = self.path.stat().st_mtime
        except FileNotFoundError:
            m = None

        if self._mtime != m or not self._data:
            self._data = load_json_file(self.path, default={})
            self._mtime = m
            _log(f"settings reload ok file={self.path.name} snapshot: {_snapshot(self._data)}")
        return self._data

_SETTINGS = _SettingsCache()

def approx_tokens(text: str) -> int:
    cfg = _SETTINGS.get()
    return max(1, math.ceil(len(text) / int(cfg["chars_per_token"])))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    cfg = _SETTINGS.get()
    overhead = int(cfg["prompt_per_message_overhead"])
    return sum(approx_tokens(m["content"]) + overhead for m in msgs)

def get_session(session_id: str):
    cfg = _SETTINGS.get()
    _log(f"get_session IN session={session_id} (settings: {_snapshot(cfg)})")
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=int(cfg["recent_maxlen"])),
        "style": STYLE_SYS,
        "short": False,
        "bullets": False,
    })
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
            _log(f"get_session loaded summary len={len(st['summary'])}")
        except Exception as e:
            _log(f"get_session summary load error {e}")
    if not st["recent"]:
        try:
            rows = store_list_messages(session_id)
            tail = rows[-st["recent"].maxlen:]
            for m in tail:
                st["recent"].append({"role": m.role, "content": strip_runjson(m.content)})
            _log(f"get_session hydrated recent={len(st['recent'])}")
        except Exception as e:
            _log(f"get_session hydrate error {e}")
    return st

def _heuristic_bullets(chunks: List[Dict[str,str]], cfg: Dict) -> str:
    max_bullets = int(cfg["heuristic_max_bullets"])
    max_words = int(cfg["heuristic_max_words"])
    prefix = cfg["bullet_prefix"]
    bullets = []
    for m in chunks:
        txt = " ".join((m.get("content") or "").split())
        if not txt:
            continue
        words = txt.replace("\n", " ").split()
        snippet = " ".join(words[:max_words]) if words else ""
        bullets.append(f"{prefix}{snippet}" if snippet else prefix.strip())
        if len(bullets) >= max_bullets:
            break
    return "\n".join(bullets) if bullets else prefix.strip()

def summarize_chunks(chunks: List[Dict[str,str]]) -> Tuple[str, bool]:
    cfg = _SETTINGS.get()
    t0 = time.time()
    use_fast = bool(cfg["use_fast_summary"])
    _log(f"summarize_chunks IN chunks={len(chunks)} FAST={use_fast}")
    if use_fast:
        txt = _heuristic_bullets(chunks, cfg)
        dt = time.time() - t0
        _log(f"summarize_chunks OUT (FAST) bullets={len([l for l in txt.splitlines() if l])} chars={len(txt)} dt={dt:.2f}s")
        return txt, False
    text = "\n".join(f'{m.get("role","")}: {m.get("content","")}' for m in chunks)
    sys_inst = cfg["summary_sys_inst"]
    user_prompt = cfg["summary_user_prefix"] + text + cfg["summary_user_suffix"]
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": sys_inst},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=int(cfg["llm_summary_max_tokens"]),
        temperature=float(cfg["llm_summary_temperature"]),
        top_p=float(cfg["llm_summary_top_p"]),
        stream=False,
        stop=list(cfg["llm_summary_stop"]),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip()
    lines = [ln.strip() for ln in raw.splitlines()]
    bullets: List[str] = []
    seen = set()
    max_words = int(cfg["heuristic_max_words"])
    max_bullets = int(cfg["heuristic_max_bullets"])
    for ln in lines:
        if not ln.startswith(cfg["bullet_prefix"]):
            continue
        norm = " ".join(ln[len(cfg["bullet_prefix"]):].lower().split())
        if not norm or norm in seen:
            continue
        seen.add(norm)
        words = ln[len(cfg["bullet_prefix"]):].split()
        if len(words) > max_words:
            ln = cfg["bullet_prefix"] + " ".join(words[:max_words])
        bullets.append(ln)
        if len(bullets) >= max_bullets:
            break
    if bullets:
        txt = "\n".join(bullets)
        dt = time.time() - t0
        _log(f"summarize_chunks OUT bullets={len(bullets)} chars={len(txt)} dt={dt:.2f}s")
        return txt, True
    s = " ".join(raw.split())[:160]
    fallback = (cfg["bullet_prefix"] + s) if s else cfg["bullet_prefix"].strip()
    dt = time.time() - t0
    _log(f"summarize_chunks OUT bullets=0 chars={len(fallback)} dt={dt:.2f}s (FALLBACK)")
    return fallback, True

def _compress_summary_block(s: str) -> str:
    cfg = _SETTINGS.get()
    max_chars = int(cfg["summary_max_chars"])
    prefix = cfg["bullet_prefix"]
    lines = [ln.strip() for ln in (s or "").splitlines()]
    out, seen = [], set()
    for ln in lines:
        if not ln.startswith(prefix):
            continue
        norm = " ".join(ln[len(prefix):].lower().split())
        if norm in seen:
            continue
        seen.add(norm)
        out.append(ln)
    text = "\n".join(out)
    _log(f"compress_summary IN chars={len(s)} kept_lines={len(out)}")
    if len(text) > max_chars:
        last, total = [], 0
        for ln in reversed(out):
            if total + len(ln) + 1 > max_chars:
                break
            last.append(ln)
            total += len(ln) + 1
        text = "\n".join(reversed(last))
    _log(f"compress_summary OUT chars={len(text)} lines={len(text.splitlines())}")
    return text

def build_system(style: str, short: bool, bullets: bool) -> str:
    cfg = _SETTINGS.get()
    _log(f"build_system flags short={short} bullets={bullets}")
    parts = [STYLE_SYS]
    if style and style != STYLE_SYS:
        parts.append(style)
    if short:
        parts.append(cfg["system_brief_directive"])
    if bullets:
        parts.append(cfg["system_bullets_directive"])
    parts.append(cfg["system_follow_user_style_directive"])
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    cfg = _SETTINGS.get()
    model_ctx = int(max_ctx or cfg["model_ctx"])
    gen_budget = int(out_budget or cfg["out_budget"])
    reserved = int(cfg["reserved_system_tokens"])
    input_budget = model_ctx - gen_budget - reserved
    if input_budget < int(cfg["min_input_budget"]):
        input_budget = int(cfg["min_input_budget"])
    sys_text = build_system(style, short, bullets)
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": cfg["summary_header_prefix"] + summary})
    packed = prologue + list(recent)
    _log(f"pack_messages SETTINGS snapshot: {_snapshot(cfg)}")
    _log(f"pack_messages OUT msgs={len(packed)} tokens~{count_prompt_tokens(packed)} "
         f"(model_ctx={model_ctx}, out_budget={gen_budget}, input_budget={input_budget})")
    return packed, input_budget

def _final_safety_trim(packed: List[Dict[str,str]], input_budget: int) -> List[Dict[str,str]]:
    cfg = _SETTINGS.get()
    keep_ratio = float(cfg["final_shrink_summary_keep_ratio"])
    min_keep = int(cfg["final_shrink_summary_min_chars"])
    def toks() -> int:
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    _log(f"final_trim START tokens={toks()} budget={input_budget}")
    keep_head = 2 if len(packed) >= 2 and isinstance(packed[1].get("content"), str) and packed[1]["content"].startswith(cfg["summary_header_prefix"]) else 1
    while toks() > input_budget and len(packed) > keep_head + 1:
        dropped = packed.pop(keep_head)
        _log(f"final_trim DROP msg role={dropped['role']} size~{approx_tokens(dropped['content'])} toks={toks()}")
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        summary_msg = packed[1]
        txt = summary_msg["content"]
        n = max(min_keep, int(len(txt) * keep_ratio))
        summary_msg["content"] = txt[-n:]
        _log(f"final_trim SHRINK summary to {len(summary_msg['content'])} chars toks={toks()}")
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        removed = packed.pop(1)
        _log(f"final_trim REMOVE summary len~{len(removed['content'])} toks={toks()}")
    while toks() > input_budget and len(packed) > 2:
        removed = packed.pop(2 if len(packed) > 3 else 1)
        _log(f"final_trim LAST_RESORT drop size~{approx_tokens(removed['content'])} toks={toks()}")
    _log(f"final_trim END tokens={toks()} msgs={len(packed)}")
    return packed

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    cfg = _SETTINGS.get()

    # --- DEBUG SNAPSHOT ---
    _log("=== roll_summary_if_needed DEBUG START ===")
    _log(f"skip_overage_lt={cfg['skip_overage_lt']}, "
         f"max_peel_per_turn={cfg['max_peel_per_turn']}, "
         f"peel_min={cfg['peel_min']}, "
         f"peel_frac={cfg['peel_frac']}, "
         f"peel_max={cfg['peel_max']}")
    _log(f"len(recent)={len(recent)}, current_summary_len={len(summary) if summary else 0}")
    _log(f"input_budget={input_budget}, reserved_system_tokens={cfg['reserved_system_tokens']}")
    _log(f"model_ctx={cfg['model_ctx']}, out_budget={cfg['out_budget']}")
    # ----------------------

    def _tok():
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999

    start_tokens = _tok()
    overage = start_tokens - input_budget
    _log(f"roll_summary_if_needed START tokens={start_tokens} "
         f"input_budget={input_budget} overage={overage}")

    if overage <= int(cfg["skip_overage_lt"]):
        _log(f"roll_summary_if_needed SKIP (overage {overage} <= {cfg['skip_overage_lt']})")
        packed = _final_safety_trim(packed, input_budget)
        return packed, summary

    peels_done = 0
    if len(recent) > 6 and peels_done < int(cfg["max_peel_per_turn"]):
        peel_min = int(cfg["peel_min"])
        peel_frac = float(cfg["peel_frac"])
        peel_max = int(cfg["peel_max"])
        target = max(peel_min, min(peel_max, int(len(recent) * peel_frac)))
        peel = []
        for _ in range(min(target, len(recent))):
            peel.append(recent.popleft())
        _log(f"roll_summary peeled={len(peel)}")
        new_sum, _used_llm = summarize_chunks(peel)
        if new_sum.startswith(cfg["bullet_prefix"]):
            summary = (summary + "\n" + new_sum).strip() if summary else new_sum
        else:
            summary = new_sum
        summary = _compress_summary_block(summary)
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": cfg["summary_header_prefix"] + summary},
            *list(recent),
        ]
        _log(f"roll_summary updated summary_len={len(summary)} tokens={_tok()}")

    packed = _final_safety_trim(packed, input_budget)
    _log(f"roll_summary_if_needed END tokens={count_prompt_tokens(packed)}")
    _log("=== roll_summary_if_needed DEBUG END ===")
    return packed, summary

# ===== aimodel/file_read/core/schemas.py =====

# core/schemas.py
from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel


class Attachment(BaseModel):
    name: str
    source: Optional[str] = None
    sessionId: Optional[str] = None

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str
    attachments: Optional[List[Attachment]] = None  # ✅ new

class MergeChatReq(BaseModel):
    sourceId: str
    targetId: Optional[str] = None
    newChat: bool = False

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None

    # ↓ make optional; defaults come from effective settings
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None

    # ↓ also optional; defaults come from settings
    autoWeb: Optional[bool] = None
    webK: Optional[int] = None
    autoRag: Optional[bool] = None   

# ===== aimodel/file_read/core/settings.py =====

# aimodel/file_read/core/settings.py
from __future__ import annotations

import json
from threading import RLock
from typing import Any, Dict, Optional

from .files import (
    DEFAULTS_SETTINGS_FILE,
    OVERRIDES_SETTINGS_FILE,
    EFFECTIVE_SETTINGS_FILE,
    load_json_file,
    save_json_file,
)


def _deep_merge(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(dst)
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)  # type: ignore[arg-type]
        else:
            out[k] = v
    return out


class _SettingsManager:
    """
    layers: defaults → adaptive(session/_global_) → overrides
    also persists the merged *global* effective to EFFECTIVE_SETTINGS_FILE
    (that’s what memory.py watches/loads)

    Dynamic access:
      - Attribute style: SETTINGS.stream_queue_maxsize
      - Dict style:      SETTINGS["stream_queue_maxsize"]
      - Safe get:        SETTINGS.get("stream_queue_maxsize", 64)

    Only keys present in the merged effective map are exposed dynamically.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._defaults: Dict[str, Any] = self._load_defaults()
        self._overrides: Dict[str, Any] = self._load_overrides()
        self._adaptive_by_session: Dict[str, Dict[str, Any]] = {}
        # write initial effective so memory.py has something on boot
        self._persist_effective_unlocked()

    # ---------- loading ----------
    def _load_defaults(self) -> Dict[str, Any]:
        return load_json_file(DEFAULTS_SETTINGS_FILE, default={})

    def _load_overrides(self) -> Dict[str, Any]:
        return load_json_file(OVERRIDES_SETTINGS_FILE, default={})

    # ---------- saving ----------
    def _save_overrides_unlocked(self) -> None:
        save_json_file(OVERRIDES_SETTINGS_FILE, self._overrides)

    def _persist_effective_unlocked(self) -> None:
        # Persist *global* effective (session-less) for memory.py
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get("_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        save_json_file(EFFECTIVE_SETTINGS_FILE, eff)

    # ---------- helpers ----------
    def _effective_unlocked(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get(session_id or "_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        return eff

    def _get_unlocked(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        eff = self._effective_unlocked(session_id)
        if key in eff:
            return eff[key]
        if default is not None:
            return default
        raise AttributeError(f"_SettingsManager has no key '{key}'")

    # ---------- public read API ----------
    @property
    def defaults(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._defaults))

    @property
    def overrides(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._overrides))

    def adaptive(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        key = session_id or "_global_"
        with self._lock:
            return json.loads(json.dumps(self._adaptive_by_session.get(key, {})))

    def effective(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        with self._lock:
            return self._effective_unlocked(session_id)

    # Dynamic attribute access (e.g., SETTINGS.stream_queue_maxsize)
    def __getattr__(self, name: str) -> Any:
        # Only called if normal attributes/methods aren't found
        with self._lock:
            return self._get_unlocked(name)

    # Dict-style access (e.g., SETTINGS["stream_queue_maxsize"])
    def __getitem__(self, key: str) -> Any:
        with self._lock:
            return self._get_unlocked(key)

    # Safe getter (optional default)
    def get(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        with self._lock:
            try:
                return self._get_unlocked(key, default=default, session_id=session_id)
            except AttributeError:
                return default

    # ---------- public write API ----------
    def patch_overrides(self, patch: Dict[str, Any]) -> None:
        if not isinstance(patch, dict):
            return
        with self._lock:
            self._overrides = _deep_merge(self._overrides, patch)
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def replace_overrides(self, new_overrides: Dict[str, Any]) -> None:
        if not isinstance(new_overrides, dict):
            new_overrides = {}
        with self._lock:
            self._overrides = json.loads(json.dumps(new_overrides))
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def reload_overrides(self) -> None:
        with self._lock:
            self._overrides = self._load_overrides()
            self._persist_effective_unlocked()

    def set_adaptive_for_session(self, session_id: Optional[str], values: Dict[str, Any]) -> None:
        key = session_id or "_global_"
        if not isinstance(values, dict):
            values = {}
        with self._lock:
            self._adaptive_by_session[key] = json.loads(json.dumps(values))
            # If updating the global adaptive layer, refresh persisted effective
            if key == "_global_":
                self._persist_effective_unlocked()

    def recompute_adaptive(self, session_id: Optional[str] = None) -> None:
        # placeholder for your controller logic later
        with self._lock:
            # after recompute, also refresh persisted effective for global
            self._persist_effective_unlocked()


SETTINGS = _SettingsManager()

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple

STYLE_SYS = (
    "You are a helpful assistant. "
    "Always follow the user's explicit instructions carefully and exactly. "
    "Do not repeat yourself. Stay coherent and complete."
)

PAT_TALK_LIKE = re.compile(r"\btalk\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_RESPOND_LIKE = re.compile(r"\brespond\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_BE = re.compile(r"\bbe\s+(?P<style>[^.;\n]+)", re.I)
PAT_FROM_NOW = re.compile(r"\bfrom\s+now\s+on[, ]+\s*(?P<style>[^.;\n]+)", re.I)

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    t = user_text.strip()
    style_match = (
        PAT_TALK_LIKE.search(t)
        or PAT_RESPOND_LIKE.search(t)
        or PAT_FROM_NOW.search(t)
        or PAT_BE.search(t)
    )
    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = (
            f"You must talk like {raw}. "
            f"Stay in character but remain helpful and accurate. "
            f"Follow the user's latest style instructions."
        )
    return style_inst, False, False

# ===== aimodel/file_read/rag/ingest/__init__.py =====

# ===== aimodel/file_read/rag/ingest/__init__.py =====
from __future__ import annotations
from typing import Tuple
import io, json
from .xls_ingest import extract_xls
from .excel_ingest import extract_excel
from .csv_ingest import extract_csv
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas
from .doc_ingest import extract_docx, extract_doc_binary
from ...core.settings import SETTINGS

__all__ = ["sniff_and_extract", "Chunk", "chunk_text", "build_metas"]

def _ing_dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[ingest]", *args)
    except Exception:
        pass

def sniff_and_extract(filename: str, data: bytes) -> Tuple[str, str]:
    name = (filename or "").lower()
    _ing_dbg("route:", name, "bytes=", len(data))

    if name.endswith((".xlsx", ".xlsm")):
        _ing_dbg("-> excel")
        return extract_excel(data)

    if name.endswith(".xls"):
        _ing_dbg("-> excel (xls via xlrd)")
        return extract_xls(data)

    if name.endswith((".csv", ".tsv")):
        _ing_dbg("-> csv/tsv")
        return extract_csv(data)

    if name.endswith(".docx"):
        _ing_dbg("-> docx")
        try:
            return extract_docx(data)
        except Exception as e:
            _ing_dbg("docx err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".doc"):
        _ing_dbg("-> doc (binary/rtf)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("doc err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".rtf"):
        _ing_dbg("-> rtf (via doc_binary)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("rtf err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".pdf"):
        _ing_dbg("-> pdf")
        try:
            from pdfminer.high_level import extract_text
            txt = extract_text(io.BytesIO(data)) or ""
            return txt.strip(), "text/plain"
        except Exception as e:
            _ing_dbg("pdfminer err:", repr(e))
            try:
                from PyPDF2 import PdfReader
                r = PdfReader(io.BytesIO(data))
                pages = [(p.extract_text() or "").strip() for p in r.pages]
                return "\n\n".join([p for p in pages if p]).strip(), "text/plain"
            except Exception as e2:
                _ing_dbg("pypdf2 err:", repr(e2))
                return _utf8(data), "text/plain"

    if name.endswith(".json"):
        _ing_dbg("-> json")
        try:
            return json.dumps(json.loads(_utf8(data)), ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("json err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".jsonl", ".jsonlines")):
        _ing_dbg("-> jsonl")
        lines = _utf8(data).splitlines()
        out = []
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            try:
                out.append(json.dumps(json.loads(ln), ensure_ascii=False, indent=2))
            except Exception:
                out.append(ln)
        return "\n".join(out).strip(), "text/plain"

    if name.endswith((".yaml", ".yml")):
        _ing_dbg("-> yaml")
        try:
            import yaml
            obj = yaml.safe_load(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("yaml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".toml"):
        _ing_dbg("-> toml")
        try:
            try:
                import tomllib
                obj = tomllib.loads(_utf8(data))
            except Exception:
                import toml
                obj = toml.loads(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("toml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".htm", ".html", ".xml")):
        _ing_dbg("-> html/xml")
        return _strip_html(_utf8(data)), "text/plain"

    # plaintext / code-ish
    if name.endswith((
        ".txt", ".log", ".md",
        ".c", ".cpp", ".h", ".hpp",
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".sh", ".ps1",
        ".rs", ".java", ".go", ".rb", ".php",
        ".swift", ".kt", ".scala", ".lua", ".perl",
    )):
        _ing_dbg("-> plaintext/code")
        return _utf8(data), "text/plain"

    _ing_dbg("-> default fallback")
    return _utf8(data), "text/plain"

# ===== aimodel/file_read/rag/ingest/common.py =====

# ===== aimodel/file_read/rag/ingest/common.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional
import re
from ...core.settings import SETTINGS

@dataclass
class Chunk:
    text: str
    meta: Dict[str, str]

# --- Small helpers kept for legacy imports (ingest.__init__ depends on these) ---
def _utf8(data: bytes) -> str:

    return (data or b"").decode("utf-8", errors="ignore")

def _strip_html(txt: str) -> str:

    if not txt:
        return ""
    # drop scripts/styles
    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", txt)
    # common block/line breaks
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p>", "\n\n", txt)
    # strip tags
    txt = re.sub(r"(?is)<.*?>", " ", txt)
    # collapse whitespace
    txt = re.sub(r"[ \t]+", " ", txt)
    return txt.strip()

# --- Section & paragraph aware chunking for table/text docs -------------------
_HDR_RE = re.compile(r"^(#{1,3})\s+.*$", flags=re.MULTILINE)
_PARA_SPLIT_RE = re.compile(r"\n\s*\n+")

def _split_sections(text: str) -> List[str]:

    text = (text or "").strip()
    if not text:
        return []
    starts = [m.start() for m in _HDR_RE.finditer(text)]
    if not starts:
        return [text]
    # ensure beginning is a split point
    if 0 not in starts:
        starts = [0] + starts
    sections: List[str] = []
    for i, s in enumerate(starts):
        e = starts[i + 1] if i + 1 < len(starts) else len(text)
        block = text[s:e].strip()
        if block:
            sections.append(block)
    return sections

def _split_paragraphs(block: str) -> List[str]:
    paras = [p.strip() for p in _PARA_SPLIT_RE.split(block or "")]
    return [p for p in paras if p]

def _hard_split(text: str, max_len: int) -> List[str]:

    approx = re.split(r"(?<=[\.\!\?\;])\s+", text or "")
    out: List[str] = []
    buf = ""
    for s in approx:
        if not s:
            continue
        if len(buf) + (1 if buf else 0) + len(s) <= max_len:
            buf = s if not buf else (buf + " " + s)
        else:
            if buf:
                out.append(buf)
            if len(s) <= max_len:
                out.append(s)
            else:
                words = re.split(r"\s+", s)
                cur = ""
                for w in words:
                    if not w:
                        continue
                    if len(cur) + (1 if cur else 0) + len(w) <= max_len:
                        cur = w if not cur else (cur + " " + w)
                    else:
                        if cur:
                            out.append(cur)
                        cur = w
                if cur:
                    out.append(cur)
            buf = ""
    if buf:
        out.append(buf)
    return out

def _pack_with_budget(pieces: List[str], *, max_chars: int) -> List[str]:
    chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for p in pieces:
        plen = len(p)
        if plen > max_chars:
            chunks.extend(_hard_split(p, max_chars))
            continue
        if cur_len == 0:
            cur, cur_len = [p], plen
            continue
        if cur_len + 2 + plen <= max_chars:  # 2 for "\n\n" when joining
            cur.append(p)
            cur_len += 2 + plen
        else:
            chunks.append("\n\n".join(cur).strip())
            cur, cur_len = [p], plen
    if cur_len:
        chunks.append("\n\n".join(cur).strip())
    return chunks

def chunk_text(
    text: str,
    meta: Optional[Dict[str, str]] = None,
    *,
    max_chars: int = int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    overlap: int = int(SETTINGS.get("rag_chunk_overlap_chars", 150)),
) -> List[Chunk]:

    base_meta = (meta or {}).copy()
    text = (text or "").strip()
    if not text:
        return []

    if len(text) <= max_chars:
        return [Chunk(text=text, meta=base_meta)]

    sections = _split_sections(text)
    if not sections:
        sections = [text]

    chunks: List[Chunk] = []
    last_tail: Optional[str] = None

    for sec in sections:
        paras = _split_paragraphs(sec)
        if not paras:
            continue
        packed = _pack_with_budget(paras, max_chars=max_chars)
        for ch in packed:
            if last_tail and overlap > 0:
                tail = last_tail[-overlap:] if len(last_tail) > overlap else last_tail
                candidate = f"{tail}\n{ch}"
                chunks.append(Chunk(text=candidate if len(candidate) <= max_chars else ch, meta=base_meta))
            else:
                chunks.append(Chunk(text=ch, meta=base_meta))
            last_tail = ch

    return chunks

def build_metas(session_id: Optional[str], filename: str, chunks: List[Chunk], *, size: int = 0) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for i, c in enumerate(chunks):
        out.append({
            "id": f"{filename}:{i}",
            "sessionId": session_id or "",
            "source": filename,
            "title": filename,
            "mime": "text/plain",
            "size": str(size),
            "chunkIndex": str(i),
            "text": c.text,  # stored for RAG snippet display
        })
    return out

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====

from __future__ import annotations
from typing import Tuple, List
import io, re, csv
from ...core.settings import SETTINGS

def _squeeze_spaces_inline(s: str) -> str:
    return re.sub(r"[ \t]+", " ", (s or "")).strip()

def extract_csv(data: bytes) -> Tuple[str, str]:

    S = SETTINGS.effective

    max_chars = int(S().get("csv_value_max_chars"))
    quote_strings = bool(S().get("csv_quote_strings"))
    header_normalize = bool(S().get("csv_header_normalize"))
    max_rows = int(S().get("csv_infer_max_rows"))
    max_cols = int(S().get("csv_infer_max_cols"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_val(v) -> str:
        if v is None:
            return ""
        s = str(v).strip()
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not header_normalize:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    # read CSV
    txt = io.StringIO(data.decode("utf-8", errors="ignore"))
    sniffer = csv.Sniffer()
    sample = txt.read(2048)
    txt.seek(0)
    dialect = sniffer.sniff(sample) if sample else csv.excel
    reader = csv.reader(txt, dialect)

    lines: List[str] = []
    lines.append("# Sheet: CSV")
    lines.append("## Inferred Table")

    rows = list(reader)
    if not rows:
        return "", "text/plain"

    headers = rows[0][:max_cols]
    norm_headers = [normalize_header(fmt_val(h)) for h in headers]
    if any(norm_headers):
        lines.append("headers: " + ", ".join(norm_headers))

    for r in rows[1:max_rows + 1]:
        vals = [fmt_val(c) for c in r[:max_cols]]
        if any(vals):
            lines.append("row: " + ", ".join(vals))

    lines.append("")
    return "\n".join(lines).strip() + "\n", "text/plain"

# ===== aimodel/file_read/rag/ingest/doc_ingest.py =====

from __future__ import annotations
from typing import Tuple, List, Optional
import io, re
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _dbg(msg: str):
    try:
        S = SETTINGS.effective
        if bool(S().get("doc_debug", False)):
            print(f"[doc_ingest] {msg}")
    except Exception:
        pass

def _is_heading(style_name: str) -> Optional[int]:
    if not style_name:
        return None
    m = re.match(r"Heading\s+(\d+)", style_name, flags=re.IGNORECASE)
    if not m:
        return None
    try:
        return max(1, min(6, int(m.group(1))))
    except Exception:
        return None

def _is_list_style(style_name: str) -> bool:
    return bool(style_name) and any(k in style_name.lower() for k in ("list", "bullet", "number"))

def _extract_paragraph_text(p) -> str:
    return _squeeze_spaces(p.text)

def extract_docx(data: bytes) -> Tuple[str, str]:
    from docx import Document
    S = SETTINGS.effective
    HEADING_MAX_LEVEL = int(S().get("docx_heading_max_level", 3))
    USE_MARKDOWN_HEADINGS = bool(S().get("docx_use_markdown_headings", True))
    PRESERVE_BULLETS = bool(S().get("docx_preserve_bullets", True))
    INCLUDE_TABLES = bool(S().get("docx_include_tables", True))
    INCLUDE_HEADERS_FOOTERS = bool(S().get("docx_include_headers_footers", False))
    MAX_PARA_CHARS = int(S().get("docx_para_max_chars", 0))
    DROP_EMPTY_LINES = bool(S().get("docx_drop_empty_lines", True))
    doc = Document(io.BytesIO(data))
    lines: List[str] = []
    try:
        title = (getattr(doc, "core_properties", None) or {}).title
        if title:
            lines.append(f"# {title}")
            lines.append("")
    except Exception:
        pass
    def _clip(s: str) -> str:
        if MAX_PARA_CHARS > 0 and len(s) > MAX_PARA_CHARS:
            return s[:MAX_PARA_CHARS] + "…"
        return s
    if INCLUDE_HEADERS_FOOTERS:
        try:
            for i, sec in enumerate(getattr(doc, "sections", []) or []):
                if i > 0:
                    break
                try:
                    hdr_ps = getattr(sec.header, "paragraphs", []) or []
                    hdr_text = "\n".join(_squeeze_spaces(p.text) for p in hdr_ps if _squeeze_spaces(p.text))
                    if hdr_text:
                        lines.append("## Header")
                        lines.append(_clip(hdr_text))
                        lines.append("")
                except Exception:
                    pass
                try:
                    ftr_ps = getattr(sec.footer, "paragraphs", []) or []
                    ftr_text = "\n".join(_squeeze_spaces(p.text) for p in ftr_ps if _squeeze_spaces(p.text))
                    if ftr_text:
                        lines.append("## Footer")
                        lines.append(_clip(ftr_text))
                        lines.append("")
                except Exception:
                    pass
        except Exception:
            pass
    for p in doc.paragraphs:
        txt = _extract_paragraph_text(p)
        if not txt and DROP_EMPTY_LINES:
            continue
        style_name = getattr(p.style, "name", "") or ""
        lvl = _is_heading(style_name)
        if lvl and lvl <= HEADING_MAX_LEVEL and USE_MARKDOWN_HEADINGS:
            prefix = "#" * max(1, min(6, lvl))
            lines.append(f"{prefix} {txt}".strip())
            continue
        if PRESERVE_BULLETS and _is_list_style(style_name):
            if txt:
                lines.append(f"- {_clip(txt)}")
            continue
        if txt:
            lines.append(_clip(txt))
        elif not DROP_EMPTY_LINES:
            lines.append("")
    if INCLUDE_TABLES and getattr(doc, "tables", None):
        for t_idx, tbl in enumerate(doc.tables):
            try:
                non_empty = any(_squeeze_spaces(cell.text) for row in tbl.rows for cell in row.cells)
            except Exception:
                non_empty = True
            if not non_empty:
                continue
            lines.append("")
            lines.append(f"## Table {t_idx + 1}")
            try:
                for row in tbl.rows:
                    cells = [_squeeze_spaces(c.text) for c in row.cells]
                    if any(cells):
                        lines.append(" | ".join(c for c in cells if c))
            except Exception:
                pass
    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

_RTF_CTRL_RE = re.compile(r"\\[a-zA-Z]+-?\d* ?")
_RTF_GROUP_RE = re.compile(r"[{}]")
_RTF_UNICODE_RE = re.compile(r"\\u(-?\d+)\??")
_RTF_HEX_RE = re.compile(r"\\'[0-9a-fA-F]{2}")
_HEX_BLOCK_RE = re.compile(r"(?:\s*[0-9A-Fa-f]{2}){120,}")

def _rtf_to_text_simple(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    def _hex_sub(m):
        try:
            return bytes.fromhex(m.group(0)[2:]).decode("latin-1", errors="ignore")
        except Exception:
            return ""
    s = _RTF_HEX_RE.sub(_hex_sub, s)
    def _uni_sub(m):
        try:
            cp = int(m.group(1))
            if cp < 0:
                cp = 65536 + cp
            return chr(cp)
        except Exception:
            return ""
    s = _RTF_UNICODE_RE.sub(_uni_sub, s)
    s = s.replace(r"\par", "\n").replace(r"\line", "\n")
    s = _RTF_CTRL_RE.sub("", s)
    s = _RTF_GROUP_RE.sub("", s)
    s = _HEX_BLOCK_RE.sub("", s)
    s = s.replace("\r", "\n")
    s = re.sub(r"\n\s*\n\s*\n+", "\n\n", s)
    s = _squeeze_spaces(s)
    return s if keep_newlines else s.replace("\n", " ")

def _rtf_to_text_via_lib(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        from striprtf.striprtf import rtf_to_text
    except Exception:
        return _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    try:
        txt = rtf_to_text(s)
    except Exception:
        txt = _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    txt = _squeeze_spaces(txt)
    return txt if keep_newlines else txt.replace("\n", " ")

def _is_ole(b: bytes) -> bool:
    return len(b) >= 8 and b[:8] == b"\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1"

def _generic_ole_text(data: bytes) -> str:
    S = SETTINGS.effective
    MIN_RUN = int(S().get("doc_ole_min_run_chars", 8))
    MAX_LINE = int(S().get("doc_ole_max_line_chars", 600))
    MIN_ALPHA_RATIO = float(S().get("doc_ole_min_alpha_ratio", 0.25))
    DROP_XMLISH = bool(S().get("doc_ole_drop_xmlish", True))
    DROP_PATHISH = bool(S().get("doc_ole_drop_pathish", True))
    DROP_SYMBOL_LINES = bool(S().get("doc_ole_drop_symbol_lines", True))
    DEDUPE_SHORT_REPEATS = bool(S().get("doc_ole_dedupe_short_repeats", True))
    XMLISH = re.compile(r"^\s*<[^>]+>", re.I)
    PATHISH = re.compile(r"[\\/].+\.(?:xml|rels|png|jpg|jpeg|gif|bmp|bin|dat)\b", re.I)
    SYMBOLLINE = re.compile(r"^[\W_]{6,}$")
    s = data.replace(b"\x00", b"")
    runs = re.findall(rb"[\t\r\n\x20-\x7E]{%d,}" % MIN_RUN, s)
    if not runs:
        return ""
    def _dec(b: bytes) -> str:
        try:
            return b.decode("cp1252", errors="ignore")
        except Exception:
            return b.decode("latin-1", errors="ignore")
    kept: List[str] = []
    for raw in runs:
        chunk = _dec(raw).replace("\r", "\n")
        for ln in re.split(r"\n+", chunk):
            t = ln.strip()
            if not t:
                continue
            if MAX_LINE > 0 and len(t) > MAX_LINE:
                t = t[:MAX_LINE].rstrip()
            t = _squeeze_spaces(t)
            letters = sum(1 for c in t if c.isalpha())
            if letters / max(1, len(t)) < MIN_ALPHA_RATIO:
                continue
            if DROP_XMLISH and XMLISH.search(t):
                continue
            if DROP_PATHISH and PATHISH.search(t):
                continue
            if DROP_SYMBOL_LINES and SYMBOLLINE.fullmatch(t):
                continue
            if DEDUPE_SHORT_REPEATS:
                t = re.sub(r"\b(\w{2,4})\1{2,}\b", r"\1\1", t)
            kept.append(t)
    out = "\n".join(kept)
    out = re.sub(r"\n\s*\n\s*\n+", "\n\n", out).strip()
    return out

def extract_doc_binary(data: bytes) -> Tuple[str, str]:
    head = (data[:64] or b"").lstrip()
    is_rtf = head.startswith(b"{\\rtf") or head.startswith(b"{\\RTF}")
    is_ole = _is_ole(data)
    _dbg(f"extract_doc_binary: bytes={len(data)} is_rtf={is_rtf} is_ole={is_ole}")
    if is_rtf:
        txt = _rtf_to_text_via_lib(data, keep_newlines=True).strip()
        return (txt + "\n" if txt else ""), "text/plain"
    if is_ole:
        txt = _generic_ole_text(data)
        return (txt + "\n" if txt else ""), "text/plain"
    try:
        txt = data.decode("utf-8", errors="ignore").strip()
    except Exception:
        txt = data.decode("latin-1", errors="ignore").strip()
    return (txt + ("\n" if txt else "")), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====
from __future__ import annotations

from typing import Tuple, List
import io, re
from datetime import datetime, date, time
from ...core.settings import SETTINGS
from .excel_ingest_core import (
    scan_blocks_by_blank_rows,
    rightmost_nonempty_header,
    select_indices,
)

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_excel(data: bytes) -> Tuple[str, str]:
    from openpyxl import load_workbook
    from openpyxl.utils import range_boundaries
    from openpyxl.worksheet.worksheet import Worksheet

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    MAX_CELLS_PER_SHEET = int(S().get("excel_max_cells_per_sheet"))
    MAX_NR_PREVIEW = int(S().get("excel_named_range_preview"))
    EMIT_MERGED = bool(S().get("excel_emit_merged"))
    EMIT_CELLS = bool(S().get("excel_emit_cells"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    EMIT_KEYVALUES = bool(S().get("excel_emit_key_values"))
    EMIT_CELL_ADDR = bool(S().get("excel_emit_cell_addresses"))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def _sheet_used_range(ws: Worksheet):
        from openpyxl.utils import range_boundaries
        if callable(getattr(ws, "calculate_dimension", None)):
            dim_ref = ws.calculate_dimension()
            try:
                min_c, min_r, max_c, max_r = range_boundaries(dim_ref)
                return min_c, min_r, max_c, max_r
            except Exception:
                pass
        return 1, 1, ws.max_column or 1, ws.max_row or 1

    def _detect_key_value(ws: Worksheet, min_c, min_r, max_c, max_r) -> bool:
        if max_c - min_c + 1 != 2:
            return False
        textish, valueish, rows = 0, 0, 0
        for r in range(min_r, min(min_r + INFER_MAX_ROWS - 1, max_r) + 1):
            a = ws.cell(row=r, column=min_c).value
            b = ws.cell(row=r, column=min_c + 1).value
            if a is None and b is None:
                continue
            rows += 1
            if isinstance(a, str):
                textish += 1
            if isinstance(b, (int, float, datetime, date, time)):
                valueish += 1
        return rows >= 3 and textish / max(1, rows) >= 0.6 and valueish / max(1, rows) >= 0.6

    def _keep_and_rename_phantom(headers: List[str]) -> tuple[List[int], List[str]]:
        if len(headers) >= 2 and re.fullmatch(r"\d+_\d+$", (headers[1] or "")) and (headers[0] or ""):
            keep_idx = [0, 1]
            new_headers = headers[:]
            new_headers[1] = "value"
            return keep_idx, new_headers
        keep_idx = [i for i, h in enumerate(headers) if h and not re.fullmatch(r"\d+_\d+$", h)]
        new_headers = [headers[i] for i in keep_idx]
        return keep_idx, new_headers

    wb_vals = load_workbook(io.BytesIO(data), data_only=True, read_only=True)

    lines: List[str] = []

    try:
        dn_obj = getattr(wb_vals, "defined_names", None)
        if dn_obj and getattr(dn_obj, "definedName", None):
            nr_out: List[str] = []
            for dn in dn_obj.definedName:
                if getattr(dn, "hidden", False):
                    continue
                try:
                    dests = list(dn.destinations)
                except Exception:
                    dests = []
                if not dests:
                    continue
                for sheet_name, ref in dests:
                    try:
                        ws = wb_vals[sheet_name]
                        min_c, min_r, max_c, max_r = range_boundaries(ref)
                        vals: List[str] = []
                        from openpyxl.utils import get_column_letter
                        gl = get_column_letter
                        for r in range(min_r, max_r + 1):
                            for c in range(min_c, max_c + 1):
                                v = ws.cell(row=r, column=c).value
                                vv = fmt_val(v)
                                if vv:
                                    vals.append(f"{gl(c)}{r}={vv}")
                                if len(vals) >= MAX_NR_PREVIEW:
                                    break
                            if len(vals) >= MAX_NR_PREVIEW:
                                break
                        preview = "; ".join(vals)
                        if preview:
                            nr_out.append(f"- {dn.name}: {sheet_name}!{ref} = {preview}")
                        else:
                            nr_out.append(f"- {dn.name}: {sheet_name}!{ref}")
                    except Exception:
                        nr_out.append(f"- {dn.name}: {sheet_name}!{ref}")
            if nr_out:
                lines.append("# Named Ranges")
                lines.extend(nr_out)
                lines.append("")
    except Exception:
        pass

    def _emit_key_values(ws: Worksheet, sheet_name: str, min_c, min_r, max_c, max_r):
        lines.append(f"# Sheet: {sheet_name}")
        lines.append("## Key/Values")
        for r in range(min_r, min(min_r + INFER_MAX_ROWS - 1, max_r) + 1):
            k = fmt_val(ws.cell(row=r, column=min_c).value)
            v = fmt_val(ws.cell(row=r, column=min_c + 1).value)
            if not k and not v:
                continue
            if k:
                lines.append(f"- {k}: {v}" if v else f"- {k}:")
        lines.append("")

    def _emit_inferred_table(ws: Worksheet, sheet_name: str, min_c, min_r, max_c, max_r):
        lines.append(f"# Sheet: {sheet_name}")
        lines.append("## Inferred Table")
        max_c_eff = min(max_c, min_c + INFER_MAX_COLS - 1)
        max_r_eff = min(max_r, min_r + INFER_MAX_ROWS - 1)

        headers: List[str] = []
        header_fill = 0
        for c in range(min_c, max_c_eff + 1):
            val = ws.cell(row=min_r, column=c).value
            s = fmt_val("" if val is None else str(val).strip())
            if s:
                header_fill += 1
            headers.append(s)

        fill_ratio = header_fill / max(1, (max_c_eff - min_c + 1))
        if fill_ratio < INFER_MIN_HEADER_FILL and (min_r + 1) <= max_r_eff:
            headers = []
            hdr_r = min_r + 1
            for c in range(min_c, max_c_eff + 1):
                val = ws.cell(row=hdr_r, column=c).value
                s = fmt_val("" if val is None else str(val).strip())
                headers.append(s)
            min_r = hdr_r

        norm_headers = [normalize_header(h) for h in headers]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[:rmax + 1]
            max_c_eff = min(max_c_eff, min_c + rmax)

        keep_idx, norm_headers = _keep_and_rename_phantom(norm_headers)
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))

        for r in range(min_r + 1, max_r_eff + 1):
            row_vals: List[str] = []
            for c in range(min_c, max_c_eff + 1):
                vv = ws.cell(row=r, column=c).value
                row_vals.append(fmt_val(vv))
            row_vals = select_indices(row_vals, keep_idx)
            while row_vals and (row_vals[-1] == "" or row_vals[-1] is None):
                row_vals.pop()
            if any(v for v in row_vals):
                lines.append("row: " + ", ".join(row_vals))
        lines.append("")

    for sheet_name in wb_vals.sheetnames:
        ws_v: Worksheet = wb_vals[sheet_name]
        emitted_any = False

        if EMIT_MERGED:
            merges = getattr(ws_v, "merged_cells", None)
            rngs = getattr(merges, "ranges", None) if merges else None
            if rngs:
                lines.append(f"# Sheet: {sheet_name}")
                lines.append("## Merged Ranges")
                for mr in rngs:
                    ref = str(getattr(mr, "coord", getattr(mr, "bounds", mr)))
                    lines.append(f"- {ref}")
                lines.append("")
                emitted_any = True

        if EMIT_CELLS and not emitted_any:
            min_c, min_r, max_c, max_r = _sheet_used_range(ws_v)
            lines.append(f"# Sheet: {sheet_name}")
            lines.append("## Cells (non-empty)")
            emitted = 0
            for r in range(min_r, max_r + 1):
                row_parts: List[str] = []
                for c in range(min_c, max_c + 1):
                    vv = ws_v.cell(row=r, column=c).value
                    if vv is None:
                        continue
                    val_str = fmt_val(vv)
                    if val_str:
                        row_parts.append(val_str if not EMIT_CELL_ADDR else f"{val_str}")
                if row_parts:
                    lines.append("- " + ", ".join(row_parts))
                    emitted += 1
                    if emitted >= MAX_CELLS_PER_SHEET:
                        lines.append("… (cells truncated)")
                        break
            lines.append("")
            emitted_any = True

        if not emitted_any:
            min_c, min_r, max_c, max_r = _sheet_used_range(ws_v)
            max_c = min(max_c, min_c + INFER_MAX_COLS - 1)
            for b_min_c, b_min_r, b_max_c, b_max_r in scan_blocks_by_blank_rows(ws_v, min_c, min_r, max_c, max_r):
                if b_min_r > b_max_r:
                    continue
                if EMIT_KEYVALUES and _detect_key_value(ws_v, b_min_c, b_min_r, b_max_c, b_max_r):
                    _emit_key_values(ws_v, sheet_name, b_min_c, b_min_r, b_max_c, b_max_r)
                else:
                    _emit_inferred_table(ws_v, sheet_name, b_min_c, b_min_r, b_max_c, b_max_r)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====
from __future__ import annotations
from typing import List, Tuple
import re

_PII_HDRS = {"ssn","social_security_number","email","phone","dob"}
_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def row_blank(ws, r: int, min_c: int, max_c: int) -> bool:
    for c in range(min_c, max_c + 1):
        v = ws.cell(row=r, column=c).value
        if v not in (None, "") and not (isinstance(v, str) and not v.strip()):
            return False
    return True

def scan_blocks_by_blank_rows(ws, min_c: int, min_r: int, max_c: int, max_r: int):
    r = min_r
    while r <= max_r:
        while r <= max_r and row_blank(ws, r, min_c, max_c):
            r += 1
        if r > max_r:
            break
        start = r
        while r <= max_r and not row_blank(ws, r, min_c, max_c):
            r += 1
        end = r - 1
        yield (min_c, start, max_c, end)

def rightmost_nonempty_header(headers: List[str]) -> int:
    for i in range(len(headers) - 1, -1, -1):
        h = headers[i]
        if h and not h.isspace():
            return i
    return -1

def drop_bad_columns(headers: List[str]) -> List[int]:
    keep = []
    for i, h in enumerate(headers):
        hn = (h or "").strip().lower()
        if not hn:
            continue
        if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
            continue
        if hn in _PII_HDRS:
            continue
        keep.append(i)
    return keep or list(range(len(headers)))

def select_indices(seq: List[str], idxs: List[int]) -> List[str]:
    out = []
    for i in idxs:
        out.append(seq[i] if i < len(seq) else "")
    return out

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
from datetime import datetime, date, time
from ...core.settings import SETTINGS

import re

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_xls(data: bytes) -> Tuple[str, str]:
    """
    Extract text from legacy .xls using xlrd and emit a format compatible with
    extract_excel() (headers/rows or key/values) so downstream RAG stays consistent.
    """
    try:
        import xlrd  # BSD-licensed
    except Exception:
        # If xlrd is not installed, return plaintext to avoid 500s.
        return (data.decode("utf-8", errors="replace"), "text/plain")

    S = SETTINGS.effective

    # --- formatting/config (mirrors excel_ingest) ---
    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    EMIT_KEYVALUES = bool(S().get("excel_emit_key_values"))
    EMIT_CELL_ADDR = bool(S().get("excel_emit_cell_addresses"))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and isinstance(dt, datetime) and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    # --- open workbook ---
    try:
        book = xlrd.open_workbook(file_contents=data)
    except Exception:
        # Misnamed/garbled file: fail soft
        return (data.decode("utf-8", errors="replace"), "text/plain")

    datemode = book.datemode

    def xlrd_cell_to_py(cell):
        # xlrd types: 0 empty, 1 text, 2 number, 3 date, 4 boolean, 5 error, 6 blank
        ctype, value = cell.ctype, cell.value
        if ctype == xlrd.XL_CELL_DATE:
            try:
                return xlrd.xldate_as_datetime(value, datemode)
            except Exception:
                return value
        if ctype == xlrd.XL_CELL_NUMBER:
            return float(value)
        if ctype == xlrd.XL_CELL_BOOLEAN:
            return bool(value)
        return value

    lines: List[str] = []

    for sheet in book.sheets():
        nrows = min(sheet.nrows or 0, INFER_MAX_ROWS)
        ncols = min(sheet.ncols or 0, INFER_MAX_COLS)
        if nrows == 0 or ncols == 0:
            continue

        # header inference
        headers_raw = []
        header_fill = 0
        for c in range(ncols):
            v = xlrd_cell_to_py(sheet.cell(0, c))
            s = "" if v is None else str(v).strip()
            if s:
                header_fill += 1
            headers_raw.append(s)

        fill_ratio = header_fill / max(1, ncols)
        start_row = 1
        if fill_ratio < INFER_MIN_HEADER_FILL and nrows >= 2:
            headers_raw = []
            for c in range(ncols):
                v = xlrd_cell_to_py(sheet.cell(1, c))
                s = "" if v is None else str(v).strip()
                headers_raw.append(s)
            start_row = 2

        norm_headers = [normalize_header(h) for h in headers_raw]

        lines.append(f"# Sheet: {sheet.name}")

        # key/value mode (two-column heuristic)
        if EMIT_KEYVALUES and ncols == 2:
            textish = valueish = rows = 0
            for r in range(start_row, nrows):
                a = xlrd_cell_to_py(sheet.cell(r, 0))
                b = xlrd_cell_to_py(sheet.cell(r, 1))
                if a is None and b is None:
                    continue
                rows += 1
                if isinstance(a, str):
                    textish += 1
                if isinstance(b, (int, float, datetime, date, time)):
                    valueish += 1
            if rows >= 3 and textish / max(1, rows) >= 0.6 and valueish / max(1, rows) >= 0.6:
                lines.append("## Key/Values")
                for r in range(start_row, nrows):
                    k = fmt_val(xlrd_cell_to_py(sheet.cell(r, 0)))
                    v = fmt_val(xlrd_cell_to_py(sheet.cell(r, 1)))
                    if not k and not v:
                        continue
                    lines.append(f"- {k}: {v}" if k else f"- : {v}")
                lines.append("")
                continue  # done with this sheet

        # table mode
        lines.append("## Inferred Table")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))

        for r in range(start_row, nrows):
            row_vals: List[str] = []
            for c in range(ncols):
                val = fmt_val(xlrd_cell_to_py(sheet.cell(r, c)))
                if val:
                    row_vals.append(val if not EMIT_CELL_ADDR else f"{val}")
            if row_vals:
                lines.append("row: " + ", ".join(row_vals))
        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/retrieve.py =====

from __future__ import annotations
from typing import List, Optional, Tuple, Dict, Any
from ..core.settings import SETTINGS
from .store import search_vectors

# -------- Embedding backend (cached) -----------------------------------------

_EMBEDDER = None
_EMBEDDER_NAME = None

def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        print(f"[RAG] sentence_transformers unavailable: {e}")
        return None, None

    model_name = str(SETTINGS.get("rag_embedding_model", "intfloat/e5-small-v2"))
    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            print(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return _EMBEDDER, _EMBEDDER_NAME

def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding encode failed: {e}")
        return []

# -------- Hit handling --------------------------------------------------------

def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    """
    Sort by score desc, then de-dupe by a stable key.
    Prefer (id) if present, else (source, chunkIndex).
    """
    hits_sorted = sorted(hits, key=lambda h: float(h.get("score", 0.0)), reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out

# -------- Pretty block rendering ---------------------------------------------

def _first_nonempty(*vals: Any) -> str:
    for v in vals:
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

def _excelish_header(h: Dict[str, Any]) -> Optional[str]:
    """
    If this hit came from Excel row ingestion, surface sheet/table/row in the header.
    We look both at top-level keys and inside 'meta' to be robust against your store schema.
    """
    meta = h.get("meta") or {}
    mime = _first_nonempty(h.get("mime"), meta.get("mime"))
    if mime not in {"text/excel+row", "text/excel+lines", "text/excel+cells"}:
        return None

    src = _first_nonempty(h.get("source"), meta.get("source"))
    sheet = _first_nonempty(h.get("sheet"), meta.get("sheet"))
    table = _first_nonempty(h.get("table"), meta.get("table"))
    row_id = _first_nonempty(h.get("row_id"), meta.get("row_id"))
    idx = _first_nonempty(str(h.get("chunkIndex") or ""), str(meta.get("chunkIndex") or ""))

    parts = [src]
    if sheet:
        parts.append(f"{sheet}")
    if table:
        parts.append(f"tbl {table}")
    if row_id:
        parts.append(f"row {row_id}")
    elif idx:
        parts.append(f"chunk {idx}")

    label = " — ".join(parts) if parts else None
    return f"- {label}" if label else None

def _default_header(h: Dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"

def _render_header(h: Dict[str, Any]) -> str:
    eh = _excelish_header(h)
    return eh if eh else _default_header(h)

def _trim_to_budget(lines: List[str], total_budget: int) -> str:
    out = []
    used = 0
    for i, ln in enumerate(lines):
        need = len(ln) + (1 if i > 0 else 0)
        if used + need > total_budget:
            break
        if i > 0:
            out.append("") if False else None  # no-op; kept for readability
        out.append(ln)
        used += need
    return "\n".join(out)

def make_rag_block(hits: List[dict], *, max_chars: int = 800) -> str:
    """
    Formats hits into a compact, model-friendly block.
    Shows Excel row context when available. Obeys total budget.
    """
    lines = ["Local knowledge:"]
    total_budget = int(SETTINGS.get("rag_total_char_budget", 2200))
    used = len(lines[0]) + 1  # header + newline

    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()

        # header
        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        # body/snippet
        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost

    return "\n".join(lines)

# -------- Public entry --------------------------------------------------------

def build_rag_block(query: str, session_id: str | None = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None
    k = int(SETTINGS.get("rag_top_k", 4))

    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k}")
    qvec = _embed_query(q)
    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None

    # Pull hits from session scope and global scope
    d = len(qvec)
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    hits_glob = search_vectors(None,       qvec, k, dim=d) or []
    hits = hits_chat + hits_glob

    print(f"[RAG SEARCH] hits={len(hits)}")
    if not hits:
        return None

    hits_top = _dedupe_and_sort(hits, k=k)
    block = make_rag_block(
        hits_top,
        max_chars=int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    )
    print(f"[RAG BLOCK] chars={len(block)}")
    return block

def build_rag_block_session_only(query: str, session_id: Optional[str], *, k: Optional[int] = None) -> Optional[str]:
    """
    Same idea as build_rag_block, but ONLY searches the current session's vectors.
    Used when a user turn includes attachments (we skip Web and global RAG).
    """
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None

    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k", SETTINGS.get("rag_top_k", 4)))

    q = (query or "").strip()
    print(f"[RAG SEARCH (session-only)] q={q!r} session={session_id} k={k}")
    qvec = _embed_query(q)
    if not qvec:
        print("[RAG SEARCH (session-only)] no qvec")
        return None

    d = len(qvec)
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    print(f"[RAG SEARCH (session-only)] hits={len(hits_chat)}")
    if not hits_chat:
        return None

    hits_top = _dedupe_and_sort(hits_chat, k=k)
    block = make_rag_block(
        hits_top,
        max_chars=int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    )
    print(f"[RAG BLOCK (session-only)] chars={len(bloc