
# ===== aimodel/file_read/__init__.py =====

from .paths import app_data_dir, read_settings, write_settings
from .model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/chats.py =====

from __future__ import annotations
from dataclasses import asdict
from typing import List, Optional, Dict
from .. import retitle_worker

from fastapi import APIRouter
from pydantic import BaseModel
from ..retitle_worker import enqueue as enqueue_retitle

from ..core.schemas import (
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    MergeChatReq,
    EditMessageReq
)

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    merge_chat as store_merge_chat,
    merge_chat_new as store_merge_chat_new,
    edit_message as edit_message
)

router = APIRouter()

# ---------- Routes ----------
@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)



@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]

@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, body: Dict[str, str]):
    role = (body.get("role") or "user").strip()
    content = (body.get("content") or "").rstrip()
    row = store_append(session_id, role, content)
    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}

@router.post("/api/chats/merge")
async def api_merge_chat(req: MergeChatReq):
    if req.newChat:
        new_id, merged = store_merge_chat_new(req.sourceId, req.targetId)
        return {
            "newChatId": new_id,
            "mergedCount": len(merged),
        }
    else:
        merged = store_merge_chat(req.sourceId, req.targetId)
        return {"mergedCount": len(merged)}


@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)


@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, body: Dict[str, str]):
    role = (body.get("role") or "user").strip()
    content = (body.get("content") or "").rstrip()
    row = store_append(session_id, role, content)

    # 🚀 Background retitle trigger
    if role == "user":
        # get all messages in this session (lightweight index only)
        msgs = store_list_messages(session_id)
        enqueue_retitle(session_id, [asdict(m) for m in msgs])

    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

# aimodel/file_read/api/generate_router.py
from __future__ import annotations
from fastapi import APIRouter, Body, Request
from ..core.schemas import ChatBody
from ..services.generate_flow import generate_stream_flow, cancel_session, cancel_session_alias
from ..services.cancel import is_active  # re-export for back-compat

router = APIRouter()

@router.post("/generate/stream")
async def generate_stream(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

# legacy alias (kept identical)
@router.post("/api/ai/generate/stream")
async def generate_stream_alias(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

@router.post("/cancel/{session_id}")
async def _cancel_session(session_id: str):
    return await cancel_session(session_id)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/health.py =====

from __future__ import annotations
from fastapi import APIRouter
from ..model_runtime import current_model_info

router = APIRouter()

@router.get("/health")
async def health():
    try:
        info = current_model_info()
        return {"ok": True, "model": info}
    except Exception as e:
        return {"ok": True, "model": {"loaded": False, "error": str(e)}}

# ===== aimodel/file_read/api/models.py =====

from __future__ import annotations
from typing import Optional, Dict
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..paths import read_settings, write_settings
from ..model_runtime import (
    list_local_models, current_model_info,
    load_model, unload_model
)

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/api/search.py =====

from __future__ import annotations
from fastapi import APIRouter, Query
from pydantic import BaseModel
from typing import List, Optional
from ..web.duckduckgo import DuckDuckGoProvider
from ..web.fetch import fetch_clean
from fastapi.responses import JSONResponse
router = APIRouter()

class WebResult(BaseModel):
    title: str
    url: str
    snippet: Optional[str] = None
    rank: int


# ...
@router.get("/api/search", response_model=List[WebResult])
async def api_search(q: str = Query(..., min_length=2), k: int = 3):
    try:
        prov = DuckDuckGoProvider()
        hits = await prov.search(q, k=k)
        if not hits:
            return JSONResponse({"error": "No results (search may be blocked or rate-limited)."}, status_code=502)
        return [WebResult(title=h.title, url=h.url, snippet=h.snippet, rank=h.rank) for h in hits]
    except Exception as e:
        return JSONResponse({"error": f"search failed: {e}"}, status_code=500)


@router.get("/api/fetch")
async def api_fetch(url: str):
    final_url, status, text = await fetch_clean(url)
    return {"url": final_url, "status": status, "preview": text[:2000]}

# ===== aimodel/file_read/app.py =====

# aimodel/file_read/app.py
from __future__ import annotations
import os, asyncio, atexit
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .paths import bootstrap
from .retitle_worker import start_worker
from .api.health import router as health_router
from .api.models import router as models_router
from .api.chats import router as chats_router
from .store import process_all_pending
from .model_runtime import load_model
from .api.search import router as search_router
from .api.generate_router import router as generate_router
from .services.cancel import is_active

bootstrap()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("APP_CORS_ORIGIN", "http://localhost:5173")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(health_router)
app.include_router(models_router)
app.include_router(chats_router)
app.include_router(generate_router)
app.include_router(search_router)

@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")

    async def worker():
        while True:
            try:
                await asyncio.to_thread(process_all_pending, is_active)
            except Exception:
                pass
            await asyncio.sleep(2.0)

    asyncio.create_task(worker(), name="pending_worker")
    asyncio.create_task(start_worker(), name="retitle_worker")

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/memory.py =====

# aimodel/file_read/memory.py
from __future__ import annotations
import math
from typing import Dict, List
from collections import deque
from ..model_runtime import get_llm     # <-- use runtime, not a global llm
from .style import STYLE_SYS
from ..store import get_summary as store_get_summary

SESSIONS: Dict[str, Dict] = {}

def get_session(session_id: str):
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=50),
        "style": STYLE_SYS,
        "short": False,
        "bullets": False,
    })
    # seed once from storage if empty
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
        except Exception:
            pass
    return st

def approx_tokens(text: str) -> int:
    return max(1, math.ceil(len(text) / 4))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    return sum(approx_tokens(m["content"]) + 4 for m in msgs)

def summarize_chunks(chunks: List[Dict[str,str]]) -> str:
    text = "\n".join(f'{m["role"]}: {m["content"]}' for m in chunks)
    prompt = f"Summarize crisply the key points and decisions:\n\n{text}\n\nSummary:"
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[{"role":"system","content":"Be concise."},
                  {"role":"user","content":prompt}],
        max_tokens=240, temperature=0.2, top_p=0.9, stream=False
    )
    return out["choices"][0]["message"]["content"].strip()

def build_system(style: str, short: bool, bullets: bool) -> str:
    parts = [STYLE_SYS]
    if style and style != STYLE_SYS:
        parts.append(style)
    if short:
        parts.append("Keep answers extremely brief: max 2 sentences OR 5 short bullets.")
    if bullets:
        parts.append("Use bullet points when possible; each bullet under 15 words.")
    parts.append("Always follow the user's most recent style instructions.")
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    input_budget = max_ctx - out_budget - 256
    sys_text = build_system(style, short, bullets)

    # Put “system” guidance into a *user* prologue for Mistral-Instruct
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": f"Conversation summary so far:\n{summary}"})

    packed = prologue + list(recent)
    return packed, input_budget

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    while count_prompt_tokens(packed) > input_budget and len(recent) > 6:
        peel = []
        for _ in range(max(4, len(recent) // 5)):
            peel.append(recent.popleft())
        new_sum = summarize_chunks(peel)
        summary = (summary + "\n" + new_sum).strip() if summary else new_sum

        # Rebuild using *user* role, matching pack_messages
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": f"Conversation summary so far:\n{summary}"},
            *list(recent),
        ]
    return packed, summary

# ===== aimodel/file_read/core/schemas.py =====

from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class MergeChatReq(BaseModel):
    sourceId: str
    targetId: Optional[str] = None
    newChat: bool = False

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str    

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None
    max_tokens: Optional[int] = 512
    temperature: float = 0.7
    top_p: float = 0.95
    # NEW:
    autoWeb: bool = True
    webK: int = 3

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple

STYLE_SYS = (
    "You are a helpful assistant. "
    "Always follow the user's explicit instructions carefully and exactly. "
    "Do not repeat yourself. Stay coherent and complete."
)

PAT_TALK_LIKE = re.compile(r"\btalk\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_RESPOND_LIKE = re.compile(r"\brespond\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_BE = re.compile(r"\bbe\s+(?P<style>[^.;\n]+)", re.I)
PAT_FROM_NOW = re.compile(r"\bfrom\s+now\s+on[, ]+\s*(?P<style>[^.;\n]+)", re.I)

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    t = user_text.strip()
    style_match = (
        PAT_TALK_LIKE.search(t)
        or PAT_RESPOND_LIKE.search(t)
        or PAT_FROM_NOW.search(t)
        or PAT_BE.search(t)
    )
    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = (
            f"You must talk like {raw}. "
            f"Stay in character but remain helpful and accurate. "
            f"Follow the user's latest style instructions."
        )
    return style_inst, False, False

# ===== aimodel/file_read/model_runtime.py =====

# aimodel/file_read/model_runtime.py
from __future__ import annotations
import os
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Optional, List

from .paths import read_settings, write_settings

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath","")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None,"") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None,"") else None),
        )

_runtime_lock = RLock()
_llm: Optional[Llama] = None
_cfg: Optional[ModelConfig] = None

def _build_kwargs(cfg: ModelConfig) -> Dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    # advanced optional tuning
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            # llama_cpp doesn't expose explicit close; allow GC
            _llm = None
    except Exception:
        _llm = None

def current_model_info() -> Dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
        }

def ensure_ready() -> None:
    """
    Lazily load a model based on current settings if nothing is loaded.
    """
    global _llm, _cfg
    with _runtime_lock:
        if _llm is not None:
            return
        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected. Load one via /models/load or set LOCALAI_MODEL_PATH.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")
        _llm = Llama(**_build_kwargs(cfg))
        _cfg = cfg

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: Dict[str, Any]) -> Dict[str, Any]:
    """
    Load/swap to a new model with given config fields (any subset).
    Persists to settings.json so the choice survives restarts.
    """
    global _llm, _cfg
    with _runtime_lock:
        s = read_settings()
        s.update({k:v for k,v in config_patch.items() if v is not None})
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")

        # swap
        _close_llm()
        _llm = Llama(**_build_kwargs(cfg))
        _cfg = cfg
        write_settings(asdict(cfg))
        return current_model_info()

def unload_model() -> None:
    """
    Unload current model, keep settings as-is.
    """
    global _llm
    with _runtime_lock:
        _close_llm()

def list_local_models() -> List[Dict[str, Any]]:
    """
    Scan modelsDir for .gguf files and return a lightweight listing.
    """
    s = read_settings()
    root = Path(s.get("modelsDir") or "")
    root.mkdir(parents=True, exist_ok=True)
    out: List[Dict[str, Any]] = []
    for p in root.rglob("*.gguf"):
        try:
            out.append({
                "path": str(p.resolve()),
                "sizeBytes": p.stat().st_size,
                "name": p.name,
                "rel": str(p.relative_to(root)),
            })
        except Exception:
            pass
    # sort largest first (usually higher quant or full precision)
    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs=9.5.4
httpx==0.27.0
trafilatura==1.9.0
lxml[html_clean]>=5.3

# ===== aimodel/file_read/retitle_worker.py =====

# aimodel/file_read/retitle_worker.py
from __future__ import annotations
import asyncio, logging
from .model_runtime import get_llm
from .store.index import load_index, save_index
from .store.base import now_iso

# Shared queue (session_id, messages)
queue: asyncio.Queue[tuple[str, list[dict]]] = asyncio.Queue()


async def start_worker():
    """Background loop to process retitle jobs one by one."""
    while True:
        session_id, messages = await queue.get()
        try:
            await _do_retitle(session_id, messages)
        except Exception as e:
            logging.exception("Retitle worker failed")
        finally:
            queue.task_done()


async def _do_retitle(session_id: str, messages: list[dict]):
    # Find first user message text
    first_user = next((m.get("content") for m in messages if m.get("role") == "user"), "")
    if not first_user:
        return

    llm = get_llm()
    out = llm.create_completion(
        prompt=f"Summarize this in 3–5 words: {first_user}",
        max_tokens=16,
        temperature=0.3,
    )
    title = out["choices"][0]["text"].strip()

    if title:
        idx = load_index()
        row = next((r for r in idx if r["sessionId"] == session_id), None)
        if row:
            row["title"] = title
            row["updatedAt"] = now_iso()
            save_index(idx)


def enqueue(session_id: str, messages: list[dict]):
    """Non-blocking add to queue."""
    if not messages:
        return
    try:
        queue.put_nowait((session_id, messages))
    except Exception as e:
        logging.warning(f"Failed to enqueue retitle: {e}")

# ===== aimodel/file_read/services/cancel.py =====

# aimodel/file_read/services/cancel.py
from __future__ import annotations
import asyncio
from threading import Event
from typing import Dict

GEN_SEMAPHORE = asyncio.Semaphore(1)
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

# aimodel/file_read/services/context_window.py
from __future__ import annotations
from typing import List, Dict, Optional, Tuple
from ..utils.streaming import safe_token_count_messages
from ..model_runtime import current_model_info

def estimate_tokens(llm, messages: List[Dict[str,str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or 4096)
    except Exception:
        return 4096

def clamp_out_budget(
    *, llm, messages: List[Dict[str,str]], requested_out: int, margin: int = 32
) -> Tuple[int, int]:
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = 1024
    n_ctx = current_n_ctx()
    available = max(16, n_ctx - prompt_est - margin)
    safe_out = max(16, min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations
import asyncio, time, json
from typing import AsyncGenerator, Dict, List
from fastapi.responses import StreamingResponse
from datetime import datetime

from ..model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody

from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .session_io import handle_incoming, persist_summary
from .packing import build_system_text, pack_with_rollup
from .context_window import clamp_out_budget

# Router + summarizer + orchestrator (web path)
from ..web.router_ai import decide_web
from ..web.query_summarizer import summarize_query
from ..web.orchestrator import build_web_block

# Tell the type checker run_stream is an async iterator of bytes (stops yellow underline)
from .streaming_worker import run_stream as _run_stream
from typing import AsyncIterator
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream  # type: ignore[assignment]

# ---- helpers for instrumentation --------------------------------------------
def _now() -> str:
    return datetime.now().isoformat(timespec="milliseconds")

def _chars_len(msgs: List[object]) -> int:
    """Robust char counter for packed messages that may include dicts or strings."""
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m  # plain string (defensive)
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total

def _dump_full_prompt(messages: List[Dict[str, object]], *, params: Dict[str, object], session_id: str) -> None:
    """Print the exact payload we send to the model: full messages + key params."""
    try:
        print(f"[{_now()}] PROMPT DUMP BEGIN session={session_id} msgs={len(messages)}")
        print(json.dumps(
            {
                "messages": messages,
                "params": params,
            },
            ensure_ascii=False,
            indent=2,
        ))
        print(f"[{_now()}] PROMPT DUMP END   session={session_id}")
    except Exception as e:
        print(f"[{_now()}] PROMPT DUMP ERROR session={session_id} err={type(e).__name__}: {e}")
# -----------------------------------------------------------------------------

async def generate_stream_flow(data: ChatBody, request) -> StreamingResponse:
    ensure_ready()
    llm = get_llm()

    session_id = data.sessionId or "default"
    if not data.messages:
        return StreamingResponse(iter([b"No messages provided."]), media_type="text/plain")

    incoming = [{"role": m.role, "content": m.content} for m in data.messages]
    print(f"[{_now()}] GEN request START session={session_id} msgs_in={len(incoming)}")

    st = handle_incoming(session_id, incoming)

    # latest user text from THIS request only
    latest_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    lut_chars = len(latest_user_text) if isinstance(latest_user_text, str) else len(str(latest_user_text) or "")

    # --- ROUTER → (maybe) SUMMARIZER → (maybe) WEB BLOCK ----------------------
    t0 = time.perf_counter()
    print(f"[{_now()}] GEN web_inject START session={session_id} latest_user_text_chars={lut_chars}")
    try:
        need_web, proposed_q = decide_web(llm, str(latest_user_text or ""))
        print(f"[{_now()}] ROUTER decision need_web={need_web} proposed_q={proposed_q!r}")

        if need_web:
            base_query = proposed_q or str(latest_user_text or "")
            q_summary = summarize_query(llm, base_query)
            q_summary = q_summary.strip().strip('"\'')

            print(f"[{_now()}] SUMMARIZER out={q_summary!r}")

            k = int(getattr(data, "webK", 3) or 3)
            print(f"[{_now()}] ORCH build start k={k} q={q_summary!r}")
            block = await build_web_block(q_summary, k=k)
            print(f"[{_now()}] ORCH build done has_block={bool(block)} block_len={(len(block) if block else 0)}")

            if block:
                # CRITICAL: wrap as a proper chat message (dict), not a raw string
                st["_ephemeral_web"] = (st.get("_ephemeral_web") or []) + [
                    {
                        "role": "assistant",
                        # *** ONLY REQUIRED CHANGE: mark findings as authoritative in the content ***
                        "content": "Web findings (authoritative — use these to answer accurately; override older knowledge):\n\n" + block,
                    }
                ]
                types_preview = [type(x).__name__ for x in (st.get("_ephemeral_web") or [])]
                print(f"[{_now()}] EPHEMERAL attached count={len(st['_ephemeral_web'])} types={types_preview}")

        dt = time.perf_counter() - t0
        eph_cnt = len(st.get("_ephemeral_web") or [])
        print(f"[{_now()}] GEN web_inject END   session={session_id} elapsed={dt:.3f}s ephemeral_blocks={eph_cnt}")
    except Exception as e:
        dt = time.perf_counter() - t0
        print(f"[{_now()}] GEN web_inject ERROR session={session_id} elapsed={dt:.3f}s err={type(e).__name__}: {e}")

    out_budget_req = data.max_tokens or 512
    system_text = build_system_text()

    # consume ephemeral web block so it doesn't stick across turns
    ephemeral_once = st.pop("_ephemeral_web", [])
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=4096,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )

    packed_chars = _chars_len(packed)
    print(f"[{_now()}] GEN pack READY       session={session_id} msgs={len(packed)} chars={packed_chars} out_budget_req={out_budget_req}")

    # dump the exact prompt (messages + params) we will send to the model
    _dump_full_prompt(
        packed,
        params={
            "requested_out": out_budget_req,
            "temperature": (data.temperature or 0.6),
            "top_p": (data.top_p or 0.9),
        },
        session_id=session_id,
    )

    persist_summary(session_id, st["summary"])

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_req, margin=32
    )
    print(f"[{_now()}] GEN clamp_out_budget  session={session_id} out_budget={out_budget} input_tokens_est={input_tokens_est}")

    stop_ev = cancel_event(session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        async with GEN_SEMAPHORE:
            mark_active(session_id, +1)
            try:
                print(f"[{_now()}] GEN run_stream START session={session_id} msgs={len(packed)} chars={packed_chars} out_budget={out_budget} tokens_in~={input_tokens_est}")
                async for chunk in run_stream(
                    llm=llm,
                    messages=packed,
                    out_budget=out_budget,
                    stop_ev=stop_ev,
                    request=request,
                    temperature=(data.temperature or 0.6),
                    top_p=(data.top_p or 0.9),
                    input_tokens_est=input_tokens_est,
                ):
                    yield chunk
            finally:
                try:
                    from ..store import apply_pending_for
                    apply_pending_for(session_id)
                except Exception:
                    pass
                print(f"[{_now()}] GEN run_stream END   session={session_id}")
                mark_active(session_id, -1)

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "Connection": "keep-alive"},
    )

async def cancel_session(session_id: str):
    from .cancel import cancel_event
    cancel_event(session_id).set()
    return {"ok": True}

async def cancel_session_alias(session_id: str):
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/packing.py =====

# aimodel/file_read/services/packing.py
from __future__ import annotations
from typing import Tuple, List, Dict, Optional
from ..core.memory import build_system, pack_messages, roll_summary_if_needed

def build_system_text() -> str:
    base = build_system(style="", short=False, bullets=False)
    guidance = (
        "\nYou may consult the prior messages to answer questions about the conversation itself "
        "(e.g., “what did I say first?”). When web context is present, consider it as evidence, "
        "prefer newer info if it conflicts with older memory, and respond in your own words."
    )
    return (base + guidance)

def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    packed, input_budget = pack_messages(
        style="", short=False, bullets=False,
        summary=summary, recent=recent, max_ctx=max_ctx, out_budget=out_budget
    )
    packed, new_summary = roll_summary_if_needed(
        packed=packed, recent=recent, summary=summary,
        input_budget=input_budget, system_text=system_text
    )

    # Inject ephemeral (web findings) BEFORE the last user message, so the final turn is still user.
    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.memory import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    # ensure ephemeral web bucket exists
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass  # preserve original non-fatal behavior

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END, STOP_STRINGS,
    build_run_json, watch_disconnect,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int]
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=64)
    SENTINEL = object()

    def produce():
        t_start = time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []

        try:
            try:
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=40,
                    repeat_penalty=1.25,
                    stop=STOP_STRINGS,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(64, out_budget // 2)
                    log.warning("generate: context overflow, retrying with max_tokens=%d", retry_tokens)
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=40,
                        repeat_penalty=1.25,
                        stop=STOP_STRINGS,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(0.005)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                llm.reset()
            except Exception:
                pass

            try:
                out_text = "".join(out_parts)
                run_json = build_run_json(
                    request_cfg={"temperature": temperature, "top_p": top_p, "max_tokens": out_budget},
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                )
                q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass

            try:
                q.put_nowait(SENTINEL)
            except Exception:
                pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            if stop_ev.is_set():
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set():
            yield b"\n\u23F9 stopped\n"
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=2.0)
        except Exception:
            pass

# ===== aimodel/file_read/services/web_block.py =====

# aimodel/file_read/services/web_block.py
from __future__ import annotations
import logging, re
from typing import Optional
from ..model_runtime import get_llm
from ..web.orchestrator import build_web_block
from ..web.query_summarizer import summarize_query

log = logging.getLogger("aimodel.api.generate")

async def always_inject_web_block(st, latest_user_text: str, *, k: int):
    """
    EXACT behavior retained:
      1) If [[search: ...]] present -> use that literal query.
      2) Else summarize the user's text into a short query.
      3) Call orchestrator.build_web_block(query) and APPEND the block to st["recent"].
    """
    if not latest_user_text or not latest_user_text.strip():
        return

    m = re.search(r"\[\[search:\s*(.+?)\s*\]\]", latest_user_text, re.I)
    if m:
        query = m.group(1).strip()
        log.info("generate: explicit [[search:]] query=%r", query)
    else:
        try:
            query = summarize_query(get_llm(), latest_user_text.strip())
            short_src = (latest_user_text[:120] + "…") if len(latest_user_text) > 120 else latest_user_text
            log.info("generate: summarized query=%r (from=%r)", query, short_src)
        except Exception as e:
            log.exception("generate: summarize_query failed: %s", e)
            return

    if not query:
        log.warning("generate: empty summarized query, skipping web")
        return

    try:
        block = await build_web_block(query, k=k, per_url_timeout_s=8.0)
        if block:
            # ⬇⬇⬇ store as EPHEMERAL, not in recent
            st.setdefault("_ephemeral_web", []).append({"role": "user", "content": block})
            log.info("generate: injected web block EPHEMERAL (chars=%d)", len(block))
        else:
            log.warning("generate: orchestrator returned no block for query=%r", query)
    except Exception as e:
        log.exception("generate: orchestrator failed for query=%r: %s", query, e)

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message, update_last, append_message,
    delete_message, delete_messages_batch, list_messages,
    list_paged, delete_batch,
    merge_chat, merge_chat_new, edit_message, set_summary, get_summary,  # ← add these
)
from .index import ChatMeta
from .pending import (
    enqueue_pending, apply_pending_for, process_all_pending, list_pending_sessions,
)

__all__ = [
    # chats
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "merge_chat", "merge_chat_new", "edit_message",  # ← add these
    # index
    "ChatMeta",
    # pending
    "enqueue_pending", "apply_pending_for", "process_all_pending", "list_pending_sessions", "set_summary", "get_summary"
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile, threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..paths import app_data_dir

# -------- Directories & Paths --------
APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"
PENDING_PATH = APP_DIR / "pending.json"              # NEW
OLD_PENDING_DELETES = APP_DIR / "pending_deletes.json"  # NEW

# -------- Lock for safe writes --------
_lock = threading.RLock()

# -------- Helpers --------
def now_iso() -> str:
    """UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    """Safely write JSON to a temp file then move into place."""
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    """Ensure app/chats directories exist and index.json is initialized."""
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    """Return path to chat file for a session ID."""
    return CHATS_DIR / f"{session_id}.json"

# -------- Exports --------
__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "PENDING_PATH",          # NEW
    "OLD_PENDING_DELETES",   # NEW
    "_lock",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta

def _load_chat(session_id: str) -> Dict:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}  # add summary
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""  # backfill older files
        return data
    
@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str

def upsert_on_first_message(session_id: str, title: str) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or "New Chat"),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row); save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)

def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)

def append_message(session_id: str, role: str, content: str) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq, "sessionId": session_id, "role": role,
        "content": content, "createdAt": now_iso(),
    }
    data["messages"].append(msg); data["seq"] = seq
    _save_chat(session_id, data)

    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if row:
        row["updatedAt"] = msg["createdAt"]
        if role == "assistant":
            row["lastMessage"] = content
        save_index(idx)

    # pending ops are applied by pending.apply_pending_for() from the router after appends
    return ChatMessageRow(**msg)

def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1

def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted

def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    return [ChatMessageRow(**m) for m in data.get("messages", [])]

def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    size = max(1, min(100, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag

def delete_batch(session_ids: List[str]) -> List[str]:
    for sid in session_ids:
        try: chat_path(sid).unlink(missing_ok=True)
        except Exception: pass
    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids

def merge_chat(source_id: str, target_id: str):
    source_msgs = list_messages(source_id)
    target_msgs = list_messages(target_id)

    # Insert source first, then re-add target to preserve order
    merged = []
    for m in source_msgs:
        row = append_message(target_id, m.role, m.content)
        merged.append(row)

    for m in target_msgs:
        row = append_message(target_id, m.role, m.content)
        merged.append(row)

    return merged

def _save_chat(session_id: str, data: Dict):
    atomic_write(chat_path(session_id), data)

def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)

def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")

def merge_chat_new(source_id: str, target_id: Optional[str] = None):
    from uuid import uuid4
    new_id = str(uuid4())
    upsert_on_first_message(new_id, "Merged Chat")

    merged = []

    # source first
    for m in list_messages(source_id):
        row = append_message(new_id, m.role, m.content)
        merged.append(row)

    # then target (if exists)
    if target_id:
        for m in list_messages(target_id):
            row = append_message(new_id, m.role, m.content)
            merged.append(row)

    return new_id, merged

def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)

    # refresh index if last message changed
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(**updated)


# expose internals for pending ops
__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch", "merge_chat", "merge_chat_new",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary", 
]

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/store/pending.py =====

from __future__ import annotations
import json
from typing import Callable, Dict, List
from .base import PENDING_PATH, OLD_PENDING_DELETES, atomic_write, _lock  # <-- import _lock
from .chats import _load_chat, _save_chat
from .index import refresh_index_after_change


def _load_pending() -> Dict[str, List[Dict[str, object]]]:
    try:
        with PENDING_PATH.open("r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                return data
    except Exception:
        pass
    return {}

def _save_pending(d: Dict[str, List[Dict[str, object]]]):
    atomic_write(PENDING_PATH, d)

def _migrate_old_pending_if_any():
    try:
        if OLD_PENDING_DELETES.exists():
            legacy = json.loads(OLD_PENDING_DELETES.read_text("utf-8"))
            if isinstance(legacy, dict):
                with _lock:
                    cur = _load_pending()
                    for sid, arr in legacy.items():
                        lst = cur.setdefault(sid, [])
                        for req in arr or []:
                            lst.append({
                                "type": "deleteMessages",
                                "payload": {
                                    "messageIds": [int(i) for i in (req.get("messageIds") or [])],
                                    "tailAssistant": bool(req.get("tailAssistant") or False),
                                },
                            })
                    _save_pending(cur)
            OLD_PENDING_DELETES.unlink(missing_ok=True)
    except Exception:
        pass

def enqueue_pending(session_id: str, op_type: str, payload: Dict[str, object]):
    _migrate_old_pending_if_any()
    with _lock:
        pend = _load_pending()
        q = pend.setdefault(session_id, [])
        q.append({"type": op_type, "payload": payload or {}})
        _save_pending(pend)

def _consume_queue(session_id: str) -> List[Dict[str, object]]:
    with _lock:
        pend = _load_pending()
        arr = pend.pop(session_id, [])
        _save_pending(pend)
        return arr

def list_pending_sessions() -> List[str]:
    _migrate_old_pending_if_any()
    with _lock:
        return list(_load_pending().keys())
    
    

# ... (rest unchanged) ...


# --- op handlers ---


def _op_delete_messages(session_id: str, payload: Dict[str, object]) -> List[int]:
    ids = set(int(i) for i in (payload.get("messageIds") or []))
    tail_assistant = bool(payload.get("tailAssistant") or False)

    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    if not isinstance(msgs, list):
        msgs = []

    found_tail = False
    if tail_assistant:
        # try to include the most recent assistant message
        for m in reversed(msgs):
            if m.get("role") == "assistant":
                try:
                    ids.add(int(m.get("id")))
                    found_tail = True
                except Exception:
                    pass
                break

    # If we intended to delete the tail assistant but it hasn't been persisted yet,
    # DEFER instead of dropping the op. `apply_pending_for` will catch this and requeue.
    if not ids and tail_assistant and not found_tail:
        raise RuntimeError("defer_tail_assistant")

    if not ids:
        return []

    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in ids:
            deleted.append(mid)
        else:
            keep.append(m)

    if deleted:
        data["messages"] = keep
        _save_chat(session_id, data)
        refresh_index_after_change(session_id, keep)
    return deleted

def _apply_op(session_id: str, op: Dict[str, object]) -> Dict[str, object]:
    t = str(op.get("type") or "")
    payload = op.get("payload") or {}
    if t == "deleteMessages":
        deleted = _op_delete_messages(session_id, payload)
        return {"type": t, "ok": True, "deleted": deleted}
    return {"type": t, "ok": False, "reason": "unknown_op"}

def apply_pending_for(session_id: str) -> List[Dict[str, object]]:
    ops = _consume_queue(session_id)
    results: List[Dict[str, object]] = []
    for op in ops:
        try:
            results.append(_apply_op(session_id, op))
        except Exception as e:
    # Requeue on failure (front)
            pend = _load_pending()
            pend.setdefault(session_id, []).insert(0, op)
            _save_pending(pend)
            results.append({"type": op.get("type"), "ok": False, "error": str(e)})
    return results

def process_all_pending(is_active: Callable[[str], bool]) -> Dict[str, List[Dict[str, object]]]:
    out: Dict[str, List[Dict[str, object]]] = {}
    _migrate_old_pending_if_any()
    sessions = list(_load_pending().keys())
    for sid in sessions:
        try:
            if is_active(sid):
                continue
            res = apply_pending_for(sid)
            if res:
                out[sid] = res
        except Exception:
            pass
    return out

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..model_runtime import current_model_info, get_llm

# Markers MUST match the frontend parser
RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

# llama.cpp common stop strings
STOP_STRINGS = ["</s>", "User:", "\nUser:"]

# ---------- token + model helpers ----------

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))  # type: ignore[arg-type]
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def build_run_json(
    *,
    request_cfg: Dict[str, object],   # temperature, top_p, max_tokens
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None

    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()

    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
        },
    }

# ---------- connection helper ----------

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/duckduckgo.py =====

# aimodel/file_read/web/duckduckgo.py
from __future__ import annotations
from typing import List, Optional, Tuple
import asyncio, time
from urllib.parse import urlparse

# Prefer new package; fallback for compatibility
try:
    from ddgs import DDGS  # type: ignore
except Exception:
    try:
        from duckduckgo_search import DDGS  # type: ignore
    except Exception:
        DDGS = None  # no provider

from .provider import SearchHit

# -------- simple in-memory cache (store superset = 10) -----------------------
_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}
CACHE_TTL_SEC = 300  # 5 minutes
CACHE_SUPERSET_K = 10

def _cache_key(query: str) -> str:
    return (query or "").strip().lower()

def _cache_get(query: str) -> Optional[List[SearchHit]]:
    key = _cache_key(query)
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > CACHE_TTL_SEC:
        _CACHE.pop(key, None)
        return None
    return hits

def _cache_set(query: str, hits: List[SearchHit]) -> None:
    _CACHE[_cache_key(query)] = (time.time(), hits)

def _host(u: str) -> str:
    try:
        h = (urlparse(u).hostname or "").lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return ""

# -------- DDGS (official client) ---------------------------------------------
def _ddg_sync_search(query: str, k: int) -> List[SearchHit]:
    results: List[SearchHit] = []
    if DDGS is None:
        print(f"[{time.strftime('%X')}] ddg: PROVIDER MISSING (DDGS=None)")
        return results
    with DDGS() as ddg:
        for i, r in enumerate(ddg.text(query, max_results=max(1, k),
                                       safesearch="moderate", region="us-en")):
            title = (r.get("title") or "").strip()
            url = (r.get("href") or "").strip()
            snippet: Optional[str] = (r.get("body") or "").strip() or None
            if not url:
                continue
            results.append(SearchHit(title=title or url, url=url, snippet=snippet, rank=i))
            if i + 1 >= k:
                break
    return results

# -------- public provider ----------------------------------------------------
class DuckDuckGoProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        """
        Returns top-k SearchHit results via DDGS.

        Added prints:
          - START with normalized query, k
          - CACHE HIT / MISS with timing
          - For fetched results: total count, top items (idx, host, title, url)
          - RETURN with timing and top titles
        """
        t0 = time.time()
        q_norm = (query or "").strip()
        if not q_norm:
            print("ddg: empty query")
            return []

        print(f"[{time.strftime('%X')}] ddg: START q={q_norm!r} k={k}")

        # superset cache
        cached = _cache_get(q_norm)
        if cached is not None:
            dt = time.time() - t0
            out = cached[:k]
            top_preview = [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]]
            print(f"[{time.strftime('%X')}] ddg: CACHE HIT dt={dt:.2f}s hits={len(out)} top={top_preview}")
            return out

        # fetch superset once
        sup_k = max(k, CACHE_SUPERSET_K)
        hits: List[SearchHit] = []
        if DDGS is not None:
            try:
                step = time.time()
                hits = await asyncio.to_thread(_ddg_sync_search, q_norm, sup_k)
                dt_fetch = time.time() - step
                print(f"[{time.strftime('%X')}] ddg: HITS RECEIVED dt={dt_fetch:.2f}s count={len(hits)}")
                # verbose preview of first few results
                for h in hits[:5]:
                    print(f"[{time.strftime('%X')}] ddg:   {h.rank:>2} | host={_host(h.url)} | title={(h.title or '')[:80]!r} | url={h.url}")
            except Exception as e:
                print(f"[{time.strftime('%X')}] ddg: ERROR {e}")
        else:
            print(f"[{time.strftime('%X')}] ddg: SKIP (DDGS unavailable)")

        # store superset and return slice
        _cache_set(q_norm, hits)
        dt = time.time() - t0
        out = hits[:k]
        top_titles = [h.title for h in out[:3]]
        print(f"[{time.strftime('%X')}] ddg: RETURN dt={dt:.2f}s hits={len(out)} top={top_titles}")
        return out

# ===== aimodel/file_read/web/fetch.py =====

from __future__ import annotations
import asyncio
from typing import Tuple, List
import httpx
import trafilatura

HEADERS = {"User-Agent": "LocalAI/0.1 (+clean-fetch)"}

async def fetch_clean(url: str, timeout_s: float = 8.0, max_chars: int = 3000) -> Tuple[str, int, str]:
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout_s) as client:
        r = await client.get(url, headers=HEADERS)
        r.raise_for_status()
        # trafilatura extracts readable article text (Apache-2.0)
        txt = trafilatura.extract(r.text, url=str(r.url)) or ""
        if not txt:
            # fallback: raw text up to cap
            txt = r.text
        txt = txt.strip().replace("\r", "")
        if len(txt) > max_chars:
            txt = txt[:max_chars]
        return (str(r.url), r.status_code, txt)

async def fetch_many(urls: List[str], per_timeout_s: float = 8.0, cap_chars: int = 3000, max_parallel: int = 3):
    sem = asyncio.Semaphore(max_parallel)
    async def _one(u: str):
        async with sem:
            try:
                return u, await fetch_clean(u, per_timeout_s, cap_chars)
            except Exception:
                return u, None
    tasks = [_one(u) for u in urls]
    return await asyncio.gather(*tasks)

# ===== aimodel/file_read/web/orchestrator.py =====

# aimodel/file_read/web/orchestrator.py
from __future__ import annotations
from typing import List, Tuple
from urllib.parse import urlparse
import time
import re

from .duckduckgo import DuckDuckGoProvider
from .provider import SearchHit
from .fetch import fetch_many

DEFAULT_K = 2                 # fewer sources by default
MAX_BLOCK_TOKENS_EST = 700    # respect ~char budget below
TOTAL_CHAR_BUDGET = 1600      # ~ 400 tokens (≈4 chars/token)
PER_DOC_CHAR_BUDGET = 900     # trim each page harder
MAX_PARALLEL_FETCH = 2        # avoid overfetching & reduce prompt bloat

def _clean_ws(s: str) -> str:
    return " ".join((s or "").split())

def _head_tail(text: str, max_chars: int) -> str:
    if not text or len(text) <= max_chars:
        return _clean_ws(text)
    head = max_chars - max(200, max_chars // 3)
    tail = max_chars - head
    return _clean_ws(text[:head] + " … " + text[-tail:])

def condense_doc(title: str, url: str, text: str, max_chars: int = PER_DOC_CHAR_BUDGET) -> str:
    # keep title, final URL, and a trimmed body
    body = _head_tail(text or "", max_chars)
    safe_title = _clean_ws(title or url)
    return f"- {safe_title}\n  {url}\n  {body}"

def _host(url: str) -> str:
    h = (urlparse(url).hostname or "").lower()
    return h[4:] if h.startswith("www.") else h

def _tokens(s: str) -> List[str]:
    return [t for t in re.findall(r"\w+", (s or "").lower()) if t]

def score_hit(hit: SearchHit, query: str) -> int:
    """
    Generic, content-based scoring (no hardcoded domains, no date/month/year rules).
    Signals:
      - exact phrase in title (+3) / substring in title (+2)
      - token coverage in title (+0..2)
      - token touch in snippet (+1 if any)
    """
    score = 0
    q = (query or "").strip().lower()
    title = (hit.title or "").strip()
    snippet = (hit.snippet or "").strip()
    title_l = title.lower()
    snip_l  = snippet.lower()

    if q:
        if title_l == q:
            score += 3
        elif q in title_l:
            score += 2

    qtoks = _tokens(q)
    if qtoks:
        cov_title = sum(1 for t in qtoks if t in title_l)
        if cov_title == len(qtoks):
            score += 2
        elif cov_title > 0:
            score += 1

        cov_snip = sum(1 for t in qtoks if t in snip_l)
        if cov_snip > 0:
            score += 1

    return score

async def build_web_block(query: str, k: int = DEFAULT_K, per_url_timeout_s: float = 8.0) -> str | None:
    start_time = time.time()
    print(f"[orchestrator] IN  @ {start_time:.3f}s | query={query!r}")

    t0 = time.perf_counter()
    provider = DuckDuckGoProvider()

    # --- SEARCH (light overfetch) ---
    overfetch = max(k + 1, int(k * 1.5))
    print(f"[orchestrator] SEARCH start overfetch={overfetch} k={k}")
    try:
        hits: List[SearchHit] = await provider.search(query, k=overfetch)
    except Exception as e:
        print(f"[orchestrator] ERROR during search for {query!r}: {e}")
        return None
    print(f"[orchestrator] SEARCH done hits={len(hits)} dt={time.perf_counter() - t0:.3f}s")

    if not hits:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no hits | elapsed={time.time()-start_time:.3f}s")
        return None

    # --- SCORING / DEDUPE ---
    print(f"[orchestrator] SCORING generic (no hardcoded boosts)")
    seen = set()
    scored: List[Tuple[int, SearchHit]] = []
    for idx, h in enumerate(hits):
        u = (h.url or "").strip()
        if not u:
            print(f"[orchestrator]   skip[{idx}] empty url")
            continue
        if u in seen:
            print(f"[orchestrator]   dup [{idx}] host={_host(u)} title={(h.title or '')[:60]!r}")
            continue
        seen.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
        print(f"[orchestrator]   meta[{idx}] score={s} host={_host(u)} title={(h.title or '')[:80]!r} url={u}")

    if not scored:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no unique hits | elapsed={time.time()-start_time:.3f}s")
        return None

    scored.sort(key=lambda x: x[0], reverse=True)
    top_hits = [h for _, h in scored[:k]]

    for i, h in enumerate(top_hits, 1):
        print(f"[orchestrator] PICK {i}/{k} score={score_hit(h, query)} host={_host(h.url)} title={(h.title or '')[:80]!r}")

    # --- FETCH (tighter caps) ---
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]
    print(f"[orchestrator] FETCH start urls={[ _host(u) for u in urls ]}")

    t_f = time.perf_counter()
    results = await fetch_many(
        urls,
        per_timeout_s=per_url_timeout_s,
        cap_chars=min(2000, PER_DOC_CHAR_BUDGET * 2),
        max_parallel=MAX_PARALLEL_FETCH,
    )
    dt_f = time.perf_counter() - t_f
    print(f"[orchestrator] FETCH done n={len(results)} dt={dt_f:.3f}s")

    chunks: List[str] = []
    for original_url, res in results:
        if not res:
            print(f"[orchestrator]   fetch MISS url={original_url}")
            continue
        final_url, status, text = res
        title = next((t for (t, u) in meta if u == original_url), final_url)
        tl = len(text or "")
        print(f"[orchestrator]   fetch OK   status={status} host={_host(final_url)} text_len={tl} title={(title or '')[:80]!r}")
        if not text:
            continue
        chunk = condense_doc(title, final_url, text, max_chars=PER_DOC_CHAR_BUDGET)
        chunks.append(chunk)
        print(f"[orchestrator]   chunk len={len(chunk)} host={_host(final_url)}")

    if not chunks:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no chunks | elapsed={time.time()-start_time:.3f}s")
        return None

    # --- ENFORCE TOTAL BUDGET ---
    header = f"Web findings for: {query}"
    available = max(200, TOTAL_CHAR_BUDGET - len(header) - 2)
    block_parts: List[str] = []
    used = 0
    for idx, ch in enumerate(chunks):
        cl = len(ch)
        sep = (2 if block_parts else 0)
        if used + cl + sep > available:
            shrunk = _head_tail(ch, max(200, available - used - sep))
            print(f"[orchestrator]   budget hit at chunk[{idx}] orig={cl} shrunk={len(shrunk)} used_before={used} avail={available}")
            if len(shrunk) > 200:
                block_parts.append(shrunk)
                used += len(shrunk) + sep
            break
        block_parts.append(ch)
        used += cl + sep
        print(f"[orchestrator]   take chunk[{idx}] len={cl} used_total={used}/{available}")

    body = "\n\n".join(block_parts)
    block = f"{header}\n\n{body}"

    end_time = time.time()
    print(f"[orchestrator] OUT @ {end_time:.3f}s | elapsed={end_time-start_time:.3f}s | chunks={len(block_parts)} | chars={len(block)}")
    return block

# ===== aimodel/file_read/web/provider.py =====

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SearchHit:
    title: str
    url: str
    snippet: Optional[str] = None
    rank: int = 0

class SearchProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        raise NotImplementedError

# ===== aimodel/file_read/web/query_summarizer.py =====

# aimodel/file_read/web/query_summarizer.py
from __future__ import annotations
from typing import Any
import re

_PROMPT = """Summarize the user's request into a concise web search query.
Keep only the key entities and terms.
Do not explain, and do not surround the result in quotation marks or other punctuation.
You may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.
Keep the original word order. Output only the query text.

User: {text}
Query:"""

def _tokens(s: str) -> set[str]:
    # simple, generic normalization (no term lists)
    return set(re.findall(r"\w+", (s or "").lower()))

def summarize_query(llm: Any, user_text: str) -> str:
    txt = (user_text or "").strip()
    print(f"[SUMMARIZER] IN user_text={txt!r}")

    # 1) Bypass LLM for tiny inputs to avoid paraphrase drift
    if len(txt) <= 32 and len(txt.split()) <= 3:
        print(f"[SUMMARIZER] BYPASS (short) -> {txt!r}")
        return txt

    out = llm.create_chat_completion(
        messages=[{"role": "user", "content": _PROMPT.format(text=txt)}],
        max_tokens=32,
        temperature=0.0,
        top_p=1.0,           # fully greedy; reduces unintended paraphrasing
        stream=False,
        stop=["\n", "</s>"],
    )
    result = (out["choices"][0]["message"]["content"] or "").strip()

    # 2) Similarity fallback (generic, no special terms)
    src_toks = _tokens(txt)
    out_toks = _tokens(result)
    if not result or not out_toks:
        print(f"[SUMMARIZER] RETAIN (empty/none) -> {txt!r}")
        print(f"[SUMMARIZER] OUT query={txt!r}")
        return txt

    jaccard = (len(src_toks & out_toks) / len(src_toks | out_toks)) if (src_toks or out_toks) else 1.0
    if jaccard < 0.6:
        print(f"[SUMMARIZER] RETAIN (low overlap {jaccard:.2f}) -> {txt!r}")
        print(f"[SUMMARIZER] OUT query={txt!r}")
        return txt

    print(f"[SUMMARIZER] OUT query={result!r} (overlap {jaccard:.2f})")
    return result

# ===== aimodel/file_read/web/router_ai.py =====

# aimodel/file_read/web/router_ai.py
from __future__ import annotations
from typing import Tuple, Optional, Any
import json
import re

# IMPORTANT: Double braces to escape JSON in .format()
_PROMPT = (
    "You ONLY output JSON. Task: Decide if the user's question needs LIVE WEB results "
    "(news, prices, schedules, laws, 'today/now/latest', changing facts). "
    'Return exactly: {{"need": true|false, "query": "<concise web query or empty>"}} '
    "No extra text.\n\nUser: {text}\nJSON:"
)

def _force_json(s: str) -> Optional[dict]:
    try:
        return json.loads(s)
    except Exception:
        m = re.search(r"\{.*\}", s, re.DOTALL)
        if not m:
            return None
        try:
            return json.loads(m.group(0))
        except Exception:
            return None

def decide_web(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    """
    General-purpose router with no hardcoded heuristics.
    - Lets the model decide if web is needed.
    - Supports explicit overrides via "web:" / "search:".
    - Returns (need_web, query_if_needed).
    """
    if not user_text or not user_text.strip():
        return (False, None)

    t = user_text.strip()
    low = t.lower()

    # Explicit override: allow caller to force web and optionally supply a query
    if low.startswith("web:") or low.startswith("search:"):
        q = t.split(":", 1)[1].strip() or t
        return (True, q)

    # Model-based decision (no regex/rule hardcodes)
    try:
        out = llm.create_chat_completion(
            messages=[{"role": "user", "content": _PROMPT.format(text=t)}],
            max_tokens=64,
            temperature=0.0,
            top_p=0.9,
            stream=False,
            stop=["</s>", "\n\n"],
        )
        text = out["choices"][0]["message"]["content"].strip()
    except Exception:
        # If the decision model fails, default to no web to avoid surprises
        return (False, None)

    data = _force_json(text) or {}
    need = bool(data.get("need", False))
    query = (str(data.get("query") or "")).strip() or (t if need else None)
    return (need, query if need else None)

# ===== frontend/src/file_read/App.tsx =====


import AgentRunner from "./pages/AgentRunner";

export default function App() {
  return (
    <main className="bg-gray-50 min-h-screen">
      <AgentRunner />
    </main>
  );
}

# ===== frontend/src/file_read/components/AssistantMetrics.tsx =====

// frontend/src/file_read/components/chat/AssistantMetrics.tsx
import { Info } from "lucide-react";
import MetricsHoverCard from "./MetricsHoverCard";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function AssistantMetrics({
  status,
  runJson,
  flat,
  align = "right",
}: { status: string; runJson?: RunJson | null; flat?: GenMetrics | null; align?: "left" | "right" }) {
  return (
    <div className="mt-2 flex justify-start">
      <div className="inline-flex items-center gap-1 px-2 py-1 rounded-full bg-white border shadow-sm text-[11px] text-gray-600">
        <Info className="w-3.5 h-3.5 opacity-70" />
        <span className="truncate max-w-[70vw] sm:max-w-none">{status || "Run details"}</span>
        <MetricsHoverCard
          data={
            runJson ??
            (flat
              ? {
                  stats: {
                    stopReason: flat.stop_reason ?? null,
                    tokensPerSecond: flat.tok_per_sec ?? null,
                    timeToFirstTokenSec: flat.ttft_ms != null ? Math.max(0, flat.ttft_ms) / 1000 : null,
                    totalTimeSec: null,
                    promptTokensCount: flat.input_tokens_est ?? null,
                    predictedTokensCount: flat.output_tokens ?? null,
                    totalTokensCount: flat.total_tokens_est ?? null,
                  },
                }
              : null)
          }
          title="Run JSON"
          align={align}
          compact
        />
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatBubble.tsx =====

import { useState } from "react";
import { Copy, Check, Trash2 } from "lucide-react";
import MarkdownMessage from "./Markdown/MarkdownMessage";

const STOP_SENTINEL_RE = /(?:\r?\n)?(?:\u23F9|\\u23F9)\s+stopped(?:\r?\n)?$/u;

export default function ChatBubble({
  role,
  text,
  showActions = true, // NEW: parent decides when to show the toolbar
  onDelete,
}: {
  role: "user" | "assistant";
  text: string;
  showActions?: boolean;
  onDelete?: () => void;
}) {
  const isUser = role === "user";
  const content = text?.trim() ?? "";
  if (role === "assistant" && !content) return null;

  // Never show the decorative stop line in assistant bubbles
  const display = isUser ? content : content.replace(STOP_SENTINEL_RE, "");

  const [copiedMsg, setCopiedMsg] = useState(false);

  const copyWholeMessage = async () => {
    try {
      await navigator.clipboard.writeText(display);
      setCopiedMsg(true);
      setTimeout(() => setCopiedMsg(false), 2000);
    } catch {}
  };

  return (
    <div className="mb-2">
      {/* Bubble */}
      <div className={`flex ${isUser ? "justify-end" : "justify-start"}`}>
        <div
          className={`max-w-[80%] w-fit break-words rounded-2xl px-4 py-2 shadow-sm
                      prose prose-base max-w-none
            ${isUser ? "bg-black text-white prose-invert" : "bg-white border text-gray-900"}`}
          style={{ wordBreak: "break-word" }}
        >
          <div className="max-w-full">
            <MarkdownMessage text={display} />
          </div>
        </div>
      </div>

      {/* Under-bubble toolbar (icon-only) */}
      {showActions && (
        <div className={`mt-1 flex ${isUser ? "justify-end" : "justify-start"}`}>
          <div className="flex items-center gap-2">
            <button
              type="button"
              onClick={copyWholeMessage}
              title={copiedMsg ? "Copied" : "Copy"}
              aria-label={copiedMsg ? "Copied" : "Copy message"}
              className="inline-flex items-center justify-center w-7 h-7 rounded border
                         bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
            >
              {copiedMsg ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
            </button>

            {/* Delete for BOTH roles when provided */}
            {onDelete && (
              <button
                type="button"
                onClick={onDelete}
                title="Delete message"
                aria-label="Delete message"
                className="inline-flex items-center justify-center w-7 h-7 rounded border
                           bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
              >
                <Trash2 className="w-4 h-4" />
              </button>
            )}
          </div>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatComposer.tsx =====

// frontend/src/file_read/components/ChatComposer.tsx
import { useEffect, useRef, useState } from "react";
import { SendHorizonal, Square } from "lucide-react";

const FORCE_SCROLL_EVT = "chat:force-scroll-bottom";

type Props = {
  input: string;
  setInput: (v: string) => void;
  loading: boolean;
  queued?: boolean;
  onSend: (text: string) => void | Promise<void>;
  onStop: () => void | Promise<void>;
  onHeightChange?: (h: number) => void;
  onRefreshChats?: () => void;
};

export default function ChatComposer({
  input,
  setInput,
  loading,
  queued = false,
  onSend,
  onStop,
  onHeightChange,
  onRefreshChats,
}: Props) {
  const wrapRef = useRef<HTMLDivElement>(null);
  const taRef = useRef<HTMLTextAreaElement>(null);
  const MAX_HEIGHT_PX = 192;
  const [isClamped, setIsClamped] = useState(false);
  const [draft, setDraft] = useState(input);

  useEffect(() => setDraft(input), [input]);

  const autogrow = () => {
    const ta = taRef.current;
    if (!ta) return;
    ta.style.height = "auto";
    const next = Math.min(ta.scrollHeight, MAX_HEIGHT_PX);
    ta.style.height = `${next}px`;
    setIsClamped(ta.scrollHeight > MAX_HEIGHT_PX);
    if (wrapRef.current && onHeightChange) {
      onHeightChange(wrapRef.current.getBoundingClientRect().height);
    }
  };

  useEffect(() => {
    autogrow();
    const onResize = () => autogrow();
    window.addEventListener("resize", onResize);
    return () => window.removeEventListener("resize", onResize);
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  useEffect(() => {
    autogrow();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [draft]);

  const hasText = draft.trim().length > 0;

  // 🔹 NEW: tell ChatView to scroll to bottom
  const forceScroll = (behavior: ScrollBehavior = "auto") => {
    window.dispatchEvent(
      new CustomEvent(FORCE_SCROLL_EVT, { detail: { behavior } })
    );
  };

  const handleSendClick = () => {
    const v = draft.trim();
    if (!v || loading || queued) return; // prevent sending while queued/streaming

    // Snap immediately to bottom so user sees the latest area
    forceScroll("auto");

    setDraft("");
    setInput("");
    void Promise.resolve(onSend(v)).finally(() => {
      onRefreshChats?.();
      // Settle at bottom after DOM updates
      requestAnimationFrame(() => forceScroll("smooth"));
    });
  };

  const handleStopClick = () => {
    if (!loading && !queued) return;
    void Promise.resolve(onStop()).finally(() => {
      onRefreshChats?.(); // refresh once the stop lands
    });
  };

  function onKeyDown(e: React.KeyboardEvent<HTMLTextAreaElement>) {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      if (!loading && !queued) handleSendClick();
    }
  }

  return (
    <div ref={wrapRef} className="relative z-50 bg-white/95 backdrop-blur border-t p-3">
      <div className="flex gap-2">
        <textarea
          ref={taRef}
          value={draft}
          onChange={(e) => {
            setDraft(e.target.value);
            setInput(e.target.value);
            autogrow();
          }}
          onInput={autogrow}
          onKeyDown={onKeyDown}
          placeholder="Ask anything…"
          className={`flex-1 border rounded-lg px-3 py-2 resize-none focus:outline-none focus:ring ${
            isClamped ? "overflow-y-auto" : "overflow-hidden"
          }`}
          rows={1}
          style={{ maxHeight: MAX_HEIGHT_PX }}
        />

        <div className="flex items-end gap-2">
          {(loading || queued) ? (
            <button
              className="p-2 rounded-lg border hover:bg-gray-50"
              onClick={handleStopClick}
              title={queued ? "Cancel queued message" : "Stop generating"}
              aria-label={queued ? "Cancel queued message" : "Stop generating"}
            >
              <Square size={18} />
            </button>
          ) : hasText ? (
            <button
              className="p-2 rounded-lg bg-black text-white hover:bg-black/90 active:translate-y-px"
              onClick={handleSendClick}
              title="Send"
              aria-label="Send"
            >
              <SendHorizonal size={18} />
            </button>
          ) : null}
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatContainer.tsx =====

import { useState, useRef, useEffect } from "react";
import ChatView from "./ChatView/ChatView";
import ChatComposer from "./ChatComposer";
import type { ChatMsg } from "../types/chat";
import type { GenMetrics, RunJson } from "../hooks/useChatStream";

interface Props {
  messages: ChatMsg[];
  input: string;
  setInput: (s: string) => void;
  loading: boolean;
  queued?: boolean;
  send: (text?: string) => Promise<void>;
  stop: () => Promise<void> | void;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onRefreshChats?: () => void;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
}

export default function ChatContainer({
  messages,
  input,
  setInput,
  loading,
  queued = false,
  send,
  stop,
  runMetrics,
  runJson,
  onRefreshChats,
  onDeleteMessages,
  autoFollow = true,
}: Props) {
  const [composerH, setComposerH] = useState(0);
  const containerRef = useRef<HTMLDivElement>(null);

  // Track if the user is scrolled up
  const [pinned, setPinned] = useState(false);

  useEffect(() => {
    const el = containerRef.current;
    if (!el) return;

    const threshold = 120;

    const isNearBottom = () => {
      return el.scrollHeight - el.scrollTop - el.clientHeight <= threshold;
    };

    const onScroll = () => {
      // pinned = true if not near bottom
      setPinned(!isNearBottom());
    };

    el.addEventListener("scroll", onScroll, { passive: true });

    // initialize
    setPinned(!isNearBottom());

    return () => el.removeEventListener("scroll", onScroll);
  }, []);

  const forceScrollToBottom = (behavior: ScrollBehavior = "smooth") => {
    const el = containerRef.current;
    if (!el) return;
    el.scrollTo({ top: el.scrollHeight, behavior });
  };

  const handleSend = async (text?: string) => {
    // Only auto-scroll if user hasn’t scrolled up
    if (!pinned) {
      forceScrollToBottom("auto");
    }
    await send(text);
    onRefreshChats?.();
    if (!pinned) {
      requestAnimationFrame(() => forceScrollToBottom("smooth"));
    }
  };

  return (
    <div className="flex flex-col h-full border rounded-lg overflow-hidden bg-white">
      <div
        ref={containerRef}
        data-chat-scroll
        className="flex-1 overflow-y-auto min-w-0"
      >
        <ChatView
          messages={messages}
          loading={loading}
          queued={queued}
          bottomPad={composerH}
          runMetrics={runMetrics}
          runJson={runJson}
          onDeleteMessages={onDeleteMessages}
          autoFollow={autoFollow}
        />
      </div>

      <ChatComposer
        input={input}
        setInput={setInput}
        loading={loading}
        queued={queued}
        onSend={handleSend}
        onStop={stop}
        onHeightChange={setComposerH}
        onRefreshChats={onRefreshChats}
      />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatItem.tsx =====

// frontend/src/file_read/components/chat/ChatItem.tsx
import ChatBubble from "./ChatBubble";
import AssistantMetrics from "./AssistantMetrics";
import { buildStatus } from "./ChatView/StatusLine";
import type { ChatMsg } from "../types/chat";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function ChatItem({
  m,
  idx,
  loading,
  lastAssistantIndex,
  runJsonLive,
  runMetricsLive,
  onDelete,
}: {
  m: ChatMsg;
  idx: number;
  loading: boolean;
  lastAssistantIndex: number;
  runJsonLive?: RunJson | null;
  runMetricsLive?: GenMetrics | null;
  onDelete?: (id: string) => void;
}) {
  const isAssistant = m.role === "assistant";
  const isCurrentStreamingAssistant = isAssistant && loading && idx === lastAssistantIndex;

  let jsonForThis: RunJson | null = null;
  let flatForThis: GenMetrics | null = null;

  if (isAssistant) {
    // @ts-ignore meta bag
    const meta = m.meta as { runJson?: RunJson | null; flat?: GenMetrics | null } | undefined;
    jsonForThis = meta?.runJson ?? null;
    flatForThis = meta?.flat ?? null;

    if (isCurrentStreamingAssistant) {
      if (runJsonLive) jsonForThis = runJsonLive;
      if (runMetricsLive) flatForThis = runMetricsLive;
    }
  }

  const status = isAssistant ? buildStatus(jsonForThis, flatForThis) : "";
  const showMetrics = isAssistant && (jsonForThis || flatForThis);

  return (
    <div>
      <ChatBubble
        role={m.role}
        text={m.text}
        showActions={m.role === "user" || (m.role === "assistant" && !isCurrentStreamingAssistant)}
        onDelete={onDelete ? () => onDelete(m.id) : undefined}
      />
      {showMetrics && <AssistantMetrics status={status} runJson={jsonForThis} flat={flatForThis} />}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/ChatSidebar.tsx =====

import { useState } from "react";
import { deleteChatsBatch } from "../../hooks/data/chatApi";
import type { ChatRow } from "../../types/chat";
import { useMultiSelect } from "../../hooks/useMultiSelect";
import { useChatsPager } from "../../hooks/useChatsPager";
import SidebarHeader from "./SidebarHeader";
import SidebarListItem from "./SidebarListItem";

const PAGE_SIZE = 10;

type Props = {
  onOpen: (id: string) => Promise<void>;
  onNew: () => Promise<void>;
  refreshKey?: number;
  activeId?: string;
  onHideSidebar?: () => void;
  onCancelSessions?: (ids: string[]) => Promise<void>;
};

export default function ChatSidebar({
  onOpen, onNew, refreshKey, activeId, onHideSidebar, onCancelSessions,
}: Props) {
  const {
    chats, page, hasMore, total, totalPages,
    initialLoading, loadingMore,
    scrollRef, sentinelRef, loadMore, setChats,
  } = useChatsPager(PAGE_SIZE, refreshKey);

  const [isEditing, setIsEditing] = useState(false);
  const [deleting, setDeleting] = useState(false);
  const [newPending, setNewPending] = useState(false);

  const allIds = chats.map(c => c.sessionId);
  const { selected, setSelected, allSelected, toggleOne, toggleAll } = useMultiSelect(allIds);

  async function handleNew() {
    if (newPending) return;
    setNewPending(true);
    try { await onNew(); } finally { setNewPending(false); }
  }

  async function onDeleteSelected(): Promise<void> {
    const count = selected.size;
    if (!count || deleting) return;

    const isAll = count === chats.length;
    const ok = window.confirm(
      isAll
        ? `Delete ALL ${count} chats? This cannot be undone.`
        : `Delete ${count} selected chat${count > 1 ? "s" : ""}?`
    );
    if (!ok) return;

    const ids = [...selected];
    try { await onCancelSessions?.(ids); await Promise.resolve(); } catch {}

    const deletingActive = activeId ? selected.has(activeId) : false;
    const fallback = chats.find(c => !selected.has(c.sessionId))?.sessionId;

    setDeleting(true);
    try {
      const deleted = await deleteChatsBatch(ids);
      if (!deleted.length) return;
      setChats(prev => prev.filter(c => !deleted.includes(c.sessionId)));
      setSelected(new Set());
      setIsEditing(false);

      if (deletingActive) {
        if (fallback) void onOpen(fallback);
        else void onOpen("");
      }
    } finally {
      setDeleting(false);
    }
  }

  return (
    <aside className="w-full md:w-72 h-full border-r bg-white p-0 flex flex-col">
      <SidebarHeader
        isEditing={isEditing}
        setIsEditing={(v) => { setIsEditing(v); setSelected(new Set()); }}
        newPending={newPending}
        onNew={handleNew}
        onHideSidebar={onHideSidebar}
        selectedCount={selected.size}
        deleting={deleting}
        onDelete={onDeleteSelected}
      />

      {/* LIST */}
      <div
        ref={scrollRef}
        className="flex-1 overflow-y-auto p-2 overscroll-contain"
        style={{ WebkitOverflowScrolling: "touch" }}
      >
        {initialLoading && (
          <div className="px-2 py-1 text-xs 