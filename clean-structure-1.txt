
# ===== aimodel/file_read/__init__.py =====

from .adaptive.config.paths import app_data_dir, read_settings, write_settings
from .runtime.model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/adaptive/config/adaptive_config.py =====

# aimodel/file_read/runtime/adaptive_config.py
from __future__ import annotations
import os, shutil, subprocess, platform
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

try:
    import psutil
except Exception:
    psutil = None
try:
    import torch
except Exception:
    torch = None

from .paths import read_settings

def _env_bool(k:str, default:bool)->bool:
    v = os.getenv(k)
    if v is None: return default
    return v.strip().lower() in ("1","true","yes","on")

def _cpu_count()->int:
    try:
        import multiprocessing as mp
        return max(1, mp.cpu_count() or os.cpu_count() or 1)
    except Exception:
        return os.cpu_count() or 1

def _avail_ram()->Optional[int]:
    if not psutil: return None
    try: return int(psutil.virtual_memory().available)
    except Exception: return None

def _cuda_vram()->Optional[int]:
    if torch and torch.cuda.is_available():
        try:
            dev = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(dev)
            return int(props.total_memory)
        except Exception:
            pass
    if shutil.which("nvidia-smi"):
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-gpu=memory.total","--format=csv,noheader,nounits"],
                text=True, stderr=subprocess.DEVNULL, timeout=2.0
            )
            mb = max(int(x.strip()) for x in out.strip().splitlines() if x.strip())
            return mb * 1024 * 1024
        except Exception:
            return None
    return None

def _gpu_kind()->str:
    if _cuda_vram(): return "cuda"
    if torch and getattr(torch.backends,"mps",None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

def _safe_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default

def _pick_dtype_quant(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> tuple[Optional[str], Optional[str]]:
    dq = a.get("dtype_quant", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = dq.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype"), best.get("quant")
        return dq.get("cuda_default_dtype"), dq.get("cuda_default_quant")
    if device == "mps":
        return dq.get("mps_default_dtype"), None
    return dq.get("cpu_default_dtype"), dq.get("cpu_default_quant")

def _pick_kv(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> Optional[str]:
    kv = a.get("kv_cache", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = kv.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype")
        return kv.get("cuda_default")
    if device == "mps":
        return kv.get("mps_default")
    return kv.get("cpu_default")

def _pick_capacity(device: str, a: Dict[str, Any], vram_bytes: Optional[int], threads:int) -> tuple[int,int,Optional[int]]:
    cap = a.get("capacity", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = cap.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return int(best.get("seq_len") or 0), int(best.get("batch") or 1), int(best.get("n_gpu_layers") or 0)
        return 0, 1, 0
    if device == "mps":
        m = cap.get("mps", {})
        return int(m.get("seq_len") or 0), int(m.get("batch") or 1), 0
    cpu = cap.get("cpu", {})
    seq_len = int(cpu.get("seq_len") or 0)
    batch = 1
    by = cpu.get("batch_by_threads") or []
    best = None
    for t in sorted(by, key=lambda x: int(x.get("min_threads", 0)), reverse=True):
        if threads >= int(t.get("min_threads") or 0):
            best = t
            break
    if best:
        batch = int(best.get("batch") or 1)
    return seq_len, batch, 0

def _gpu_mem_fraction(device:str, a: Dict[str, Any]) -> float:
    table = a.get("gpu_fraction", {}) if isinstance(a, dict) else {}
    v = table.get(device)
    return _safe_float(v, 0.0)

def _torch_flags(device:str, a: Dict[str, Any]) -> tuple[bool,bool]:
    flags = a.get("flags", {}) if isinstance(a, dict) else {}
    flash = bool(flags.get("enable_flash_attn_cuda")) if device == "cuda" else False
    tc = bool(flags.get("use_torch_compile_on_cuda_linux")) if (device == "cuda" and platform.system().lower()=="linux") else False
    return flash, tc

def _threads(a: Dict[str, Any]) -> tuple[int,int,int,int]:
    policy = a.get("cpu_threads_policy", {}) if isinstance(a, dict) else {}
    mode = str(policy.get("mode") or "").lower()
    ncpu = _cpu_count()
    if mode == "fixed":
        v = int(policy.get("value") or max(1, ncpu-1))
        t = max(1, min(v, ncpu))
    elif mode == "percent":
        pct = _safe_float(policy.get("value"), 0.0)
        t = max(1, min(ncpu, int(round(ncpu*pct/100.0))))
        if t < 1: t = 1
    else:
        t = max(1, ncpu-1)
    intra = t
    inter = max(1, ncpu//2)
    return ncpu, t, intra, inter

@dataclass
class AdaptiveConfig:
    device: str
    dtype: Optional[str]
    quant: Optional[str]
    kv_cache_dtype: Optional[str]
    max_seq_len: int
    max_batch_size: int
    gpu_memory_fraction: float
    cpu_threads: int
    torch_intraop_threads: int
    torch_interop_threads: int
    enable_flash_attn: bool
    use_torch_compile: bool
    total_vram_bytes: Optional[int]
    avail_ram_bytes: Optional[int]
    cpu_count: int
    def as_dict(self)->Dict[str,Any]:
        return asdict(self)

def compute_adaptive_config()->AdaptiveConfig:
    settings = read_settings()
    a = settings.get("adaptive", {}) if isinstance(settings, dict) else {}
    device = _gpu_kind()
    vram = _cuda_vram() if device=="cuda" else None
    ram = _avail_ram()
    ncpu, threads, intra, inter = _threads(a)
    dtype, quant = _pick_dtype_quant(device, a, vram)
    kv = _pick_kv(device, a, vram)
    seq_len, batch, n_gpu_layers = _pick_capacity(device, a, vram, threads)
    frac = _gpu_mem_fraction(device, a)
    flash, tcompile = _torch_flags(device, a)
    return AdaptiveConfig(
        device=device,
        dtype=dtype,
        quant=quant,
        kv_cache_dtype=kv,
        max_seq_len=int(seq_len or 0),
        max_batch_size=int(batch or 1),
        gpu_memory_fraction=frac,
        cpu_threads=threads,
        torch_intraop_threads=intra,
        torch_interop_threads=inter,
        enable_flash_attn=flash,
        use_torch_compile=tcompile,
        total_vram_bytes=vram,
        avail_ram_bytes=ram,
        cpu_count=ncpu
    )

# ===== aimodel/file_read/adaptive/config/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/adaptive/config/settings.json =====

{
  "adaptiveEnabled": false,
  "adaptive": {
    "enabled": true,
    "cpu_threads_policy": {
      "mode": "leave_one"
    },
    "dtype_quant": {
      "cuda_default_dtype": "float16",
      "cuda_default_quant": null,
      "mps_default_dtype": "float16",
      "cpu_default_dtype": "int8",
      "cpu_default_quant": "q4_K_M",
      "cuda_tiers": [
        { "min_vram_gb": 24, "dtype": "bfloat16", "quant": null },
        { "min_vram_gb": 12, "dtype": "float16", "quant": null },
        { "min_vram_gb": 6,  "dtype": "float16", "quant": "bnb-int8" },
        { "min_vram_gb": 4,  "dtype": "float16", "quant": "bnb-int8" }
      ]
    },
    "kv_cache": {
      "cuda_default": "fp8",
      "mps_default": "fp16",
      "cpu_default": "fp32",
      "cuda_tiers": [
        { "min_vram_gb": 16, "dtype": "fp16" },
        { "min_vram_gb": 0,  "dtype": "fp8" }
      ]
    },
    "capacity": {
      "cuda_tiers": [
        { "min_vram_gb": 24, "seq_len": 8192, "batch": 8, "n_gpu_layers": 9999 },
        { "min_vram_gb": 12, "seq_len": 4096, "batch": 4, "n_gpu_layers": 48 },
        { "min_vram_gb": 8,  "seq_len": 3072, "batch": 2, "n_gpu_layers": 40 },
        { "min_vram_gb": 6,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 32 },
        { "min_vram_gb": 4,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 28 }
      ],
      "mps": { "seq_len": 2048, "batch": 1 },
      "cpu": {
        "seq_len": 2048,
        "batch_by_threads": [
          { "min_threads": 16, "batch": 8 },
          { "min_threads": 8,  "batch": 4 },
          { "min_threads": 1,  "batch": 2 }
        ]
      }
    },
    "gpu_fraction": {
      "cuda": 0.8,
      "mps": 0.7,
      "cpu": 0.0
    },
    "flags": {
      "enable_flash_attn_cuda": true,
      "use_torch_compile_on_cuda_linux": true
    },
    "batchTokenMap": [
      { "minConcurrency": 8, "n_batch": 512 },
      { "minConcurrency": 4, "n_batch": 384 },
      { "minConcurrency": 2, "n_batch": 256 },
      { "minConcurrency": 1, "n_batch": 192 }
    ]
  },
  "modelPath": "",
  "nCtx": null,
  "nThreads": null,
  "nGpuLayers": null,
  "nBatch": null
}

# ===== aimodel/file_read/adaptive/controller.py =====



# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/billing.py =====

# ===== aimodel/file_read/api/billing.py =====
from __future__ import annotations
import os
from fastapi import APIRouter, HTTPException, Depends
import httpx

from .local_auth import _decode_bearer as decode_bearer

# Frontend calls /api/billing/... on this LAN backend.
# This backend proxies to your Cloudflare Worker (which holds Stripe + license secrets).
router = APIRouter(prefix="/api", tags=["billing"])

LIC_SERVER = (os.getenv("LIC_SERVER_BASE") or "").rstrip("/")
if not LIC_SERVER:
    # Fail fast in dev if you forgot to set it
    raise RuntimeError("LIC_SERVER_BASE env var is required (e.g. https://lic-server.localmind.workers.dev)")

# --- Billing status for the UI
# We treat the user as "active" if a valid license is applied locally.
# (Your Worker already mints the LM1 license; your app applies it via /api/license/apply.)
try:
    # Import your local license verifier to reuse its code
    from .licensing import status as license_status  # returns {"plan":..., "valid": bool, "exp": ...}
except Exception:
    license_status = None

@router.get("/billing/status")
async def billing_status(auth=Depends(decode_bearer)):
    email = (auth.get("sub") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    # Prefer local license truth (offline-capable)
    if license_status:
        try:
            lic = license_status()
            active = bool(lic.get("valid"))
            return {
                "status": "active" if active else "inactive",
                "current_period_end": int(lic.get("exp") or 0),
            }
        except Exception:
            pass

    # Fallback: inactive if we can't read local license
    return {"status": "inactive", "current_period_end": 0}

# --- Start Stripe Checkout via Worker
@router.post("/billing/checkout")
async def start_checkout(auth=Depends(decode_bearer)):
    email = (auth.get("sub") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    url = f"{LIC_SERVER}/api/checkout/session"
    async with httpx.AsyncClient(timeout=20.0) as client:
        r = await client.post(url, json={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    if not isinstance(data, dict) or "url" not in data:
        raise HTTPException(502, "Bad response from licensing server")
    return {"url": data["url"]}

# --- Open Stripe Billing Portal via Worker
@router.post("/billing/portal")
async def open_portal(auth=Depends(decode_bearer)):
    email = (auth.get("sub") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    # Your Worker route must look up customerId by email (see earlier Worker snippet)
    url = f"{LIC_SERVER}/api/portal/session"
    async with httpx.AsyncClient(timeout=20.0) as client:
        r = await client.post(url, json={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    if not isinstance(data, dict) or "url" not in data:
        raise HTTPException(502, "Bad response from licensing server")
    return {"url": data["url"]}

# --- Optional: let the app auto-apply license by session id (proxy to Worker)
@router.get("/license/by-session")
async def license_by_session(session_id: str):
    if not session_id:
        raise HTTPException(400, "Missing session_id")
    url = f"{LIC_SERVER}/api/license/by-session"
    async with httpx.AsyncClient(timeout=15.0) as client:
        r = await client.get(url, params={"session_id": session_id}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    return r.json()

# ===== aimodel/file_read/api/chats.py =====

# ===== aimodel/file_read/api/chats.py =====
from __future__ import annotations

from ..core.schemas import ChatMessage
from dataclasses import asdict
from typing import List, Optional, Dict
from ..utils.streaming import strip_runjson
from fastapi import APIRouter
from pydantic import BaseModel
from ..services.cancel import GEN_SEMAPHORE
from ..workers.retitle_worker import enqueue as enqueue_retitle  # ✅ import the enqueuer

from ..core.schemas import (
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    EditMessageReq,
)

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    edit_message as edit_message,
)

router = APIRouter()

@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)

@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]


@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, msg: ChatMessage):
    role = msg.role
    content = (msg.content or "").rstrip()
    attachments = msg.attachments or []

    row = store_append(session_id, role, content, attachments=attachments)

    if role == "assistant":
        try:
            msgs = store_list_messages(session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            msgs_clean = []
            for m in msgs:
                dm = asdict(m)
                dm["content"] = strip_runjson(dm.get("content") or "")
                msgs_clean.append(dm)
            enqueue_retitle(session_id, msgs_clean, job_seq=last_seq)
        except Exception:
            pass

    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}


@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

from fastapi import APIRouter, Body, Request
from ..core.schemas import ChatBody
from ..services.generate_flow import generate_stream_flow, cancel_session_alias

router = APIRouter()

@router.post("/api/ai/generate/stream")
async def generate_stream_alias(request: Request, data: ChatBody = Body(...)):
    return await generate_stream_flow(data, request)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/licensing.py =====

from fastapi import APIRouter, HTTPException, Depends, Query
from pydantic import BaseModel
import base64, json, time, os, stat, sys
from pathlib import Path
from nacl.signing import VerifyKey
from nacl.exceptions import BadSignatureError
import httpx
from .local_auth import require_auth

router = APIRouter(prefix="/api/license", tags=["license"])

def _app_data_dir() -> Path:
    override = os.getenv("LOCALMIND_DATA_DIR", "").strip()
    if override:
        return Path(override)
    if sys.platform.startswith("win"):
        base = os.getenv("APPDATA") or str(Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"
    elif sys.platform == "darwin":
        return Path.home() / "Library" / "Application Support" / "LocalAI"
    else:
        return Path.home() / ".config" / "LocalAI"

APP_DIR = _app_data_dir() / "license"
APP_DIR.mkdir(parents=True, exist_ok=True)
LIC_PATH = APP_DIR / "license.json"
THROTTLE_PATH = APP_DIR / "license.throttle.json"
COOLDOWN_SEC = 12 * 3600
EXP_SOON_SEC = 30 * 24 * 3600

_old = Path(os.path.expanduser("~/.localmind/license.json"))
if _old.exists() and not LIC_PATH.exists():
    try:
        print(f"[license] migrate old -> {LIC_PATH}")
        LIC_PATH.write_text(_old.read_text(encoding="utf-8"), encoding="utf-8")
        try:
            _old.unlink(missing_ok=True)  # type: ignore[arg-type]
        except Exception as e:
            print(f"[license] migrate unlink warn {e!r}")
    except Exception as e:
        print(f"[license] migrate error {e!r}")

LIC_PUB_HEX = os.getenv("LIC_ED25519_PUB_HEX", "").strip()
print(f"[license] using file {LIC_PATH}")
print(f"[license] pubkey set={bool(LIC_PUB_HEX)}")

def _b64u_decode(s: str) -> bytes:
    pad = "=" * (-len(s) % 4)
    return base64.urlsafe_b64decode(s + pad)

def _verify(lic: str) -> dict:
    print("[license] _verify: start")
    if not lic or not lic.startswith("LM1."):
        print("[license] _verify: bad_format")
        raise ValueError("Bad format")
    try:
        _, payload_b64, sig_b64 = lic.split(".", 2)
    except ValueError:
        print("[license] _verify: malformed_token")
        raise ValueError("Malformed")
    payload = _b64u_decode(payload_b64)
    sig = _b64u_decode(sig_b64)
    if not LIC_PUB_HEX:
        print("[license] _verify: missing_public_key")
        raise ValueError("Verifier not configured")
    vk = VerifyKey(bytes.fromhex(LIC_PUB_HEX))
    try:
        vk.verify(payload, sig)
    except BadSignatureError:
        print("[license] _verify: bad_signature")
        raise ValueError("Invalid signature")
    data = json.loads(payload.decode("utf-8"))
    if "plan" not in data:
        data["plan"] = "pro"
    now = int(time.time())
    exp = int(data.get("exp") or 0)
    if exp and now > exp:
        print(f"[license] _verify: expired exp={exp} now={now}")
        raise ValueError("Expired")
    print("[license] _verify: ok")
    return data

def _save_secure(path: Path, obj: dict):
    print(f"[license] _save_secure: path={path}")
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = str(path) + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f)
    os.replace(tmp, path)
    try:
        os.chmod(path, stat.S_IRUSR | stat.S_IWUSR)
    except Exception as e:
        print(f"[license] _save_secure: chmod_warn {e!r}")

def _load_current() -> dict | None:
    exists = LIC_PATH.exists()
    print(f"[license] _load_current: file={LIC_PATH} exists={exists}")
    if not exists:
        return None
    with open(LIC_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def _lic_base() -> str:
    base = (os.getenv("LIC_SERVER_BASE") or "").strip()
    print(f"[license] _lic_base: {base or 'MISSING'}")
    if not base:
        raise HTTPException(500, "LIC_SERVER_BASE not configured")
    return base.rstrip("/")

def _email_from_auth(auth_payload: dict | None) -> str:
    email = (auth_payload or {}).get("sub") or ""
    return email.strip().lower()

def _throttle_ok(kind: str) -> bool:
    now = int(time.time())
    rec = {}
    try:
        with open(THROTTLE_PATH, "r", encoding="utf-8") as f:
            rec = json.load(f)
    except Exception:
        rec = {}
    last = int(rec.get(kind) or 0)
    if now - last < COOLDOWN_SEC:
        print(f"[license] throttle: skip kind={kind} last={last} now={now}")
        return False
    rec[kind] = now
    THROTTLE_PATH.parent.mkdir(parents=True, exist_ok=True)
    tmp = str(THROTTLE_PATH) + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(rec, f)
    os.replace(tmp, THROTTLE_PATH)
    print(f"[license] throttle: ok kind={kind} now={now}")
    return True

class ApplyReq(BaseModel):
    license: str

@router.post("/apply")
def apply_license(body: ApplyReq):
    print("[license] POST /apply")
    claims = _verify(body.license.strip())
    _save_secure(LIC_PATH, {"license": body.license.strip(), "claims": claims})
    print("[license] POST /apply ok")
    return {"ok": True, "plan": claims.get("plan","pro"), "exp": claims.get("exp")}

@router.get("/apply")
def apply_license_get(license: str = Query(..., min_length=10)):
    print("[license] GET /apply")
    claims = _verify(license.strip())
    _save_secure(LIC_PATH, {"license": license.strip(), "claims": claims})
    print("[license] GET /apply ok")
    return {"ok": True, "plan": claims.get("plan","pro"), "exp": claims.get("exp")}

@router.get("/status", dependencies=[Depends(require_auth)])
def status():
    print("[license] GET /status")
    try:
        rec = _load_current()
        if not rec:
            print("[license] status: no_file → free/invalid")
            return {"plan": "free", "valid": False}
        claims = _verify(rec["license"])
        exp = int(claims.get("exp") or 0)
        now = int(time.time())
        out = {
            "plan": claims.get("plan","pro"),
            "valid": True,
            "exp": exp,
            "exp_remaining_sec": max(0, exp - now) if exp else None,
            "claims": {k: claims.get(k) for k in ("license_id","sub","entitlements","issued_at")}
        }
        print(f"[license] status: ok plan={out['plan']} exp={out['exp']}")
        return out
    except Exception as e:
        print(f"[license] status: verify_fail {e!r}")
        return {"plan": "free", "valid": False}

@router.get("/claims", dependencies=[Depends(require_auth)])
def claims():
    print("[license] GET /claims")
    rec = _load_current()
    if not rec:
        print("[license] claims: not_found")
        raise HTTPException(404, "No license installed")
    data = _verify(rec["license"])
    print("[license] claims: ok")
    return data

@router.delete("/", dependencies=[Depends(require_auth)])
def remove_license():
    print("[license] DELETE /api/license")
    try:
        if LIC_PATH.exists():
            LIC_PATH.unlink()
            print("[license] delete: removed")
        else:
            print("[license] delete: not_exists")
        return {"ok": True}
    except Exception as e:
        print(f"[license] delete: error {e!r}")
        raise HTTPException(500, f"Could not remove license: {e}")

@router.get("/by-session", dependencies=[Depends(require_auth)])
async def license_by_session(session_id: str = Query(..., min_length=6)):
    print(f"[license] GET /by-session session_id={session_id}")
    base = _lic_base()
    url = f"{base}/api/license/by-session"
    try:
        async with httpx.AsyncClient(timeout=15) as client:
            r = await client.get(url, params={"session_id": session_id}, headers={"Accept": "application/json"})
        print(f"[license] by-session: status={r.status_code}")
        if r.status_code >= 400:
            txt = r.text[:300]
            print(f"[license] by-session: error_body={txt!r}")
            raise HTTPException(r.status_code, r.text)
        data = r.json()
        print("[license] by-session: ok (no install)")
        return data
    except Exception as e:
        print(f"[license] by-session: exception {e!r}")
        raise

@router.post("/install-from-session", dependencies=[Depends(require_auth)])
async def install_from_session(session_id: str = Query(..., min_length=6)):
    print(f"[license] POST /install-from-session session_id={session_id}")
    base = _lic_base()
    url = f"{base}/api/license/by-session"
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(url, params={"session_id": session_id}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        print(f"[license] install-from-session: error {r.status_code} {r.text[:200]!r}")
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    lic = (data or {}).get("license") or ""
    if not lic:
        print("[license] install-from-session: no license yet")
        raise HTTPException(404, "License not available yet")
    claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": claims})
    print("[license] install-from-session: saved")
    return {"ok": True, "plan": claims.get("plan","pro"), "exp": claims.get("exp")}

@router.post("/recover")
async def recover(auth=Depends(require_auth)):
    email = _email_from_auth(auth)
    print(f"[license] POST /recover email={email or 'MISSING'}")
    if not email:
        raise HTTPException(400, "Email required")
    if not _throttle_ok("recover"):
        return {"ok": True, "status": "skipped_cooldown"}
    base = _lic_base()
    url = f"{base}/api/license/by-customer"
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(url, params={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        print(f"[license] recover: error {r.status_code} {r.text[:200]!r}")
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    lic = (data or {}).get("license") or ""
    if not lic:
        print("[license] recover: no license")
        return {"ok": True, "status": "not_found"}
    claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": claims})
    print("[license] recover: saved")
    return {"ok": True, "status": "installed", "plan": claims.get("plan","pro"), "exp": claims.get("exp")}

@router.post("/refresh")
async def refresh(auth=Depends(require_auth), force: bool = Query(False)):
    print(f"[license] POST /refresh force={force}")
    rec = _load_current()
    if not rec:
        print("[license] refresh: no file → recover")
        return await recover(auth=auth)
    try:
        claims = _verify(rec["license"])
        now = int(time.time())
        exp = int(claims.get("exp") or 0)
        if not force and exp and exp - now > EXP_SOON_SEC:
            print("[license] refresh: fresh_enough")
            return {"ok": True, "status": "fresh_enough"}
    except Exception as e:
        print(f"[license] refresh: local verify failed {e!r} → recover")
        return await recover(auth=auth)
    if not force and not _throttle_ok("refresh"):
        return {"ok": True, "status": "skipped_cooldown"}
    email = _email_from_auth(auth)
    base = _lic_base()
    url = f"{base}/api/license/by-customer"
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(url, params={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        print(f"[license] refresh: error {r.status_code} {r.text[:200]!r}")
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    lic = (data or {}).get("license") or ""
    if not lic:
        print("[license] refresh: no license on server")
        return {"ok": True, "status": "not_found"}
    new_claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": new_claims})
    print("[license] refresh: saved")
    return {"ok": True, "status": "updated", "plan": new_claims.get("plan","pro"), "exp": new_claims.get("exp")}

# ===== aimodel/file_read/api/local_auth.py =====

# ===== aimodel/file_read/api/local_auth.py =====
from __future__ import annotations

import os
import time
import sqlite3
from pathlib import Path
from typing import Optional, Dict, Any

from fastapi import APIRouter, HTTPException, Header, Depends
from pydantic import BaseModel, EmailStr, field_validator
from passlib.hash import bcrypt
import jwt

# Optional: context hooks used elsewhere in your app (best-effort)
from ..core import request_ctx  # type: ignore[attr-defined]

router = APIRouter()

# --- Storage ---------------------------------------------------------------
DB_PATH = Path(".runtime/auth.sqlite")
DB_PATH.parent.mkdir(exist_ok=True, parents=True)

def _db() -> sqlite3.Connection:
    con = sqlite3.connect(DB_PATH)
    con.execute(
        """
        CREATE TABLE IF NOT EXISTS users(
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            email TEXT UNIQUE NOT NULL,
            pw TEXT NOT NULL,
            created_at INTEGER NOT NULL
        )
        """
    )
    con.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_users_email ON users(email)")
    return con

# --- Settings --------------------------------------------------------------
SECRET = os.getenv("APP_JWT_SECRET", "dev-secret-change-me")
ISSUER = os.getenv("APP_JWT_ISSUER", "ai-agent")
# 24h default
TOKEN_TTL = int(os.getenv("APP_JWT_TTL", "86400"))
AUTH_REQUIRED = os.getenv("APP_AUTH_REQUIRED", "false").lower() == "true"

if not SECRET or SECRET == "dev-secret-change-me":
    # In dev we allow this, but warn loudly in logs.
    try:
        print("[local_auth] WARNING: Using default APP_JWT_SECRET. Set a strong secret in production.")
    except Exception:
        pass

# --- Models ----------------------------------------------------------------
class RegisterReq(BaseModel):
    email: EmailStr
    password: str

    @field_validator("password")
    @classmethod
    def _check_pw_len(cls, v: str) -> str:
        if len(v) < 6:
            raise ValueError("Password must be at least 6 characters")
        return v

class LoginReq(BaseModel):
    email: EmailStr
    password: str

# --- JWT helpers -----------------------------------------------------------
def _make_token(email: str) -> str:
    now = int(time.time())
    payload: Dict[str, Any] = {
        "sub": email,
        "iss": ISSUER,
        "iat": now,
        "exp": now + TOKEN_TTL,
    }
    return jwt.encode(payload, SECRET, algorithm="HS256")

def _decode_bearer(authorization: Optional[str] = Header(None)) -> Dict[str, Any]:
    """
    FastAPI dependency. Decodes and validates a Bearer JWT from the Authorization header.
    Usage in routes:  auth = Depends(_decode_bearer)
    """
    if not authorization or not authorization.lower().startswith("bearer "):
        raise HTTPException(401, "Missing token")
    token = authorization.split(None, 1)[1]
    try:
        payload = jwt.decode(
            token,
            SECRET,
            algorithms=["HS256"],
            options={"require": ["exp", "iat"]},
        )
        # Optional issuer check (enable if you want to pin)
        # if payload.get("iss") != ISSUER:
        #     raise HTTPException(401, "Invalid token issuer")
        return payload
    except jwt.ExpiredSignatureError:
        raise HTTPException(401, "Token expired")
    except jwt.PyJWTError:
        raise HTTPException(401, "Invalid token")

# Helpful small dependencies for routes that just need the email string
def get_current_email(authorization: Optional[str] = Header(None)) -> str:
    payload = _decode_bearer(authorization)
    email = (payload.get("sub") or "").strip().lower()
    if not email:
        raise HTTPException(401, "Invalid token payload")
    return email

def optional_email(authorization: Optional[str] = Header(None)) -> Optional[str]:
    try:
        payload = _decode_bearer(authorization)
        return (payload.get("sub") or "").strip().lower() or None
    except HTTPException:
        return None

# --- Routes: auth ----------------------------------------------------------
@router.post("/auth/register")
def register(req: RegisterReq):
    """
    Create a local user (email + password). Returns { ok: true } or 400 if exists.
    """
    email = req.email.lower().strip()
    pw_hash = bcrypt.hash(req.password)
    con = _db()
    try:
        con.execute(
            "INSERT INTO users(email, pw, created_at) VALUES (?,?,?)",
            (email, pw_hash, int(time.time())),
        )
        con.commit()
    except sqlite3.IntegrityError:
        raise HTTPException(400, "Email already registered")
    return {"ok": True}

@router.post("/auth/login")
def login(req: LoginReq):
    """
    Verify credentials and return a JWT bearer token.
    """
    email = req.email.lower().strip()
    con = _db()
    cur = con.execute("SELECT pw FROM users WHERE email=?", (email,))
    row = cur.fetchone()
    if not row or not bcrypt.verify(req.password, row[0]):
        raise HTTPException(401, "Invalid email or password")
    token = _make_token(email)
    return {"access_token": token, "token_type": "bearer"}

@router.get("/auth/me")
def me(authorization: Optional[str] = Header(None)):
    """
    Return the current user's profile (email, createdAt).
    """
    payload = _decode_bearer(authorization)
    email = (payload.get("sub") or "").strip().lower()
    if not email:
        raise HTTPException(401, "Invalid token payload")

    con = _db()
    cur = con.execute("SELECT id, created_at FROM users WHERE email=?", (email,))
    row = cur.fetchone()
    return {
        "email": email,
        "createdAt": row[1] if row else None,
    }

# --- Framework-friendly dependency for other modules -----------------------
def require_auth(
    authorization: Optional[str] = Header(None),
    x_id: Optional[str] = Header(None),
    x_license: Optional[str] = Header(None),
    x_brave_key: Optional[str] = Header(None),  # legacy / unused here; kept for compat
):
    """
    Dependency to attach request context and optionally require auth.

    - Always records headers best-effort into request_ctx (if present).
    - If APP_AUTH_REQUIRED=false (default in dev), it won't enforce a token,
      but will still try to decode and stash the user email for convenience.
    - If APP_AUTH_REQUIRED=true (prod), it will enforce a valid Bearer token.
    """
    # Best-effort: stash headers / tokens in request-scoped context for other layers
    try:
        if hasattr(request_ctx, "set_x_id"):
            request_ctx.set_x_id(x_id or "")
        if authorization and authorization.lower().startswith("bearer "):
            token = authorization.split(None, 1)[1]
            if hasattr(request_ctx, "set_id_token"):
                request_ctx.set_id_token(token)
        else:
            if hasattr(request_ctx, "set_id_token"):
                request_ctx.set_id_token("")
        # legacy extras
        if hasattr(request_ctx, "set_brave_key"):
            request_ctx.set_brave_key(x_brave_key or "")
        if hasattr(request_ctx, "set_license_key"):
            request_ctx.set_license_key(x_license or "")
    except Exception:
        # Context attachment should never break auth
        pass

    if not AUTH_REQUIRED:
        # Dev mode: try to decode for convenience, but don't enforce
        try:
            if authorization and authorization.lower().startswith("bearer "):
                payload = _decode_bearer(authorization)
                email = (payload.get("sub") or "").strip().lower()
                if hasattr(request_ctx, "set_user_email"):
                    request_ctx.set_user_email(email)
            else:
                if hasattr(request_ctx, "set_user_email"):
                    request_ctx.set_user_email("")
        except Exception:
            if hasattr(request_ctx, "set_user_email"):
                request_ctx.set_user_email("")
        return None

    # Prod mode: enforce a valid token
    if not authorization or not authorization.lower().startswith("bearer "):
        raise HTTPException(401, "Missing token")

    try:
        payload = _decode_bearer(authorization)
        email = (payload.get("sub") or "").strip().lower()
        if hasattr(request_ctx, "set_user_email"):
            request_ctx.set_user_email(email)
        return payload
    except HTTPException:
        raise
    except Exception:
        raise HTTPException(401, "Invalid token")

# ===== aimodel/file_read/api/metrics.py =====

# aimodel/file_read/api/metrics.py
from __future__ import annotations
from fastapi import APIRouter, HTTPException, Query
from typing import Optional
from ..runtime.model_runtime import get_llm
from ..services.packing import build_system_text, pack_with_rollup
from ..services.budget import analyze_budget
from ..core.settings import SETTINGS
from ..store import list_messages, get_summary, set_summary

router = APIRouter(prefix="/metrics", tags=["metrics"])

@router.get("/budget")
def get_budget(sessionId: Optional[str] = Query(default=None), maxTokens: Optional[int] = None):
    eff0 = SETTINGS.effective()
    sid = sessionId or eff0["default_session_id"]
    llm = get_llm()
    eff = SETTINGS.effective(session_id=sid)

    msgs = [{"role": m.role, "content": m.content} for m in list_messages(sid)]
    summary = get_summary(sid)
    system_text = build_system_text()
    packed, new_summary, _ = pack_with_rollup(
        system_text=system_text,
        summary=summary,
        recent=msgs,
        max_ctx=int(eff["model_ctx"]),
        out_budget=int(eff["default_max_tokens"]),
    )
    if new_summary != summary:
        set_summary(sid, new_summary)

    requested_out = int(maxTokens or eff["default_max_tokens"])
    budget = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=requested_out,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()
    return {"sessionId": sid, "budget": budget}

# ===== aimodel/file_read/api/models.py =====

from __future__ import annotations
from typing import Optional, Dict
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..adaptive.config.paths import read_settings, write_settings
from ..runtime.model_runtime import (
    list_local_models, current_model_info,
    load_model, unload_model
)

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/api/rag.py =====

# aimodel/file_read/api/rag.py
from __future__ import annotations
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from typing import Optional, List, Dict
import numpy as np
from sentence_transformers import SentenceTransformer
from threading import RLock
from ..rag.uploads import list_sources as rag_list_sources, hard_delete_source
from ..rag.schemas import SearchReq, SearchHit
from ..rag.ingest import sniff_and_extract, chunk_text, build_metas
from ..rag.store import add_vectors, search_vectors

router = APIRouter(prefix="/api/rag", tags=["rag"])

_st_model: SentenceTransformer | None = None
_st_lock = RLock()

def _get_st_model() -> SentenceTransformer:
    global _st_model
    if _st_model is None:
        with _st_lock:
            if _st_model is None:
                print("[RAG EMBED] loading e5-small-v2… (one-time)")
                _st_model = SentenceTransformer("intfloat/e5-small-v2")
                print("[RAG EMBED] model ready")
    return _st_model

def _embed(texts: List[str]) -> np.ndarray:
    model = _get_st_model()
    arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
    return arr.astype("float32")

@router.post("/upload")
async def upload_doc(sessionId: Optional[str] = Form(default=None), file: UploadFile = File(...)):
    print(f"[RAG UPLOAD] sessionId={sessionId}, filename={file.filename}, content_type={file.content_type}")

    data = await file.read()
    print(f"[RAG UPLOAD] file size={len(data)} bytes")

    text, mime = sniff_and_extract(file.filename, data)
    print(f"[RAG UPLOAD] extracted mime={mime}, text_len={len(text)}")

    if not text.strip():
        raise HTTPException(status_code=400, detail="Empty/unsupported file")

    chunks = chunk_text(text, {"mime": mime})
    print(f"[RAG UPLOAD] chunk_count={len(chunks)}")

    metas = build_metas(sessionId, file.filename, chunks, size=len(data))
    embeds = _embed([c.text for c in chunks])
    print(f"[RAG UPLOAD] embed_shape={embeds.shape}")

    add_vectors(sessionId, embeds, metas, dim=embeds.shape[1])
    return {"ok": True, "added": len(chunks)}

@router.post("/search")
async def search(req: SearchReq):
    q = (req.query or "").strip()
    if not q:
        return {"hits": []}

    qv = _embed([q])[0]
    chat_hits = search_vectors(req.sessionId, qv, req.kChat, dim=qv.shape[0]) if req.kChat else []
    global_hits = search_vectors(None, qv, req.kGlobal, dim=qv.shape[0]) if req.kGlobal else []

    fused = sorted(chat_hits + global_hits, key=lambda r: r["score"], reverse=True)

    out: List[SearchHit] = []
    for r in fused:
        out.append(SearchHit(
            id=r["id"],
            text=r["text"],
            score=float(r["score"]),
            source=r.get("source"),
            title=r.get("title"),
            sessionId=r.get("sessionId"),
        ))
    return {"hits": [h.model_dump() for h in out]}

@router.get("/list")
async def list_items(sessionId: Optional[str] = None, k: int = 20):
    qv = _embed(["list"])[0]
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    items = []
    for h in hits:
        txt = (h.get("text") or "")
        items.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": txt,
        })
    print(f"[RAG LIST] sessionId={sessionId} k={k} -> {len(items)} items")
    return {"items": items}

@router.get("/dump")
async def dump_items(sessionId: Optional[str] = None, k: int = 50):
    qv = _embed(["dump"])[0]
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    chunks = []
    for h in hits:
        chunks.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": h.get("text") or "",
        })
    print(f"[RAG DUMP] sessionId={sessionId} k={k} -> {len(chunks)} items")
    return {"chunks": chunks}

@router.get("/uploads")
async def api_list_uploads(sessionId: Optional[str] = None, scope: str = "all"):
    include_global = scope != "session"
    return {"uploads": rag_list_sources(sessionId, include_global=include_global)}

@router.post("/uploads/delete-hard")
async def api_delete_upload_hard(body: dict[str, str]):
    source = (body.get("source") or "").strip()
    session_id = (body.get("sessionId") or None)
    if not source:
        return {"error": "source required"}

    out = hard_delete_source(source, session_id=session_id, embedder=_embed)
    return out

# ===== aimodel/file_read/api/settings.py =====

from fastapi import APIRouter, Query, Body
from typing import Dict, Any, Optional
from ..core.settings import SETTINGS

router = APIRouter(prefix="/api/settings", tags=["settings"])

@router.get("/defaults")
def get_defaults():
    return SETTINGS.defaults

@router.get("/overrides")
def get_overrides():
    return SETTINGS.overrides

@router.patch("/overrides")
def patch_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.patch_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.put("/overrides")
def put_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.replace_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.get("/adaptive")
def get_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.adaptive(session_id=session_id)

@router.post("/adaptive/recompute")
def recompute_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    SETTINGS.recompute_adaptive(session_id=session_id)
    return {"ok": True, "adaptive": SETTINGS.adaptive(session_id=session_id)}

@router.get("/effective")
def get_effective(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.effective(session_id=session_id)

# ===== aimodel/file_read/app.py =====

# ===== aimodel/file_read/app.py =====
from __future__ import annotations

# --- Load .env BEFORE any other imports that use os.getenv ---
import os, asyncio
from pathlib import Path

try:
    from dotenv import load_dotenv  # pip install python-dotenv
    # .env sits next to this file (same folder as app.py, per your note)
    _ENV_PATH = Path(__file__).resolve().parent / ".env"
    load_dotenv(dotenv_path=_ENV_PATH, override=False)
except Exception as _e:
    # Don't crash if dotenv isn't present; env may be set by the shell
    print(f"[env] NOTE: could not load .env: {_e}")

from fastapi import FastAPI, Depends, Request
from fastapi.middleware.cors import CORSMiddleware

# Internal libs
from .core import request_ctx
from .adaptive.config.paths import bootstrap
from .workers.retitle_worker import start_worker
from .runtime.model_runtime import load_model

# --- Routers (import AFTER env is loaded) ---
from .api.models import router as models_router
from .api.chats import router as chats_router
from .api.generate_router import router as generate_router
from .api import metrics as metrics_api
from .api.rag import router as rag_router
from .api import settings as settings_router
from .api.billing import router as billing_router
from .api.licensing import router as licensing_router
from .api.local_auth import router as auth_router, require_auth

bootstrap()
app = FastAPI()

# --- CORS (dev over LAN & localhost) ---
# Supports comma-separated list in APP_CORS_ORIGIN
origins = [
    o.strip()
    for o in os.getenv("APP_CORS_ORIGIN", "http://localhost:5173").split(",")
    if o.strip()
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === Routers ===
# Public
app.include_router(models_router)

# Protected (toggle with APP_AUTH_REQUIRED=true/false via require_auth)
deps = [Depends(require_auth)]
app.include_router(chats_router,              dependencies=deps)
app.include_router(generate_router,           dependencies=deps)
app.include_router(settings_router.router,    dependencies=deps)
app.include_router(rag_router,                dependencies=deps)
app.include_router(metrics_api.router,        dependencies=deps)
app.include_router(billing_router,            dependencies=deps)
app.include_router(licensing_router)

# Local auth (register/login/me)
app.include_router(auth_router, prefix="/api")

# --- Startup: warm load model & background worker ---
@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")
    asyncio.create_task(start_worker(), name="retitle_worker")

# --- Per-request header capture (optional) ---
@app.middleware("http")
async def _capture_auth_headers(request: Request, call_next):
    auth = (request.headers.get("authorization") or "").strip()
    if auth.lower().startswith("bearer "):
        request_ctx.set_id_token(auth.split(None, 1)[1])
    else:
        request_ctx.set_id_token("")
    request_ctx.set_x_id((request.headers.get("x-id") or "").strip())
    return await call_next(request)

# ===== aimodel/file_read/backend/.runtime/ports.json =====

{"api_port": 8001}

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/files.py =====

# aimodel/file_read/core/files.py
from __future__ import annotations
from pathlib import Path
import json, os
from typing import Any


CORE_DIR = Path(__file__).resolve().parent
STORE_DIR = CORE_DIR.parent / "store"

EFFECTIVE_SETTINGS_FILE = Path(
    os.getenv("EFFECTIVE_SETTINGS_PATH", str(STORE_DIR / "effective_settings.json"))
)
OVERRIDES_SETTINGS_FILE = Path(
    os.getenv("OVERRIDES_SETTINGS_PATH", str(STORE_DIR / "override_settings.json"))
)
DEFAULTS_SETTINGS_FILE = Path(
    os.getenv("DEFAULT_SETTINGS_PATH", str(STORE_DIR / "default_settings.json"))
)

def load_json_file(path: Path, default: Any = None) -> Any:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {} if default is None else default

def save_json_file(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ===== aimodel/file_read/core/packing_memory_core.py =====

from __future__ import annotations
import math, time
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque

from ..runtime.model_runtime import get_llm
from .style import get_style_sys
from ..store import get_summary as store_get_summary
from ..store import list_messages as store_list_messages
from ..utils.streaming import strip_runjson
from ..core.files import EFFECTIVE_SETTINGS_FILE, load_json_file

SESSIONS: Dict[str, Dict] = {}
PACK_TELEMETRY: Dict[str, object] = {
    "packSec": 0.0,
    "summarySec": 0.0,
    "finalTrimSec": 0.0,
    "compressSec": 0.0,
    "summaryTokensApprox": 0,
    "summaryUsedLLM": False,
    "summaryBullets": 0,
    "summaryAddedChars": 0,
    "summaryOutTokensApprox": 0,
    "summaryCompressedFromChars": 0,
    "summaryCompressedToChars": 0,
    "summaryCompressedDroppedChars": 0,
}
SUMMARY_TEL = PACK_TELEMETRY

class _SettingsCache:
    def __init__(self) -> None:
        self.path: Path = EFFECTIVE_SETTINGS_FILE
        self._mtime: float | None = None
        self._data: Dict = {}

    def get(self) -> Dict:
        try:
            m = self.path.stat().st_mtime
        except FileNotFoundError:
            m = None
        if self._mtime != m or not self._data:
            self._data = load_json_file(self.path, default={})
            self._mtime = m
        return self._data

_SETTINGS = _SettingsCache()

def approx_tokens(text: str) -> int:
    cfg = _SETTINGS.get()
    return max(1, math.ceil(len(text) / int(cfg["chars_per_token"])))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    cfg = _SETTINGS.get()
    overhead = int(cfg["prompt_per_message_overhead"])
    return sum(approx_tokens(m["content"]) + overhead for m in msgs)

def get_session(session_id: str):
    cfg = _SETTINGS.get()
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=int(cfg["recent_maxlen"])),
        "style": get_style_sys(),
        "short": False,
        "bullets": False,
    })
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
        except Exception:
            pass
    if not st["recent"]:
        try:
            rows = store_list_messages(session_id)
            tail = rows[-st["recent"].maxlen:]
            for m in tail:
                st["recent"].append({"role": m.role, "content": strip_runjson(m.content)})
        except Exception:
            pass
    return st

def _heuristic_bullets(chunks: List[Dict[str,str]], cfg: Dict) -> str:
    max_bullets = int(cfg["heuristic_max_bullets"])
    max_words = int(cfg["heuristic_max_words"])
    prefix = cfg["bullet_prefix"]
    bullets = []
    for m in chunks:
        txt = " ".join((m.get("content") or "").split())
        if not txt:
            continue
        words = txt.replace("\n", " ").split()
        snippet = " ".join(words[:max_words]) if words else ""
        bullets.append(f"{prefix}{snippet}" if snippet else prefix.strip())
        if len(bullets) >= max_bullets:
            break
    return "\n".join(bullets) if bullets else prefix.strip()

def summarize_chunks(chunks: List[Dict[str,str]]) -> Tuple[str, bool]:
    cfg = _SETTINGS.get()
    t0 = time.time()
    PACK_TELEMETRY["summarySec"] = 0.0
    PACK_TELEMETRY["summaryTokensApprox"] = 0
    PACK_TELEMETRY["summaryUsedLLM"] = False
    PACK_TELEMETRY["summaryBullets"] = 0
    PACK_TELEMETRY["summaryAddedChars"] = 0
    PACK_TELEMETRY["summaryOutTokensApprox"] = 0
    use_fast = bool(cfg["use_fast_summary"])
    if use_fast:
        txt = _heuristic_bullets(chunks, cfg)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = False
        PACK_TELEMETRY["summaryBullets"] = len([l for l in txt.splitlines() if l.strip()])
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        return txt, False
    text = "\n".join(f'{m.get("role","")}: {m.get("content","")}' for m in chunks)
    sys_inst = cfg["summary_sys_inst"]
    user_prompt = cfg["summary_user_prefix"] + text + cfg["summary_user_suffix"]
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": sys_inst},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=int(cfg["llm_summary_max_tokens"]),
        temperature=float(cfg["llm_summary_temperature"]),
        top_p=float(cfg["llm_summary_top_p"]),
        stream=False,
        stop=list(cfg["llm_summary_stop"]),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip()
    lines = [ln.strip() for ln in raw.splitlines()]
    bullets: List[str] = []
    seen = set()
    max_words = int(cfg["heuristic_max_words"])
    max_bullets = int(cfg["heuristic_max_bullets"])
    for ln in lines:
        if not ln.startswith(cfg["bullet_prefix"]):
            continue
        norm = " ".join(ln[len(cfg["bullet_prefix"]):].lower().split())
        if not norm or norm in seen:
            continue
        seen.add(norm)
        words = ln[len(cfg["bullet_prefix"]):].split()
        if len(words) > max_words:
            ln = cfg["bullet_prefix"] + " ".join(words[:max_words])
        bullets.append(ln)
        if len(bullets) >= max_bullets:
            break
    if bullets:
        txt = "\n".join(bullets)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = True
        PACK_TELEMETRY["summaryBullets"] = len(bullets)
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        return txt, True
    s = " ".join(raw.split())[:160]
    fallback = (cfg["bullet_prefix"] + s) if s else cfg["bullet_prefix"].strip()
    dt = time.time() - t0
    PACK_TELEMETRY["summarySec"] = float(dt)
    PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(fallback))
    PACK_TELEMETRY["summaryUsedLLM"] = True
    PACK_TELEMETRY["summaryBullets"] = len([l for l in fallback.splitlines() if l.strip()])
    PACK_TELEMETRY["summaryAddedChars"] = len(fallback)
    PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(fallback))
    return fallback, True

def _compress_summary_block(s: str) -> str:
    cfg = _SETTINGS.get()
    max_chars = int(cfg["summary_max_chars"])
    prefix = cfg["bullet_prefix"]
    lines = [ln.strip() for ln in (s or "").splitlines()]
    out, seen = [], set()
    for ln in lines:
        if not ln.startswith(prefix):
            continue
        norm = " ".join(ln[len(prefix):].lower().split())
        if norm in seen:
            continue
        seen.add(norm)
        out.append(ln)
    text = "\n".join(out)
    PACK_TELEMETRY["summaryCompressedFromChars"] = int(len(s or ""))
    if len(text) > max_chars:
        last, total = [], 0
        for ln in reversed(out):
            if total + len(ln) + 1 > max_chars:
                break
            last.append(ln)
            total += len(ln) + 1
        text = "\n".join(reversed(last))
    PACK_TELEMETRY["summaryCompressedToChars"] = int(len(text))
    PACK_TELEMETRY["summaryCompressedDroppedChars"] = int(max(0, int(PACK_TELEMETRY["summaryCompressedFromChars"]) - int(PACK_TELEMETRY["summaryCompressedToChars"])))
    return text

# ===== aimodel/file_read/core/packing_ops.py =====

from __future__ import annotations
import time
from typing import Dict, List

from .packing_memory_core import (
    _SETTINGS,
    count_prompt_tokens,
    approx_tokens,
    summarize_chunks,
    _compress_summary_block,
    PACK_TELEMETRY,
)
from .style import get_style_sys

def build_system(style: str, short: bool, bullets: bool) -> str:
    cfg = _SETTINGS.get()
    base = get_style_sys()
    parts = [base]
    if style and style != base:
        parts.append(style)
    if short:
        parts.append(cfg["system_brief_directive"])
    if bullets:
        parts.append(cfg["system_bullets_directive"])
    parts.append(cfg["system_follow_user_style_directive"])
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    t0_pack = time.time()
    cfg = _SETTINGS.get()
    model_ctx = int(max_ctx or cfg["model_ctx"])
    gen_budget = int(out_budget or cfg["out_budget"])
    reserved = int(cfg["reserved_system_tokens"])
    input_budget = model_ctx - gen_budget - reserved
    if input_budget < int(cfg["min_input_budget"]):
        input_budget = int(cfg["min_input_budget"])
    sys_text = build_system(style, short, bullets)
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": cfg["summary_header_prefix"] + summary})
    packed = prologue + list(recent)
    try:
        PACK_TELEMETRY["packInputTokensApprox"] = int(count_prompt_tokens(packed))
        PACK_TELEMETRY["packMsgs"] = int(len(packed))
    except Exception:
        pass
    PACK_TELEMETRY["packSec"] += float(time.time() - t0_pack)
    return packed, input_budget

def _final_safety_trim(packed: List[Dict[str,str]], input_budget: int) -> List[Dict[str,str]]:
    t0 = time.time()
    cfg = _SETTINGS.get()
    keep_ratio = float(cfg["final_shrink_summary_keep_ratio"])
    min_keep = int(cfg["final_shrink_summary_min_chars"])
    def toks() -> int:
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    t_before = toks()
    PACK_TELEMETRY["finalTrimTokensBefore"] = int(t_before)
    dropped_msgs = 0
    dropped_tokens = 0
    keep_head = 2 if len(packed) >= 2 and isinstance(packed[1].get("content"), str) and packed[1]["content"].startswith(cfg["summary_header_prefix"]) else 1
    while toks() > input_budget and len(packed) > keep_head + 1:
        dropped = packed.pop(keep_head)
        try:
            dropped_tokens += int(approx_tokens(dropped["content"]))
            dropped_msgs += 1
        except Exception:
            pass
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        summary_msg = packed[1]
        txt = summary_msg["content"]
        n = max(min_keep, int(len(txt) * keep_ratio))
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"] = int(len(txt))
        except Exception:
            pass
        summary_msg["content"] = txt[-n:]
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkToChars"] = int(len(summary_msg["content"]))
            PACK_TELEMETRY["finalTrimSummaryDroppedChars"] = int(max(0, int(PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"]) - int(PACK_TELEMETRY["finalTrimSummaryShrunkToChars"])))
        except Exception:
            pass
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        removed = packed.pop(1)
        try:
            dropped_tokens += int(approx_tokens(removed["content"]))
            dropped_msgs += 1
        except Exception:
            pass
    while toks() > input_budget and len(packed) > 2:
        removed = packed.pop(2 if len(packed) > 3 else 1)
        try:
            dropped_tokens += int(approx_tokens(removed["content"]))
            dropped_msgs += 1
        except Exception:
            pass
    t_after = toks()
    PACK_TELEMETRY["finalTrimTokensAfter"] = int(t_after)
    PACK_TELEMETRY["finalTrimDroppedMsgs"] = int(dropped_msgs)
    PACK_TELEMETRY["finalTrimDroppedApproxTokens"] = int(max(0, dropped_tokens))
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0)
    return packed

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    cfg = _SETTINGS.get()
    def _tok():
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    start_tokens = _tok()
    overage = start_tokens - input_budget
    PACK_TELEMETRY["rollStartTokens"] = int(start_tokens)
    PACK_TELEMETRY["rollOverageTokens"] = int(overage)
    if overage <= int(cfg["skip_overage_lt"]):
        packed = _final_safety_trim(packed, input_budget)
        PACK_TELEMETRY["rollEndTokens"] = int(count_prompt_tokens(packed))
        return packed, summary
    peels_done = 0
    peeled_n = 0
    if len(recent) > 6 and peels_done < int(cfg["max_peel_per_turn"]):
        peel_min = int(cfg["peel_min"])
        peel_frac = float(cfg["peel_frac"])
        peel_max = int(cfg["peel_max"])
        target = max(peel_min, min(peel_max, int(len(recent) * peel_frac)))
        peel = []
        for _ in range(min(target, len(recent))):
            peel.append(recent.popleft())
        peeled_n = len(peel)
        t0_sum = time.time()
        new_sum, _used_llm = summarize_chunks(peel)
        PACK_TELEMETRY["summarySec"] += float(time.time() - t0_sum)
        if new_sum.startswith(cfg["bullet_prefix"]):
            summary = (summary + "\n" + new_sum).strip() if summary else new_sum
        else:
            summary = new_sum
        t0_comp = time.time()
        summary = _compress_summary_block(summary)
        PACK_TELEMETRY["compressSec"] += float(time.time() - t0_comp)
        try:
            PACK_TELEMETRY["rollPeeledMsgs"] = int(peeled_n)
            PACK_TELEMETRY["rollNewSummaryChars"] = int(len(summary))
            PACK_TELEMETRY["rollNewSummaryTokensApprox"] = int(approx_tokens(summary))
        except Exception:
            pass
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": cfg["summary_header_prefix"] + summary},
            *list(recent),
        ]
    t0_trim = time.time()
    packed = _final_safety_trim(packed, input_budget)
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0_trim)
    end_tokens = count_prompt_tokens(packed)
    PACK_TELEMETRY["rollEndTokens"] = int(end_tokens)
    return packed, summary

# ===== aimodel/file_read/core/request_ctx.py =====

# aimodel/file_read/core/request_ctx.py
from contextvars import ContextVar

x_id_ctx: ContextVar[str] = ContextVar("x_id_ctx", default="")
id_token_ctx: ContextVar[str] = ContextVar("id_token_ctx", default="")
user_email_ctx: ContextVar[str] = ContextVar("user_email_ctx", default="")

def get_x_id() -> str: return x_id_ctx.get()
def set_x_id(val: str): x_id_ctx.set((val or "").strip())

def get_id_token() -> str: return id_token_ctx.get()
def set_id_token(val: str): id_token_ctx.set((val or "").strip())

def get_user_email() -> str: return user_email_ctx.get()
def set_user_email(val: str): user_email_ctx.set((val or "").strip())

# ===== aimodel/file_read/core/schemas.py =====

# core/schemas.py
from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel


class Attachment(BaseModel):
    name: str
    source: Optional[str] = None
    sessionId: Optional[str] = None

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str
    attachments: Optional[List[Attachment]] = None  

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None

    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None

    autoWeb: Optional[bool] = None
    webK: Optional[int] = None
    autoRag: Optional[bool] = None   

# ===== aimodel/file_read/core/settings.py =====

# aimodel/file_read/core/settings.py
from __future__ import annotations
import json
from threading import RLock
from typing import Any, Dict, Optional

from .files import (
    DEFAULTS_SETTINGS_FILE,
    OVERRIDES_SETTINGS_FILE,
    EFFECTIVE_SETTINGS_FILE,
    load_json_file,
    save_json_file,
)


def _deep_merge(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(dst)
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v) 
        else:
            out[k] = v
    return out


class _SettingsManager:

    def __init__(self) -> None:
        self._lock = RLock()
        self._defaults: Dict[str, Any] = self._load_defaults()
        self._overrides: Dict[str, Any] = self._load_overrides()
        self._adaptive_by_session: Dict[str, Dict[str, Any]] = {}
        self._persist_effective_unlocked()

    def _load_defaults(self) -> Dict[str, Any]:
        return load_json_file(DEFAULTS_SETTINGS_FILE, default={})

    def _load_overrides(self) -> Dict[str, Any]:
        return load_json_file(OVERRIDES_SETTINGS_FILE, default={})

    def _save_overrides_unlocked(self) -> None:
        save_json_file(OVERRIDES_SETTINGS_FILE, self._overrides)

    def _persist_effective_unlocked(self) -> None:
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get("_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        save_json_file(EFFECTIVE_SETTINGS_FILE, eff)

    def _effective_unlocked(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get(session_id or "_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        return eff

    def _get_unlocked(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        eff = self._effective_unlocked(session_id)
        if key in eff:
            return eff[key]
        if default is not None:
            return default
        raise AttributeError(f"_SettingsManager has no key '{key}'")
    
    @property
    def defaults(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._defaults))

    @property
    def overrides(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._overrides))

    def adaptive(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        key = session_id or "_global_"
        with self._lock:
            return json.loads(json.dumps(self._adaptive_by_session.get(key, {})))

    def effective(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        with self._lock:
            return self._effective_unlocked(session_id)


    def __getattr__(self, name: str) -> Any:
        with self._lock:
            return self._get_unlocked(name)


    def __getitem__(self, key: str) -> Any:
        with self._lock:
            return self._get_unlocked(key)

    def get(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        with self._lock:
            try:
                return self._get_unlocked(key, default=default, session_id=session_id)
            except AttributeError:
                return default

    def patch_overrides(self, patch: Dict[str, Any]) -> None:
        if not isinstance(patch, dict):
            return
        with self._lock:
            self._overrides = _deep_merge(self._overrides, patch)
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def replace_overrides(self, new_overrides: Dict[str, Any]) -> None:
        if not isinstance(new_overrides, dict):
            new_overrides = {}
        with self._lock:
            self._overrides = json.loads(json.dumps(new_overrides))
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def reload_overrides(self) -> None:
        with self._lock:
            self._overrides = self._load_overrides()
            self._persist_effective_unlocked()

    def set_adaptive_for_session(self, session_id: Optional[str], values: Dict[str, Any]) -> None:
        key = session_id or "_global_"
        if not isinstance(values, dict):
            values = {}
        with self._lock:
            self._adaptive_by_session[key] = json.loads(json.dumps(values))
            if key == "_global_":
                self._persist_effective_unlocked()

    def recompute_adaptive(self, session_id: Optional[str] = None) -> None:
        with self._lock:
            self._persist_effective_unlocked()


SETTINGS = _SettingsManager()

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple
from .settings import SETTINGS

def get_style_sys() -> str:
    return SETTINGS.get("style_sys", "")

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    S = SETTINGS.effective()
    pats = S.get("style_patterns", {})
    template = S.get("style_template", "You must talk like {style}.")

    compiled = []
    for key in ["talk_like", "respond_like", "from_now", "be"]:
        if key in pats:
            compiled.append(re.compile(pats[key], re.I))

    t = user_text.strip()
    style_match = None
    for pat in compiled:
        style_match = pat.search(t)
        if style_match:
            break

    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = template.format(style=raw)

    return style_inst, False, False

# ===== aimodel/file_read/rag/__init__.py =====



# ===== aimodel/file_read/rag/ingest/__init__.py =====

# aimodel/file_read/rag/ingest/__init__.py

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import io, re, csv, hashlib, json
from datetime import datetime, date, time

import pytesseract
from PIL import Image
import pypdfium2 as pdfium

from ...core.settings import SETTINGS

from .ocr import ocr_image_bytes, is_bad_text, ocr_pdf
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas
from .excel_ingest_core import (
    scan_blocks_by_blank_rows,
    rightmost_nonempty_header,
    select_indices,
)
from .xls_ingest import extract_xls
from .excel_ingest import extract_excel
from .csv_ingest import extract_csv
from .docx_ingest import extract_docx
from .doc_binary_ingest import extract_doc_binary
from .ppt_ingest import extract_pptx, extract_ppt
from .pdf_ingest import extract_pdf
from .main import sniff_and_extract

__all__ = [
    "dataclass",
    "List", "Dict", "Optional", "Tuple",
    "io", "re", "csv", "hashlib", "json",
    "datetime", "date", "time",
    "pytesseract", "Image", "pdfium",
    "SETTINGS",
    "ocr_image_bytes", "is_bad_text", "ocr_pdf",
    "_utf8", "_strip_html", "Chunk", "chunk_text", "build_metas",
    "scan_blocks_by_blank_rows", "rightmost_nonempty_header", "select_indices",
    "extract_xls", "extract_excel", "extract_csv",
    "extract_docx", "extract_doc_binary", "extract_pptx", "extract_ppt", "extract_pdf",
    "sniff_and_extract",
]

# ===== aimodel/file_read/rag/ingest/common.py =====

# ===== aimodel/file_read/rag/ingest/common.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional
import re
from ...core.settings import SETTINGS

@dataclass
class Chunk:
    text: str
    meta: Dict[str, str]


def _utf8(data: bytes) -> str:

    return (data or b"").decode("utf-8", errors="ignore")

def _strip_html(txt: str) -> str:

    if not txt:
        return ""

    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", txt)
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p>", "\n\n", txt)
    txt = re.sub(r"(?is)<.*?>", " ", txt)
    txt = re.sub(r"[ \t]+", " ", txt)
    return txt.strip()

_HDR_RE = re.compile(r"^(#{1,3})\s+.*$", flags=re.MULTILINE)
_PARA_SPLIT_RE = re.compile(r"\n\s*\n+")

def _split_sections(text: str) -> List[str]:

    text = (text or "").strip()
    if not text:
        return []
    starts = [m.start() for m in _HDR_RE.finditer(text)]
    if not starts:
        return [text]
    if 0 not in starts:
        starts = [0] + starts
    sections: List[str] = []
    for i, s in enumerate(starts):
        e = starts[i + 1] if i + 1 < len(starts) else len(text)
        block = text[s:e].strip()
        if block:
            sections.append(block)
    return sections

def _split_paragraphs(block: str) -> List[str]:
    paras = [p.strip() for p in _PARA_SPLIT_RE.split(block or "")]
    return [p for p in paras if p]

def _hard_split(text: str, max_len: int) -> List[str]:

    approx = re.split(r"(?<=[\.\!\?\;])\s+", text or "")
    out: List[str] = []
    buf = ""
    for s in approx:
        if not s:
            continue
        if len(buf) + (1 if buf else 0) + len(s) <= max_len:
            buf = s if not buf else (buf + " " + s)
        else:
            if buf:
                out.append(buf)
            if len(s) <= max_len:
                out.append(s)
            else:
                words = re.split(r"\s+", s)
                cur = ""
                for w in words:
                    if not w:
                        continue
                    if len(cur) + (1 if cur else 0) + len(w) <= max_len:
                        cur = w if not cur else (cur + " " + w)
                    else:
                        if cur:
                            out.append(cur)
                        cur = w
                if cur:
                    out.append(cur)
            buf = ""
    if buf:
        out.append(buf)
    return out

def _pack_with_budget(pieces: List[str], *, max_chars: int) -> List[str]:
    chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for p in pieces:
        plen = len(p)
        if plen > max_chars:
            chunks.extend(_hard_split(p, max_chars))
            continue
        if cur_len == 0:
            cur, cur_len = [p], plen
            continue
        if cur_len + 2 + plen <= max_chars:  
            cur.append(p)
            cur_len += 2 + plen
        else:
            chunks.append("\n\n".join(cur).strip())
            cur, cur_len = [p], plen
    if cur_len:
        chunks.append("\n\n".join(cur).strip())
    return chunks

def chunk_text(
    text: str,
    meta: Optional[Dict[str, str]] = None,
    *,
    max_chars: int = int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    overlap: int = int(SETTINGS.get("rag_chunk_overlap_chars", 150)),
) -> List[Chunk]:

    base_meta = (meta or {}).copy()
    text = (text or "").strip()
    if not text:
        return []

    if len(text) <= max_chars:
        return [Chunk(text=text, meta=base_meta)]

    sections = _split_sections(text)
    if not sections:
        sections = [text]

    chunks: List[Chunk] = []
    last_tail: Optional[str] = None

    for sec in sections:
        paras = _split_paragraphs(sec)
        if not paras:
            continue
        packed = _pack_with_budget(paras, max_chars=max_chars)
        for ch in packed:
            if last_tail and overlap > 0:
                tail = last_tail[-overlap:] if len(last_tail) > overlap else last_tail
                candidate = f"{tail}\n{ch}"
                chunks.append(Chunk(text=candidate if len(candidate) <= max_chars else ch, meta=base_meta))
            else:
                chunks.append(Chunk(text=ch, meta=base_meta))
            last_tail = ch

    return chunks

def build_metas(session_id: Optional[str], filename: str, chunks: List[Chunk], *, size: int = 0) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for i, c in enumerate(chunks):
        out.append({
            "id": f"{filename}:{i}",
            "sessionId": session_id or "",
            "source": filename,
            "title": filename,
            "mime": "text/plain",
            "size": str(size),
            "chunkIndex": str(i),
            "text": c.text,  
        })
    return out

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
import io, re, csv
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_csv(data: bytes) -> Tuple[str, str]:
    S = SETTINGS.effective
    max_chars = int(S().get("csv_value_max_chars"))
    quote_strings = bool(S().get("csv_quote_strings"))
    header_normalize = bool(S().get("csv_header_normalize"))
    max_rows = int(S().get("csv_infer_max_rows"))
    max_cols = int(S().get("csv_infer_max_cols"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_val(v) -> str:
        if v is None:
            return ""
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not header_normalize:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def rightmost_nonempty_header(headers: List[str]) -> int:
        for i in range(len(headers) - 1, -1, -1):
            h = headers[i]
            if h and not h.isspace():
                return i
        return -1

    def keep_headers(headers: List[str]) -> List[int]:
        keep = []
        for i, h in enumerate(headers):
            hn = (h or "").strip().lower()
            if not hn:
                continue
            if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
                continue
            keep.append(i)
        return keep or list(range(len(headers)))

    def _row_blank_csv(row: List[str]) -> bool:
        if row is None:
            return True
        for c in row:
            if c is None:
                continue
            if str(c).strip():
                return False
        return True

    txt = io.StringIO(data.decode("utf-8", errors="ignore"))
    sample = txt.read(2048)
    txt.seek(0)
    try:
        dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
    except Exception:
        dialect = csv.excel
    reader = csv.reader(txt, dialect)
    rows = list(reader)
    if not rows:
        return "", "text/plain"

    n = len(rows)
    lines: List[str] = []
    lines.append("# Sheet: CSV")

    i = 0
    while i < n:
        while i < n and _row_blank_csv(rows[i]):
            i += 1
        if i >= n:
            break

        start = i
        while i < n and not _row_blank_csv(rows[i]):
            i += 1
        end = i - 1
        if start > end:
            continue

        headers_raw = (rows[start] if start < n else [])[:max_cols]
        norm_headers = [normalize_header(fmt_val(h)) for h in headers_raw]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[: rmax + 1]
        norm_headers = norm_headers[:max_cols]
        keep_idx = keep_headers(norm_headers)
        kept_headers = [norm_headers[j] for j in keep_idx]

        total_rows_block = (end - start + 1)
        use_rows = total_rows_block if max_rows <= 0 else min(total_rows_block, max_rows + 1)
        total_cols_block = len(kept_headers)
        if max_cols > 0:
            total_cols_block = min(total_cols_block, max_cols)

        lines.append(f"## Table: CSV!R{start+1}-{start+use_rows},C1-{max(total_cols_block,1)}")
        if any(kept_headers):
            lines.append("headers: " + ", ".join(h for h in kept_headers if h))
        lines.append("")

        data_start = start + 1
        data_end = min(end, start + use_rows - 1)
        usable_cols_for_slice = min(len(norm_headers), max_cols if max_cols > 0 else len(norm_headers))
        for r in range(data_start, data_end + 1):
            row_vals_raw = rows[r][:usable_cols_for_slice] if r < n else []
            vals = [fmt_val(c) for c in row_vals_raw]
            vals = [vals[j] if j < len(vals) else "" for j in keep_idx]
            while vals and (vals[-1] == "" or vals[-1] is None):
                vals.pop()
            if not any(vals):
                continue

            pairs: List[str] = []
            for h, v in zip(kept_headers, vals):
                if h and v:
                    pairs.append(f"{h}={v}")

            excel_row_num = r + 1
            lines.append(f"### Row {excel_row_num} — CSV")
            lines.append(", ".join(pairs) if pairs else ", ".join(vals))
            lines.append("")

    return "\n".join(lines).strip() + "\n", "text/plain"

# ===== aimodel/file_read/rag/ingest/doc_binary_ingest.py =====

# RTF and legacy .doc/.ole helpers and extraction (no DOCX here)
from __future__ import annotations
from typing import Tuple, List
import re
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _is_ole(b: bytes) -> bool:
    return len(b) >= 8 and b[:8] == b"\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1"

def _dbg(msg: str):
    try:
        S = SETTINGS.effective
        if bool(S().get("doc_debug", False)):
            print(f"[doc_ingest] {msg}")
    except Exception:
        pass

_RTF_CTRL_RE = re.compile(r"\\[a-zA-Z]+-?\d* ?")
_RTF_GROUP_RE = re.compile(r"[{}]")
_RTF_UNICODE_RE = re.compile(r"\\u(-?\d+)\??")
_RTF_HEX_RE = re.compile(r"\\'[0-9a-fA-F]{2}")
_HEX_BLOCK_RE = re.compile(r"(?:\s*[0-9A-Fa-f]{2}){120,}")

def _rtf_to_text_simple(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    def _hex_sub(m):
        try:
            return bytes.fromhex(m.group(0)[2:]).decode("latin-1", errors="ignore")
        except Exception:
            return ""
    s = _RTF_HEX_RE.sub(_hex_sub, s)
    def _uni_sub(m):
        try:
            cp = int(m.group(1))
            if cp < 0:
                cp = 65536 + cp
            return chr(cp)
        except Exception:
            return ""
    s = _RTF_UNICODE_RE.sub(_uni_sub, s)
    s = s.replace(r"\par", "\n").replace(r"\line", "\n")
    s = _RTF_CTRL_RE.sub("", s)
    s = _RTF_GROUP_RE.sub("", s)
    s = _HEX_BLOCK_RE.sub("", s)
    s = s.replace("\r", "\n")
    s = re.sub(r"\n\s*\n\s*\n+", "\n\n", s)
    s = _squeeze_spaces(s)
    return s if keep_newlines else s.replace("\n", " ")

def _rtf_to_text_via_lib(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        from striprtf.striprtf import rtf_to_text
    except Exception:
        return _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    try:
        txt = rtf_to_text(s)
    except Exception:
        txt = _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    txt = _squeeze_spaces(txt)
    return txt if keep_newlines else txt.replace("\n", " ")

def _generic_ole_text(data: bytes) -> str:
    S = SETTINGS.effective
    MIN_RUN = int(S().get("doc_ole_min_run_chars", 8))
    MAX_LINE = int(S().get("doc_ole_max_line_chars", 600))
    MIN_ALPHA_RATIO = float(S().get("doc_ole_min_alpha_ratio", 0.25))
    DROP_XMLISH = bool(S().get("doc_ole_drop_xmlish", True))
    DROP_PATHISH = bool(S().get("doc_ole_drop_pathish", True))
    DROP_SYMBOL_LINES = bool(S().get("doc_ole_drop_symbol_lines", True))
    DEDUPE_SHORT_REPEATS = bool(S().get("doc_ole_dedupe_short_repeats", True))
    XMLISH = re.compile(r"^\s*<[^>]+>", re.I)
    PATHISH = re.compile(r"[\\/].+\.(?:xml|rels|png|jpg|jpeg|gif|bmp|bin|dat)\b", re.I)
    SYMBOLLINE = re.compile(r"^[\W_]{6,}$")
    s = data.replace(b"\x00", b"")
    runs = re.findall(rb"[\t\r\n\x20-\x7E]{%d,}" % MIN_RUN, s)
    if not runs:
        return ""
    def _dec(b: bytes) -> str:
        try:
            return b.decode("cp1252", errors="ignore")
        except Exception:
            return b.decode("latin-1", errors="ignore")
    kept: List[str] = []
    for raw in runs:
        chunk = _dec(raw).replace("\r", "\n")
        for ln in re.split(r"\n+", chunk):
            t = ln.strip()
            if not t:
                continue
            if MAX_LINE > 0 and len(t) > MAX_LINE:
                t = t[:MAX_LINE].rstrip()
            t = _squeeze_spaces(t)
            letters = sum(1 for c in t if c.isalpha())
            if letters / max(1, len(t)) < MIN_ALPHA_RATIO:
                continue
            if DROP_XMLISH and XMLISH.search(t):
                continue
            if DROP_PATHISH and PATHISH.search(t):
                continue
            if DROP_SYMBOL_LINES and SYMBOLLINE.fullmatch(t):
                continue
            if DEDUPE_SHORT_REPEATS:
                t = re.sub(r"\b(\w{2,4})\1{2,}\b", r"\1\1", t)
            kept.append(t)
    out = "\n".join(kept)
    out = re.sub(r"\n\s*\n\s*\n+", "\n\n", out).strip()
    return out

def extract_doc_binary(data: bytes) -> Tuple[str, str]:
    head = (data[:64] or b"").lstrip()
    is_rtf = head.startswith(b"{\\rtf") or head.startswith(b"{\\RTF}")
    is_ole = _is_ole(data)
    _dbg(f"extract_doc_binary: bytes={len(data)} is_rtf={is_rtf} is_ole={is_ole}")
    if is_rtf:
        txt = _rtf_to_text_via_lib(data, keep_newlines=True).strip()
        return (txt + "\n" if txt else ""), "text/plain"
    if is_ole:
        txt = _generic_ole_text(data)
        return (txt + "\n" if txt else ""), "text/plain"
    try:
        txt = data.decode("utf-8", errors="ignore").strip()
    except Exception:
        txt = data.decode("latin-1", errors="ignore").strip()
    return (txt + ("\n" if txt else "")), "text/plain"

# ===== aimodel/file_read/rag/ingest/docx_ingest.py =====

# DOCX-only extraction (no .doc/RTF here)
from __future__ import annotations
from typing import Tuple, List, Optional
import io, re
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _is_heading(style_name: str) -> Optional[int]:
    if not style_name:
        return None
    m = re.match(r"Heading\s+(\d+)", style_name, flags=re.IGNORECASE)
    if not m:
        return None
    try:
        return max(1, min(6, int(m.group(1))))
    except Exception:
        return None

def _is_list_style(style_name: str) -> bool:
    return bool(style_name) and any(k in style_name.lower() for k in ("list", "bullet", "number"))

def _extract_paragraph_text(p) -> str:
    return _squeeze_spaces(p.text)

def _docx_image_blobs(doc) -> List[bytes]:
    blobs: List[bytes] = []
    seen_rids = set()
    try:
        part = doc.part

        # inline images
        for ish in getattr(doc, "inline_shapes", []) or []:
            try:
                rId = ish._inline.graphic.graphicData.pic.blipFill.blip.embed
                if rId and rId not in seen_rids:
                    blob = part.related_parts[rId].blob
                    if blob:
                        blobs.append(blob)
                        seen_rids.add(rId)
            except Exception:
                pass

        for p in doc.paragraphs:
            for r in p.runs:
                for d in getattr(r._element, "xpath", lambda *_: [])(".//a:blip"):
                    try:
                        rId = d.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed")
                        if rId and rId not in seen_rids:
                            blob = part.related_parts[rId].blob
                            if blob:
                                blobs.append(blob)
                                seen_rids.add(rId)
                    except Exception:
                        pass

    except Exception:
        pass
    return blobs

def extract_docx(data: bytes) -> Tuple[str, str]:
    from docx import Document
    S = SETTINGS.effective
    HEADING_MAX_LEVEL = int(S().get("docx_heading_max_level", 3))
    USE_MARKDOWN_HEADINGS = bool(S().get("docx_use_markdown_headings", True))
    PRESERVE_BULLETS = bool(S().get("docx_preserve_bullets", True))
    INCLUDE_TABLES = bool(S().get("docx_include_tables", True))
    INCLUDE_HEADERS_FOOTERS = bool(S().get("docx_include_headers_footers", False))
    MAX_PARA_CHARS = int(S().get("docx_para_max_chars", 0))
    DROP_EMPTY_LINES = bool(S().get("docx_drop_empty_lines", True))

    doc = Document(io.BytesIO(data))
    lines: List[str] = []

    try:
        title = (getattr(doc, "core_properties", None) or {}).title
        if title:
            lines.append(f"# {title}")
            lines.append("")
    except Exception:
        pass

    def _clip(s: str) -> str:
        if MAX_PARA_CHARS > 0 and len(s) > MAX_PARA_CHARS:
            return s[:MAX_PARA_CHARS] + "…"
        return s

    if INCLUDE_HEADERS_FOOTERS:
        try:
            for i, sec in enumerate(getattr(doc, "sections", []) or []):
                if i > 0:
                    break
                try:
                    hdr_ps = getattr(sec.header, "paragraphs", []) or []
                    hdr_text = "\n".join(_squeeze_spaces(p.text) for p in hdr_ps if _squeeze_spaces(p.text))
                    if hdr_text:
                        lines.append("## Header")
                        lines.append(_clip(hdr_text))
                        lines.append("")
                except Exception:
                    pass
                try:
                    ftr_ps = getattr(sec.footer, "paragraphs", []) or []
                    ftr_text = "\n".join(_squeeze_spaces(p.text) for p in ftr_ps if _squeeze_spaces(p.text))
                    if ftr_text:
                        lines.append("## Footer")
                        lines.append(_clip(ftr_text))
                        lines.append("")
                except Exception:
                    pass
        except Exception:
            pass

    for p in doc.paragraphs:
        txt = _extract_paragraph_text(p)
        if not txt and DROP_EMPTY_LINES:
            continue
        style_name = getattr(p.style, "name", "") or ""
        lvl = _is_heading(style_name)
        if lvl and lvl <= HEADING_MAX_LEVEL and USE_MARKDOWN_HEADINGS:
            prefix = "#" * max(1, min(6, lvl))
            lines.append(f"{prefix} {txt}".strip())
            continue
        if PRESERVE_BULLETS and _is_list_style(style_name):
            if txt:
                lines.append(f"- {_clip(txt)}")
            continue
        if txt:
            lines.append(_clip(txt))
        elif not DROP_EMPTY_LINES:
            lines.append("")

    if INCLUDE_TABLES and getattr(doc, "tables", None):
        for t_idx, tbl in enumerate(doc.tables):
            try:
                non_empty = any(_squeeze_spaces(cell.text) for row in tbl.rows for cell in row.cells)
            except Exception:
                non_empty = True
            if not non_empty:
                continue
            lines.append("")
            lines.append(f"## Table {t_idx + 1}")
            try:
                for row in tbl.rows:
                    cells = [_squeeze_spaces(c.text) for c in row.cells]
                    if any(cells):
                        lines.append(" | ".join(c for c in cells if c))
            except Exception:
                pass

    if bool(S().get("docx_ocr_images", False)):
        min_bytes = int(S().get("ocr_min_image_bytes", 16384))
        seen_ocr_text = set()
        for blob in _docx_image_blobs(doc):
            if len(blob) >= min_bytes:
                t = (ocr_image_bytes(blob) or "").strip()
                if t:
                    key = t.lower()
                    if key not in seen_ocr_text:
                        lines.append(t)
                        seen_ocr_text.add(key)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
import io, re, hashlib
from datetime import datetime, date, time
from ...core.settings import SETTINGS
from .excel_ingest_core import (
    scan_blocks_by_blank_rows,
    rightmost_nonempty_header,
    select_indices,
)

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_excel(data: bytes) -> Tuple[str, str]:
    from openpyxl import load_workbook
    from openpyxl.utils import range_boundaries
    from openpyxl.worksheet.worksheet import Worksheet
    from openpyxl.utils.datetime import from_excel as _from_excel

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def _sheet_used_range(ws: Worksheet):
        from openpyxl.utils import range_boundaries
        if callable(getattr(ws, "calculate_dimension", None)):
            dim_ref = ws.calculate_dimension()
            try:
                min_c, min_r, max_c, max_r = range_boundaries(dim_ref)
                return min_c, min_r, max_c, max_r
            except Exception:
                pass
        return 1, 1, ws.max_column or 1, ws.max_row or 1

    def _cap_rows(min_r: int, max_r: int) -> int:
        return max_r if INFER_MAX_ROWS <= 0 else min(max_r, min_r + INFER_MAX_ROWS - 1)

    def _keep_and_rename_phantom(headers: List[str]) -> tuple[List[int], List[str]]:
        if len(headers) >= 2 and re.fullmatch(r"\d+_\d+$", (headers[1] or "")) and (headers[0] or ""):
            keep_idx = [0, 1]
            new_headers = headers[:]
            new_headers[1] = "value"
            return keep_idx, new_headers
        keep_idx = [i for i, h in enumerate(headers) if h and not re.fullmatch(r"\d+_\d+$", h)]
        new_headers = [headers[i] for i in keep_idx]
        return keep_idx, new_headers

    wb_vals = load_workbook(io.BytesIO(data), data_only=True, read_only=True)

    def _coerce_excel_datetime(cell, v):
        try:
            if getattr(cell, "is_date", False) and isinstance(v, (int, float)):
                return _from_excel(v, wb_vals.epoch)
        except Exception:
            pass
        return v

    lines: List[str] = []
    ingest_id = hashlib.sha1(data).hexdigest()[:16]
    lines.append(f"# Ingest-ID: {ingest_id}")

    def _emit_inferred_table(ws: Worksheet, sheet_name: str, min_c, min_r, max_c, max_r):
        lines.append(f"# Sheet: {sheet_name}")
        max_c_eff = min(max_c, min_c + INFER_MAX_COLS - 1)
        max_r_eff = _cap_rows(min_r, max_r)
        headers: List[str] = []
        header_fill = 0
        for c in range(min_c, max_c_eff + 1):
            val = ws.cell(row=min_r, column=c).value
            s = fmt_val("" if val is None else str(val).strip())
            if s:
                header_fill += 1
            headers.append(s)
        fill_ratio = header_fill / max(1, (max_c_eff - min_c + 1))
        if fill_ratio < INFER_MIN_HEADER_FILL and (min_r + 1) <= max_r_eff:
            headers = []
            hdr_r = min_r + 1
            for c in range(min_c, max_c_eff + 1):
                val = ws.cell(row=hdr_r, column=c).value
                s = fmt_val("" if val is None else str(val).strip())
                headers.append(s)
            min_r = hdr_r
        norm_headers = [normalize_header(h) for h in headers]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[:rmax + 1]
            max_c_eff = min(max_c_eff, min_c + rmax)
        keep_idx, norm_headers = _keep_and_rename_phantom(norm_headers)
        lines.append("## Table: " + f"{sheet_name}!R{min_r}-{max_r_eff},C{min_c}-{max_c_eff}")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))
        lines.append("")
        for r in range(min_r + 1, max_r_eff + 1):
            raw_vals: List[str] = []
            for c in range(min_c, max_c_eff + 1):
                cell = ws.cell(row=r, column=c)
                vv = _coerce_excel_datetime(cell, cell.value)
                raw_vals.append(fmt_val(vv))
            row_vals = select_indices(raw_vals, keep_idx)
            while row_vals and (row_vals[-1] == "" or row_vals[-1] is None):
                row_vals.pop()
            if not any(v for v in row_vals):
                continue
            pairs: List[str] = []
            for h, v in zip(norm_headers, row_vals):
                if h and v:
                    pairs.append(f"{h}={v}")
            lines.append(f"### Row {r} — {sheet_name}")
            lines.append(", ".join(pairs) if pairs else ", ".join(row_vals))
            lines.append("")

    for sheet_name in wb_vals.sheetnames:
        ws_v: Worksheet = wb_vals[sheet_name]
        min_c, min_r, max_c, max_r = _sheet_used_range(ws_v)
        max_c = min(max_c, min_c + INFER_MAX_COLS - 1)
        for b_min_c, b_min_r, b_max_c, b_max_r in scan_blocks_by_blank_rows(ws_v, min_c, min_r, max_c, max_r):
            if b_min_r > b_max_r:
                continue
            _emit_inferred_table(ws_v, sheet_name, b_min_c, b_min_r, b_max_c, b_max_r)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====
from __future__ import annotations
from typing import List
import re

_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def row_blank(ws, r: int, min_c: int, max_c: int) -> bool:
    for c in range(min_c, max_c + 1):
        v = ws.cell(row=r, column=c).value
        if v not in (None, "") and not (isinstance(v, str) and not v.strip()):
            return False
    return True

def scan_blocks_by_blank_rows(ws, min_c: int, min_r: int, max_c: int, max_r: int):
    r = min_r
    while r <= max_r:
        while r <= max_r and row_blank(ws, r, min_c, max_c):
            r += 1
        if r > max_r:
            break
        start = r
        while r <= max_r and not row_blank(ws, r, min_c, max_c):
            r += 1
        end = r - 1
        yield (min_c, start, max_c, end)

def rightmost_nonempty_header(headers: List[str]) -> int:
    for i in range(len(headers) - 1, -1, -1):
        h = headers[i]
        if h and not h.isspace():
            return i
    return -1

def drop_bad_columns(headers: List[str]) -> List[int]:
    keep = []
    for i, h in enumerate(headers):
        hn = (h or "").strip().lower()
        if not hn:
            continue
        if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
            continue
        keep.append(i)
    return keep or list(range(len(headers)))

def select_indices(seq: List[str], idxs: List[int]) -> List[str]:
    out = []
    for i in idxs:
        out.append(seq[i] if i < len(seq) else "")
    return out

# ===== aimodel/file_read/rag/ingest/main.py =====

# aimodel/file_read/rag/ingest/main.py
from __future__ import annotations
from typing import Tuple
import io, json
from .xls_ingest import extract_xls
from .excel_ingest import extract_excel
from .csv_ingest import extract_csv
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas
from .docx_ingest import extract_docx
from .doc_binary_ingest import extract_doc_binary
from .ppt_ingest import extract_pptx, extract_ppt
from .pdf_ingest import extract_pdf   
from ...core.settings import SETTINGS

__all__ = ["sniff_and_extract", "Chunk", "chunk_text", "build_metas"]

def _ing_dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[ingest]", *args)
    except Exception:
        pass

def sniff_and_extract(filename: str, data: bytes) -> Tuple[str, str]:
    name = (filename or "").lower()
    _ing_dbg("route:", name, "bytes=", len(data))

    if name.endswith((".pptx", ".pptm")):
        _ing_dbg("-> pptx/pptm")
        return extract_pptx(data)

    if name.endswith(".ppt"):
        _ing_dbg("-> ppt (ole)")
        return extract_ppt(data)

    if name.endswith((".xlsx", ".xlsm")):
        _ing_dbg("-> excel")
        return extract_excel(data)

    if name.endswith(".xls"):
        _ing_dbg("-> excel (xls via xlrd)")
        return extract_xls(data)

    if name.endswith((".csv", ".tsv")):
        _ing_dbg("-> csv/tsv")
        return extract_csv(data)

    if name.endswith(".docx"):
        _ing_dbg("-> docx")
        try:
            return extract_docx(data)
        except Exception as e:
            _ing_dbg("docx err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".doc"):
        _ing_dbg("-> doc (binary/rtf)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("doc err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".rtf"):
        _ing_dbg("-> rtf (via doc_binary)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("rtf err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".pdf"):
        _ing_dbg("-> pdf (delegating to extract_pdf)") 
        return extract_pdf(data)

    if name.endswith(".json"):
        _ing_dbg("-> json")
        try:
            return json.dumps(json.loads(_utf8(data)), ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("json err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".jsonl", ".jsonlines")):
        _ing_dbg("-> jsonl")
        lines = _utf8(data).splitlines()
        out = []
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            try:
                out.append(json.dumps(json.loads(ln), ensure_ascii=False, indent=2))
            except Exception:
                out.append(ln)
        return "\n".join(out).strip(), "text/plain"

    if name.endswith((".yaml", ".yml")):
        _ing_dbg("-> yaml")
        try:
            import yaml
            obj = yaml.safe_load(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("yaml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".toml"):
        _ing_dbg("-> toml")
        try:
            try:
                import tomllib
                obj = tomllib.loads(_utf8(data))
            except Exception:
                import toml
                obj = toml.loads(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("toml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".htm", ".html", ".xml")):
        _ing_dbg("-> html/xml")
        return _strip_html(_utf8(data)), "text/plain"

    if name.endswith((
        ".txt", ".log", ".md",
        ".c", ".cpp", ".h", ".hpp",
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".sh", ".ps1",
        ".rs", ".java", ".go", ".rb", ".php",
        ".swift", ".kt", ".scala", ".lua", ".perl",
    )):
        _ing_dbg("-> plaintext/code")
        return _utf8(data), "text/plain"

    _ing_dbg("-> default fallback")
    return _utf8(data), "text/plain"

# ===== aimodel/file_read/rag/ingest/ocr.py =====

# aimodel/file_read/rag/ingest/ocr.py
from __future__ import annotations
from typing import List
import io, re
import pytesseract
from PIL import Image
import pypdfium2 as pdfium
from ...core.settings import SETTINGS

_cmd = str(SETTINGS.effective().get("tesseract_cmd", "")).strip()
if _cmd:
    pytesseract.pytesseract.tesseract_cmd = _cmd

_ALNUM = re.compile(r"[A-Za-z0-9]")

def _alnum_ratio(s: str) -> float:
    if not s:
        return 0.0
    a = len(_ALNUM.findall(s))
    return a / max(1, len(s))

def is_bad_text(s: str) -> bool:
    S = SETTINGS.effective
    min_len = int(S().get("ocr_min_chars_for_ok", 32))
    min_ratio = float(S().get("ocr_min_alnum_ratio_for_ok", 0.15))
    s = (s or "").strip()
    return (len(s) < min_len) or (_alnum_ratio(s) < min_ratio)

def ocr_image_bytes(img_bytes: bytes) -> str:
    S = SETTINGS.effective
    lang = str(S().get("ocr_lang", "eng"))
    psm = str(S().get("ocr_psm", "3"))
    oem = str(S().get("ocr_oem", "3"))
    cfg = f"--oem {oem} --psm {psm}"
    with Image.open(io.BytesIO(img_bytes)) as im:
        im = im.convert("L")
        return pytesseract.image_to_string(im, lang=lang, config=cfg) or ""

def ocr_pdf(data: bytes) -> str:
    S = SETTINGS.effective
    dpi = int(S().get("pdf_ocr_dpi", 300))
    max_pages = int(S().get("pdf_ocr_max_pages", 0))
    lang = str(S().get("ocr_lang", "eng"))
    oem = str(S().get("ocr_oem", "3"))
    psm_default = str(S().get("ocr_psm", "6"))
    try_psm = [psm_default, "4", "7", "3"]  
    min_side = 1200  

    def _dbg(*args):
        try:
            if bool(S().get("ingest_debug", False)):
                print("[ocr_pdf]", *args, flush=True)
        except Exception:
            pass

    try:
        doc = pdfium.PdfDocument(io.BytesIO(data))
    except Exception as e:
        _dbg("PdfDocument ERROR:", repr(e))
        return ""

    n = len(doc)
    limit = n if max_pages <= 0 else min(n, max_pages)
    _dbg(f"pages={n}", f"limit={limit}", f"dpi={dpi}", f"lang={lang}", f"psm_default={psm_default}")

    out: List[str] = []
    for i in range(limit):
        try:
            page = doc[i]
            pil = page.render(scale=dpi/72, rotation=0).to_pil().convert("L")
            base_w, base_h = pil.width, pil.height

            variants = []

            img1 = pil
            if min(base_w, base_h) < min_side:
                f = max(1.0, min_side / float(min(base_w, base_h)))
                img1 = pil.resize((int(base_w * f), int(base_h * f)))
            variants.append(("gray", img1))

            img2 = img1.point(lambda x: 255 if x > 180 else 0)
            variants.append(("bin180", img2))

            img3 = img1.point(lambda x: 255 - x)
            variants.append(("inv", img3))

            got = ""
            for tag, imgv in variants:
                for psm in try_psm:
                    cfg = f"--oem {oem} --psm {psm}"   
                    txt = pytesseract.image_to_string(imgv, lang=lang, config=cfg) or ""
                    txt = txt.strip()
                    _dbg(f"page={i+1}/{limit}", f"{tag} {imgv.width}x{imgv.height}", f"psm={psm}", f"len={len(txt)}", f"prev={repr(txt[:80])}")
                    if txt:
                        got = txt
                        break
                if got:
                    break

            if got:
                out.append(got)
        except Exception as e:
            _dbg(f"page={i+1} ERROR:", repr(e))

    final = "\n\n".join(out).strip()
    _dbg("final_len=", len(final))
    return final

# ===== aimodel/file_read/rag/ingest/pdf_ingest.py =====

# aimodel/file_read/rag/ingest/pdf_ingest.py
from __future__ import annotations
from typing import Tuple
import io
from ...core.settings import SETTINGS
from .ocr import is_bad_text, ocr_pdf
from .common import _utf8

def _dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[pdf_ingest]", *args, flush=True)
    except Exception:
        pass

def extract_pdf(data: bytes) -> Tuple[str, str]:
    print("[pdf_ingest] ENTER extract_pdf", flush=True)

    S = SETTINGS.effective
    OCR_ENABLED = bool(S().get("pdf_ocr_enable", False))
    OCR_MODE = str(S().get("pdf_ocr_mode", "auto")).lower()   
    WHEN_BAD  = bool(S().get("pdf_ocr_when_bad", True))
    DPI       = int(S().get("pdf_ocr_dpi", 300))
    MAX_PAGES = int(S().get("pdf_ocr_max_pages", 0))

    print(f"[pdf_ingest] cfg ocr_enabled={OCR_ENABLED} mode={OCR_MODE} when_bad={WHEN_BAD} dpi={DPI} max_pages={MAX_PAGES}", flush=True)

    def _do_ocr() -> str:
        print("[pdf_ingest] OCR_CALL begin", flush=True)  
        txt = (ocr_pdf(data) or "").strip()
        print(f"[pdf_ingest] OCR_CALL end text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
        return txt

    if OCR_ENABLED and OCR_MODE == "force":
        _dbg("mode=force -> OCR first")
        ocr_txt = _do_ocr()
        if ocr_txt:
            print("[pdf_ingest] EXIT (force OCR success)", flush=True)
            return ocr_txt, "text/plain"
        _dbg("mode=force -> OCR empty, trying text extract")

    txt = ""
    try:
        from pdfminer.high_level import extract_text
        txt = (extract_text(io.BytesIO(data)) or "").strip()
        print(f"[pdf_ingest] pdfminer text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
    except Exception as e:
        print(f"[pdf_ingest] pdfminer ERROR {repr(e)}", flush=True)
        txt = ""

    if OCR_ENABLED and OCR_MODE != "never":
        try_ocr = (not txt) or (WHEN_BAD and is_bad_text(txt))
        print(f"[pdf_ingest] auto-eval try_ocr={try_ocr} has_text={bool(txt)} is_bad={(is_bad_text(txt) if txt else 'n/a')}", flush=True)
        if try_ocr:
            ocr_txt = _do_ocr()
            if ocr_txt:
                print("[pdf_ingest] EXIT (auto OCR success)", flush=True)
                return ocr_txt, "text/plain"

    if not txt:
        try:
            from PyPDF2 import PdfReader
            r = PdfReader(io.BytesIO(data))
            pages = [(p.extract_text() or "").strip() for p in r.pages]
            txt = "\n\n".join([p for p in pages if p]).strip()
            print(f"[pdf_ingest] pypdf2 text_len={len(txt)} preview={repr((txt or '')[:120])}", flush=True)
        except Exception as e2:
            print(f"[pdf_ingest] pypdf2 ERROR {repr(e2)}", flush=True)
            txt = _utf8(data)
            print(f"[pdf_ingest] bytes-fallback text_len={len(txt)}", flush=True)

    final = (txt.strip() if txt else "")
    print(f"[pdf_ingest] EXIT (returned_len={len(final)})", flush=True)
    return final, "text/plain"

# ===== aimodel/file_read/rag/ingest/ppt_ingest.py =====

# OCR-enabled PPT ingest (minimal changes per request)
from __future__ import annotations
from typing import Tuple, List
import io, re
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile(r"[ \t]+")
def _squeeze(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _clip(s: str, limit: int) -> str:
    if limit > 0 and len(s) > limit:
        return s[:limit] + "…"
    return s

def _shape_text(shape) -> List[str]:
    out: List[str] = []
    if getattr(shape, "has_text_frame", False):
        for p in shape.text_frame.paragraphs:
            txt = _squeeze("".join(r.text for r in p.runs))
            if txt:
                out.append(txt)
    if getattr(shape, "has_table", False):
        tbl = shape.table
        for r in tbl.rows:
            cells = [_squeeze(c.text) for c in r.cells]
            if any(cells):
                out.append(" | ".join(c for c in cells if c))
    if getattr(shape, "shape_type", None) and str(shape.shape_type) == "GROUP":
        try:
            for sh in getattr(shape, "shapes", []):
                out.extend(_shape_text(sh))
        except Exception:
            pass
    try:
        S = SETTINGS.effective
        if bool(S().get("pptx_ocr_images", False)):
            is_pic = getattr(shape, "shape_type", None)
            if is_pic and "PICTURE" in str(is_pic):
                img = getattr(shape, "image", None)
                blob = getattr(img, "blob", None) if img is not None else None
                if blob and len(blob) >= int(S().get("ocr_min_image_bytes", 16384)):
                    print("[OCR] candidate image size:", len(blob))
                    t = (ocr_image_bytes(blob) or "").strip()
                    print("[OCR] result:", repr(t[:200]))
                    if t:
                        out.append(t)
    except Exception as e:
        print("[OCR] error:", e)
    return out

def extract_pptx(data: bytes) -> Tuple[str, str]:
    from pptx import Presentation
    S = SETTINGS.effective
    USE_MD = bool(S().get("pptx_use_markdown_headings", True))
    INCLUDE_NOTES = bool(S().get("pptx_include_notes", True))
    INCLUDE_TABLES = bool(S().get("pptx_include_tables", True))
    DROP_EMPTY = bool(S().get("pptx_drop_empty_lines", True))
    MAX_PARA = int(S().get("pptx_para_max_chars", 0))
    NUMBER_SLIDES = bool(S().get("pptx_number_slides", True))

    prs = Presentation(io.BytesIO(data))
    lines: List[str] = []

    for i, slide in enumerate(prs.slides, start=1):
        title = ""
        try:
            if getattr(slide, "shapes", None):
                for sh in slide.shapes:
                    if getattr(sh, "is_placeholder", False) and str(getattr(sh, "placeholder_format", "").type).lower().endswith("title"):
                        title = _squeeze(getattr(sh, "text", "") or "")
                        break
        except Exception:
            pass

        head = f"Slide {i}" + (f": {title}" if title else "")
        if USE_MD:
            lines.append(("## " if NUMBER_SLIDES else "## ") + head)
        else:
            lines.append(head)

        body: List[str] = []
        for sh in getattr(slide, "shapes", []):
            if getattr(sh, "has_table", False) and not INCLUDE_TABLES:
                continue
            body.extend(_shape_text(sh))

        for t in body:
            t = _clip(t, MAX_PARA)
            if t or not DROP_EMPTY:
                lines.append(t)

        if INCLUDE_NOTES:
            try:
                notes = slide.notes_slide
                if notes and getattr(notes, "notes_text_frame", None):
                    note_txt = _squeeze(notes.notes_text_frame.text)
                    if note_txt:
                        lines.append("")
                        lines.append("### Notes")
                        for ln in note_txt.splitlines():
                            ln = _squeeze(ln)
                            if ln or not DROP_EMPTY:
                                lines.append(_clip(ln, MAX_PARA))
            except Exception:
                pass

        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

def extract_ppt(data: bytes) -> Tuple[str, str]:
    from .doc_binary_ingest import _generic_ole_text
    S = SETTINGS.effective
    DROP_EMPTY = bool(S().get("ppt_drop_empty_lines", True))
    DEDUPE = bool(S().get("ppt_dedupe_lines", True))
    MAX_PARA = int(S().get("ppt_max_line_chars", 600))
    MIN_ALPHA = float(S().get("ppt_min_alpha_ratio", 0.4))
    MAX_PUNCT = float(S().get("ppt_max_punct_ratio", 0.5))
    TOKEN_MAX = int(S().get("ppt_token_max_chars", 40))

    raw = _generic_ole_text(data)
    if not raw:
        try:
            raw = data.decode("utf-8", errors="ignore")
        except Exception:
            raw = ""

    out: List[str] = []
    seen = set()
    for ln in (raw.splitlines() if raw else []):
        s = _squeeze(ln)
        if not s and DROP_EMPTY:
            continue
        if MAX_PARA > 0 and len(s) > MAX_PARA:
            s = s[:MAX_PARA] + "…"
        if s:
            letters = sum(1 for c in s if c.isalpha())
            alen = max(1, len(s))
            if letters / alen < MIN_ALPHA:
                continue
            punct = sum(1 for c in s if not c.isalnum() and not c.isspace())
            if punct / alen > MAX_PUNCT:
                continue
            if " " not in s and len(s) <= TOKEN_MAX and re.fullmatch(r"[\w.\-]+", s):
                continue
        if DEDUPE:
            if s in seen:
                continue
            seen.add(s)
        if s or not DROP_EMPTY:
            out.append(s)

    text = "\n".join(out).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====

# ===== aimodel/file_read/rag/ingest/xls_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
from datetime import datetime, date, time
from ...core.settings import SETTINGS
import re

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_xls(data: bytes) -> Tuple[str, str]:
    try:
        import xlrd  
    except Exception:
        return (data.decode("utf-8", errors="replace"), "text/plain")

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    EMIT_KEYVALUES = bool(S().get("excel_emit_key_values"))
    EMIT_CELL_ADDR = bool(S().get("excel_emit_cell_addresses"))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and isinstance(dt, datetime) and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    try:
        book = xlrd.open_workbook(file_contents=data)
    except Exception:
        return (data.decode("utf-8", errors="replace"), "text/plain")

    datemode = book.datemode

    def xlrd_cell_to_py(cell):
        ctype, value = cell.ctype, cell.value
        if ctype == xlrd.XL_CELL_DATE:
            try:
                return xlrd.xldate_as_datetime(value, datemode)
            except Exception:
                return value
        if ctype == xlrd.XL_CELL_NUMBER:
            return float(value)
        if ctype == xlrd.XL_CELL_BOOLEAN:
            return bool(value)
        return value

    lines: List[str] = []

    for sheet in book.sheets():
        nrows = min(sheet.nrows or 0, INFER_MAX_ROWS)
        ncols = min(sheet.ncols or 0, INFER_MAX_COLS)
        if nrows == 0 or ncols == 0:
            continue

        headers_raw = []
        header_fill = 0
        for c in range(ncols):
            v = xlrd_cell_to_py(sheet.cell(0, c))
            s = "" if v is None else str(v).strip()
            if s:
                header_fill += 1
            headers_raw.append(s)

        fill_ratio = header_fill / max(1, ncols)
        start_row = 1
        if fill_ratio < INFER_MIN_HEADER_FILL and nrows >= 2:
            headers_raw = []
            for c in range(ncols):
                v = xlrd_cell_to_py(sheet.cell(1, c))
                s = "" if v is None else str(v).strip()
                headers_raw.append(s)
            start_row = 2

        norm_headers = [normalize_header(h) for h in headers_raw]

        lines.append(f"# Sheet: {sheet.name}")

        if EMIT_KEYVALUES and ncols == 2:
            textish = valueish = rows = 0
            for r in range(start_row, nrows):
                a = xlrd_cell_to_py(sheet.cell(r, 0))
                b = xlrd_cell_to_py(sheet.cell(r, 1))
                if a is None and b is None:
                    continue
                rows += 1
                if isinstance(a, str):
                    textish += 1
                if isinstance(b, (int, float, datetime, date, time)):
                    valueish += 1
            if rows >= 3 and textish / max(1, rows) >= 0.6 and valueish / max(1, rows) >= 0.6:
                lines.append("## Key/Values")
                for r in range(start_row, nrows):
                    k = fmt_val(xlrd_cell_to_py(sheet.cell(r, 0)))
                    v = fmt_val(xlrd_cell_to_py(sheet.cell(r, 1)))
                    if not k and not v:
                        continue
                    lines.append(f"- {k}: {v}" if k else f"- : {v}")
                lines.append("")
                continue 
            
        lines.append("## Inferred Table")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))

        for r in range(start_row, nrows):
            row_vals: List[str] = []
            for c in range(ncols):
                val = fmt_val(xlrd_cell_to_py(sheet.cell(r, c)))
                if val:
                    row_vals.append(val if not EMIT_CELL_ADDR else f"{val}")
            if row_vals:
                lines.append("row: " + ", ".join(row_vals))
        lines.append("")

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/rerank.py =====

from __future__ import annotations
from typing import List, Dict, Optional
from ..core.settings import SETTINGS

_RERANKER = None
_RERANKER_NAME = None

def _load_reranker():
    global _RERANKER, _RERANKER_NAME
    model_name = SETTINGS.get("rag_rerank_model")
    if not model_name:
        return None
    if _RERANKER is not None and _RERANKER_NAME == model_name:
        return _RERANKER
    try:
        from sentence_transformers import CrossEncoder
        _RERANKER = CrossEncoder(model_name)
        _RERANKER_NAME = model_name
        return _RERANKER
    except Exception as e:
        print(f"[RAG RERANK] failed to load reranker {model_name}: {e}")
        _RERANKER = None
        _RERANKER_NAME = None
        return None

def rerank_hits(query: str, hits: List[dict], *, top_m: Optional[int] = None) -> List[dict]:
    if not hits:
        return hits
    model = _load_reranker()
    if model is None:
        return hits

    pairs = [(query, (h.get("text") or "")) for h in hits]
    try:
        scores = model.predict(pairs)
    except Exception as e:
        print(f"[RAG RERANK] predict failed: {e}")
        return hits

    out: List[dict] = []
    for h, s in zip(hits, scores):
        hh = dict(h)
        hh["rerankScore"] = float(s)
        out.append(hh)

    out.sort(key=lambda x: x.get("rerankScore", 0.0), reverse=True)
    if isinstance(top_m, int) and top_m > 0:
        out = out[:top_m]
    return out

def cap_per_source(hits: List[dict], per_source_cap: int) -> List[dict]:
    if per_source_cap is None or per_source_cap <= 0:
        return hits
    bucket: dict[str, int] = {}
    out: List[dict] = []
    for h in hits:
        src = str(h.get("source") or "")
        seen = bucket.get(src, 0)
        if seen < per_source_cap:
            out.append(h)
            bucket[src] = seen + 1
    return out

def min_score_fraction(hits: List[Dict], key: str, frac: float) -> List[Dict]:
    if not hits:
        return hits
    vals = []
    for h in hits:
        try:
            v = float(h.get(key) or 0.0)
        except Exception:
            v = 0.0
        vals.append(v)

    s_min = min(vals)
    s_max = max(vals)
    if s_max == s_min:
        return hits

    kept = []
    for h, v in zip(hits, vals):
        norm = (v - s_min) / (s_max - s_min)
        if norm >= float(frac):
            kept.append(h)
    return kept

# ===== aimodel/file_read/rag/retrieve_common.py =====

# aimodel/file_read/rag/retrieve_common.py
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any, Optional
from ..core.settings import SETTINGS
from .retrieve_tabular import make_rag_block_tabular

_EMBEDDER = None
_EMBEDDER_NAME = None
PRINT_MAX = 10 

def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        print(f"[RAG] sentence_transformers unavailable: {e}")
        return None, None

    model_name = SETTINGS.get("rag_embedding_model")
    if not model_name:
        print("[RAG] no rag_embedding_model configured")
        return None, None

    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            print(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return _EMBEDDER, _EMBEDDER_NAME


def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding encode failed: {e}")
        return []

def _primary_score(h: Dict[str, Any]) -> float:
    s = h.get("rerankScore")
    if s is not None:
        try:
            return float(s)
        except Exception:
            pass
    try:
        return float(h.get("score") or 0.0)
    except Exception:
        return 0.0

def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    hits_sorted = sorted(hits, key=_primary_score, reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out

def _default_header(h: Dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"


def _render_header(h: Dict[str, Any]) -> str:
    return _default_header(h)


def make_rag_block_generic(hits: List[dict], *, max_chars: int) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"
    total_budget = int(SETTINGS.get("rag_total_char_budget"))

    lines = [preamble]
    used = len(lines[0]) + 1

    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()

        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost

    return "\n".join(lines)


def build_block_for_hits(hits_top: List[dict], preferred_sources: Optional[List[str]] = None) -> str:
    block = make_rag_block_tabular(hits_top, preferred_sources=preferred_sources)
    if block is None:
        block = make_rag_block_generic(
            hits_top,
            max_chars=int(SETTINGS.get("rag_max_chars_per_chunk")),
        )
    return block or ""

def _rescore_for_preferred_sources(hits: List[dict], preferred_sources: Optional[List[str]] = None) -> List[dict]:
    if not preferred_sources:
        return hits
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))
    pref = set(s.strip().lower() for s in preferred_sources if s)
    out = []
    for h in hits:
        sc = float(h.get("score") or 0.0)
        src = str(h.get("source") or "").strip().lower()
        if src in pref:
            sc *= (1.0 + boost)
        hh = dict(h)
        hh["score"] = sc
        out.append(hh)
    return out


def _fmt_hit(h: Dict[str, Any]) -> str:
    return (
        f"id={h.get('id')!s} src={h.get('source')!s} "
        f"chunk={h.get('chunkIndex')!s} row={h.get('row')!s} "
        f"score={h.get('score')!s} rerank={h.get('rerankScore')!s}"
    )


def _print_hits(label: str, hits: List[Dict[str, Any]], limit: int = PRINT_MAX) -> None:
    print(f"[RAG DEBUG] {label}: count={len(hits)}")
    for i, h in enumerate(hits[:limit]):
        print(f"[RAG DEBUG]   {i+1:02d}: {_fmt_hit(h)}")
    if len(hits) > limit:
        print(f"[RAG DEBUG]   … (+{len(hits)-limit} more)")


def _nohit_block(q: str) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "Local knowledge")
    preamble = preamble if preamble.endswith(":") else preamble + ":"
    msg = str(SETTINGS.get("rag_nohit_message") or "⛔ No relevant local entries found for this query. Do not guess.")
    if q:
        return f"{preamble}\n- {msg}\n- query={q!r}"
    return f"{preamble}\n- {msg}"

@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    rerankSec: float = 0.0
    usedReranker: bool = False
    keptAfterRerank: int = 0
    capPerSource: int = 0
    keptAfterCap: int = 0
    minScoreFrac: float = 0.0
    keptAfterMinFrac: int = 0
    fallbackUsed: bool = False
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"

# ===== aimodel/file_read/rag/retrieve_core.py =====

# aimodel/file_read/rag/retrieve_core.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Any, Optional
import time
from ..core.settings import SETTINGS
from .store import search_vectors
from .retrieve_tabular import make_rag_block_tabular
from .rerank import rerank_hits, cap_per_source, min_score_fraction

_EMBEDDER = None
_EMBEDDER_NAME = None
PRINT_MAX = 10  


def _get_embedder():
    global _EMBEDDER, _EMBEDDER_NAME
    try:
        from sentence_transformers import SentenceTransformer
    except Exception as e:
        print(f"[RAG] sentence_transformers unavailable: {e}")
        return None, None

    model_name = SETTINGS.get("rag_embedding_model")
    if not model_name:
        print("[RAG] no rag_embedding_model configured")
        return None, None

    if _EMBEDDER is None or _EMBEDDER_NAME != model_name:
        try:
            _EMBEDDER = SentenceTransformer(model_name)
            _EMBEDDER_NAME = model_name
        except Exception as e:
            print(f"[RAG] failed to load embedding model {model_name}: {e}")
            _EMBEDDER = None
            _EMBEDDER_NAME = None
    return _EMBEDDER, _EMBEDDER_NAME


def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    model, _ = _get_embedder()
    if model is None:
        return []
    try:
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding encode failed: {e}")
        return []


def _primary_score(h: Dict[str, Any]) -> float:
    s = h.get("rerankScore")
    if s is not None:
        try:
            return float(s)
        except Exception:
            pass
    try:
        return float(h.get("score") or 0.0)
    except Exception:
        return 0.0


def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    hits_sorted = sorted(hits, key=_primary_score, reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        kid = str(h.get("id") or "")
        key = (kid, "") if kid else (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out


def _default_header(h: Dict[str, Any]) -> str:
    src = str(h.get("source") or "")
    idx = h.get("chunkIndex")
    return f"- {src} — chunk {idx}" if idx is not None else f"- {src}"


def _render_header(h: Dict[str, Any]) -> str:
    return _default_header(h)


def make_rag_block_generic(hits: List[dict], *, max_chars: int) -> str:
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"
    total_budget = int(SETTINGS.get("rag_total_char_budget"))

    lines = [preamble]
    used = len(lines[0]) + 1

    for h in hits:
        head = _render_header(h)
        body = (h.get("text") or "").strip()

        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        if body:
            snippet = body[:max_chars]
            snippet_line = "  " + snippet
            snippet_cost = len(snippet_line) + 1
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 1)
                if remain > 0:
                    lines.append(("  " + snippet)[:remain])
                    used += remain + 1
                break
            lines.append(snippet_line)
            used += snippet_cost

    return "\n".join(lines)


@dataclass
class RagTelemetry:
    embedSec: float = 0.0
    searchChatSec: float = 0.0
    searchGlobalSec: float = 0.0
    hitsChat: int = 0
    hitsGlobal: int = 0
    rerankSec: float = 0.0
    usedReranker: bool = False
    keptAfterRerank: int = 0
    capPerSource: int = 0
    keptAfterCap: int = 0
    minScoreFrac: float = 0.0
    keptAfterMinFrac: int = 0
    fallbackUsed: bool = False
    dedupeSec: float = 0.0
    blockBuildSec: float = 0.0
    topKRequested: int = 0
    blockChars: int = 0
    mode: str = "global"


def _rescore_for_preferred_sources(hits: List[dict], preferred_sources: Optional[List[str]] = None) -> List[dict]:
    if not preferred_sources:
        return hits
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))
    pref = set(s.strip().lower() for s in preferred_sources if s)
    out = []
    for h in hits:
        sc = float(h.get("score") or 0.0)
        src = str(h.get("source") or "").strip().lower()
        if src in pref:
            sc *= (1.0 + boost)
        hh = dict(h)
        hh["score"] = sc
        out.append(hh)
    return out


def build_block_for_hits(hits_top: List[dict], preferred_sources: Optional[List[str]] = None) -> str:
    block = make_rag_block_tabular(hits_top, preferred_sources=preferred_sources)
    if block is None:
        block = make_rag_block_generic(
            hits_top,
            max_chars=int(SETTINGS.get("rag_max_chars_per_chunk")),
        )
    return block or ""


def _fmt_hit(h: Dict[str, Any]) -> str:
    return (
        f"id={h.get('id')!s} src={h.get('source')!s} "
        f"chunk={h.get('chunkIndex')!s} row={h.get('row')!s} "
        f"score={h.get('score')!s} rerank={h.get('rerankScore')!s}"
    )


def _print_hits(label: str, hits: List[Dict[str, Any]], limit: int = PRINT_MAX) -> None:
    print(f"[RAG DEBUG] {label}: count={len(hits)}")
    for i, h in enumerate(hits[:limit]):
        print(f"[RAG DEBUG]   {i+1:02d}: {_fmt_hit(h)}")
    if len(hits) > limit:
        print(f"[RAG DEBUG]   … (+{len(hits)-limit} more)")


def _build_rag_block_core(
    query: str,
    *,
    session_id: str | None,
    k: int,
    session_only: bool,
    preferred_sources: Optional[List[str]] = None,
) -> Tuple[Optional[str], RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode=("session-only" if session_only else "global"))
    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")

    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)

    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None, tel

    d = len(qvec)

    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)

    hits_glob: List[dict] = []
    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)

    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)

    _print_hits("ANN hits (chat)", hits_chat)
    if not session_only:
        _print_hits("ANN hits (global)", hits_glob)

    all_hits = hits_chat + ([] if session_only else hits_glob)
    if not all_hits:
        print("[RAG SEARCH] no hits")
        return None, tel

    all_hits = _rescore_for_preferred_sources(all_hits, preferred_sources=preferred_sources)
    _print_hits("After preference boost", all_hits)

    pre_prune_hits = list(all_hits)

    t_rr = time.perf_counter()
    top_m_cfg = SETTINGS.get("rag_rerank_top_m")
    try:
        top_m = int(top_m_cfg) if top_m_cfg not in (None, "", False) else None
    except Exception:
        top_m = None
    if isinstance(top_m, int):
        top_m = max(1, min(top_m, len(all_hits)))

    try:
        all_hits = rerank_hits(q, all_hits, top_m=top_m)
    except Exception as e:
        print(f"[RAG] rerank error: {e}; skipping rerank")
        pass

    tel.rerankSec = round(time.perf_counter() - t_rr, 6)
    tel.usedReranker = any("rerankScore" in h for h in all_hits)
    tel.keptAfterRerank = len(all_hits)
    _print_hits(f"After rerank (top_m={top_m})", all_hits)

    frac_cfg = SETTINGS.get("rag_min_score_frac")
    try:
        frac = float(frac_cfg) if frac_cfg not in (None, "", False) else None
    except Exception:
        frac = None

    tel.minScoreFrac = float(frac) if isinstance(frac, (int, float)) else 0.0
    if isinstance(frac, (int, float)):
        key = "rerankScore" if any("rerankScore" in h for h in all_hits) else "score"
        before = list(all_hits)
        all_hits = min_score_fraction(all_hits, key, float(frac))
        _print_hits(f"After minScoreFrac={frac} on key={key}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by minScoreFrac", dropped)
    tel.keptAfterMinFrac = len(all_hits)

    cap_cfg = SETTINGS.get("rag_per_source_cap")
    try:
        cap_val = int(cap_cfg) if cap_cfg not in (None, "", False) else 0
    except Exception:
        cap_val = 0
    tel.capPerSource = cap_val
    if cap_val and cap_val > 0:
        before = list(all_hits)
        all_hits = cap_per_source(all_hits, cap_val)
        _print_hits(f"After per-source cap={cap_val}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by per-source cap", dropped)
    tel.keptAfterCap = len(all_hits)

    if not all_hits:
        best = sorted(pre_prune_hits, key=_primary_score, reverse=True)[:1]
        all_hits = best
        tel.fallbackUsed = True
        print("[RAG] pruning yielded 0 hits; falling back to best pre-prune hit")
        _print_hits("Fallback best pre-prune", all_hits)

    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)
    _print_hits(f"Final top-k (k={k})", hits_top)

    t4 = time.perf_counter()
    block = build_block_for_hits(hits_top, preferred_sources=preferred_sources)
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    preview = (block or "")[:400].replace("\n", "\\n")
    print(f"[RAG BLOCK] chars={tel.blockChars} preview=\"{preview}\"")
    print(f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}")

    return block, tel


def build_rag_block(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    k = int(SETTINGS.get("rag_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block


def build_rag_block_with_telemetry(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    k = int(SETTINGS.get("rag_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block, asdict(tel)


def build_rag_block_session_only(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Optional[str]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block


def build_rag_block_session_only_with_telemetry(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block, asdict(tel)

# ===== aimodel/file_read/rag/retrieve_pipeline.py =====

# ===== aimodel/file_read/rag/retrieve_pipeline.py =====
from __future__ import annotations
from dataclasses import asdict
from typing import List, Tuple, Dict, Any, Optional
import time
from ..core.settings import SETTINGS
from .store import search_vectors
from .rerank import rerank_hits, cap_per_source, min_score_fraction
from .retrieve_common import (
    RagTelemetry,
    _embed_query,
    _primary_score,
    _dedupe_and_sort,
    _rescore_for_preferred_sources,
    _print_hits,
    build_block_for_hits,
    _nohit_block,
)

def _mk_preview(text: Optional[str], limit: int = 400) -> str:
    t = (text or "")[:limit]
    return t.replace("\n", "\\n")

def _build_rag_block_core(
    query: str,
    *,
    session_id: str | None,
    k: int,
    session_only: bool,
    preferred_sources: Optional[List[str]] = None,
) -> Tuple[Optional[str], RagTelemetry]:
    tel = RagTelemetry(topKRequested=k, mode=("session-only" if session_only else "global"))
    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k} session_only={session_only}")

    t0 = time.perf_counter()
    qvec = _embed_query(q)
    tel.embedSec = round(time.perf_counter() - t0, 6)
    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None, tel

    d = len(qvec)

    t1 = time.perf_counter()
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    tel.searchChatSec = round(time.perf_counter() - t1, 6)

    hits_glob: List[dict] = []
    if not session_only:
        t2 = time.perf_counter()
        hits_glob = search_vectors(None, qvec, k, dim=d) or []
        tel.searchGlobalSec = round(time.perf_counter() - t2, 6)

    tel.hitsChat = len(hits_chat)
    tel.hitsGlobal = len(hits_glob)

    _print_hits("ANN hits (chat)", hits_chat)
    if not session_only:
        _print_hits("ANN hits (global)", hits_glob)

    all_hits = hits_chat + ([] if session_only else hits_glob)
    if not all_hits:
        print("[RAG SEARCH] no hits")
        return None, tel

    all_hits = _rescore_for_preferred_sources(all_hits, preferred_sources=preferred_sources)
    _print_hits("After preference boost", all_hits)

    pre_prune_hits = list(all_hits)

    t_rr = time.perf_counter()
    top_m_cfg = SETTINGS.get("rag_rerank_top_m")
    try:
        top_m = int(top_m_cfg) if top_m_cfg not in (None, "", False) else None
    except Exception:
        top_m = None
    if isinstance(top_m, int):
        top_m = max(1, min(top_m, len(all_hits)))

    try:
        all_hits = rerank_hits(q, all_hits, top_m=top_m)
    except Exception as e:
        print(f"[RAG] rerank error: {e}; skipping rerank")
        pass

    tel.rerankSec = round(time.perf_counter() - t_rr, 6)
    tel.usedReranker = any("rerankScore" in h for h in all_hits)
    tel.keptAfterRerank = len(all_hits)
    _print_hits(f"After rerank (top_m={top_m})", all_hits)

    min_abs = SETTINGS.get("rag_min_abs_rerank")
    try:
        min_abs = float(min_abs) if min_abs not in (None, "", False) else None
    except Exception:
        min_abs = None
    if isinstance(min_abs, float):
        before = list(all_hits)
        all_hits = [h for h in all_hits if float(h.get("rerankScore") or -1e12) >= min_abs]
        if len(all_hits) != len(before):
            _print_hits(f"After abs rerank cutoff >= {min_abs}", all_hits)

    frac_cfg = SETTINGS.get("rag_min_score_frac")
    try:
        frac = float(frac_cfg) if frac_cfg not in (None, "", False) else None
    except Exception:
        frac = None
    tel.minScoreFrac = float(frac) if isinstance(frac, (int, float)) else 0.0
    if isinstance(frac, (int, float)):
        key = "rerankScore" if any("rerankScore" in h for h in all_hits) else "score"
        before = list(all_hits)
        all_hits = min_score_fraction(all_hits, key, float(frac))
        _print_hits(f"After minScoreFrac={frac} on key={key}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by minScoreFrac", dropped)
    tel.keptAfterMinFrac = len(all_hits)

    cap_cfg = SETTINGS.get("rag_per_source_cap")
    try:
        cap_val = int(cap_cfg) if cap_cfg not in (None, "", False) else 0
    except Exception:
        cap_val = 0
    tel.capPerSource = cap_val
    if cap_val and cap_val > 0:
        before = list(all_hits)
        all_hits = cap_per_source(all_hits, cap_val)
        _print_hits(f"After per-source cap={cap_val}", all_hits)
        dropped = [h for h in before if h not in all_hits]
        if dropped:
            _print_hits("Dropped by per-source cap", dropped)
    tel.keptAfterCap = len(all_hits)

    if not all_hits:
        if pre_prune_hits:
            all_hits = sorted(pre_prune_hits, key=_primary_score, reverse=True)[:1]
            tel.fallbackUsed = True
            print("[RAG] pruning yielded 0; keeping best pre-prune (low-confidence fallback)")
            _print_hits("Fallback best pre-prune", all_hits)
        else:
            tel.fallbackUsed = False
            print("[RAG] no ANN hits; returning no-hit block")
            block = _nohit_block(q)
            tel.blockChars = len(block or "")
            preview = _mk_preview(block)
            print(f'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
            print(f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}")
            return block, tel

    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)
    _print_hits(f"Final top-k (k={k})", hits_top)

    t4 = time.perf_counter()
    block = build_block_for_hits(hits_top, preferred_sources=preferred_sources)
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    preview = _mk_preview(block)
    print(f'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
    print(f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}")

    return block, tel

def build_rag_block(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    k = int(SETTINGS.get("rag_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block

def build_rag_block_with_telemetry(query: str, session_id: str | None = None, *, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    k = int(SETTINGS.get("rag_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources)
    return block, asdict(tel)

def build_rag_block_session_only(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Optional[str]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, _ = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block

def build_rag_block_session_only_with_telemetry(query: str, session_id: Optional[str], *, k: Optional[int] = None, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return None, {}
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, tel = _build_rag_block_core(query, session_id=session_id, k=int(k), session_only=True, preferred_sources=preferred_sources)
    return block, asdict(tel)

# ===== aimodel/file_read/rag/retrieve_tabular.py =====

from __future__ import annotations
from typing import List, Dict, Any, Optional, Tuple
import re, os
from ..core.settings import SETTINGS

_TABLE_RE = re.compile(r"^##\s*Table:\s*(?P<sheet>[^!]+)!\s*R(?P<r1>\d+)-(?P<r2>\d+),C(?P<c1>\d+)-(?P<c2>\d+)", re.MULTILINE)
_ROW_RE = re.compile(r"^#{0,3}\s*Row\s+(?P<row>\d+)\s+—\s+(?P<sheet>[^\r\n]+)", re.MULTILINE)


def is_csv_source(src: str) -> bool:
    try:
        _, ext = os.path.splitext(src.lower())
        return ext in {".csv", ".tsv"}
    except Exception:
        return False


def is_xlsx_source(src: str) -> bool:
    try:
        _, ext = os.path.splitext(src.lower())
        return ext in {".xlsx", ".xlsm", ".xls"}
    except Exception:
        return False


def _capture_table_block(text: str) -> Optional[str]:
    m = _TABLE_RE.search(text or "")
    if not m:
        return None
    start = m.start()
    end = len(text)
    nxt = re.search(r"^\s*$", text[m.end():], re.MULTILINE)
    if nxt:
        end = m.end() + nxt.start()
    return text[start:end].strip()


def _capture_row_block(text: str, row_num: int, sheet: str) -> Optional[str]:
    if not text:
        return None
    pat = re.compile(rf"^#{0,3}\s*Row\s+{row_num}\s+—\s+{re.escape(sheet)}[^\n]*\n(?P<body>.*?)(?:\n\s*\n|$)", re.MULTILINE | re.DOTALL)
    m = pat.search(text)
    if not m:
        pat2 = re.compile(rf"^\s*{row_num}\s+—\s+{re.escape(sheet)}[^\n]*\n(?P<body>.*?)(?:\n\s*\n|$)", re.MULTILINE | re.DOTALL)
        m = pat2.search(text)
        if not m:
            return None
    head = f"### Row {row_num} — {sheet}"
    body = (m.group("body") or "").strip()
    if not body:
        return head
    return f"{head}\n{body}"


def _collect_tabular_hits(hits: List[dict]) -> Dict[str, Any]:
    headers: Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]] = {}
    rows: List[Dict[str, Any]] = []
    for h in hits:
        src = str(h.get("source") or "")
        body = (h.get("text") or "").strip()

        for mt in _TABLE_RE.finditer(body):
            sheet = mt.group("sheet").strip()
            r1 = int(mt.group("r1")); r2 = int(mt.group("r2"))
            c1 = int(mt.group("c1")); c2 = int(mt.group("c2"))
            key = (src, sheet, r1, r2, c1, c2)
            if key not in headers:
                tb = _capture_table_block(body)
                headers[key] = {
                    "source": src, "sheet": sheet,
                    "r1": r1, "r2": r2, "c1": c1, "c2": c2,
                    "text": tb or "", "score": float(h.get("score") or 0.0)
                }
            else:
                headers[key]["score"] = max(headers[key]["score"], float(h.get("score") or 0.0))

        for mr in _ROW_RE.finditer(body):
            rn = int(mr.group("row"))
            sheet = mr.group("sheet").strip()
            rows.append({"source": src, "sheet": sheet, "row": rn, "hit": h, "score": float(h.get("score") or 0.0)})
    return {"headers": headers, "rows": rows}


def _pair_rows_with_headers(collected: Dict[str, Any]) -> Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]]:
    headers = collected["headers"]
    rows = collected["rows"]
    groups: Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]] = {}
    for r in rows:
        src = r["source"]; sheet = r["sheet"]; rown = r["row"]
        match_key = None
        for key in headers.keys():
            s, sh, r1, r2, c1, c2 = key
            if s == src and sh == sheet and r1 <= rown <= r2:
                match_key = key
                break
        if not match_key:
            continue
        g = groups.setdefault(match_key, {"header": headers[match_key], "rows": []})
        g["rows"].append(r)
    return groups


def _render_tabular_groups(
    groups: Dict[Tuple[str, str, int, int, int, int], Dict[str, Any]],
    preferred_sources: Optional[List[str]] = None
) -> List[str]:
    total_budget = int(SETTINGS.get("rag_total_char_budget"))
    max_row_snippets = int(SETTINGS.get("rag_tabular_rows_per_table"))
    per_row_max = int(SETTINGS.get("rag_max_chars_per_chunk"))
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"

    pref = set(s.strip().lower() for s in (preferred_sources or []) if s)
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))

    lines: List[str] = [preamble]
    used = len(lines[0]) + 1

    def _group_score(key):
        base = groups[key]["header"]["score"]
        src = str(groups[key]["header"]["source"] or "").strip().lower()
        return base * (1.0 + boost) if src in pref else base

    keys_sorted = sorted(groups.keys(), key=_group_score, reverse=True)

    for key in keys_sorted:
        hdr = groups[key]["header"]
        hdr_text = (hdr.get("text") or "").strip()
        if not hdr_text:
            hdr_text = f"## Table: {hdr['sheet']}!R{hdr['r1']}-{hdr['r2']},C{hdr['c1']}-{hdr['c2']}"
        hdr_cost = len(hdr_text) + 1
        if used + hdr_cost > total_budget:
            break
        lines.append(hdr_text)
        used += hdr_cost

        row_list = sorted(groups[key]["rows"], key=lambda r: r["score"], reverse=True)[:max_row_snippets]
        for r in row_list:
            body = (r["hit"].get("text") or "")
            row_block = _capture_row_block(body, r["row"], r["sheet"]) or ""
            if not row_block:
                continue
            row_snip = row_block[:per_row_max].strip()
            row_cost = len(row_snip) + 1
            if used + row_cost > total_budget:
                break
            lines.append(row_snip)
            used += row_cost

        if used >= total_budget:
            break

    return lines


def make_rag_block_tabular(hits: List[dict], preferred_sources: Optional[List[str]] = None) -> Optional[str]:
    if not hits:
        return None
    # Only keep sources that look like CSV/Excel to avoid mixing generic text
    tabular_hits = [
        h for h in hits
        if is_xlsx_source(str(h.get("source") or "")) or is_csv_source(str(h.get("source") or ""))
    ]
    if not tabular_hits:
        return None
    collected = _collect_tabular_hits(tabular_hits)
    groups = _pair_rows_with_headers(collected)
    if not groups:
        return None
    lines = _render_tabular_groups(groups, preferred_sources=preferred_sources)
    return "\n".join(lines)

# ===== aimodel/file_read/rag/router_ai.py =====

# ===== aimodel/file_read/rag/router_ai.py =====
from __future__ import annotations
from typing import Tuple, Optional, Any
import json, re, traceback
from ..core.settings import SETTINGS

def _dbg(msg: str):
    print(f"[RAG ROUTER] {msg}")

def _force_json_strict(s: str) -> dict:
    if not s:
        return {}
    try:
        v = json.loads(s)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass
    rgx = SETTINGS.get("router_rag_json_extract_regex")
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
                v = json.loads(cand)
                return v if isinstance(v, dict) else {}
        except Exception:
            pass
    return {}

def _strip_wrappers(text: str) -> str:
    t = text or ""
    if SETTINGS.get("router_rag_trim_whitespace") is True:
        t = t.strip()
    if SETTINGS.get("router_rag_strip_wrappers_enabled") is not True:
        return t
    head = t
    if SETTINGS.get("router_rag_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]
    pat = SETTINGS.get("router_rag_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

def _normalize_keys(d: dict) -> dict:
    return {str(k).strip().strip('"').strip("'").strip().lower(): v for k, v in d.items()}

def _as_bool(v) -> Optional[bool]:
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        s = v.strip().strip('"').strip("'").lower()
        if s in ("true", "yes", "y", "1"):  return True
        if s in ("false", "no", "n", "0"):  return False
    return None

def decide_rag(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    try:
        if not user_text or not user_text.strip():
            return (False, None)

        core_text = _strip_wrappers(user_text.strip())

        prompt_tpl = SETTINGS.get("router_rag_decide_prompt")
        if not isinstance(prompt_tpl, str) or ("$text" not in prompt_tpl and "{text}" not in prompt_tpl):
            _dbg("router_rag_decide_prompt missing/invalid")
            return (False, None)

        from string import Template
        if "$text" in prompt_tpl:
            the_prompt = Template(prompt_tpl).safe_substitute(text=core_text)
        else:
            the_prompt = prompt_tpl.format(text=core_text)

        params = {
            "max_tokens": SETTINGS.get("router_rag_decide_max_tokens"),
            "temperature": SETTINGS.get("router_rag_decide_temperature"),
            "top_p": SETTINGS.get("router_rag_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_rag_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}

        raw = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw.get("choices", [{}])[0].get("message", {}).get("content") or "").strip()

        data = _force_json_strict(text_out)
        if not isinstance(data, dict):
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)
        data = _normalize_keys(data)

        need_raw = data.get("need")
        need_bool = _as_bool(need_raw) if not isinstance(need_raw, bool) else need_raw
        if need_bool is None:
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)

        need = bool(need_bool)
        if not need:
            return (False, None)

        query_field = data.get("query", "")
        query_clean = _strip_wrappers(str(query_field or "").strip())
        if not query_clean:
            query_clean = core_text[:512]

        return (True, query_clean)

    except Exception as e:
        _dbg(f"FATAL {type(e).__name__}: {e}")
        traceback.print_exc()
        need_default = SETTINGS.get("router_rag_default_need_when_invalid")
        return (bool(need_default) if isinstance(need_default, bool) else False, None)

# ===== aimodel/file_read/rag/schemas.py =====

from pydantic import BaseModel, Field
from typing import Optional, Dict

class SearchReq(BaseModel):
    query: str
    sessionId: Optional[str] = None
    kChat: int = 6
    kGlobal: int = 4
    hybrid_alpha: float = 0.5  

class ItemRow(BaseModel):
    id: str
    sessionId: Optional[str]
    source: str
    title: Optional[str]
    mime: Optional[str]
    size: Optional[int]
    createdAt: str
    meta: Dict[str, str] = Field(default_factory=dict)

class SearchHit(BaseModel):
    id: str
    text: str
    score: float
    source: Optional[str] = None   
    title: Optional[str] = None
    sessionId: Optional[str] = None
    url: Optional[str] = None

# ===== aimodel/file_read/rag/search.py =====

from __future__ import annotations
from typing import List, Dict

def reciprocal_rank_fusion(results: List[List[Dict]], k: int = 60) -> List[Dict]:
    scores: Dict[str, float] = {}
    lookup: Dict[str, Dict] = {}
    for lst in results:
        for rank, r in enumerate(lst, start=1):
            rid = r["id"]
            scores[rid] = scores.get(rid, 0.0) + 1.0 / (k + rank)
            lookup[rid] = r
    fused = [{"score": s, **lookup[rid]} for rid, s in scores.items()]
    fused.sort(key=lambda x: x["score"], reverse=True)
    return fused

# ===== aimodel/file_read/rag/store.py =====

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import faiss, json
import numpy as np
from ..store.base import APP_DIR   
import shutil

BASE = APP_DIR / "rag"

def _ns_dir(session_id: Optional[str]) -> Path:
    if session_id:
        return BASE / "by_session" / session_id
    return BASE / "global"

def _paths(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / "index.faiss", d / "meta.jsonl"

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def _load_index(dim: int, p: Path) -> faiss.Index:
    if p.exists():
        return faiss.read_index(str(p))
    return faiss.IndexFlatIP(dim)

def _save_index(idx: faiss.Index, p: Path) -> None:
    faiss.write_index(idx, str(p))

def add_vectors(session_id: Optional[str], embeds: np.ndarray, metas: List[Dict], dim: int):
    idx_path, meta_path = _paths(session_id)
    idx = _load_index(dim, idx_path)

    if not isinstance(idx, faiss.IndexFlatIP):
        idx = faiss.IndexFlatIP(dim) if idx.ntotal == 0 else idx

    embeds = _norm(embeds)

    existing_ids = set()
    if meta_path.exists():
        with meta_path.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    existing_ids.add(j["id"])
                except:
                    pass

    start = idx.ntotal
    new_embeds = []
    new_metas = []

    for i, m in enumerate(metas):
        if m["id"] in existing_ids:
            continue
        m["row"] = start + len(new_embeds)
        new_embeds.append(embeds[i])
        new_metas.append(m)

    if new_embeds:
        idx.add(np.vstack(new_embeds))
        _save_index(idx, idx_path)
        with meta_path.open("a", encoding="utf-8") as f:
            for m in new_metas:
                f.write(json.dumps(m, ensure_ascii=False) + "\n")

def search_vectors(session_id: Optional[str], query_vec: np.ndarray, topk: int, dim: int) -> List[Dict]:
    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return []

    idx = _load_index(dim, idx_path)

    query_vec = np.asarray(query_vec, dtype="float32")
    q = _norm(query_vec.reshape(1, -1))

    D, I = idx.search(q, topk)
    out: List[Dict] = []
    rows: Dict[int, Dict] = {}
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                j = json.loads(line)
                rows[int(j["row"])] = j
            except:
                pass
    for score, row in zip(D[0].tolist(), I[0].tolist()):
        if row < 0:
            continue
        m = rows.get(row)
        if not m:
            continue
        m = dict(m)
        m["score"] = float(score)
        out.append(m)
    return out

def search_similar(qvec: List[float] | np.ndarray, *, k: int = 5, session_id: Optional[str] = None) -> List[Dict]:
    """
    Compatibility wrapper used by retrieve.py.
    qvec: a single embedding vector (list or np.ndarray)
    """
    arr = np.asarray(qvec, dtype="float32")
    dim = int(arr.shape[-1])
    return search_vectors(session_id, arr, k, dim)

def add_texts(
    texts: List[str],
    metas: List[Dict],
    *,
    session_id: Optional[str],
    embed_fn,  
) -> int:
    if not texts:
        return 0
    vecs = embed_fn(texts) 
    if not isinstance(vecs, np.ndarray):
        vecs = np.asarray(vecs, dtype="float32")
    dim = int(vecs.shape[-1])
    add_vectors(session_id, vecs, metas, dim)
    return len(texts)

def delete_namespace(session_id: str) -> bool:
    d = _ns_dir(session_id)
    try:
        if d.exists():
            shutil.rmtree(d, ignore_errors=True)
            return True
        return False
    except Exception:
        return False

def session_has_any_vectors(session_id: Optional[str]) -> bool:
    if not session_id:
        return False

    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return False

    try:
        idx = _load_index(dim=1, p=idx_path)  
        if getattr(idx, "ntotal", 0) <= 0:
            return False
    except Exception as e:
        print(f"[RAG STORE] failed to read index for {session_id}: {e}")
        return False

    try:
        with meta_path.open("r", encoding="utf-8") as f:
            for _ in f:
                return True
        return False
    except Exception as e:
        print(f"[RAG STORE] failed to read meta for {session_id}: {e}")
        return False

# ===== aimodel/file_read/rag/uploads.py =====

from __future__ import annotations
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import json
import numpy as np
import faiss

from .store import _ns_dir  

_META_FN = "meta.jsonl"
_INDEX_FN = "index.faiss"

def _meta_path_ro(session_id: Optional[str]) -> Path:
    return _ns_dir(session_id) / _META_FN

def _index_path_ro(session_id: Optional[str]) -> Path:
    return _ns_dir(session_id) / _INDEX_FN

def _paths_mut(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / _INDEX_FN, d / _META_FN

def _read_meta(meta_path: Path) -> List[dict]:
    if not meta_path.exists():
        return []
    out: List[dict] = []
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                j = json.loads(line)
                if isinstance(j, dict):
                    out.append(j)
            except:
                pass
    return out

def _write_meta(meta_path: Path, rows: List[dict]) -> None:
    tmp = meta_path.with_suffix(".jsonl.tmp")
    with tmp.open("w", encoding="utf-8") as f:
        for j in rows:
            f.write(json.dumps(j, ensure_ascii=False) + "\n")
    tmp.replace(meta_path)

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def list_sources(session_id: Optional[str], include_global: bool = True) -> List[dict]:
    def _agg(ns: Optional[str]) -> Dict[str, int]:
        mp = _meta_path_ro(ns)  
        agg: Dict[str, int] = {}
        for j in _read_meta(mp):
            src = (j.get("source") or "").strip()
            if not src:
                continue
            agg[src] = agg.get(src, 0) + 1
        return agg

    rows: List[dict] = []
    # session first
    if session_id is not None:
        for src, n in _agg(session_id).items():
            rows.append({"source": src, "sessionId": session_id, "chunks": n})
    if include_global:
        for src, n in _agg(None).items():
            rows.append({"source": src, "sessionId": None, "chunks": n})
    return rows

def hard_delete_source(source: str, *, session_id: Optional[str], embedder) -> dict:
    idx_path, meta_path = _paths_mut(session_id) 
    rows = _read_meta(meta_path)
    if not rows:
        return {"ok": True, "removed": 0, "remaining": 0}

    keep: List[dict] = []
    removed = 0
    for j in rows:
        if str(j.get("source") or "").strip() == source:
            removed += 1
        else:
            keep.append(j)

    if removed == 0:
        return {"ok": True, "removed": 0, "remaining": len(keep)}

    for i, j in enumerate(keep):
        j["row"] = i

    if len(keep) == 0:
        if idx_path.exists():
            try:
                idx_path.unlink()
            except:
                pass
        _write_meta(meta_path, [])
        return {"ok": True, "removed": removed, "remaining": 0}

    texts = [str(j.get("text") or "") for j in keep]
    B = 128  # batch size
    parts: List[np.ndarray] = []
    for i in range(0, len(texts), B):
        vec = embedder(texts[i:i + B])
        if not isinstance(vec, np.ndarray):
            vec = np.asarray(vec, dtype="float32")
        parts.append(vec.astype("float32"))
    embeds = np.vstack(parts)
    embeds = _norm(embeds)

    dim = int(embeds.shape[-1])
    new_index = faiss.IndexFlatIP(dim)
    new_index.add(embeds)

    faiss.write_index(new_index, str(idx_path))
    _write_meta(meta_path, keep)

    return {"ok": True, "removed": removed, "remaining": len(keep)}

# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs==9.5.4
httpx==0.27.0
# HTML parsing / readability (permissive)
lxml==6.0.1
readability-lxml==0.8.1
selectolax==0.3.21
beautifulsoup4==4.12.3

# RAG
faiss-cpu==1.8.0.post1
numpy==1.26.4
sentence-transformers==3.0.1
tzlocal==5.2
openpyxl==3.1.5 
python-multipart==0.0.20

python-docx==1.1.2
PyYAML==6.0.2
toml==0.10.2
pdfminer.six==20240706

PyPDF2==3.0.1
pandas==2.2.2

striprtf==0.0.26

python-pptx==0.6.23

pytesseract==0.3.13
pypdfium2==4.30.0
Pillow==10.4.

pydantic[email]==2.11.7
python-jose==3.3.0
passlib[bcrypt]==1.7.4


pynacl==1.5.0

# ===== aimodel/file_read/runtime/__init__.py =====



# ===== aimodel/file_read/runtime/model_runtime.py =====

# aimodel/file_read/model_runtime.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Optional, List
from ..adaptive.config.paths import read_settings, write_settings

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath","")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None,"") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None,"") else None),
        )

_runtime_lock = RLock()
_llm: Optional[Llama] = None
_cfg: Optional[ModelConfig] = None

def _build_kwargs(cfg: ModelConfig) -> Dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _attach_introspection(llm: Llama) -> None:
    def get_last_timings():
        try:
            t = getattr(llm, "get_timings", None)
            if callable(t):
                v = t()
                if isinstance(v, dict):
                    return v
        except Exception:
            pass
        try:
            v = getattr(llm, "timings", None)
            if isinstance(v, dict):
                return v
        except Exception:
            pass
        try:
            v = getattr(llm, "perf", None)
            if isinstance(v, dict):
                return v
        except Exception:
            pass
        return None
    try:
        setattr(llm, "get_last_timings", get_last_timings)
    except Exception:
        pass

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            _llm = None
    except Exception:
        _llm = None

def current_model_info() -> Dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
        }

def ensure_ready() -> None:
    global _llm, _cfg
    with _runtime_lock:
        if _llm is not None:
            return
        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected. Load one via /models/load or set LOCALAI_MODEL_PATH.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")
        _llm = Llama(**_build_kwargs(cfg))
        _attach_introspection(_llm)
        _cfg = cfg

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: Dict[str, Any]) -> Dict[str, Any]:
    global _llm, _cfg
    with _runtime_lock:
        s = read_settings()
        s.update({k:v for k,v in config_patch.items() if v is not None})
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")
        _close_llm()
        _llm = Llama(**_build_kwargs(cfg))
        _attach_introspection(_llm)
        _cfg = cfg
        write_settings(asdict(cfg))
        return current_model_info()

def unload_model() -> None:
    global _llm
    with _runtime_lock:
        _close_llm()

def list_local_models() -> List[Dict[str, Any]]:
    s = read_settings()
    root = Path(s.get("modelsDir") or "")
    root.mkdir(parents=True, exist_ok=True)
    out: List[Dict[str, Any]] = []
    for p in root.rglob("*.gguf"):
        try:
            out.append({
                "path": str(p.resolve()),
                "sizeBytes": p.stat().st_size,
                "name": p.name,
                "rel": str(p.relative_to(root)),
            })
        except Exception:
            pass
    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/services/__init__.py =====



# ===== aimodel/file_read/services/attachments.py =====

# aimodel/file_read/services/attachments.py
from __future__ import annotations
from typing import Any, Iterable, Optional, List


def att_get(att: Any, key: str, default=None):
    try:
        return att.get(key, default)          
    except AttributeError:
        return getattr(att, key, default)    


def join_attachment_names(attachments: Optional[Iterable[Any]]) -> str:
    if not attachments:
        return ""
    names: List[str] = [att_get(a, "name") for a in attachments]  
    names = [n for n in names if n]
    return ", ".join(names)

# ===== aimodel/file_read/services/budget.py =====

# aimodel/file_read/services/budget.py
from __future__ import annotations
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
from .context_window import current_n_ctx, estimate_tokens

@dataclass
class TurnBudget:
    n_ctx: int
    input_tokens_est: Optional[int]
    requested_out_tokens: int
    clamped_out_tokens: int
    clamp_margin: int
    reserved_system_tokens: Optional[int] = None
    available_for_out_tokens: Optional[int] = None
    headroom_tokens: Optional[int] = None
    overage_tokens: Optional[int] = None
    reason: str = "ok"

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

def analyze_budget(
    llm: Any,
    messages: List[Dict[str, str]],
    *,
    requested_out_tokens: int,
    clamp_margin: int,
    reserved_system_tokens: Optional[int] = None,
) -> TurnBudget:
    n_ctx = current_n_ctx()
    try:
        inp = estimate_tokens(llm, messages)
    except Exception:
        inp = None

    rst = int(reserved_system_tokens or 0)
    min_out = 16

    if inp is None:
        available = None
        clamped = requested_out_tokens
        headroom = None
        overage = None
        reason = "input_tokens_unknown"
    else:
        available_raw = n_ctx - inp - clamp_margin - rst
        available = max(min_out, available_raw)
        clamped = max(min_out, min(requested_out_tokens, available))
        headroom = max(0, available - clamped)
        overage = max(0, requested_out_tokens - available)
        reason = "ok" if overage == 0 else "requested_exceeds_available"

    return TurnBudget(
        n_ctx=n_ctx,
        input_tokens_est=inp,
        requested_out_tokens=requested_out_tokens,
        clamped_out_tokens=clamped,
        clamp_margin=clamp_margin,
        reserved_system_tokens=reserved_system_tokens,
        available_for_out_tokens=available,
        headroom_tokens=headroom,
        overage_tokens=overage,
        reason=reason,
    )

# ===== aimodel/file_read/services/cancel.py =====

from __future__ import annotations
import asyncio
from threading import Event
from typing import Dict
from ..core.settings import SETTINGS

eff = SETTINGS.effective()
GEN_SEMAPHORE = asyncio.Semaphore(int(eff["gen_semaphore_permits"]))
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

# aimodel/file_read/services/context_window.py
from __future__ import annotations
from typing import List, Dict, Optional, Tuple, Any
from ..utils.streaming import safe_token_count_messages
from ..runtime.model_runtime import current_model_info
from ..core.settings import SETTINGS

def estimate_tokens(llm, messages: List[Dict[str, str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    eff = SETTINGS.effective()
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or eff["nctx_fallback"])
    except Exception:
        return int(eff["nctx_fallback"])

def clamp_out_budget(
    *, llm, messages: List[Dict[str, str]], requested_out: int, margin: int = 32, reserved_system_tokens: Optional[int] = None
) -> Tuple[int, Optional[int]]:
    eff = SETTINGS.effective()
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = int(eff["token_estimate_fallback"])
    n_ctx = current_n_ctx()
    rst = int(reserved_system_tokens or 0)
    min_out = int(eff.get("min_out_tokens", 16))
    available = max(min_out, n_ctx - prompt_est - margin - rst)
    safe_out = max(min_out, min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

def compute_budget_view(
    llm,
    messages: List[Dict[str, str]],
    requested_out: Optional[int] = None,
    clamp_margin: Optional[int] = None,
    reserved_system_tokens: Optional[int] = None,
) -> Dict[str, Any]:
    eff = SETTINGS.effective()
    n_ctx = current_n_ctx()
    margin = int(clamp_margin if clamp_margin is not None else eff.get("clamp_margin", 32))
    rst = int(reserved_system_tokens if reserved_system_tokens is not None else eff.get("reserved_system_tokens", 0))
    min_out = int(eff.get("min_out_tokens", 16))
    default_out = int(eff.get("out_budget", 512))
    req_out = int(requested_out if requested_out is not None else default_out)

    inp_opt = estimate_tokens(llm, messages)
    if inp_opt is None:
        try:
            prompt_est = safe_token_count_messages(llm, messages)
        except Exception:
            prompt_est = int(eff["token_estimate_fallback"])
    else:
        prompt_est = int(inp_opt)

    available = max(min_out, n_ctx - prompt_est - margin - rst)
    out_budget_chosen = max(min_out, min(req_out, available))
    over_by_tokens = max(0, (prompt_est + req_out + margin + rst) - n_ctx)
    usable_ctx = max(0, n_ctx - margin - rst)

    return {
        "modelCtx": n_ctx,
        "clampMargin": margin,
        "usableCtx": usable_ctx,
        "reservedSystemTokens": rst,
        "inputTokensEst": prompt_est,
        "outBudgetChosen": out_budget_chosen,
        "outBudgetDefault": default_out,
        "outBudgetRequested": req_out,
        "outBudgetMaxAllowed": available,
        "overByTokens": over_by_tokens,
        "minOutTokens": min_out,
        "queueWaitSec": None,
    }

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations
from typing import AsyncGenerator, AsyncIterator, Dict
from dataclasses import asdict
from fastapi.responses import StreamingResponse
import time
from ..core.settings import SETTINGS
from ..utils.streaming import RUNJSON_START, RUNJSON_END
from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .streaming_worker import run_stream as _run_stream
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream

from .generate_pipeline import prepare_generation_with_telemetry

async def generate_stream_flow(data, request) -> StreamingResponse:
    prep = await prepare_generation_with_telemetry(data)
    eff = SETTINGS.effective(session_id=prep.session_id)
    stop_ev = cancel_event(prep.session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        q_start = time.perf_counter()
        async with GEN_SEMAPHORE:
            try:
                q_wait = time.perf_counter() - q_start
                if isinstance(prep.budget_view, dict):
                    prep.budget_view["queueWaitSec"] = round(q_wait, 3)
            except Exception:
                pass

            mark_active(prep.session_id, +1)
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
                s = chunk_bytes.decode("utf-8", errors="ignore")
                if RUNJSON_START in s and RUNJSON_END in s:
                    return
                if s.strip() == eff["stopped_line_marker"]:
                    return
                out_buf.extend(chunk_bytes)

            try:
                async for chunk in run_stream(
                    llm=prep.llm,
                    messages=prep.packed,
                    out_budget=prep.out_budget,
                    stop_ev=stop_ev,
                    request=request,
                    temperature=prep.temperature,
                    top_p=prep.top_p,
                    input_tokens_est=prep.input_tokens_est,
                    t0_request=prep.t_request_start,
                    budget_view=prep.budget_view,
                ):
                    if isinstance(chunk, (bytes, bytearray)):
                        _accum_visible(chunk)
                    else:
                        _accum_visible(chunk.encode("utf-8"))
                    yield chunk
            finally:
                try:
                    full_text = out_buf.decode("utf-8", errors="ignore").strip()
                    start = full_text.find(RUNJSON_START)
                    if start != -1:
                        end = full_text.find(RUNJSON_END, start)
                        if end != -1:
                            full_text = (full_text[:start] + full_text[end + len(RUNJSON_END):]).strip()
                    if full_text:
                        prep.st["recent"].append({"role": "assistant", "content": full_text})
                except Exception:
                    pass

                try:
                    from ..store import apply_pending_for
                    apply_pending_for(prep.session_id)
                except Exception:
                    pass

                try:
                    from ..store import list_messages as store_list_messages
                    from ..workers.retitle_worker import enqueue as enqueue_retitle
                    msgs = store_list_messages(prep.session_id)
                    last_seq = max((int(m.id) for m in msgs), default=0)
                    enqueue_retitle(prep.session_id, [asdict(m) for m in msgs], job_seq=last_seq)
                except Exception:
                    pass

                mark_active(prep.session_id, -1)

    return StreamingResponse(
        streamer(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no", "Connection": "keep-alive"},
    )

async def cancel_session(session_id: str) -> Dict[str, bool]:
    from .cancel import cancel_event
    cancel_event(session_id).set()
    return {"ok": True}

async def cancel_session_alias(session_id: str) -> Dict[str, bool]:
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/generate_pipeline.py =====

# aimodel/file_read/services/generate_pipeline.py
from __future__ import annotations
import time
from typing import Any, Dict, List, Optional
from ..core.settings import SETTINGS
from ..runtime.model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody
from .session_io import handle_incoming
from .packing import build_system_text, pack_with_rollup
from .prompt_utils import chars_len
from .router_text import compose_router_text
from .attachments import att_get
from ..web.router_ai import decide_web_and_fetch
from ..rag.retrieve_pipeline import build_rag_block_session_only_with_telemetry
from .generate_pipeline_support import (
    Prep,
    _bool,
    _approx_block_tokens,
)
from ..core.packing_memory_core import PACK_TELEMETRY
from .generate_pipeline_part2 import _finish_prepare_generation_with_telemetry

async def prepare_generation_with_telemetry(data: ChatBody) -> Prep:
    ensure_ready()
    llm = get_llm()
    t_request_start = time.perf_counter()
    eff0 = SETTINGS.effective()
    session_id = data.sessionId or eff0["default_session_id"]
    eff = SETTINGS.effective(session_id=session_id)

    rag_global_enabled = bool(eff.get("rag_global_enabled", True))
    rag_session_enabled = bool(eff.get("rag_session_enabled", True))
    force_session_only = bool(eff.get("rag_force_session_only")) or (not rag_global_enabled)

    temperature = float(eff["default_temperature"] if getattr(data, "temperature") is None else data.temperature)
    top_p = float(eff["default_top_p"] if getattr(data, "top_p") is None else data.top_p)
    out_budget_req = int(eff["default_max_tokens"] if getattr(data, "max_tokens") is None else data.max_tokens)
    auto_web = _bool(eff["default_auto_web"])
    if getattr(data, "autoWeb") is not None:
        auto_web = _bool(data.autoWeb)
    web_k = int(eff["default_web_k"] if getattr(data, "webK") is None else data.webK)
    web_k = max(int(eff["web_k_min"]), min(web_k, int(eff["web_k_max"])))
    auto_rag = _bool(eff["default_auto_rag"])
    if getattr(data, "autoRag") is not None:
        auto_rag = _bool(data.autoRag)
    model_ctx = int(eff["model_ctx"])

    incoming = [
        {
            "role": m.role,
            "content": m.content,
            "attachments": getattr(m, "attachments", None),
        }
        for m in (data.messages or [])
    ]
    print(f"[PIPE] incoming_msgs={len(incoming)}")
    latest_user = next((m for m in reversed(incoming) if m["role"] == "user"), {})
    latest_user_text = (latest_user.get("content") or "").strip()
    atts = (latest_user.get("attachments") or [])
    has_atts = bool(atts)
    print(f"[PIPE] latest_user_text_len={len(latest_user_text)} has_atts={has_atts} att_count={len(atts)}")

    if not latest_user_text and has_atts:
        names = [att_get(a, "name") for a in atts]
        names = [n for n in names if n]
        latest_user_text = "User uploaded: " + (", ".join(names) if names else "files")

    st = handle_incoming(session_id, incoming)

    base_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    router_text = compose_router_text(
        st.get("recent", []),
        str(base_user_text or ""),
        st.get("summary", "") or "",
        tail_turns=int(eff["router_tail_turns"]),
        summary_chars=int(eff["router_summary_chars"]),
        max_chars=int(eff["router_max_chars"]),
    )

    telemetry: Dict[str, Any] = {"web": {}, "rag": {}, "pack": {}, "prepSec": round(time.perf_counter() - t_request_start, 6)}
    ephemeral_once: List[Dict[str, str]] = []
    telemetry["web"]["injectElapsedSec"] = 0.0
    telemetry["web"]["ephemeralBlocks"] = 0

    try:
        web_block: Optional[str] = None
        web_tel: Dict[str, Any] = {}
        if auto_web and not (has_atts and bool(eff.get("disable_global_rag_on_attachments"))):
            res = await decide_web_and_fetch(llm, router_text, k=web_k)
            if isinstance(res, tuple):
                web_block = res[0] if len(res) > 0 else None
                tel_candidate = res[1] if len(res) > 1 else None
                if isinstance(tel_candidate, dict):
                    web_tel = tel_candidate
            elif isinstance(res, str):
                web_block = res
            else:
                web_block = None
        telemetry["web"].update(web_tel or {})
        need_flag = (web_tel or {}).get("needed")
        injected_candidate = isinstance(web_block, str) and bool(web_block.strip())
        if (need_flag is True) and (not injected_candidate):
            try:
                from ..web.orchestrator import build_web_block
                fb, fb_tel = await build_web_block(router_text, k=web_k)
                print(f"[PIPE][WEB] orchestrator block preview: {fb[:200]!r}" if fb else "[PIPE][WEB] orchestrator returned no block")
                print(f"[PIPE][WEB] orchestrator telemetry: {fb_tel}")
                if fb and fb.strip():
                    web_block = fb
                    injected_candidate = True
            except Exception as e:
                print(f"[PIPE][WEB] orchestrator fallback error: {e}")
        if injected_candidate:
            t0_inject = time.perf_counter()
            web_text = str(eff["web_block_preamble"]) + "\n\n" + web_block.strip()

            max_chars = int(eff.get("web_inject_max_chars") or 0)
            if max_chars > 0 and len(web_text) > max_chars:
                web_text = web_text[:max_chars]

            telemetry["web"]["blockChars"] = len(web_text)
            tok = _approx_block_tokens(llm, "assistant", web_text)
            if tok is not None:
                telemetry["web"]["blockTokensApprox"] = tok
            telemetry["web"]["injected"] = True

            ephemeral_only = bool(eff.get("web_ephemeral_only", True))
            telemetry["web"]["ephemeral"] = ephemeral_only
            telemetry["web"]["droppedFromSummary"] = ephemeral_only

            PACK_TELEMETRY["ignore_ephemeral_in_summary"] = ephemeral_only

            ephemeral_once.append({
                "role": "assistant",
                "content": web_text,
                "_ephemeral": True if ephemeral_only else False,
                "_source": "web"
            })

            telemetry["web"]["injectElapsedSec"] = round(time.perf_counter() - t0_inject, 6)
        else:
            telemetry["web"]["injected"] = False
            telemetry["web"]["injectElapsedSec"] = 0.0
    except Exception as e:
        print(f"[PIPE][WEB] error: {e}")
        telemetry["web"].setdefault("injected", False)
        telemetry["web"].setdefault("injectElapsedSec", 0.0)

    telemetry["web"]["ephemeralBlocks"] = len(ephemeral_once)
    print(f"[PIPE] has_atts={has_atts} disable_global_rag_on_attachments={bool(eff.get('disable_global_rag_on_attachments'))}")

    if has_atts and bool(eff.get("disable_global_rag_on_attachments")):
        att_names = [att_get(a, "name") for a in atts if att_get(a, "name")]
        query_for_atts = (base_user_text or "").strip() or " ".join(att_names) or "document"
        print(f"[PIPE] session-only RAG path query_for_atts={query_for_atts!r} att_names={att_names}")
        t0_att = time.perf_counter()
        try:
            att_block, att_tel = build_rag_block_session_only_with_telemetry(query_for_atts, session_id)
            print(f"[PIPE][RAG] session-only query: {query_for_atts!r}")
            print(f"[PIPE][RAG] session-only block preview: {att_block[:200]!r}" if att_block else "[PIPE][RAG] no session-only block")
        except Exception:
            att_block, att_tel = (None, {})
        if att_tel:
            telemetry["rag"].update(att_tel)
        print(f"[PIPE] session-only RAG built={bool(att_block)} block_chars={len(att_block or '')}")
        if att_block:
            rag_text = str(eff["rag_block_preamble"]) + "\n\n" + att_block
            telemetry["rag"]["sessionOnly"] = True
            telemetry["rag"]["mode"] = "session-only"
            telemetry["rag"]["blockChars"] = len(rag_text)
            tok = _approx_block_tokens(llm, "assistant", rag_text)
            if tok is not None:
                telemetry["rag"]["sessionOnlyTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            ephemeral_once.append({"role": "assistant", "content": rag_text, "_ephemeral": True})
        else:
            telemetry["rag"]["sessionOnly"] = False
            telemetry["rag"].setdefault("injected", False)
        telemetry["rag"]["sessionOnlyBuildSec"] = round(time.perf_counter() - t0_att, 6)

    system_text = build_system_text()
    t_pack0 = time.perf_counter()
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )
    telemetry["pack"]["packSec"] = round(time.perf_counter() - t_pack0, 6)

    return await _finish_prepare_generation_with_telemetry(
        llm, eff, data, st, router_text, latest_user_text, base_user_text, has_atts,
        force_session_only, rag_session_enabled, rag_global_enabled, auto_rag,
        telemetry, packed, out_budget_req, temperature, top_p, t_request_start, session_id
    )

# ===== aimodel/file_read/services/generate_pipeline_part2.py =====

# aimodel/file_read/services/generate_pipeline_part2.py
from __future__ import annotations
import time
from typing import Any, Dict, Optional, List
from .prompt_utils import chars_len
from .generate_pipeline_support import (
    Prep, _tok_count, _approx_block_tokens, _diff_find_inserted_block,
    _web_breakdown, _web_unattributed, _enforce_fit
)
from ..rag.router_ai import decide_rag
from .packing import maybe_inject_rag_block
from .session_io import persist_summary
from .budget import analyze_budget
from ..core.packing_memory_core import PACK_TELEMETRY
from .context_window import clamp_out_budget

async def _finish_prepare_generation_with_telemetry(
    llm, eff, data, st, router_text, latest_user_text, base_user_text, has_atts,
    force_session_only, rag_session_enabled, rag_global_enabled, auto_rag,
    telemetry, packed, out_budget_req, temperature, top_p, t_request_start, session_id
) -> Prep:
    must_inject_session = bool(
        force_session_only and rag_session_enabled and not has_atts and not telemetry.get("web", {}).get("ephemeralBlocks")
    )

    rag_router_allowed = ((rag_session_enabled or rag_global_enabled) and not (
        has_atts and bool(eff["disable_global_rag_on_attachments"])
    )) or must_inject_session

    # NEW: if web router said it's needed or we already injected web, skip RAG router
    web_needed = bool((telemetry.get("web") or {}).get("needed"))
    web_injected = bool((telemetry.get("web") or {}).get("injected"))
    if web_needed or web_injected:
        rag_router_allowed = False
        telemetry.setdefault("rag", {})
        telemetry["rag"]["routerSkipped"] = True
        telemetry["rag"]["routerSkippedReason"] = "web_needed" if web_needed else "web_block_present"

    ephemeral_once: List[Dict[str, str]] = []
    if rag_router_allowed and bool(eff["rag_enabled"]) and not ephemeral_once:
        rag_need = False
        rag_query: Optional[str] = None

        if must_inject_session:
            rag_need = True
            rag_query = (latest_user_text or base_user_text or "").strip()
            telemetry["rag"]["routerDecideSec"] = 0.0
            telemetry["rag"]["routerNeeded"] = True
            telemetry["rag"]["routerForcedSession"] = True
            telemetry["rag"]["routerQuery"] = rag_query
        else:
            t_router0 = time.perf_counter()
            if auto_rag:
                try:
                    rag_need, rag_query = decide_rag(llm, router_text)
                except Exception:
                    rag_need, rag_query = (False, None)
            telemetry["rag"]["routerDecideSec"] = round(time.perf_counter() - t_router0, 6)
            telemetry["rag"]["routerNeeded"] = bool(rag_need)
            if rag_query is not None:
                telemetry["rag"]["routerQuery"] = rag_query

        skip_rag = bool(ephemeral_once) or (not rag_need)
        tokens_before = _tok_count(llm, packed)
        t_inject0 = time.perf_counter()

        print(f"[PIPE][RAG] router query: {rag_query!r} skip_rag={skip_rag}")
        res = maybe_inject_rag_block(
            packed,
            session_id=session_id,
            skip_rag=skip_rag,
            rag_query=rag_query,
            force_session_only=force_session_only,
        )

        telemetry["rag"]["injectBuildSec"] = round(time.perf_counter() - t_inject0, 6)

        if isinstance(res, tuple):
            packed2 = res[0]
            tel = res[1] if len(res) > 1 and isinstance(res[1], dict) else {}
            block_text = res[2] if len(res) > 2 and isinstance(res[2], str) else None
        else:
            packed2 = res
            tel = {}
            block_text = None

        if tel:
            telemetry["rag"].update(tel)
        if block_text:
            print(f"[PIPE][RAG] injected block preview: {block_text[:200]!r}")
            telemetry["rag"]["blockChars"] = len(block_text)
            tok = _approx_block_tokens(llm, "user", block_text)
            if tok is not None:
                telemetry["rag"]["blockTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or ("session-only" if force_session_only else "global")
        else:
            inserted = _diff_find_inserted_block(packed, packed2)
            if inserted and isinstance(inserted.get("content"), str):
                print(f"[PIPE][RAG] diff-inserted block preview: {inserted['content'][:200]!r}")
                text = inserted["content"]
                telemetry["rag"]["blockChars"] = len(text)
                tok = _approx_block_tokens(llm, "user", text)
                if tok is not None:
                    telemetry["rag"]["blockTokensApprox"] = tok
                telemetry["rag"]["injected"] = True
                telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or ("session-only" if force_session_only else "global")

        tokens_after = _tok_count(llm, packed2)
        if tokens_before is not None:
            telemetry["rag"]["packedTokensBefore"] = tokens_before
        if tokens_after is not None:
            telemetry["rag"]["packedTokensAfter"] = tokens_after
        if tokens_before is not None and tokens_after is not None:
            telemetry["rag"]["ragTokensAdded"] = max(0, tokens_after - tokens_before)
        packed = packed2
    else:
        telemetry["rag"]["routerSkipped"] = True
        if telemetry.get("web", {}).get("ephemeralBlocks"):
            telemetry["rag"]["routerSkippedReason"] = "ephemeral_block_present"
        elif not rag_router_allowed:
            telemetry["rag"]["routerSkippedReason"] = "attachments_disable_global_or_rag_disabled"
        elif not bool(eff["rag_enabled"]):
            telemetry["rag"]["routerSkippedReason"] = "rag_disabled"
        print(f"[PIPE] rag_router_skipped reason={telemetry['rag'].get('routerSkippedReason')}")

    packed, out_budget_adj = _enforce_fit(llm, eff, packed, out_budget_req)
    packed_chars = chars_len(packed)
    telemetry["pack"]["packedChars"] = packed_chars
    telemetry["pack"]["messages"] = len(packed)
    telemetry["pack"]["summarySec"] = float(PACK_TELEMETRY.get("summarySec") or 0.0)
    telemetry["pack"]["summaryTokensApprox"] = int(PACK_TELEMETRY.get("summaryTokensApprox") or 0)
    telemetry["pack"]["summaryUsedLLM"] = bool(PACK_TELEMETRY.get("summaryUsedLLM") or False)
    telemetry["pack"]["finalTrimSec"] = float(PACK_TELEMETRY.get("finalTrimSec") or 0.0)
    telemetry["pack"]["compressSec"] = float(PACK_TELEMETRY.get("compressSec") or 0.0)
    telemetry["pack"]["packInputTokensApprox"] = int(PACK_TELEMETRY.get("packInputTokensApprox") or 0)
    telemetry["pack"]["packMsgs"] = int(PACK_TELEMETRY.get("packMsgs") or 0)
    telemetry["pack"]["finalTrimTokensBefore"] = int(PACK_TELEMETRY.get("finalTrimTokensBefore") or 0)
    telemetry["pack"]["finalTrimTokensAfter"] = int(PACK_TELEMETRY.get("finalTrimTokensAfter") or 0)
    telemetry["pack"]["finalTrimDroppedMsgs"] = int(PACK_TELEMETRY.get("finalTrimDroppedMsgs") or 0)
    telemetry["pack"]["finalTrimDroppedApproxTokens"] = int(PACK_TELEMETRY.get("finalTrimDroppedApproxTokens") or 0)
    telemetry["pack"]["finalTrimSummaryShrunkFromChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryShrunkFromChars") or 0)
    telemetry["pack"]["finalTrimSummaryShrunkToChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryShrunkToChars") or 0)
    telemetry["pack"]["finalTrimSummaryDroppedChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryDroppedChars") or 0)
    telemetry["pack"]["rollStartTokens"] = int(PACK_TELEMETRY.get("rollStartTokens") or 0)
    telemetry["pack"]["rollOverageTokens"] = int(PACK_TELEMETRY.get("rollOverageTokens") or 0)

    persist_summary(session_id, st["summary"])

    budget_view = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=out_budget_adj,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()

    wb = _web_breakdown(telemetry.get("web", {}))
    telemetry.setdefault("web", {})["breakdown"] = wb
    telemetry["web"]["breakdown"]["unattributedWebSec"] = _web_unattributed(telemetry.get("web", {}), wb)
    telemetry["web"]["breakdown"]["prepSec"] = float(telemetry.get("prepSec") or 0.0)

    budget_view.setdefault("web", {}).update(telemetry.get("web", {}))
    budget_view.setdefault("rag", {}).update(telemetry.get("rag", {}))
    budget_view.setdefault("pack", {}).update(telemetry.get("pack", {}))

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_adj, margin=int(eff["clamp_margin"])
    )
    budget_view.setdefault("request", {})
    budget_view["request"]["outBudgetRequested"] = out_budget_adj
    budget_view["request"]["temperature"] = temperature
    budget_view["request"]["top_p"] = top_p

    return Prep(
        llm=llm,
        session_id=session_id,
        packed=packed,
        st=st,
        out_budget=out_budget,
        input_tokens_est=input_tokens_est,
        budget_view=budget_view,
        temperature=temperature,
        top_p=top_p,
        t_request_start=t_request_start,
    )

# ===== aimodel/file_read/services/generate_pipeline_support.py =====

# aimodel/file_read/services/generate_pipeline_support.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from ..utils.streaming import safe_token_count_messages

@dataclass
class Prep:
    llm: Any
    session_id: str
    packed: List[Dict[str, str]]
    st: Dict[str, Any]
    out_budget: int
    input_tokens_est: Optional[int]
    budget_view: Dict[str, Any]
    temperature: float
    top_p: float
    t_request_start: float

def _bool(v, default: bool = False) -> bool:
    try:
        return bool(v)
    except Exception:
        return bool(default)

def _tok_count(llm, messages: List[Dict[str, str]]) -> Optional[int]:
    try:
        return int(safe_token_count_messages(llm, messages))
    except Exception:
        return None

def _approx_block_tokens(llm, role: str, text: str) -> Optional[int]:
    return _tok_count(llm, [{"role": role, "content": text}])

def _diff_find_inserted_block(before: List[Dict[str, str]], after: List[Dict[str, str]]) -> Optional[Dict[str, str]]:
    if len(after) - len(before) != 1:
        return None
    i = 0
    while i < len(before) and before[i] == after[i]:
        i += 1
    if i < len(after):
        return after[i]
    return None

def _dump_msgs(label: str, msgs: List[Dict[str, str]], head_chars: int = 180):
    return

def _web_breakdown(web: Dict[str, Any]) -> Dict[str, float]:
    w = web or {}
    orch = w.get("orchestrator") or {}
    router = float(w.get("elapsedSec") or 0.0)
    summarize = float((w.get("summarizer") or {}).get("elapsedSec") or 0.0)
    inject = float(w.get("injectElapsedSec") or 0.0)
    search_total = 0.0
    s1 = (orch.get("search") or {})
    for k in ("elapsedSecTotal", "elapsedSec"):
        if isinstance(s1.get(k), (int, float)):
            search_total = float(s1[k])
            break
    fetch1 = float((orch.get("fetch1") or {}).get("totalSec") or 0.0)
    fetch2 = float((orch.get("fetch2") or {}).get("totalSec") or 0.0)
    orch_elapsed = float(orch.get("elapsedSec") or w.get("fetchElapsedSec") or 0.0)
    assemble = orch_elapsed - (search_total + fetch1 + fetch2)
    if assemble < 0:
        assemble = 0.0
    total_pre_ttft = router + summarize + orch_elapsed + inject
    return {
        "routerSec": round(router, 6),
        "summarizeSec": round(summarize, 6),
        "searchSec": round(search_total, 6),
        "fetchSec": round(fetch1, 6),
        "jsFetchSec": round(fetch2, 6),
        "assembleSec": round(assemble, 6),
        "orchestratorSec": round(orch_elapsed, 6),
        "injectSec": round(inject, 6),
        "totalWebPreTtftSec": round(total_pre_ttft, 6),
    }

def _web_unattributed(web: Dict[str, Any], breakdown: Dict[str, float]) -> float:
    total = float((web or {}).get("fetchElapsedSec") or 0.0)
    explained = float(breakdown.get("searchSec", 0.0)) + float(breakdown.get("fetchSec", 0.0)) + float(breakdown.get("jsFetchSec", 0.0)) + float(breakdown.get("assembleSec", 0.0))
    ua = total - explained
    return round(ua if ua > 0 else 0.0, 6)

def _enforce_fit(llm, eff: Dict[str, Any], packed: List[Dict[str, str]], out_budget_req: int) -> tuple[list[dict], int]:
    tok = _tok_count(llm, packed) or 0
    capacity = int(eff["model_ctx"]) - int(eff["clamp_margin"])
    def drop_one(px):
        keep_head = 2 if len(px) >= 2 and isinstance(px[1].get("content"), str) and px[1]["content"].startswith(eff["summary_header_prefix"]) else 1
        if len(px) > keep_head + 1:
            px.pop(keep_head)
            return True
        return False
    def remove_ephemeral_blocks(px):
        i = 0
        removed = False
        while i < len(px):
            m = px[i]
            if m.get("_ephemeral") is True:
                px.pop(i)
                removed = True
            else:
                i += 1
        return removed
    if tok + out_budget_req > capacity:
        if remove_ephemeral_blocks(packed):
            tok = _tok_count(llm, packed) or 0
    while tok + out_budget_req > capacity and drop_one(packed):
        tok = _tok_count(llm, packed) or 0
    if tok >= capacity:
        ob2 = 0
    else:
        ob2 = min(out_budget_req, max(0, capacity - tok))
    return packed, ob2

# ===== aimodel/file_read/services/packing.py =====

# ===== aimodel/file_read/services/packing.py =====
from __future__ import annotations
from typing import Tuple, List, Dict, Optional, Any
from ..rag.retrieve_pipeline import build_rag_block_with_telemetry, build_rag_block_session_only_with_telemetry
from ..core.settings import SETTINGS
from ..core.packing_ops import build_system, pack_messages, roll_summary_if_needed

def build_system_text() -> str:
    eff = SETTINGS.effective()
    base = build_system(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
    )
    guidance = str(eff["packing_guidance"])
    return base + guidance

def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    eff = SETTINGS.effective()

    packed, input_budget = pack_messages(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
        summary=summary,
        recent=recent,
        max_ctx=max_ctx,
        out_budget=out_budget,
    )

    packed, new_summary = roll_summary_if_needed(
        packed=packed,
        recent=recent,
        summary=summary,
        input_budget=input_budget,
        system_text=system_text,
    )

    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

def maybe_inject_rag_block(
    messages: list[dict],
    *,
    session_id: str | None,
    skip_rag: bool = False,
    rag_query: str | None = None,
    force_session_only: bool = False,  # NEW
) -> tuple[list[dict], Optional[Dict[str, Any]], Optional[str]]:
    if skip_rag:
        return messages, None, None
    if not SETTINGS.get("rag_enabled", True):
        return messages, None, None
    if not messages or messages[-1].get("role") != "user":
        return messages, None, None

    user_q = rag_query or (messages[-1].get("content") or "")

    # NEW: choose session-only vs global
    use_session_only = force_session_only or (not SETTINGS.get("rag_global_enabled", True))

    if use_session_only and SETTINGS.get("rag_session_enabled", True):
        from ..rag.retrieve_pipeline import build_rag_block_session_only_with_telemetry
        block, tel = build_rag_block_session_only_with_telemetry(user_q, session_id=session_id)
        mode = "session-only"
    else:
        from ..rag.retrieve_pipeline import build_rag_block_with_telemetry
        block, tel = build_rag_block_with_telemetry(user_q, session_id=session_id)
        mode = "global"

    if not block:
        print(f"[RAG INJECT] no hits (session={session_id}) q={(user_q or '')!r}")
        return messages, None, None

    print(f"[RAG INJECT] injecting (session={session_id}) chars={len(block)} mode={mode}")
    injected = messages[:-1] + [{"role": "user", "content": block}, messages[-1]]
    tel = dict(tel or {})
    tel["injected"] = True
    tel["mode"] = mode
    return injected, tel, block

# ===== aimodel/file_read/services/prompt_utils.py =====

from __future__ import annotations
import json
from datetime import datetime
from typing import Dict, List


def now_str() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total


# ===== aimodel/file_read/services/router_text.py =====

from __future__ import annotations
from typing import Optional, List
from ..core.settings import SETTINGS


def compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: Optional[int] = None,
    summary_chars: Optional[int] = None,
    max_chars: Optional[int] = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: List[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.packing_memory_core import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List
from ..core.settings import SETTINGS
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END,
    build_run_json, watch_disconnect, collect_engine_timings,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int],  t0_request: Optional[float] = None, budget_view: Optional[dict] = None,
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=SETTINGS.stream_queue_maxsize)
    SENTINEL = object()

    def produce():
        t_start = t0_request or time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        t_call: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []
        stage: dict = {"queueWaitSec": None, "genSec": None}

        try:
            try:
                t_call = time.perf_counter()
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=SETTINGS.stream_top_k,
                    repeat_penalty=SETTINGS.stream_repeat_penalty,
                    stop=SETTINGS.stream_stop_strings,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(
                        SETTINGS.stream_retry_min_tokens,
                        int(out_budget * SETTINGS.stream_retry_fraction)
                    )
                    log.warning(
                        "generate: context overflow, retrying with max_tokens=%d",
                        retry_tokens
                    )
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=SETTINGS.stream_top_k,
                        repeat_penalty=SETTINGS.stream_repeat_penalty,
                        stop=SETTINGS.stream_stop_strings,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(SETTINGS.stream_backpressure_sleep_sec)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                out_text = "".join(out_parts)

                if t_first is not None and t_last is not None:
                    stage["genSec"] = round(t_last - t_first, 3)
                if t_start is not None and t_first is not None:
                    stage["ttftSec"] = round(t_first - t_start, 3)
                if t_start is not None and t_last is not None:
                    stage["totalSec"] = round(t_last - t_start, 3)

                if t_call is not None and t_start is not None:
                    stage["preModelSec"] = round(t_call - t_start, 6)
                if t_call is not None and t_first is not None:
                    stage["modelQueueSec"] = round(t_first - t_call, 6)

                if isinstance(budget_view, dict) and "queueWaitSec" in budget_view:
                    stage["queueWaitSec"] = budget_view.get("queueWaitSec")

                try:
                    engine = collect_engine_timings(llm)
                except Exception:
                    engine = None
                if engine:
                    stage["engine"] = engine

                if isinstance(budget_view, dict):
                    def _fnum(x) -> float:
                        try:
                            return float(x) if x is not None else 0.0
                        except Exception:
                            return 0.0

                    ttft_raw = stage.get("ttftSec")
                    ttft_val = _fnum(ttft_raw)

                    pack   = budget_view.get("pack") or {}
                    rag    = budget_view.get("rag") or {}
                    web_bd = ((budget_view.get("web") or {}).get("breakdown")) or {}

                    pack_sec = _fnum(pack.get("packSec"))
                    trim_sec = _fnum(pack.get("finalTrimSec"))
                    comp_sec = _fnum(pack.get("compressSec"))

                    rag_router = _fnum(rag.get("routerDecideSec"))

                    build_candidates = (
                        rag.get("injectBuildSec"),
                        rag.get("sessionOnlyBuildSec"),
                        rag.get("blockBuildSec"),
                    )
                    first_build = next((v for v in build_candidates if v is not None), None)
                    rag_build_agg = _fnum(first_build)

                    rag_embed  = _fnum(rag.get("embedSec"))
                    rag_s_chat = _fnum(rag.get("searchChatSec"))
                    rag_s_glob = _fnum(rag.get("searchGlobalSec"))
                    rag_dedupe = _fnum(rag.get("dedupeSec"))

                    if rag_build_agg > 0.0:
                        rag_pipeline_sec = rag_build_agg
                    else:
                        rag_pipeline_sec = rag_embed + rag_s_chat + rag_s_glob + rag_dedupe

                    prep_sec    = _fnum(web_bd.get("prepSec"))
                    web_pre     = _fnum(web_bd.get("totalWebPreTtftSec"))

                    model_queue = _fnum(stage.get("modelQueueSec"))

                    pre_accounted = (
                        pack_sec + trim_sec + comp_sec
                        + rag_router + rag_pipeline_sec
                        + web_pre + prep_sec
                        + model_queue
                    )
                    unattr_ttft = ttft_val - pre_accounted
                    if unattr_ttft < 0.0:
                        unattr_ttft = 0.0

                    budget_view.setdefault("breakdown", {})
                    budget_view["breakdown"].update({
                        "ttftSec": ttft_val,
                        "preTtftAccountedSec": round(pre_accounted, 6),
                        "unattributedTtftSec": round(unattr_ttft, 6),
                    })

                run_json = build_run_json(
                    request_cfg={"temperature": temperature, "top_p": top_p, "max_tokens": out_budget},
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                    budget_view=budget_view,
                    extra_timings=stage,
                    error_text=err_text,
                )
                if SETTINGS.runjson_emit:
                    q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass
            finally:
                try:
                    llm.reset()
                except Exception:
                    pass
                try:
                    q.put_nowait(SENTINEL)
                except Exception:
                    pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set() and SETTINGS.stream_emit_stopped_line:
            yield (f"\n{SETTINGS.stopped_line_marker}\n").encode("utf-8")
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=SETTINGS.stream_producer_join_timeout_sec)
        except Exception:
            pass

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message,
    update_last,
    append_message,
    delete_message,
    delete_messages_batch,
    list_messages,
    list_paged,
    delete_batch,
    edit_message,
    set_summary,
    get_summary,
)
from .index import ChatMeta

__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message",
    "update_last",
    "append_message",
    "delete_message",
    "delete_messages_batch",
    "list_messages",
    "list_paged",
    "delete_batch",
    "edit_message",
    "set_summary",
    "get_summary",
    "ChatMeta",
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..adaptive.config.paths import app_data_dir

APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"

def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    return CHATS_DIR / f"{session_id}.json"

__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

# ===== aimodel/file_read/store/chats.py =====
from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
from ..core.settings import SETTINGS
from ..utils.streaming import strip_runjson
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta
from ..rag.store import delete_namespace as rag_delete_namespace


def _load_chat(session_id: str) -> Dict[str, Any]:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""
        return data


@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str
    attachments: Optional[List[Dict]] = None

def _normalize_attachments(atts: Optional[list[Any]]) -> Optional[list[dict]]:
    if not atts:
        return None
    out = []
    for a in atts:
        if isinstance(a, dict):
            out.append({
                "name": a.get("name"),
                "source": a.get("source"),
                "sessionId": a.get("sessionId"),
            })
        else:
            try:
                out.append({
                    "name": getattr(a, "name", None),
                    "source": getattr(a, "source", None),
                    "sessionId": getattr(a, "sessionId", None),
                })
            except Exception:
                continue
    return out or None

def upsert_on_first_message(session_id: str, title: str) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or SETTINGS["chat_default_title"]),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row)
    save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)


def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)


def append_message(session_id: str, role: str, content: str, attachments: Optional[list[Any]] = None) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq,
        "sessionId": session_id,
        "role": role,
        "content": content,
        "createdAt": now_iso(),
    }
    norm_atts = _normalize_attachments(attachments)
    if norm_atts:
        msg["attachments"] = norm_atts

    data["messages"].append(msg)
    data["seq"] = seq
    _save_chat(session_id, data)
    return ChatMessageRow(
        id=seq,
        sessionId=session_id,
        role=role,
        content=content,
        createdAt=msg["createdAt"],
        attachments=norm_atts,
    )


def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1


def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted


def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    rows: List[ChatMessageRow] = []
    for m in data.get("messages", []):
        rows.append(ChatMessageRow(
            id=m["id"],
            sessionId=m["sessionId"],
            role=m["role"],
            content=m["content"],
            createdAt=m.get("createdAt"),
            attachments=m.get("attachments", []),
        ))
    return rows


def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    min_size = int(SETTINGS["chat_page_min_size"])
    max_size = int(SETTINGS["chat_page_max_size"])
    size = max(min_size, min(max_size, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag


def delete_batch(session_ids: List[str]) -> List[str]:
    for sid in session_ids:
        try:
            chat_path(sid).unlink(missing_ok=True)
        except Exception:
            pass

    for sid in session_ids:
        try:
            rag_delete_namespace(sid)
        except Exception:
            pass

    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids


def _save_chat(session_id: str, data: Dict[str, Any]):
    atomic_write(chat_path(session_id), data)


def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)


def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")


def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            if "attachments" in m and m["attachments"] is not None:
                m["attachments"] = _normalize_attachments(m["attachments"])
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(
        id=updated["id"],
        sessionId=updated["sessionId"],
        role=updated["role"],
        content=updated["content"],
        createdAt=updated.get("createdAt"),
        attachments=updated.get("attachments", []),
    )


__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary",
]

# ===== aimodel/file_read/store/default_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "brave",
  "brave_api_key": "",
  "brave_worker_url": "https://brave-proxy.localmind.workers.dev/brave",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "brave_api_base": "https://api.search.brave.com/res/v1/web/search",
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "web_fetch_timeout_sec": 4,
  "web_fetch_max_chars": 2400,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 4,
  "web_orch_total_char_budget": 1800,
  "web_orch_per_doc_char_budget": 600,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 4,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.3,
  "web_orch_js_retry_low_q": 0.18,
  "web_orch_js_retry_lowish_ratio": 0.6,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": 1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.4,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_orch_diversity": "=== Diversity & Adaptive Chunking ===",
  "web_orch_min_hosts": 3,
  "__comment_web_misc": "=== Misc flags ===",
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "retitle_llm_hard_prefix": "You generate a concise chat title.\nReturn ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). No trailing punctuation. Max 6 words.",
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe",
  "style_sys": "You are a helpful assistant. Always follow the user's explicit instructions carefully and exactly. Do not repeat yourself. Stay coherent and complete.",
  "style_patterns": {
    "talk_like": "\\btalk\\s+like\\s+(?P<style>[^.;\\n]+)",
    "respond_like": "\\brespond\\s+like\\s+(?P<style>[^.;\\n]+)",
    "be": "\\bbe\\s+(?P<style>[^.;\\n]+)",
    "from_now": "\\bfrom\\s+now\\s+on[, ]+\\s*(?P<style>[^.;\\n]+)"
  },
  "style_template": "You must talk like {style}. Stay in character but remain helpful and accurate. Follow the user's latest style instructions.",
  "router_decide_prompt": "You are a router deciding whether answering the text requires the public web.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n- When uncertain whether real-time state is required, prefer need=true.\n\nText:\n{text}\nJSON:"
}

# ===== aimodel/file_read/store/effective_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "brave",
  "brave_api_key": "BSAJsnvyOOOULDffIM8myC1IUk34u-d",
  "brave_worker_url": "",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "brave_api_base": "https://api.search.brave.com/res/v1/web/search",
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "web_fetch_timeout_sec": 4,
  "web_fetch_max_chars": 2400,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 4,
  "web_orch_total_char_budget": 1800,
  "web_orch_per_doc_char_budget": 600,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 4,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.3,
  "web_orch_js_retry_low_q": 0.18,
  "web_orch_js_retry_lowish_ratio": 0.6,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": 1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.4,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_orch_diversity": "=== Diversity & Adaptive Chunking ===",
  "web_orch_min_hosts": 3,
  "__comment_web_misc": "=== Misc flags ===",
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "retitle_llm_hard_prefix": "You generate a concise chat title.\nReturn ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). No trailing punctuation. Max 6 words.",
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe",
  "style_sys": "You are a helpful assistant. Always follow the user's explicit instructions carefully and exactly. Do not repeat yourself. Stay coherent and complete.",
  "style_patterns": {
    "talk_like": "\\btalk\\s+like\\s+(?P<style>[^.;\\n]+)",
    "respond_like": "\\brespond\\s+like\\s+(?P<style>[^.;\\n]+)",
    "be": "\\bbe\\s+(?P<style>[^.;\\n]+)",
    "from_now": "\\bfrom\\s+now\\s+on[, ]+\\s*(?P<style>[^.;\\n]+)"
  },
  "style_template": "You must talk like {style}. Stay in character but remain helpful and accurate. Follow the user's latest style instructions.",
  "router_decide_prompt": "You are a router deciding whether answering the text requires the public web.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n- When uncertain whether real-time state is required, prefer need=true.\n\nText:\n{text}\nJSON:",
  "brave_api_key_present": true
}

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/store/override_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "brave",
  "brave_api_key": "BSAJsnvyOOOULDffIM8myC1IUk34u-d",
  "brave_worker_url": "",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "brave_api_base": "https://api.search.brave.com/res/v1/web/search",
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "web_fetch_timeout_sec": 4,
  "web_fetch_max_chars": 2400,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 4,
  "web_orch_total_char_budget": 1800,
  "web_orch_per_doc_char_budget": 600,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 4,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.3,
  "web_orch_js_retry_low_q": 0.18,
  "web_orch_js_retry_lowish_ratio": 0.6,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": 1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.4,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_orch_diversity": "=== Diversity & Adaptive Chunking ===",
  "web_orch_min_hosts": 3,
  "__comment_web_misc": "=== Misc flags ===",
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "retitle_llm_hard_prefix": "You generate a concise chat title.\nReturn ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). No trailing punctuation. Max 6 words.",
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe",
  "style_sys": "You are a helpful assistant. Always follow the user's explicit instructions carefully and exactly. Do not repeat yourself. Stay coherent and complete.",
  "style_patterns": {
    "talk_like": "\\btalk\\s+like\\s+(?P<style>[^.;\\n]+)",
    "respond_like": "\\brespond\\s+like\\s+(?P<style>[^.;\\n]+)",
    "be": "\\bbe\\s+(?P<style>[^.;\\n]+)",
    "from_now": "\\bfrom\\s+now\\s+on[, ]+\\s*(?P<style>[^.;\\n]+)"
  },
  "style_template": "You must talk like {style}. Stay in character but remain helpful and accurate. Follow the user's latest style instructions.",
  "router_decide_prompt": "You are a router deciding whether answering the text requires the public web.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n- When uncertain whether real-time state is required, prefer need=true.\n\nText:\n{text}\nJSON:",
  "brave_api_key_present": true
}

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from ..runtime.model_runtime import current_model_info, get_llm

RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

STOP_STRINGS = ["</s>", "User:", "\nUser:"]

def strip_runjson(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    out, i = [], 0
    while True:
        start = s.find(RUNJSON_START, i)
        if start == -1:
            out.append(s[i:])
            break
        out.append(s[i:start])
        end = s.find(RUNJSON_END, start)
        if end == -1:
            break
        i = end + len(RUNJSON_END)
    return "".join(out).strip()

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def _first(obj: dict, keys: List[str]):
    for k in keys:
        if k in obj and obj[k] is not None:
            return obj[k]
    return None

def _sec_from_ms(v: Any) -> Optional[float]:
    try:
        x = float(v)
        return round(x / 1000.0, 6)
    except Exception:
        return None

def collect_engine_timings(llm: Any) -> Optional[Dict[str, Optional[float]]]:
    src = None
    try:
        if hasattr(llm, "get_last_timings") and callable(llm.get_last_timings):
            src = llm.get_last_timings()
        elif hasattr(llm, "get_timings") and callable(llm.get_timings):
            src = llm.get_timings()
        elif hasattr(llm, "timings"):
            t = llm.timings
            src = t() if callable(t) else t
        elif hasattr(llm, "perf"):
            p = llm.perf
            src = p() if callable(p) else p
        elif hasattr(llm, "stats"):
            s = llm.stats
            src = s() if callable(s) else s
        elif hasattr(llm, "get_stats") and callable(llm.get_stats):
            src = llm.get_stats()
    except Exception:
        src = None

    if not isinstance(src, dict):
        return None

    load_ms = _first(src, ["load_ms", "loadMs", "model_load_ms", "load_time_ms"])
    prompt_ms = _first(src, ["prompt_ms", "promptMs", "prompt_eval_ms", "prompt_time_ms", "prefill_ms"])
    eval_ms = _first(src, ["eval_ms", "evalMs", "decode_ms", "eval_time_ms"])
    prompt_n = _first(src, ["prompt_n", "promptN", "prompt_tokens", "n_prompt_tokens"])
    eval_n = _first(src, ["eval_n", "evalN", "eval_tokens", "n_eval_tokens"])

    out: Dict[str, Optional[float]] = {
        "loadSec": _sec_from_ms(load_ms),
        "promptSec": _sec_from_ms(prompt_ms),
        "evalSec": _sec_from_ms(eval_ms),
        "promptN": None,
        "evalN": None,
    }
    try:
        out["promptN"] = int(prompt_n) if prompt_n is not None else None
    except Exception:
        out["promptN"] = None
    try:
        out["evalN"] = int(eval_n) if eval_n is not None else None
    except Exception:
        out["evalN"] = None
    return out

def build_run_json(
    *,
    request_cfg: Dict[str, object],
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
    budget_view: Optional[dict] = None,
    extra_timings: Optional[dict] = None,
    error_text: Optional[str] = None,
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None
    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()
    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    timings_payload = dict(extra_timings or {})
    if "engine" not in timings_payload or timings_payload.get("engine") is None:
        timings_payload["engine"] = collect_engine_timings(llm)

    encode_t = float(input_tokens_est or 0.0)
    decode_t = float(out_tokens or 0.0)
    total_t = encode_t + decode_t
    model_queue_s = float(timings_payload.get("modelQueueSec") or 0.0)
    engine_prompt_s = float((timings_payload.get("engine") or {}).get("promptSec") or 0.0)
    encode_sec = model_queue_s or engine_prompt_s or 0.0
    decode_sec = float(gen_secs or 0.0)
    total_sec = float(t_end - t_start)
    encode_tps = (encode_t / encode_sec) if encode_sec > 0 else None
    decode_tps = (decode_t / decode_sec) if decode_sec > 0 else None
    overall_tps = (total_t / total_sec) if total_sec > 0 else None

    bv = budget_view or {}
    bv_break = (bv.get("breakdown") or {}) if isinstance(bv, dict) else {}
    web = (bv.get("web") or {}) if isinstance(bv, dict) else {}
    web_bd = (web.get("breakdown") or {}) if isinstance(web, dict) else {}
    rag = (bv.get("rag") or {}) if isinstance(bv, dict) else {}

    nctx = bv.get("modelCtx") or bv.get("n_ctx") or 0
    clamp = bv.get("clampMargin") or bv.get("clamp_margin") or 0
    inp_est = bv.get("inputTokensEst") or bv.get("input_tokens_est") or (input_tokens_est or 0)
    out_chosen = bv.get("outBudgetChosen") or bv.get("clamped_out_tokens") or 0
    out_actual = out_tokens
    out_shown = out_actual or out_chosen
    used_ctx = (inp_est or 0) + (out_shown or 0) + (clamp or 0)
    ctx_pct = (float(used_ctx) / float(nctx) * 100.0) if nctx else 0.0

    rag_delta = 0
    for k in ("ragTokensAdded", "blockTokens", "blockTokensApprox", "sessionOnlyTokensApprox"):
        v = rag.get(k)
        if isinstance(v, (int, float)) and v > 0:
            rag_delta = int(v)
            break
    rag_pct_of_input = int(round((rag_delta / inp_est) * 100)) if inp_est else 0

    web_pre = (
        web_bd.get("totalWebPreTtftSec")
        or ((web.get("elapsedSec") or 0) + (web.get("fetchElapsedSec") or 0) + (web.get("injectElapsedSec") or 0))
        or 0
    )

    pre_accounted = bv_break.get("preTtftAccountedSec")
    unattributed = (
        bv_break.get("unattributedTtftSec")
        if "unattributedTtftSec" in bv_break
        else (max(0.0, (ttft_ms / 1000.0) - float(pre_accounted))) if pre_accounted is not None else None
    )

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
            "budget": budget_view or {},
            "timings": timings_payload,
            "error": error_text or None,
        },
        "budget_view": (budget_view or {}),
        "_derived": {
            "context": {
                "modelCtx": nctx,
                "clampMargin": clamp,
                "inputTokensEst": inp_est,
                "outBudgetChosen": out_chosen,
                "outActual": out_actual,
                "outShown": out_shown,
                "usedCtx": used_ctx,
                "ctxPct": ctx_pct,
            },
            "rag": {
                "ragDelta": rag_delta,
                "ragPctOfInput": rag_pct_of_input,
            },
            "web": {
                "webPre": web_pre,
            },
            "timing": {
                "accountedPreTtftSec": pre_accounted,
                "unattributedPreTtftSec": unattributed,
                "preModelSec": timings_payload.get("preModelSec"),
                "modelQueueSec": timings_payload.get("modelQueueSec"),
                "genSec": gen_secs,
                "ttftSec": round((ttft_ms or 0) / 1000.0, 3),
            },
            "throughput": {
                "encodeTokPerSec": encode_tps,
                "decodeTokPerSec": decode_tps,
                "overallTokPerSec": overall_tps,
            },
        },
    }

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/utils/text.py =====

from __future__ import annotations
import re
from typing import Optional

def clean_ws(s: Optional[str]) -> str:
    return " ".join((s or "").split())

def strip_wrappers(text: str, *, trim_whitespace: bool, split_on_blank: bool, header_regex: Optional[str]) -> str:
    t = text or ""
    if trim_whitespace:
        t = t.strip()
    if not header_regex and not split_on_blank:
        return t
    head = t
    if split_on_blank:
        head = t.split("\n\n", 1)[0]
    if header_regex:
        try:
            rx = re.compile(header_regex)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/brave.py =====

from __future__ import annotations
from typing import List, Optional, Tuple, Dict, Any
import time, urllib.parse, hashlib

try:
    import httpx
except Exception:
    httpx = None

from ..core.settings import SETTINGS
from .provider import SearchHit
from .orchestrator_common import _host

_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}


def _cache_key(query: str, base: str, key_marker: str) -> str:
    q = (query or "").strip().lower()
    b = (base or "").strip().lower()
    m = (key_marker or "").strip().lower()
    return f"{q}||{b}||{m}"


def _cache_get(key: str) -> Optional[List[SearchHit]]:
    eff = SETTINGS.effective()
    ttl = int(eff["web_search_cache_ttl_sec"])
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > ttl:
        _CACHE.pop(key, None)
        return None
    return hits


def _cache_set(key: str, hits: List[SearchHit]) -> None:
    _CACHE[key] = (time.time(), hits)


def _set_hits_telemetry(tel: Dict[str, Any], all_hits: List[SearchHit], out: List[SearchHit]) -> None:
    tel["hits"] = {
        "total": len(all_hits),
        "returned": len(out),
        "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
    }


def _build_url(base: str, q: str, k: int) -> str:
    params = {"q": q, "count": str(max(1, k))}
    return f"{base}?{urllib.parse.urlencode(params)}"


def _num(x: Any) -> Optional[int]:
    try:
        return int(str(x))
    except Exception:
        return None


class BraveProvider:
    async def search(
        self,
        query: str,
        k: int = 3,
        telemetry: Optional[Dict[str, Any]] = None,
        xid: Optional[str] = None
    ) -> List[SearchHit]:
        t_start = time.perf_counter()
        eff = SETTINGS.effective()
        q_norm = (query or "").strip()
        if not q_norm:
            if telemetry is not None:
                telemetry.update({
                    "query": q_norm,
                    "k": int(k),
                    "supersetK": int(k),
                    "elapsedSec": round(time.perf_counter() - t_start, 6),
                    "cache": {"hit": False}
                })
            return []

        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        tel: Dict[str, Any] = {"query": q_norm, "k": int(k), "supersetK": superset_k}

        brave_base = (eff.get("brave_api_base") or "https://api.search.brave.com/res/v1/web/search").strip()
        # Global key from SETTINGS (admin-configured)
        key = (SETTINGS.get("brave_api_key", "") or "").strip()
        key_hash = hashlib.sha1(key.encode("utf-8")).hexdigest()[:8] if key else "nokey"
        ckey = _cache_key(q_norm, brave_base, key_hash)

        t_cache = time.perf_counter()
        cached = _cache_get(ckey)
        tel["cache"] = {"hit": cached is not None, "elapsedSec": round(time.perf_counter() - t_cache, 6)}
        if cached is not None:
            print("BRAVE cache hit", {"query": q_norm, "k": k, "base": brave_base, "keyHash": key_hash})
            out = cached[:k]
            _set_hits_telemetry(tel, cached, out)
            tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
            if telemetry is not None:
                telemetry.update(tel)
            return out

        hits: List[SearchHit] = []
        prov_info: Dict[str, Any] = {"available": httpx is not None}
        t_fetch = time.perf_counter()

        if httpx is None:
            prov_info["errorType"] = "ProviderUnavailable"
            prov_info["errorMsg"] = "httpx not installed"
        else:
            print("BRAVE cfg", {"base": brave_base, "byok": False, "keyHash": key_hash})
            url = _build_url(brave_base, q_norm, superset_k)
            headers: Dict[str, str] = {}
            if key:
                headers["X-Subscription-Token"] = key
            print("BRAVE headers", {"hasKey": bool(key)})

            if not key:
                prov_info["errorType"] = "Unauthorized"
                prov_info["errorMsg"] = "No Brave API key configured in settings"
            else:
                try:
                    timeout = float(eff.get("web_fetch_timeout_sec", 8))
                    print("BRAVE call", {"url": url, "timeoutSec": timeout})
                    async with httpx.AsyncClient(timeout=timeout) as client:
                        r = await client.get(url, headers=headers)
                        print("BRAVE resp", {
                            "status": r.status_code,
                            "len": len(r.text or ""),
                            "preview": (r.text[:200] if r.text else ""),
                        })
                        r.raise_for_status()
                        data = r.json()
                    rate = {
                        "minute": {
                            "limit": _num(r.headers.get("X-RateLimit-Limit-Minute")),
                            "remaining": _num(r.headers.get("X-RateLimit-Remaining-Minute")),
                            "resetMs": _num(r.headers.get("X-RateLimit-Reset-Minute")),
                        },
                        "day": {
                            "limit": _num(r.headers.get("X-RateLimit-Limit-Day")),
                            "remaining": _num(r.headers.get("X-RateLimit-Remaining-Day")),
                            "resetMs": _num(r.headers.get("X-RateLimit-Reset-Day")),
                        },
                    }
                    prov_info["rate"] = rate
                    web = (data or {}).get("web") or {}
                    results = web.get("results") or []
                    for i, item in enumerate(results[:superset_k], start=1):
                        title = (item.get("title") or "").strip()
                        url_i = (item.get("url") or "").strip()
                        snippet = (item.get("description") or "").strip() or None
                        if not url_i:
                            continue
                        hits.append(SearchHit(title=title or url_i, url=url_i, snippet=snippet, rank=i))
                    prov_info["errorType"] = None
                    prov_info["errorMsg"] = None
                except httpx.HTTPStatusError as e:
                    print("BRAVE HTTP error", {"status": r.status_code, "body": (r.text or "")[:200]})
                    prov_info["errorType"] = "HTTPStatusError"
                    prov_info["errorMsg"] = f"{e.response.status_code} {e.response.text}"
                except Exception as e:
                    print("BRAVE exception", {"type": type(e).__name__, "msg": str(e)})
                    prov_info["errorType"] = type(e).__name__
                    prov_info["errorMsg"] = str(e)

        prov_info["elapsedSec"] = round(time.perf_counter() - t_fetch, 6)
        tel["provider"] = prov_info

        _cache_set(ckey, hits)
        out = hits[:k]
        _set_hits_telemetry(tel, hits, out)
        tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
        if telemetry is not None:
            telemetry.update(tel)
        return out

# ===== aimodel/file_read/web/fetch.py =====

# aimodel/file_read/web/fetch.py
from __future__ import annotations
import asyncio
from typing import Tuple, List, Optional, Dict, Any
import time
import httpx
from urllib.parse import urlparse

try:
    from readability import Document
except Exception:
    Document = None  # optional

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None  # optional

try:
    from selectolax.parser import HTMLParser
except Exception:
    HTMLParser = None  # optional

from ..core.settings import SETTINGS


def _req(key: str):
    return SETTINGS[key]

def _ua() -> str:
    return str(_req("web_fetch_user_agent"))

def _timeout() -> float:
    return float(_req("web_fetch_timeout_sec"))

def _max_chars() -> int:
    return int(_req("web_fetch_max_chars"))

def _max_bytes() -> int:
    return int(_req("web_fetch_max_bytes"))

def _max_parallel() -> int:
    return max(1, int(_req("web_fetch_max_parallel")))


# -------------------- Adaptive cooldown (generic, no host hardcoding) --------------------
# host -> (fail_count, cooldown_until_ts)
_BAD_HOSTS: Dict[str, Tuple[int, float]] = {}

def _now() -> float:
    return time.time()

def _host_of(u: str) -> str:
    try:
        return (urlparse(u).hostname or "").lower()
    except Exception:
        return ""

def _cooldown_secs(fails: int) -> float:
    # 15m, 30m, 60m, ... capped at 24h
    base = 15 * 60.0
    cap = 24 * 60 * 60.0
    return min(cap, base * (2 ** max(0, fails - 1)))

def _mark_bad(host: str) -> None:
    if not host:
        return
    fails, until = _BAD_HOSTS.get(host, (0, 0.0))
    fails += 1
    _BAD_HOSTS[host] = (fails, _now() + _cooldown_secs(fails))

def _mark_good(host: str) -> None:
    if not host:
        return
    if host in _BAD_HOSTS:
        fails, until = _BAD_HOSTS[host]
        fails = max(0, fails - 1)
        if fails == 0:
            _BAD_HOSTS.pop(host, None)
        else:
            _BAD_HOSTS[host] = (fails, _now() + _cooldown_secs(fails))

def _is_on_cooldown(host: str) -> bool:
    ent = _BAD_HOSTS.get(host)
    return bool(ent and ent[1] > _now())
# ----------------------------------------------------------------------------------------


async def _read_capped_bytes(resp: httpx.Response, cap_bytes: int) -> bytes:
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    html = raw_html or ""
    # Try readability first (often best for article-like pages)
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
        except Exception:
            pass
    # Try selectolax (fast, robust)
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = tree.body.text(separator=" ", strip=True) if tree.body else tree.text(separator=" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    # Fallback to BeautifulSoup full parse
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    # Last resort: return raw html (will be trimmed by char cap)
    return html


async def fetch_clean(
    url: str,
    timeout_s: Optional[float] = None,
    max_chars: Optional[int] = None,
    max_bytes: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
) -> Tuple[str, int, str]:
    t0 = time.perf_counter()
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout, headers=headers) as client:
        r = await client.get(url)
        r.raise_for_status()
        ctype = (r.headers.get("content-type") or "").lower()

        raw_bytes = await _read_capped_bytes(r, cap_bytes)
        enc = r.encoding or "utf-8"
        raw_text = raw_bytes.decode(enc, errors="ignore")
        txt = _extract_text_from_html(raw_text, str(r.url))
        txt = (txt or "").strip().replace("\r", "")
        if len(txt) > cap_chars:
            txt = txt[:cap_chars]

        # Generic usefulness test: skip non-HTML or extremely short bodies
        MIN_USEFUL_CHARS = 80
        host_final = _host_of(str(r.url))
        if ("text/html" not in ctype) or (len(txt) < MIN_USEFUL_CHARS):
            _mark_bad(host_final)
        else:
            _mark_good(host_final)

        if telemetry is not None:
            telemetry.update({
                "reqUrl": url,
                "finalUrl": str(r.url),
                "status": int(r.status_code),
                "elapsedSec": round(time.perf_counter() - t0, 6),
                "bytes": len(raw_bytes),
                "chars": len(txt),
                "timeoutSec": timeout,
                "capBytes": cap_bytes,
                "capChars": cap_chars,
                "contentType": ctype,
                "cooldownFails": _BAD_HOSTS.get(host_final, (0, 0.0))[0] if host_final in _BAD_HOSTS else 0,
            })
        return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: List[str],
    per_timeout_s: Optional[float] = None,
    cap_chars: Optional[int] = None,
    cap_bytes: Optional[int] = None,
    max_parallel: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
):
    t_total0 = time.perf_counter()
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))
    tel_items: List[Dict[str, Any]] = []

    async def _one(u: str):
        item_tel: Dict[str, Any] = {"reqUrl": u}
        host = _host_of(u)

        # Skip hosts currently on adaptive cooldown (generic, no lists)
        if _is_on_cooldown(host):
            item_tel.update({
                "ok": False,
                "skipped": True,
                "skipReason": "cooldown",
                "host": host,
            })
            tel_items.append(item_tel)
            return u, None

        t0 = time.perf_counter()
        async with sem:
            try:
                res = await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                    telemetry=item_tel,
                )
                item_tel.setdefault("elapsedSec", round(time.perf_counter() - t0, 6))
                item_tel["ok"] = True
                item_tel["host"] = host
                tel_items.append(item_tel)
                return u, res
            except Exception as e:
                _mark_bad(host)  # network/HTTP error counts as a fail
                item_tel.update({
                    "ok": False,
                    "errorType": type(e).__name__,
                    "errorMsg": str(e),
                    "elapsedSec": round(time.perf_counter() - t0, 6),
                    "timeoutSec": (float(per_timeout_s) if per_timeout_s is not None else _timeout()),
                    "capBytes": (int(cap_bytes) if cap_bytes is not None else _max_bytes()),
                    "capChars": (int(cap_chars) if cap_chars is not None else _max_chars()),
                    "host": host,
                })
                tel_items.append(item_tel)
                return u, None

    tasks = [_one(u) for u in urls]
    results = await asyncio.gather(*tasks)

    if telemetry is not None:
        ok_cnt = sum(1 for it in tel_items if it.get("ok"))
        telemetry.update({
            "totalSec": round(time.perf_counter() - t_total0, 6),
            "requested": len(urls),
            "ok": ok_cnt,
            "miss": len(urls) - ok_cnt,
            "items": tel_items,
            "settings": {
                "userAgent": _ua(),
                "defaultTimeoutSec": _timeout(),
                "defaultCapChars": _max_chars(),
                "defaultCapBytes": _max_bytes(),
                "maxParallel": _max_parallel() if max_parallel is None else int(max_parallel),
            },
        })

    return results

# ===== aimodel/file_read/web/orchestrator.py =====

from __future__ import annotations
from typing import List, Tuple, Optional, Dict, Any
import time
from collections import defaultdict

from .brave import BraveProvider
from .provider import SearchHit
from .orchestrator_common import (
    _as_int, _as_float, _as_bool, _as_str,
    condense_doc, content_quality_score,
    _dedupe_by_host, score_hit, _head_tail,
    _fetch_round, _host,   # NOTE: make sure _host is imported
)
from ..core.request_ctx import get_x_id


async def build_web_block(query: str, k: Optional[int] = None, per_url_timeout_s: Optional[float] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    tel: Dict[str, Any] = {"query": (query or "").strip()}
    cfg_k               = (int(k) if k is not None else _as_int("web_orch_default_k"))
    total_char_budget   = _as_int("web_orch_total_char_budget")
    per_doc_budget      = _as_int("web_orch_per_doc_char_budget")
    max_parallel        = _as_int("web_orch_max_parallel_fetch")
    overfetch_factor    = _as_float("web_orch_overfetch_factor")
    overfetch_min_extra = _as_int("web_orch_overfetch_min_extra")
    enable_js_retry     = _as_bool("web_orch_enable_js_retry")
    js_avg_q_thresh     = _as_float("web_orch_js_retry_avg_q")
    js_low_q_thresh     = _as_float("web_orch_js_retry_low_q")
    js_lowish_ratio     = _as_float("web_orch_js_retry_lowish_ratio")
    js_timeout_add      = _as_float("web_orch_js_retry_timeout_add")
    js_timeout_cap      = _as_float("web_orch_js_retry_timeout_cap")
    js_parallel_delta   = _as_int("web_orch_js_retry_parallel_delta")
    js_min_parallel     = _as_int("web_orch_js_retry_min_parallel")
    header_tpl          = _as_str("web_block_header")
    sep_str             = _as_str("web_orch_block_separator")
    min_chunk_after     = _as_int("web_orch_min_chunk_after_shrink")
    min_block_reserve   = _as_int("web_orch_min_block_reserve")
    per_timeout = (float(per_url_timeout_s) if per_url_timeout_s is not None else _as_float("web_fetch_timeout_sec"))
    start_time = time.perf_counter()

    provider = BraveProvider()
    # widen search result pool a bit for better host variety
    overfetch = max(cfg_k + overfetch_min_extra, int(round(cfg_k * overfetch_factor)))
    tel["search"] = {"requestedK": cfg_k, "overfetch": overfetch}
    t0 = time.perf_counter()
    try:
        hits: List[SearchHit] = await provider.search(query, k=overfetch, telemetry=tel["search"], xid=get_x_id())
    except Exception as e:
        tel["error"] = {"stage": "search", "type": type(e).__name__, "msg": str(e)}
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) due to search error:", tel["error"])
        return None, tel
    tel["search"]["elapsedSecTotal"] = round(time.perf_counter() - t0, 6)

    if not hits:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) — no hits")
        return None, tel

    # score & keep uniques by URL
    seen_urls = set()
    scored: List[Tuple[int, SearchHit]] = []
    for h in hits:
        u = (h.url or "").strip()
        if not u or u in seen_urls:
            continue
        seen_urls.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
    tel["scoring"] = {"inputHits": len(hits), "scored": len(scored)}

    if not scored:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) — no scored hits")
        return None, tel

    # IMPORTANT: dedupe by host to a PREFETCH set (larger than k) to increase variety
    prefetch = max(cfg_k * 2, cfg_k + 6)
    top_hits = _dedupe_by_host(scored, prefetch)
    tel["scoring"]["picked"] = len(top_hits)
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]

    # fetch round 1
    t_f = time.perf_counter()
    tel["fetch1"] = {}
    results = await _fetch_round(
        urls, meta, per_url_timeout_s=per_timeout, max_parallel=max_parallel, use_js=False, telemetry=tel["fetch1"]
    )
    tel["fetch1"]["roundSec"] = round(time.perf_counter() - t_f, 6)

    texts: List[Tuple[str, str, str]] = []
    quality_scores: List[float] = []
    for original_url, res in results:
        if not res:
            continue
        final_url, status, text = res
        title = next((t for (t, u) in meta if u == original_url), final_url)
        qscore = content_quality_score(text or "")
        quality_scores.append(qscore)
        if text:
            texts.append((title, final_url, text))
    tel["fetch1"]["docs"] = {"ok": len(texts), "qAvg": (sum(quality_scores)/len(quality_scores) if quality_scores else 0.0)}

    # optional JS retry
    try_js = False
    if enable_js_retry and quality_scores:
        avg_q = sum(quality_scores) / len(quality_scores)
        lowish = sum(1 for q in quality_scores if q < js_low_q_thresh)
        if avg_q < js_avg_q_thresh or (lowish / max(1, len(quality_scores))) >= js_lowish_ratio:
            try_js = True
        tel["jsRetry"] = {
            "considered": True, "triggered": try_js,
            "avgQ": round(avg_q, 4),
            "lowishRatio": round((lowish / max(1, len(quality_scores))) * 1.0, 4),
            "thresholds": {"avg": js_avg_q_thresh, "low": js_low_q_thresh, "ratio": js_lowish_ratio},
        }
    else:
        tel["jsRetry"] = {"considered": bool(enable_js_retry), "triggered": False}

    if try_js:
        js_timeout   = min(per_timeout + js_timeout_add, js_timeout_cap)
        js_parallel  = max(js_min_parallel, max_parallel + js_parallel_delta)
        tel["fetch2"] = {"timeoutSec": js_timeout, "maxParallel": js_parallel}
        results_js = await _fetch_round(
            urls, meta, per_url_timeout_s=js_timeout, max_parallel=js_parallel, use_js=True, telemetry=tel["fetch2"]
        )
        texts_js: List[Tuple[str, str, str]] = []
        for original_url, res in results_js:
            if not res:
                continue
            final_url, status, text = res
            title = next((t for (t, u) in meta if u == original_url), final_url)
            if text:
                texts_js.append((title, final_url, text))
        if texts_js:
            texts = texts_js

    if not texts:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) — no fetched texts")
        return None, tel

    # sort by content quality (generic)
    texts.sort(key=lambda t: content_quality_score(t[2]), reverse=True)

    # ====== PER-HOST QUOTA ASSEMBLY ======
    # group fetched docs by host
    by_host: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
    for title, url, text in texts:
        by_host[_host(url)].append((title, url, text))

    # keep each host's best doc first
    for h in by_host:
        by_host[h].sort(key=lambda x: content_quality_score(x[2]), reverse=True)

    # order hosts by the strength of their best doc
    hosts_ordered = sorted(by_host.keys(), key=lambda h: content_quality_score(by_host[h][0][2]), reverse=True)

    header = header_tpl.format(query=query)
    sep = sep_str
    available = max(min_block_reserve, total_char_budget - len(header) - len(sep))

    # compute a fair per-host quota so no site can hog the whole block
    min_hosts = max(1, min(_as_int("web_orch_min_hosts"), len(hosts_ordered))) if "web_orch_min_hosts" in globals() or "web_orch_min_hosts" in locals() else 3
    # equal-share starting point; clamp by per_doc_budget
    per_host_quota = max(min_chunk_after * 2, available // max(min_hosts, cfg_k))
    per_host_quota = min(per_host_quota, per_doc_budget)

    block_parts: List[str] = []
    used = 0
    included_hosts: List[str] = []

    # pass 1: guarantee at least one chunk per top hosts within their quota
    for h in hosts_ordered:
        title, url, text = by_host[h][0]
        chunk = condense_doc(title, url, text, max_chars=per_host_quota)
        sep_len = len(sep) if block_parts else 0
        if used + sep_len + len(chunk) > available:
            rem = available - used - sep_len
            if rem > min_chunk_after:
                chunk = _head_tail(chunk, rem)
            else:
                break
        block_parts.append(chunk)
        included_hosts.append(h)
        used += sep_len + len(chunk)
        if len(included_hosts) >= min_hosts and used >= int(available * 0.66):
            # decent diversity; move on to optional extras
            break

    # pass 2: fill remaining space with next-best docs across hosts (still capped by per_doc_budget)
    # build a round-robin of second-best, third-best, ...
    layer = 1
    while used < available:
        added_any = False
        for h in hosts_ordered:
            if layer >= len(by_host[h]):
                continue
            title, url, text = by_host[h][layer]
            sep_len = len(sep) if block_parts else 0
            chunk = condense_doc(title, url, text, max_chars=per_doc_budget)
            if used + sep_len + len(chunk) > available:
                rem = available - used - sep_len
                if rem <= min_chunk_after:
                    continue
                chunk = _head_tail(chunk, rem)
                if used + sep_len + len(chunk) > available:
                    continue
            block_parts.append(chunk)
            used += sep_len + len(chunk)
            added_any = True
            if used >= available:
                break
        if not added_any:
            break
        layer += 1
    # ====== /PER-HOST QUOTA ASSEMBLY ======

    body = sep.join(block_parts)
    block = f"{header}{sep}{body}" if body else header

    tel["assembly"] = {
        "chunksPicked": len(block_parts),
        "chars": len(block),
        "available": available,
        "headerChars": len(header),
        "hostsIncluded": len(included_hosts),
        "perHostQuota": per_host_quota,
    }
    tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)

    # Always print what is being sent
    print("[web-block] -------- BEGIN --------")
    print(block)
    print("[web-block] --------  END  --------")
    try:
        srcs = [{"title": t, "url": u} for (t, u, _) in texts[:10]]
        print("[web-block] sources:", srcs)
    except Exception as _e:
        print("[web-block] sources: <unavailable>", str(_e))

    return block, tel

# ===== aimodel/file_read/web/orchestrator_common.py =====

# aimodel/file_read/web/orchestrator_common.py
from __future__ import annotations
from typing import List, Tuple, Optional, Dict, Any
from urllib.parse import urlparse
import re
from ..utils.text import clean_ws
from ..core.settings import SETTINGS
from .provider import SearchHit
from .fetch import fetch_many

def _req(key: str):
    return SETTINGS[key]

def _as_int(key: str) -> int: return int(_req(key))
def _as_float(key: str) -> float: return float(_req(key))
def _as_bool(key: str) -> bool: return bool(_req(key))
def _as_str(key: str) -> str:
    v = _req(key)
    return "" if v is None else str(v)

def _host(url: str) -> str:
    h = (urlparse(url).hostname or "").lower()
    pref = _as_str("web_orch_www_prefix")
    return h[len(pref):] if pref and h.startswith(pref) else h

def _tokens(s: str) -> List[str]:
    return re.findall(r"\w+", (s or "").lower())

def _head_tail(text: str, max_chars: int) -> str:
    t = text or ""
    if max_chars <= 0 or len(t) <= max_chars:
        return clean_ws(t)

    ellipsis = _as_str("web_orch_ellipsis")
    # 40% head, 20% middle, 40% tail (generic)
    reserve = len(ellipsis) * 2
    avail = max(0, max_chars - reserve)
    if avail <= 0:
        return clean_ws(t[:max_chars])

    head_len = max(1, int(avail * 0.4))
    mid_len  = max(1, int(avail * 0.2))
    tail_len = max(1, avail - head_len - mid_len)

    n = len(t)
    # head
    h0, h1 = 0, min(head_len, n)
    head = t[h0:h1]

    # middle (centered)
    m0 = max(0, (n // 2) - (mid_len // 2))
    m1 = min(n, m0 + mid_len)
    # ensure middle starts after head if they overlap heavily
    if m0 < h1 and (h1 + mid_len) <= n:
        m0, m1 = h1, min(n, h1 + mid_len)
    mid = t[m0:m1]

    # tail
    t0 = max(m1, n - tail_len)  # avoid overlapping mid/tail
    tail = t[t0:n] if tail_len > 0 else ""

    return clean_ws(head + ellipsis + mid + ellipsis + tail)

def condense_doc(title: str, url: str, text: str, *, max_chars: int) -> str:
    body = _head_tail(text or "", max_chars)
    safe_title = clean_ws(title or url)
    bullet = _as_str("web_orch_bullet_prefix") or "- "
    indent = _as_str("web_orch_indent_prefix") or "  "
    return f"{bullet}{safe_title}\n{indent}{url}\n{indent}{body}"

def score_hit(hit: SearchHit, query: str) -> int:
    w_exact = _as_int("web_orch_score_w_exact")
    w_substr = _as_int("web_orch_score_w_substr")
    w_title_full = _as_int("web_orch_score_w_title_full")
    w_title_part = _as_int("web_orch_score_w_title_part")
    w_snip_touch = _as_int("web_orch_score_w_snip_touch")
    score = 0
    q = (query or "").strip().lower()
    title = (hit.title or "").strip()
    snippet = (hit.snippet or "").strip()
    title_l = title.lower()
    snip_l = snippet.lower()
    if q:
        if title_l == q:
            score += w_exact
        elif q in title_l:
            score += w_substr
    qtoks = _tokens(q)
    if qtoks:
        cov_title = sum(1 for t in qtoks if t in title_l)
        if cov_title == len(qtoks):
            score += w_title_full
        elif cov_title > 0:
            score += w_title_part
        if any(t in snip_l for t in qtoks):
            score += w_snip_touch
    return score

def _type_ratio(text: str, sub: str) -> float:
    if not text:
        return 1.0
    cnt = text.lower().count(sub)
    return float(cnt) / max(1, len(text))

def content_quality_score(text: str) -> float:
    if not text:
        return 0.0
    t = text.strip()
    n = len(t)
    len_div = _as_float("web_orch_q_len_norm_divisor")
    w_len = _as_float("web_orch_q_len_weight")
    w_div = _as_float("web_orch_q_diversity_weight")
    length_score = min(1.0, n / len_div) if len_div > 0 else 0.0
    toks = _tokens(t)
    if not toks:
        return 0.1 * length_score
    uniq = len(set(toks))
    diversity = uniq / max(1.0, float(len(toks)))
    pen = 0.0
    for rule in _req("web_orch_q_penalties"):
        token = str(rule.get("token") or "")
        mult = float(rule.get("mult") or 0.0)
        cap = float(rule.get("cap") or 1.0)
        pen += min(cap, _type_ratio(t, token) * mult)
    raw = (w_len * length_score) + (w_div * diversity) - pen
    return max(0.0, min(1.0, raw))

def _dedupe_by_host(scored_hits: List[Tuple[int, SearchHit]], k: int) -> List[SearchHit]:
    picked: List[SearchHit] = []
    seen_hosts = set()
    for s, h in sorted(scored_hits, key=lambda x: x[0], reverse=True):
        u = (h.url or "").strip()
        if not u:
            continue
        host = _host(u)
        if host in seen_hosts:
            continue
        seen_hosts.add(host)
        picked.append(h)
        if len(picked) >= k:
            break
    return picked

async def _fetch_round(
    urls: List[str],
    meta: List[Tuple[str, str]],
    per_url_timeout_s: float,
    max_parallel: int,
    use_js: bool = False,
    telemetry: Optional[Dict[str, Any]] = None,
) -> List[Tuple[str, Optional[Tuple[str, int, str]]]]:
    fetch_fn = fetch_many
    if use_js:
        try:
            from . import fetch as _fetch_mod
            fetch_fn = getattr(_fetch_mod, "fetch_many_js", fetch_many)
        except Exception:
            fetch_fn = fetch_many
    cap_mult = _as_float("web_orch_fetch_cap_multiplier")
    per_doc_budget = _as_int("web_orch_per_doc_char_budget")
    fetch_max_chars = _as_int("web_fetch_max_chars")
    per_doc_cap = min(int(per_doc_budget * cap_mult), fetch_max_chars)

    results = await fetch_fn(
        urls,
        per_timeout_s=per_url_timeout_s,
        cap_chars=per_doc_cap,
        max_parallel=max_parallel,
        telemetry=telemetry,
    )
    return results

# ===== aimodel/file_read/web/provider.py =====

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SearchHit:
    title: str
    url: str
    snippet: Optional[str] = None
    rank: int = 0

class SearchProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        raise NotImplementedError

# ===== aimodel/file_read/web/query_summarizer.py =====

# aimodel/file_read/web/query_summarizer.py
from __future__ import annotations
from typing import Any, Iterable, Dict, Tuple
import re, time
from ..core.settings import SETTINGS
from ..utils.streaming import safe_token_count_messages

def _tokens(s: str) -> set[str]:
    return set(re.findall(r"\w+", (s or "").lower()))

def _as_list(v) -> list:
    if v is None:
        return []
    if isinstance(v, (list, tuple)):
        return list(v)
    return [v]

def summarize_query(llm: Any, user_text: str) -> Tuple[str, Dict[str, Any]]:
    telemetry: Dict[str, Any] = {}
    txt = (user_text or "").strip()

    bypass_enabled = SETTINGS.get("query_sum_bypass_short_enabled")
    short_chars = SETTINGS.get("query_sum_short_max_chars")
    short_words = SETTINGS.get("query_sum_short_max_words")
    if bypass_enabled is True and isinstance(short_chars, int) and isinstance(short_words, int):
        if len(txt) <= short_chars and len(txt.split()) <= short_words:
            telemetry.update({"bypass": True})
            return txt, telemetry
    telemetry.update({"bypass": False})

    prompt = SETTINGS.get("query_sum_prompt")
    if isinstance(prompt, str) and "{text}" in prompt:
        params = {}
        max_tokens = SETTINGS.get("query_sum_max_tokens")
        if isinstance(max_tokens, int):
            params["max_tokens"] = max_tokens
        temperature = SETTINGS.get("query_sum_temperature")
        if isinstance(temperature, (int, float)):
            params["temperature"] = float(temperature)
        top_p = SETTINGS.get("query_sum_top_p")
        if isinstance(top_p, (int, float)):
            params["top_p"] = float(top_p)
        stops = _as_list(SETTINGS.get("query_sum_stop"))
        if stops:
            params["stop"] = [str(s) for s in stops if isinstance(s, str)]
        params["stream"] = False

        t_start = time.perf_counter()
        out = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt.format(text=txt)}],
            **params,
        )
        elapsed = time.perf_counter() - t_start
        result = (out["choices"][0]["message"]["content"] or "").strip()
        in_tokens = safe_token_count_messages(llm, [{"role": "user", "content": prompt.format(text=txt)}]) or 0
        out_tokens = safe_token_count_messages(llm, [{"role": "assistant", "content": result}]) or 0
        telemetry.update({
            "elapsedSec": round(elapsed, 4),
            "inputTokens": in_tokens,
            "outputTokens": out_tokens,
        })
    else:
        return txt, telemetry

    overlap_enabled = SETTINGS.get("query_sum_overlap_check_enabled")
    j_min = SETTINGS.get("query_sum_overlap_jaccard_min")
    if overlap_enabled is True and isinstance(j_min, (int, float)):
        src_toks = _tokens(txt)
        out_toks = _tokens(result)
        if not result or not out_toks:
            telemetry.update({"overlapRetained": True, "overlapScore": 0.0})
            return txt, telemetry
        jaccard = (len(src_toks & out_toks) / len(src_toks | out_toks)) if (src_toks or out_toks) else 1.0
        telemetry.update({"overlapScore": round(jaccard, 4)})
        if jaccard < float(j_min):
            telemetry.update({"overlapRetained": True})
            return txt, telemetry
        telemetry.update({"overlapRetained": False})
        return result, telemetry

    return result, telemetry

# ===== aimodel/file_read/web/router_ai.py =====

from __future__ import annotations
from typing import Tuple, Optional, Any, Dict
import json, re, time
from ..core.settings import SETTINGS
from ..utils.streaming import safe_token_count_messages
from ..utils.text import strip_wrappers as _strip_wrappers

def _force_json(s: str) -> dict:
    if not s:
        return {}
    raw = s.strip()
    try:
        cf = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", raw, re.IGNORECASE)
        if cf:
            raw = cf.group(1).strip()
    except Exception:
        pass
    try:
        v = json.loads(raw)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass
    try:
        m = None
        for m in re.finditer(r"\{[^{}]*\"need\"\s*:\s*(?:true|false|\"true\"|\"false\")[^{}]*\}", raw, re.IGNORECASE):
            pass
        if m:
            frag = m.group(0)
            v = json.loads(frag)
            return v if isinstance(v, dict) else {}
    except Exception:
        pass
    try:
        last = None
        for last in re.finditer(r"\{[\s\S]*\}", raw):
            pass
        if last:
            frag = last.group(0)
            v = json.loads(frag)
            return v if isinstance(v, dict) else {}
    except Exception:
        pass
    return {}

def decide_web(llm: Any, user_text: str) -> Tuple[bool, Optional[str], Dict[str, Any]]:
    telemetry: Dict[str, Any] = {}
    try:
        if not user_text or not user_text.strip():
            return (False, None, telemetry)
        t_start = time.perf_counter()
        t_raw = user_text.strip()
        if SETTINGS.get("router_strip_wrappers_enabled") is True:
            core_text = _strip_wrappers(
                t_raw,
                trim_whitespace=SETTINGS.get("router_trim_whitespace") is True,
                split_on_blank=SETTINGS.get("router_strip_split_on_blank") is True,
                header_regex=SETTINGS.get("router_strip_header_regex"),
            )
        else:
            core_text = t_raw.strip() if SETTINGS.get("router_trim_whitespace") is True else t_raw
        prompt_tpl = SETTINGS.get("router_decide_prompt")
        if not isinstance(prompt_tpl, str) or not prompt_tpl.strip():
            return (False, None, telemetry)
        the_prompt = _safe_prompt_format(prompt_tpl, text=core_text)
        params = {
            "max_tokens": SETTINGS.get("router_decide_max_tokens"),
            "temperature": SETTINGS.get("router_decide_temperature"),
            "top_p": SETTINGS.get("router_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}
        raw_out_obj = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw_out_obj.get("choices", [{}])[0]
                                  .get("message", {})
                                  .get("content") or "").strip()
        telemetry["rawRouterOut"] = text_out[:2000]
        data = _force_json(text_out) or {}
        need_val = data.get("need", None)
        if isinstance(need_val, str):
            nv = need_val.strip().lower()
            if nv in ("true", "yes", "y", "1"):
                need_val = True
            elif nv in ("false", "no", "n", "0"):
                need_val = False
        if isinstance(need_val, bool):
            need = need_val
            parsed_ok = True
        else:
            parsed_ok = False
            need_default = SETTINGS.get("router_default_need_when_invalid")
            need = bool(need_default) if isinstance(need_default, bool) else False
        query_field = data.get("query", "")
        try:
            if SETTINGS.get("router_strip_wrappers_enabled") is True:
                query = _strip_wrappers(
                    str(query_field or "").strip(),
                    trim_whitespace=SETTINGS.get("router_trim_whitespace") is True,
                    split_on_blank=SETTINGS.get("router_strip_split_on_blank") is True,
                    header_regex=SETTINGS.get("router_strip_header_regex"),
                )
            else:
                query = str(query_field or "").strip()
        except Exception:
            query = ""
        if not need:
            query = None
        t_elapsed = time.perf_counter() - t_start
        in_tokens = safe_token_count_messages(llm, [{"role": "user", "content": the_prompt}]) or 0
        out_tokens = safe_token_count_messages(llm, [{"role": "assistant", "content": text_out}]) or 0
        telemetry.update({
            "needed": bool(need),
            "routerQuery": query if need else None,
            "elapsedSec": round(t_elapsed, 4),
            "inputTokens": in_tokens,
            "outputTokens": out_tokens,
            "parsedOk": parsed_ok,
        })
        return (need, query, telemetry)
    except Exception:
        return (False, None, telemetry)

async def decide_web_and_fetch(llm: Any, user_text: str, *, k: int = 3) -> Tuple[Optional[str], Dict[str, Any]]:
    telemetry: Dict[str, Any] = {}
    need, proposed_q, tel_decide = decide_web(llm, (user_text or "").strip())
    telemetry.update(tel_decide)
    if not need:
        return None, telemetry
    from .query_summarizer import summarize_query
    from .orchestrator import build_web_block
    if SETTINGS.get("router_strip_wrappers_enabled") is True:
        base_query = _strip_wrappers(
            (proposed_q or user_text).strip(),
            trim_whitespace=SETTINGS.get("router_trim_whitespace") is True,
            split_on_blank=SETTINGS.get("router_strip_split_on_blank") is True,
            header_regex=SETTINGS.get("router_strip_header_regex"),
        )
    else:
        base_query = (proposed_q or user_text).strip()
    try:
        q_summary, tel_sum = summarize_query(llm, base_query)
        if SETTINGS.get("router_strip_wrappers_enabled") is True:
            q_summary = _strip_wrappers(
                (q_summary or "").strip(),
                trim_whitespace=SETTINGS.get("router_trim_whitespace") is True,
                split_on_blank=SETTINGS.get("router_strip_split_on_blank") is True,
                header_regex=SETTINGS.get("router_strip_header_regex"),
            ) or base_query
        else:
            q_summary = (q_summary or "").strip() or base_query
        telemetry["summarizer"] = tel_sum
        telemetry["summarizedQuery"] = q_summary
    except Exception:
        q_summary = base_query
    t_start = time.perf_counter()
    try:
        block_res = await build_web_block(q_summary, k=k)
        if isinstance(block_res, tuple):
            block, tel_orch = block_res
            telemetry["orchestrator"] = tel_orch or {}
        else:
            block = block_res
    except Exception:
        block = None
    t_elapsed = time.perf_counter() - t_start
    telemetry.update({
        "fetchElapsedSec": round(t_elapsed, 4),
        "blockChars": len(block) if block else 0,
    })
    return (block or None, telemetry)

def _safe_prompt_format(tpl: str, **kwargs) -> str:
    marker = "__ROUTER_TEXT_FIELD__"
    tmp = tpl.replace("{text}", marker)
    tmp = tmp.replace("{", "{{").replace("}", "}}")
    tmp = tmp.replace(marker, "{text}")
    return tmp.format(**kwargs)

# ===== aimodel/file_read/workers/retitle_worker.py =====

from __future__ import annotations
import asyncio, logging, re
from typing import Dict, List, Optional, Tuple
from ..runtime.model_runtime import get_llm
from ..store.index import load_index, save_index
from ..store.base import now_iso
from ..services.cancel import is_active, GEN_SEMAPHORE
from ..store.chats import _load_chat
from ..core.settings import SETTINGS

def S(key: str):
    return SETTINGS[key]

_PENDING: Dict[str, dict] = {}
_ENQUEUED: set[str] = set()
_queue: asyncio.Queue[str] = asyncio.Queue(maxsize=int(S("retitle_queue_maxsize")))
_lock = asyncio.Lock()

def _preview(s: str) -> str:
    n = int(S("retitle_preview_chars"))
    ell = S("retitle_preview_ellipsis")
    s = (s or "")
    return (s[:n] + ell) if len(s) > n else s

def _is_substantial(text: str) -> bool:
    t = (text or "").strip()
    min_chars = int(S("retitle_min_substantial_chars"))
    require_alpha = bool(S("retitle_require_alpha"))
    if len(t) < min_chars:
        return False
    return (re.search(r"[A-Za-z]", t) is not None) if require_alpha else True

def _pick_source(messages: List[dict]) -> Optional[str]:
    if not messages:
        return None
    min_user_len = int(S("retitle_min_user_chars"))
    for m in reversed(messages):
        if m.get("role") == "user":
            txt = (m.get("content") or "").strip()
            if len(txt) >= min_user_len and _is_substantial(txt):
                return txt
    for m in reversed(messages):
        if m.get("role") == "assistant":
            txt = (m.get("content") or "").strip()
            if _is_substantial(txt):
                return txt
    return None

def _sanitize_title(s: str) -> str:
    if not s:
        return ""
    s = s.strip()
    drop_prefix_re = S("retitle_sanitize_drop_prefix_regex")
    if drop_prefix_re:
        s = re.sub(drop_prefix_re, "", s)
    if bool(S("retitle_sanitize_strip_quotes")):
        s = s.strip().strip('"').strip("'").strip()
    replace_not_allowed_re = S("retitle_sanitize_replace_not_allowed_regex")
    replace_with = S("retitle_sanitize_replace_with")
    if replace_not_allowed_re:
        s = re.sub(replace_not_allowed_re, replace_with, s)
    s = re.sub(r"\s+", " ", s).strip()
    max_words = int(S("retitle_sanitize_max_words"))
    max_chars = int(S("retitle_sanitize_max_chars"))
    if max_words > 0:
        words = s.split()
        s = " ".join(words[:max_words])
    if max_chars > 0 and len(s) > max_chars:
        s = s[:max_chars].rstrip()
    return s

def _make_title(llm, src: str) -> str:
    hard = SETTINGS.get("retitle_llm_hard_prefix") or ""
    sys_extra = SETTINGS.get("retitle_llm_sys_inst") or ""
    sys = f"{hard}\n\n{sys_extra}".strip()
    user_text = f"{S('retitle_user_prefix')}{src}{S('retitle_user_suffix')}"
    messages = [
        {"role": "system", "content": sys},
        {"role": "user", "content": user_text},
    ]
    out = llm.create_chat_completion(
        messages=messages,
        max_tokens=int(S("retitle_llm_max_tokens")),
        temperature=float(S("retitle_llm_temperature")),
        top_p=float(S("retitle_llm_top_p")),
        stream=False,
        stop=S("retitle_llm_stop"),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip().strip('"').strip("'")
    strip_regex = SETTINGS.get("retitle_strip_regex")
    if strip_regex:
        raw = re.sub(strip_regex, "", raw).strip()
    raw = re.sub(r"^`{1,3}|`{1,3}$", "", raw).strip()
    raw = re.sub(r"[.:;,\-\s]+$", "", raw)
    return raw

async def start_worker():
    while True:
        sid = await _queue.get()
        try:
            await _process_session(sid)
        except Exception:
            logging.exception("Retitle worker failed")
        finally:
            _queue.task_done()

def _extract_job(snapshot: dict) -> Tuple[List[dict], int]:
    msgs = snapshot.get("messages") or []
    job_seq = int(snapshot.get("job_seq") or 0)
    return msgs, job_seq

async def _process_session(session_id: str):
    if not bool(S("retitle_enable")):
        return
    await asyncio.sleep(int(S("retitle_grace_ms")) / 1000.0)
    waited = 0
    backoff = int(S("retitle_active_backoff_start_ms"))
    backoff_max = int(S("retitle_active_backoff_max_ms"))
    backoff_total = int(S("retitle_active_backoff_total_ms"))
    growth = float(S("retitle_active_backoff_growth"))
    while is_active(session_id) and waited < backoff_total:
        await asyncio.sleep(backoff / 1000.0)
        waited += backoff
        backoff = min(int(backoff * growth), backoff_max)
    async with _lock:
        snapshot = _PENDING.pop(session_id, None)
        _ENQUEUED.discard(session_id)
    if not snapshot:
        return
    messages, job_seq = _extract_job(snapshot)
    try:
        cur_seq = int((_load_chat(session_id) or {}).get("seq") or 0)
    except Exception:
        cur_seq = job_seq
    if cur_seq > job_seq:
        return
    src = _pick_source(messages) or ""
    if not src.strip():
        return
    async with GEN_SEMAPHORE:
        llm = get_llm()
        try:
            title_raw = await asyncio.to_thread(_make_title, llm, src)
        except Exception as e:
            logging.exception("retitle: LLM error: %s", e)
            return
        finally:
            try:
                llm.reset()
            except Exception:
                pass
    title = _sanitize_title(title_raw) if bool(S("retitle_enable_sanitize")) else title_raw
    if not title:
        return
    idx = load_index()
    row = next((r for r in idx if r.get("sessionId") == session_id), None)
    if not row:
        return
    if (row.get("title") or "").strip() == title:
        return
    row["title"] = title
    row["updatedAt"] = now_iso()
    save_index(idx)

def enqueue(session_id: str, messages: List[dict], *, job_seq: Optional[int] = None):
    if not session_id:
        return
    if not isinstance(messages, list):
        messages = []
    if job_seq is None:
        try:
            job_seq = max(int(m.get("id") or 0) for m in messages) if messages else 0
        except Exception:
            job_seq = 0
    snap = {"messages": messages, "job_seq": int(job_seq)}
    async def _put():
        async with _lock:
            _PENDING[session_id] = snap
            if session_id not in _ENQUEUED:
                _ENQUEUED.add(session_id)
                try:
                    _queue.put_nowait(session_id)
                except Exception as e:
                    logging.warning(f"Failed to enqueue retitle: {e}")
    try:
        loop = asyncio.get_running_loop()
        loop.create_task(_put())
    except RuntimeError:
        asyncio.run(_put())

# ===== frontend/src/file_read/api/billing.ts =====

// frontend/src/api/billing.ts
import { buildUrl } from "../services/http";

export async function getBillingStatus(token: string) {
  const r = await fetch(buildUrl("/billing/status"), {
    headers: { Authorization: `Bearer ${token}` },
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json() as Promise<{ status: string; current_period_end: number }>;
}

export async function startCheckout(token: string, priceId?: string) {
  const r = await fetch(buildUrl("/billing/checkout"), {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
    // server accepts price_id OR priceId; we’ll send price_id
    body: JSON.stringify(priceId ? { price_id: priceId } : {}),
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json() as Promise<{ url: string }>;
}

export async function openPortal(token: string) {
  const r = await fetch(buildUrl("/billing/portal"), {
    method: "POST",
    headers: { Authorization: `Bearer ${token}` },
  });
  if (!r.ok) throw new Error(await r.text());
  return r.json() as Promise<{ url: string }>;
}

# ===== frontend/src/file_read/api/licensingWorker.ts =====



# ===== frontend/src/file_read/App.tsx =====

import AgentRunner from "./pages/AgentRunner";
import { useAuth } from "./auth/AuthContext";
import SignIn from "./auth/SignIn";
import SignUp from "./auth/SignUp";
import ForgotPassword from "./auth/ForgotPassword";
import { useState } from "react";

export default function App() {
  const { user, loading } = useAuth();
  const [mode, setMode] = useState<"signin" | "signup" | "forgot">("signin");

  // BYPASS: render app even if not signed in
  if (import.meta.env.VITE_BYPASS_AUTH === "true") {
    return (
      <main className="bg-gray-50 h-screen overflow-hidden">
        <AgentRunner />
      </main>
    );
  }

  if (loading) {
    return <main className="min-h-screen grid place-items-center">Loading…</main>;
  }

  if (!user) {
    return (
      <main className="min-h-screen grid place-items-center bg-gray-50 px-4">
        <div className="w-full max-w-sm bg-white p-6 rounded-2xl shadow">
          {mode === "signin" && <SignIn />}
          {mode === "signup" && <SignUp />}
          {mode === "forgot" && <ForgotPassword />}

          <div className="mt-4 text-sm text-center text-gray-700">
            {mode !== "signin" && (
              <button onClick={() => setMode("signin")} className="underline mx-2">Sign in</button>
            )}
            {mode !== "signup" && (
              <button onClick={() => setMode("signup")} className="underline mx-2">Create account</button>
            )}
            {mode !== "forgot" && (
              <button onClick={() => setMode("forgot")} className="underline mx-2">Forgot password</button>
            )}
          </div>
        </div>
      </main>
    );
  }

  return (
    <main className="bg-gray-50 h-screen overflow-hidden">
      <AgentRunner />
    </main>
  );
}

# ===== frontend/src/file_read/auth/AuthContext.tsx =====

// frontend/src/file_read/auth/AuthContext.tsx
import React, { createContext, useContext, useEffect, useMemo, useState } from "react";
import { getJSON } from "../services/http";

type LocalUser = {
  email?: string;
  name?: string;
  // add more fields if your /auth/me returns them
};

type Ctx = {
  user: LocalUser | null;
  loading: boolean;
  token: string | null;                // NEW
  setToken: (t: string | null) => void;
  logout: () => void;
};

const AuthContext = createContext<Ctx>({
  user: null,
  loading: true,
  token: null,                         // NEW
  setToken: () => {},
  logout: () => {},
});

export const AuthProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [token, _setToken] = useState<string | null>(null);
  const [user, setUser] = useState<LocalUser | null>(null);
  const [loading, setLoading] = useState(true);

  // Single writer for token + localStorage
  const setToken = (t: string | null) => {
    _setToken(t);
    try {
      if (t) localStorage.setItem("local_jwt", t);
      else localStorage.removeItem("local_jwt");
    } catch {}
  };

  const logout = () => {
    setToken(null);
    setUser(null);
  };

  // On mount: read token and load profile (if any)
  useEffect(() => {
    let cancelled = false;
    (async () => {
      try {
        const t = localStorage.getItem("local_jwt");
        if (t) {
          _setToken(t);
          // Optional profile fetch; your backend should accept Authorization: Bearer <jwt>
          try {
            const me = await getJSON<LocalUser>("/auth/me");
            if (!cancelled) setUser(me || null);
            if (me?.email) localStorage.setItem("profile_email", me.email);
          } catch {
            const stored = localStorage.getItem("profile_email") || "";
            if (!cancelled) setUser(stored ? { email: stored } : { email: undefined });
          }
        } else {
          if (!cancelled) setUser(null);
        }
      } finally {
        if (!cancelled) setLoading(false);
      }
    })();
    return () => { cancelled = true; };
  }, []);

  // If token changes later (SignIn/SignUp), refresh /auth/me
  useEffect(() => {
    let cancelled = false;
    (async () => {
      if (!token) return;
      try {
        const me = await getJSON<LocalUser>("/auth/me");
        if (!cancelled) setUser(me || null);
        if (me?.email) localStorage.setItem("profile_email", me.email);
      } catch {
        const stored = localStorage.getItem("profile_email") || "";
        if (!cancelled) setUser(stored ? { email: stored } : { email: undefined });
      }
    })();
    return () => { cancelled = true; };
  }, [token]);

  const ctx = useMemo<Ctx>(
    () => ({ user, loading, token, setToken, logout }),   // CHANGED: include token
    [user, loading, token]
  );

  return <AuthContext.Provider value={ctx}>{children}</AuthContext.Provider>;
};

export const useAuth = () => useContext(AuthContext);

# ===== frontend/src/file_read/auth/ForgotPassword.tsx =====

// frontend/src/file_read/auth/ForgotPassword.tsx
import { useState } from "react";
import { postJSON } from "../services/http";

export default function ForgotPassword() {
  const [email, setEmail] = useState("");
  const [sent, setSent] = useState(false);
  const [err, setErr] = useState<string | null>(null);
  const [submitting, setSubmitting] = useState(false);

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    setErr(null);
    setSent(false);
    setSubmitting(true);
    try {
      // Backend should implement: POST /auth/forgot  { email }
      // Behavior suggestion:
      //  - If SMTP configured: send email with reset link/token
      //  - If offline: generate token and print reset URL to server logs (admin shares it)
      await postJSON("/auth/forgot", { email: email.trim().toLowerCase() });
      setSent(true);
    } catch (e: any) {
      // Common cases: 404 if endpoint not implemented, or 400/422 for bad email
      const msg = (e?.message as string) || "";
      if (/HTTP 404/i.test(msg)) {
        setErr("Password reset isn’t enabled on this box. Ask the admin to reset your password.");
      } else {
        setErr(msg.replace(/^HTTP \d+\s*–\s*/, "") || "Could not send reset request");
      }
    } finally {
      setSubmitting(false);
    }
  }

  return (
    <form onSubmit={onSubmit} className="space-y-4">
      <h1 className="text-xl font-semibold text-center">Reset password</h1>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Email</label>
        <input
          value={email}
          onChange={(e) => setEmail(e.target.value)}
          type="email"
          inputMode="email"
          autoComplete="email"
          required
          className="w-full rounded-lg border border-gray-300 px-3 py-2 outline-none focus:ring-2 focus:ring-black"
          placeholder="you@example.com"
        />
      </div>

      {sent && (
        <div className="text-sm text-green-700 bg-green-50 border border-green-200 rounded-lg px-3 py-2">
          If password reset is enabled, a link has been sent (or printed in the server logs if email isn’t configured).
          Contact your admin if you don’t receive it.
        </div>
      )}

      {err && (
        <div className="text-sm text-red-600 bg-red-50 border border-red-200 rounded-lg px-3 py-2">
          {err}
        </div>
      )}

      <button
        type="submit"
        disabled={submitting}
        className="w-full rounded-lg bg-black text-white py-2.5 font-medium disabled:opacity-60"
      >
        {submitting ? "Sending…" : "Send reset request"}
      </button>
    </form>
  );
}

# ===== frontend/src/file_read/auth/localAuth.ts =====

import { buildUrl } from "../services/http";

const KEY = "local_jwt";
export function getLocalToken(): string | null { return localStorage.getItem(KEY); }
export function setLocalToken(tok: string | null) { tok ? localStorage.setItem(KEY, tok) : localStorage.removeItem(KEY); }

export async function localRegister(email: string, password: string) {
  const r = await fetch(buildUrl("/auth/register"), {
    method: "POST", headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ email, password }),
  });
  if (!r.ok) throw new Error(await r.text());
}

export async function localLogin(email: string, password: string) {
  const r = await fetch(buildUrl("/auth/login"), {
    method: "POST", headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ email, password }),
  });
  if (!r.ok) throw new Error(await r.text());
  const data = await r.json();
  setLocalToken(data.access_token);
}

export function localLogout() { setLocalToken(null); }

# ===== frontend/src/file_read/auth/SignIn.tsx =====

import { useState } from "react";
import { localLogin, localRegister } from "./localAuth";
import { useAuth } from "./AuthContext";

export default function SignIn() {
  const { setToken } = useAuth();
  const [email, setEmail] = useState("");
  const [pw, setPw] = useState("");
  const [err, setErr] = useState<string | null>(null);
  const [mode, setMode] = useState<"signin" | "signup">("signin");
  const [busy, setBusy] = useState(false);

  async function submit(e: React.FormEvent) {
    e.preventDefault();
    setErr(null); setBusy(true);
    try {
      if (mode === "signup") {
        await localRegister(email, pw);
      }
      await localLogin(email, pw);
      // token is stored by localLogin; poke context to re-evaluate
      setToken(localStorage.getItem("local_jwt"));
    } catch (e: any) {
      setErr(e?.message || "Auth failed");
    } finally { setBusy(false); }
  }

  return (
    <form onSubmit={submit} className="space-y-3">
      <h1 className="text-lg font-semibold">{mode === "signin" ? "Sign in" : "Create account"}</h1>
      <input className="w-full border rounded px-3 py-2" placeholder="Email" value={email} onChange={e=>setEmail(e.target.value)} />
      <input className="w-full border rounded px-3 py-2" placeholder="Password" type="password" value={pw} onChange={e=>setPw(e.target.value)} />
      {err && <div className="text-sm text-red-600">{err}</div>}
      <button disabled={busy} className="w-full rounded bg-black text-white py-2">{busy ? "Please wait…" : (mode === "signin" ? "Sign in" : "Sign up")}</button>
      <div className="text-sm text-center">
        {mode === "signin" ? (
          <button type="button" className="underline" onClick={()=>setMode("signup")}>Create account</button>
        ) : (
          <button type="button" className="underline" onClick={()=>setMode("signin")}>Have an account? Sign in</button>
        )}
      </div>
    </form>
  );
}

# ===== frontend/src/file_read/auth/SignUp.tsx =====

// frontend/src/file_read/auth/SignUp.tsx
import { useState } from "react";
import { Eye, EyeOff } from "lucide-react";
import { localRegister, localLogin } from "./localAuth";
import { useAuth } from "./AuthContext";

export default function SignUp() {
  const { setToken } = useAuth();
  const [email, setEmail] = useState("");
  const [pw, setPw] = useState("");
  const [confirmPw, setConfirmPw] = useState("");
  const [err, setErr] = useState<string | null>(null);
  const [submitting, setSubmitting] = useState(false);
  const [showPw, setShowPw] = useState(false);
  const [showConfirm, setShowConfirm] = useState(false);

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    setErr(null);

    const em = email.trim().toLowerCase();
    if (!em) return setErr("Email is required");
    if (pw.length < 6) return setErr("Password must be at least 6 characters");
    if (pw !== confirmPw) return setErr("Passwords do not match");

    setSubmitting(true);
    try {
      await localRegister(em, pw);     // create user locally
      await localLogin(em, pw);        // get JWT + store in localStorage
      setToken(localStorage.getItem("local_jwt")); // notify context (logged in)
    } catch (e: any) {
      setErr(e?.message || "Sign up failed");
      setSubmitting(false);
    }
  }

  return (
    <form onSubmit={onSubmit} className="space-y-4">
      <h1 className="text-xl font-semibold text-center">Create account</h1>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Email</label>
        <input
          value={email}
          onChange={(e) => setEmail(e.target.value)}
          type="email"
          inputMode="email"
          autoComplete="email"
          required
          className="w-full rounded-lg border border-gray-300 px-3 py-2 outline-none focus:ring-2 focus:ring-black"
          placeholder="you@example.com"
        />
      </div>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Password</label>
        <div className="relative">
          <input
            value={pw}
            onChange={(e) => setPw(e.target.value)}
            type={showPw ? "text" : "password"}
            autoComplete="new-password"
            minLength={6}
            required
            className="w-full rounded-lg border border-gray-300 px-3 py-2 pr-10 outline-none focus:ring-2 focus:ring-black"
            placeholder="At least 6 characters"
          />
          <button
            type="button"
            onClick={() => setShowPw((s) => !s)}
            className="absolute inset-y-0 right-0 flex items-center pr-3 text-gray-500"
            aria-label={showPw ? "Hide password" : "Show password"}
          >
            {showPw ? <EyeOff size={18} /> : <Eye size={18} />}
          </button>
        </div>
      </div>

      <div className="space-y-1">
        <label className="block text-sm text-gray-700">Confirm Password</label>
        <div className="relative">
          <input
            value={confirmPw}
            onChange={(e) => setConfirmPw(e.target.value)}
            type={showConfirm ? "text" : "password"}
            autoComplete="new-password"
            minLength={6}
            required
            className="w-full rounded-lg border border-gray-300 px-3 py-2 pr-10 outline-none focus:ring-2 focus:ring-black"
            placeholder="Re-enter password"
          />
          <button
            type="button"
            onClick={() => setShowConfirm((s) => !s)}
            className="absolute inset-y-0 right-0 flex items-center pr-3 text-gray-500"
            aria-label={showConfirm ? "Hide confirm password" : "Show confirm password"}
          >
            {showConfirm ? <EyeOff size={18} /> : <Eye size={18} />}
          </button>
        </div>
      </div>

      {err && (
        <div className="text-sm text-red-600 bg-red-50 border border-red-200 rounded-lg px-3 py-2">
          {err}
        </div>
      )}

      <button
        type="submit"
        disabled={submitting}
        className="w-full rounded-lg bg-black text-white py-2.5 font-medium disabled:opacity-60"
      >
        {submitting ? "Creating…" : "Create account"}
      </button>
    </form>
  );
}

# ===== frontend/src/file_read/components/AssistantMetrics.tsx =====

// frontend/src/file_read/components/chat/AssistantMetrics.tsx
import { Info } from "lucide-react";
import MetricsHoverCard from "./MetricsHoverCard";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function AssistantMetrics({
  status,
  runJson,
  flat,
  align = "right",
}: { status: string; runJson?: RunJson | null; flat?: GenMetrics | null; align?: "left" | "right" }) {
  return (
    <div className="mt-2 flex justify-start">
      <div className="inline-flex items-center gap-1 px-2 py-1 rounded-full bg-white border shadow-sm text-[11px] text-gray-600">
        <Info className="w-3.5 h-3.5 opacity-70" />
        <span className="truncate max-w-[70vw] sm:max-w-none">{status || "Run details"}</span>
        <MetricsHoverCard
          data={
            runJson ??
            (flat
              ? {
                  stats: {
                    stopReason: flat.stop_reason ?? null,
                    tokensPerSecond: flat.tok_per_sec ?? null,
                    timeToFirstTokenSec: flat.ttft_ms != null ? Math.max(0, flat.ttft_ms) / 1000 : null,
                    totalTimeSec: null,
                    promptTokensCount: flat.input_tokens_est ?? null,
                    predictedTokensCount: flat.output_tokens ?? null,
                    totalTokensCount: flat.total_tokens_est ?? null,
                  },
                }
              : null)
          }
          title="Run JSON"
          align={align}
          compact
        />
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Budget/BudgetBar.tsx =====

// frontend/src/file_read/components/Budget/BudgetBar.tsx

import { useState } from "react";
import {
  type RunJson,
  getNormalizedBudget,
  getRagTelemetry,
  getWebTelemetry,
  getTimingMetrics,
  getPackTelemetry,
  getThroughput,
} from "../../shared/lib/runjson";
import {
  RagPanel,
  WebPanel,
  TimingPanel,
} from "./BudgetBarPanelsExtras";
import {
  num,
  PackPanel,
} from "./BudgetBarPanelsCore";

import { ChevronDown, ChevronUp } from "lucide-react";

function pct(n: number, d: number) {
  if (!Number.isFinite(n) || !Number.isFinite(d) || d <= 0) return 0;
  return Math.max(0, Math.min(100, (n / d) * 100));
}

export default function BudgetBar({ runJson }: { runJson?: RunJson | null }) {
  const nb = getNormalizedBudget(runJson ?? undefined);
  if (!nb) return null;

  const [open, setOpen] = useState(false);

  const rag = getRagTelemetry(runJson ?? undefined) as any | null;
  const web = getWebTelemetry(runJson ?? undefined) as any | null;
  const pack = getPackTelemetry(runJson ?? undefined) as any | null;
  const timing = getTimingMetrics(runJson ?? undefined) as any | null;
  const tps = getThroughput(runJson ?? undefined);

  const breakdown =
    (runJson as any)?.budget_view?.breakdown ??
    (runJson as any)?.stats?.budget?.breakdown ??
    null;

  const modelCtx = num(nb.modelCtx);
  const clampMargin = num(nb.clampMargin);
  const inputTokensEst = num(nb.inputTokensEst);
  const outBudgetChosen = num(nb.outBudgetChosen);
  const outActual = num(runJson?.stats?.predictedTokensCount);
  const outShown = outActual || outBudgetChosen;

  const used = inputTokensEst + outShown + clampMargin;
  const fullPct = pct(used, modelCtx);

  const ragDelta = Math.max(
    0,
    num(rag?.ragTokensAdded) ||
      num(rag?.blockTokens) ||
      num(rag?.blockTokensApprox) ||
      num(rag?.sessionOnlyTokensApprox)
  );
  const ragWasInjected = !!(rag?.injected || rag?.sessionOnly || ragDelta > 0);
  const ragPctOfInput = inputTokensEst > 0 ? Math.round((ragDelta / inputTokensEst) * 100) : 0;
  const ragBlockBuildTime =
    rag?.injectBuildSec ?? rag?.blockBuildSec ?? rag?.sessionOnlyBuildSec;

  const webRouteSec = web?.elapsedSec;
  const webFetchSec = web?.fetchElapsedSec;
  const webInjectSec = web?.injectElapsedSec;
  const webPre =
    num((web as any)?.breakdown?.totalWebPreTtftSec) ||
    (num(webRouteSec) + num(webFetchSec) + num(webInjectSec));

  const packPackSec = num(pack?.packSec);
  const packSummarySec = num(pack?.summarySec);
  const packFinalTrimSec = num(pack?.finalTrimSec);
  const packCompressSec = num(pack?.compressSec);
  const packSummaryTokens = num(pack?.summaryTokensApprox);
  const packSummaryUsedLLM = !!pack?.summaryUsedLLM;

  const droppedMsgs = num((pack as any)?.finalTrimDroppedMsgs);
  const droppedApproxTok = num((pack as any)?.finalTrimDroppedApproxTokens);
  const sumShrinkFrom = num((pack as any)?.finalTrimSummaryShrunkFromChars);
  const sumShrinkTo = num((pack as any)?.finalTrimSummaryShrunkToChars);
  const sumShrinkDropped = num((pack as any)?.finalTrimSummaryDroppedChars);
  const rolledPeeledMsgs = num((pack as any)?.rollPeeledMsgs);
  const rollNewSummaryTokens = num((pack as any)?.rollNewSummaryTokensApprox);

  const engine = timing?.engine || null;
  const engineLoadSec = num(engine?.loadSec);
  const enginePromptSec = num(engine?.promptSec);
  const engineEvalSec = num(engine?.evalSec);
  const enginePromptN = engine?.promptN;
  const engineEvalN = engine?.evalN;

  const preModelSec = num(timing?.preModelSec);
  const modelQueueSec = num(timing?.modelQueueSec);

  const preAccountedFromBackend = num(breakdown?.preTtftAccountedSec);
  const accountedFallback =
    webPre +
    num(rag?.routerDecideSec) +
    num(ragBlockBuildTime) +
    packPackSec +
    packSummarySec +
    packFinalTrimSec +
    packCompressSec +
    preModelSec +
    modelQueueSec;
  const accounted = preAccountedFromBackend || accountedFallback;

  const unattributed =
    (breakdown && Number.isFinite(breakdown.unattributedTtftSec))
      ? num(breakdown.unattributedTtftSec)
      : Math.max(0, num(timing?.ttftSec) - accounted);

  return (
    <div className="px-3 py-2 border-t bg-white/90 backdrop-blur sticky bottom-0 z-40">
      <div className="flex items-center gap-2 text-[11px] text-gray-700">
        <button
          type="button"
          onClick={() => setOpen((v) => !v)}
          aria-expanded={open ? "true" : "false"}
          className="shrink-0 inline-flex items-center gap-1 px-2 h-6 rounded border bg-white hover:bg-gray-50"
          title={open ? "Hide details" : "Show details"}
        >
          {open ? <ChevronDown className="w-3.5 h-3.5" /> : <ChevronUp className="w-3.5 h-3.5" />}
          <span className="hidden sm:inline">Details</span>
        </button>

        <div
          className="flex-1 h-1.5 rounded bg-gray-200 overflow-hidden"
          title={`Context ${fullPct.toFixed(1)}%`}
        >
          <div className="h-1.5 bg-black" style={{ width: `${fullPct}%` }} />
        </div>

        <div className="whitespace-nowrap hidden xs:block">
          In: <span className="font-medium">{inputTokensEst}</span>
        </div>
        <div className="whitespace-nowrap hidden xs:block">
          Out: <span className="font-medium">{outShown}</span>
        </div>
        <div className="whitespace-nowrap hidden sm:block">
          Ctx: <span className="font-medium">{modelCtx}</span>
        </div>
        <div className="whitespace-nowrap text-gray-500 hidden md:block">
          {`Context is ${fullPct.toFixed(1)}% full`}
        </div>
      </div>

      {open && (
        <div className="mt-2 max-h-40 sm:max-h-48 md:max-h-56 overflow-y-auto pr-1 pb-1 -mr-1">
          {pack && (
            <PackPanel
              pack={pack}
              packPackSec={packPackSec}
              packSummarySec={packSummarySec}
              packFinalTrimSec={packFinalTrimSec}
              packCompressSec={packCompressSec}
              packSummaryTokens={packSummaryTokens}
              packSummaryUsedLLM={packSummaryUsedLLM}
              droppedMsgs={droppedMsgs}
              droppedApproxTok={droppedApproxTok}
              sumShrinkFrom={sumShrinkFrom}
              sumShrinkTo={sumShrinkTo}
              sumShrinkDropped={sumShrinkDropped}
              rolledPeeledMsgs={rolledPeeledMsgs}
              rollNewSummaryTokens={rollNewSummaryTokens}
            />
          )}

          {rag && (
            <RagPanel
              rag={rag}
              ragWasInjected={ragWasInjected}
              ragBlockBuildTime={ragBlockBuildTime}
              ragDelta={ragDelta}
              ragPctOfInput={ragPctOfInput}
              inputTokensEst={inputTokensEst}
            />
          )}

          {web && <WebPanel web={web} />}

          {timing && (
            <TimingPanel
              timing={timing}
              enginePromptSec={enginePromptSec}
              engineEvalSec={engineEvalSec}
              engineLoadSec={engineLoadSec}
              enginePromptN={enginePromptN}
              engineEvalN={engineEvalN}
              preModelSec={preModelSec}
              modelQueueSec={modelQueueSec}
              unattributed={unattributed}
              encodeTps={tps?.encodeTps ?? null}
              decodeTps={tps?.decodeTps ?? null}
              overallTps={tps?.overallTps ?? null}
            />
          )}
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/Budget/BudgetBarPanelsCore.tsx =====

// frontend/src/file_read/components/Budget/BudgetBarPanelsCore.tsx
export const num = (v: unknown) =>
  typeof v === "number" && Number.isFinite(v) ? v : 0;

export function fmtSec(v?: number) {
  if (v == null || !Number.isFinite(v)) return "—";
  if (v < 0.01) return "<0.01s";
  return `${v.toFixed(2)}s`;
}

export function fmtTps(v?: number | null) {
  if (v == null || !Number.isFinite(v)) return "—";
  if (v < 1) return v.toFixed(2);
  if (v < 10) return v.toFixed(1);
  return Math.round(v).toString();
}

type PackPanelProps = {
  pack: any;
  packPackSec: number;
  packSummarySec: number;
  packFinalTrimSec: number;
  packCompressSec: number;
  packSummaryTokens: number;
  packSummaryUsedLLM: boolean;
  droppedMsgs: number;
  droppedApproxTok: number;
  sumShrinkFrom: number;
  sumShrinkTo: number;
  sumShrinkDropped: number;
  rolledPeeledMsgs: number;
  rollNewSummaryTokens: number;
};

export function PackPanel({
  pack,
  packPackSec,
  packSummarySec,
  packFinalTrimSec,
  packCompressSec,
  packSummaryTokens,
  packSummaryUsedLLM,
  droppedMsgs,
  droppedApproxTok,
  sumShrinkFrom,
  sumShrinkTo,
  sumShrinkDropped,
  rolledPeeledMsgs,
  rollNewSummaryTokens,
}: PackPanelProps) {
  return (
    <div className="mt-2 text-[11px] text-gray-700">
      <div className="flex flex-wrap items-center gap-x-4 gap-y-1">
        {"packSec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            pack {fmtSec(packPackSec)}
          </span>
        )}
        {"summarySec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            summary {fmtSec(packSummarySec)} {packSummaryUsedLLM ? "(llm)" : "(fast)"}
          </span>
        )}
        {"finalTrimSec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            trim {fmtSec(packFinalTrimSec)}
          </span>
        )}
        {"compressSec" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            compress {fmtSec(packCompressSec)}
          </span>
        )}
        {"summaryTokensApprox" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            sumTokens≈<b>{packSummaryTokens}</b>
          </span>
        )}
        {"packedChars" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            packed chars=<b>{(pack as any).packedChars}</b>
          </span>
        )}
        {"messages" in pack && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            msgs=<b>{(pack as any).messages}</b>
          </span>
        )}

        {(droppedMsgs > 0 || droppedApproxTok > 0) && (
          <span className="px-1.5 py-0.5 rounded bg-red-50 border border-red-200 text-red-700">
            dropped msgs=<b>{droppedMsgs}</b>
            {droppedApproxTok ? (
              <>
                {" "}
                / ≈<b>{droppedApproxTok}</b> tok
              </>
            ) : null}
          </span>
        )}

        {sumShrinkDropped > 0 && (
          <span className="px-1.5 py-0.5 rounded bg-amber-50 border border-amber-200 text-amber-800">
            summary shrink {sumShrinkFrom}→{sumShrinkTo} chars (−<b>{sumShrinkDropped}</b>)
          </span>
        )}

        {rolledPeeledMsgs > 0 && (
          <span className="px-1.5 py-0.5 rounded bg-blue-50 border border-blue-200 text-blue-800">
            rolled: <b>{rolledPeeledMsgs}</b> msgs → +sum≈<b>{rollNewSummaryTokens}</b> tok
          </span>
        )}
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Budget/BudgetBarPanelsExtras.tsx =====

// frontend/src/file_read/components/Budget/BudgetBarPanelsExtras.tsx
import { fmtSec, fmtTps } from "./BudgetBarPanelsCore";

type RagPanelProps = {
  rag: any;
  ragWasInjected: boolean;
  ragBlockBuildTime?: number;
  ragDelta: number;
  ragPctOfInput: number;
  inputTokensEst: number;
};

export function RagPanel({
  rag,
  ragWasInjected,
  ragBlockBuildTime,
  ragDelta,
  ragPctOfInput,
  inputTokensEst,
}: RagPanelProps) {
  const routerNeeded = rag?.routerNeeded;
  const routerSkipped = rag?.routerSkipped;
  const routerSkippedReason = rag?.routerSkippedReason;
  const routerDecideSec = rag?.routerDecideSec;
  const embedSec = rag?.embedSec;
  const searchChatSec = rag?.searchChatSec;
  const searchGlobalSec = rag?.searchGlobalSec;
  const dedupeSec = rag?.dedupeSec;
  const topKRequested = rag?.topKRequested;
  const hitsChat = rag?.hitsChat;
  const hitsGlobal = rag?.hitsGlobal;
  const blockChars = rag?.blockChars ?? rag?.sessionOnlyChars;
  const routerQuery = rag?.routerQuery;

  return (
    <div className="mt-2 text-[11px] text-gray-700">
      <div className="flex flex-wrap items-center gap-x-4 gap-y-1">
        <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
          RAG: <b>{ragWasInjected ? "injected" : "skipped"}</b> {rag?.mode ? `(${rag.mode})` : rag?.sessionOnly ? "(session-only)" : ""}
        </span>
        {"routerNeeded" in rag && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            router: <b>{routerNeeded ? "yes" : "no"}</b>
          </span>
        )}
        {routerSkipped && (
          <span className="px-1.5 py-0.5 rounded bg-amber-50 border border-amber-200 text-amber-800">
            skipped {routerSkippedReason ? `(${routerSkippedReason})` : ""}
          </span>
        )}
        {"routerDecideSec" in rag && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            route {fmtSec(routerDecideSec)}
          </span>
        )}
        {"embedSec" in rag && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            embed {fmtSec(embedSec)}
          </span>
        )}
        {("searchChatSec" in rag || "searchGlobalSec" in rag) && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            search {fmtSec(searchChatSec)} / {fmtSec(searchGlobalSec)}
          </span>
        )}
        {"dedupeSec" in rag && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            dedupe {fmtSec(dedupeSec)}
          </span>
        )}
        {ragBlockBuildTime !== undefined && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            block {fmtSec(ragBlockBuildTime)}
          </span>
        )}
        {topKRequested != null && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">k=<b>{topKRequested}</b></span>
        )}
        {hitsChat != null && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">hits chat=<b>{hitsChat}</b></span>
        )}
        {hitsGlobal != null && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">global=<b>{hitsGlobal}</b></span>
        )}
        {(rag.ragTokensAdded != null || rag.blockTokens != null || rag.blockTokensApprox != null || rag.sessionOnlyTokensApprox != null) && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            +RAG tokens=<b>{ragDelta}</b>
            {inputTokensEst ? ` (${ragPctOfInput}% of input)` : ""}
          </span>
        )}
        {blockChars != null && (
          <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
            block chars=<b>{blockChars}</b>
          </span>
        )}
      </div>
      {routerQuery && (
        <div className="mt-1 text-[10px] text-gray-500 truncate" title={routerQuery}>
          query: {routerQuery}
        </div>
      )}
    </div>
  );
}

type WebPanelProps = {
  web: any;
};

export function WebPanel({ web }: WebPanelProps) {
  const webWasInjected = !!web?.injected;
  const webNeeded = web?.needed;
  const webRouteSec = web?.elapsedSec;
  const webFetchSec = web?.fetchElapsedSec;
  const webInjectSec = web?.injectElapsedSec;
  const webBlockChars = web?.blockChars;
  const webEphemeralBlocks = web?.ephemeralBlocks;
  const summarizedQuery = web?.summarizedQuery;

  return (
    <div className="mt-2 text-[11px] text-gray-700">
      <div className="flex flex-wrap items-center gap-x-4 gap-y-1">
        <span className="px-1.5 py-0.5 rounded bg-gray-100 border">
          WEB: <b>{webWasInjected ? "injected" : "skipped"}</b>
          {webNeeded !== undefined ? <> (router: <b>{webNeeded ? "need" : "no"}</b>)</> : null}
        </span>
        {"elapsedSec" in web && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">route {fmtSec(webRouteSec)}</span>}
        {"fetchElapsedSec" in web && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">fetch {fmtSec(webFetchSec)}</span>}
        {"injectElapsedSec" in web && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">inject {fmtSec(webInjectSec)}</span>}
        {"blockChars" in web && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">block chars=<b>{webBlockChars}</b></span>}
        {"ephemeralBlocks" in web && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">eph blocks=<b>{webEphemeralBlocks}</b></span>}
        {web.droppedFromSummary && (
  <span className="px-1.5 py-0.5 rounded bg-purple-50 border border-purple-200 text-purple-800">
    dropped after turn
  </span>
)}
      </div>
      {summarizedQuery && (
        <div className="mt-1 text-[10px] text-gray-500 truncate" title={summarizedQuery}>
          query: {summarizedQuery}
        </div>
      )}
    </div>
  );
}

type TimingPanelProps = {
  timing: any;
  enginePromptSec?: number;
  engineEvalSec?: number;
  engineLoadSec?: number;
  enginePromptN?: number | null;
  engineEvalN?: number | null;
  preModelSec?: number;
  modelQueueSec?: number;
  unattributed?: number;
  encodeTps?: number | null;
  decodeTps?: number | null;
  overallTps?: number | null;
};

export function TimingPanel({
  timing,
  enginePromptSec,
  engineEvalSec,
  engineLoadSec,
  enginePromptN,
  engineEvalN,
  preModelSec,
  modelQueueSec,
  unattributed,
  encodeTps,
  decodeTps,
  overallTps,
}: TimingPanelProps) {
  return (
    <div className="mt-2 text-[11px] text-gray-700">
      <div className="flex flex-wrap items-center gap-x-4 gap-y-1">
        <span className="px-1.5 py-0.5 rounded bg-gray-100 border">ttft {fmtSec(timing.ttftSec ?? undefined)}</span>
        {"queueWaitSec" in timing && timing.queueWaitSec != null && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">queue {fmtSec(timing.queueWaitSec)}</span>}
        {"genSec" in timing && timing.genSec != null && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">gen {fmtSec(timing.genSec)}</span>}
        {"totalSec" in timing && timing.totalSec != null && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">total {fmtSec(timing.totalSec)}</span>}
        {!!enginePromptSec && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">prefill {fmtSec(enginePromptSec)}</span>}
        {!!engineEvalSec && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">eval {fmtSec(engineEvalSec)}</span>}
        {!!engineLoadSec && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">load {fmtSec(engineLoadSec)}</span>}
        {enginePromptN != null && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">promptN={enginePromptN}</span>}
        {engineEvalN != null && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">evalN={engineEvalN}</span>}
        {!!preModelSec && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">pre-model {fmtSec(preModelSec)}</span>}
        {!!modelQueueSec && <span className="px-1.5 py-0.5 rounded bg-gray-100 border">model-queue {fmtSec(modelQueueSec)}</span>}
        <span className="px-1.5 py-0.5 rounded bg-gray-100 border">unattributed {fmtSec(unattributed)}</span>
        {encodeTps != null && <span className="px-1.5 py-0.5 rounded bg-green-50 border border-green-200 text-green-800">encode <b>{fmtTps(encodeTps)}</b> tok/s</span>}
        {decodeTps != null && <span className="px-1.5 py-0.5 rounded bg-indigo-50 border border-indigo-200 text-indigo-800">decode <b>{fmtTps(decodeTps)}</b> tok/s</span>}
        {overallTps != null && <span className="px-1.5 py-0.5 rounded bg-gray-50 border">overall <b>{fmtTps(overallTps)}</b> tok/s</span>}
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatBubble.tsx =====

import { useState } from "react";
import { Copy, Check, Trash2 } from "lucide-react";
import MarkdownMessage from "./Markdown/MarkdownMessage";
import { stripRunJson } from "../shared/lib/runjson";
import type { Attachment } from "../types/chat";   // ✅ import Attachment type

const STOP_SENTINEL_RE = /(?:\r?\n)?(?:\u23F9|\\u23F9)\s+stopped(?:\r?\n)?$/u;

export default function ChatBubble({
  role,
  text,
  attachments = [],   // ✅ new prop
  showActions = true,
  onDelete,
}: {
  role: "user" | "assistant";
  text: string;
  attachments?: Attachment[];   // ✅ allow attachments
  showActions?: boolean;
  onDelete?: () => void;
}) {
  const isUser = role === "user";
  const raw = text ?? "";
  const { text: stripped } = stripRunJson(raw);
  let content = stripped.trim();

  if (!isUser) content = content.replace(STOP_SENTINEL_RE, "");

  const hasOnlyAttachments =
    isUser && (!content || content.length === 0) && attachments.length > 0;

  if (role === "assistant" && !content && attachments.length === 0) return null;

  const [copiedMsg, setCopiedMsg] = useState(false);
  const copyWholeMessage = async () => {
    try {
      await navigator.clipboard.writeText(content);
      setCopiedMsg(true);
      setTimeout(() => setCopiedMsg(false), 2000);
    } catch {}
  };

  return (
    <div className="mb-2">
      <div className={`flex ${isUser ? "justify-end" : "justify-start"}`}>
        <div
          className={`max-w-[80%] w-fit break-words rounded-2xl px-4 py-2 shadow-sm
                      prose prose-base max-w-none
            ${isUser ? "bg-black text-white prose-invert" : "bg-white border text-gray-900"}`}
        >
          {/* ✅ Render attachments */}
          {attachments.length > 0 && (
            <div className="mb-2 flex flex-wrap gap-2">
              {attachments.map((att) => (
                <div
                  key={`${att.sessionId || "global"}:${att.source || att.name}`}
                  className={`border rounded px-2 py-1 text-sm flex items-center gap-2 ${
                    isUser
                      ? "bg-white/10 border-white/30"
                      : "bg-white"
                  }`}
                  title={att.name || att.source}
                >
                  📎 <span className="truncate max-w-[220px]">{att.name}</span>
                </div>
              ))}
            </div>
          )}

          {content ? (
            <div className="max-w-full">
              <MarkdownMessage text={content} />
            </div>
          ) : hasOnlyAttachments ? null : isUser ? null : (
            <span className="opacity-60">…</span>
          )}
        </div>
      </div>

      {showActions && (
        <div className={`mt-1 flex ${isUser ? "justify-end" : "justify-start"}`}>
          <div className="flex items-center gap-2">
            <button
              type="button"
              onClick={copyWholeMessage}
              title={copiedMsg ? "Copied" : "Copy"}
              aria-label={copiedMsg ? "Copied" : "Copy message"}
              className="inline-flex items-center justify-center w-7 h-7 rounded border
                         bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
            >
              {copiedMsg ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
            </button>
            {onDelete && (
              <button
                type="button"
                onClick={onDelete}
                title="Delete message"
                aria-label="Delete message"
                className="inline-flex items-center justify-center w-7 h-7 rounded border
                           bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
              >
                <Trash2 className="w-4 h-4" />
              </button>
            )}
          </div>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatComposer.tsx =====

// frontend/src/file_read/components/ChatComposer.tsx
import { useEffect, useRef, useState } from "react";
import ComposerActions from "./Composer/ComposerActions";
import AttachmentChip from "./Composer/AttachmentChip";
import { useAttachmentUploads } from "../hooks/useAttachmentUploads";
import type { Attachment } from "../types/chat";
import type { UIAttachment } from "../hooks/useAttachmentUploads";

const FORCE_SCROLL_EVT = "chat:force-scroll-bottom";

type Props = {
  input: string;
  setInput: (v: string) => void;
  loading: boolean;
  queued?: boolean;
  onSend: (text: string, attachments?: Attachment[]) => void | Promise<void>;
  onStop: () => void | Promise<void>;
  onHeightChange?: (h: number) => void;
  onRefreshChats?: () => void;
  sessionId?: string;
};

export default function ChatComposer({
  input,
  setInput,
  loading,
  queued = false,
  onSend,
  onStop,
  onHeightChange,
  onRefreshChats,
  sessionId,
}: Props) {
  const wrapRef = useRef<HTMLDivElement>(null);
  const taRef = useRef<HTMLTextAreaElement>(null);
  const fileRef = useRef<HTMLInputElement>(null);
  const MAX_HEIGHT_PX = 192;

  const [isClamped, setIsClamped] = useState(false);
  const [draft, setDraft] = useState(input);

  const { atts, addFiles, removeAtt, anyUploading, anyReady, attachmentsForPost, reset } =
    useAttachmentUploads(sessionId, onRefreshChats);

  useEffect(() => setDraft(input), [input]);

  const autogrow = () => {
    const ta = taRef.current;
    if (!ta) return;
    ta.style.height = "auto";
    const next = Math.min(ta.scrollHeight, MAX_HEIGHT_PX);
    ta.style.height = `${next}px`;
    setIsClamped(ta.scrollHeight > MAX_HEIGHT_PX);
    if (wrapRef.current && onHeightChange) {
      onHeightChange(wrapRef.current.getBoundingClientRect().height);
    }
  };

  useEffect(() => {
    autogrow();
    const onResize = () => autogrow();
    window.addEventListener("resize", onResize);
    return () => window.removeEventListener("resize", onResize);
  }, []);

  useEffect(() => {
    autogrow();
  }, [draft, atts.length]);

  const hasText = draft.trim().length > 0;

  const forceScroll = (behavior: ScrollBehavior = "auto") => {
    window.dispatchEvent(new CustomEvent(FORCE_SCROLL_EVT, { detail: { behavior } }));
  };

  const handleSendClick = async () => {
    const v = draft.trim();
    if ((loading || queued) || (!v && !anyReady) || anyUploading) return;
    forceScroll("auto");
    setDraft("");
    setInput("");
    reset();
    try {
      await onSend(v, attachmentsForPost());
    } finally {
      onRefreshChats?.();
      requestAnimationFrame(() => forceScroll("smooth"));
    }
  };

  const handleStopClick = () => {
    if (!loading && !queued) return;
    void Promise.resolve(onStop()).finally(() => onRefreshChats?.());
  };

  const pickFile = () => fileRef.current?.click();

  const onFilePicked: React.ChangeEventHandler<HTMLInputElement> = async (e) => {
    const files = e.target.files;
    if (!files || files.length === 0) return;
    if (!sessionId) {
      e.target.value = "";
      return;
    }
    await addFiles(files);
    e.target.value = "";
  };

  function onKeyDown(e: React.KeyboardEvent<HTMLTextAreaElement>) {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      void handleSendClick();
    }
  }

  const disableActions = loading || queued || anyUploading;
  const showSend = hasText || anyReady;

  return (
    <div ref={wrapRef} className="relative z-50 bg-white/95 backdrop-blur border-t p-3">
      {atts.length > 0 && (
        <div className="mb-2 flex flex-wrap gap-2">
          {atts.map((a: UIAttachment) => (
            <AttachmentChip key={a.uiId} a={a} onRemove={removeAtt} />
          ))}
        </div>
      )}

      <div className="flex gap-2">
        <input ref={fileRef} type="file" multiple className="hidden" onChange={onFilePicked} />

        <textarea
          ref={taRef}
          value={draft}
          onChange={(e) => {
            setDraft(e.target.value);
            setInput(e.target.value);
            autogrow();
          }}
          onInput={autogrow}
          onKeyDown={onKeyDown}
          placeholder="Ask anything…"
          className={`flex-1 border rounded-lg px-3 py-2 resize-none focus:outline-none focus:ring ${
            isClamped ? "overflow-y-auto" : "overflow-hidden"
          }`}
          rows={1}
          style={{ maxHeight: MAX_HEIGHT_PX }}
          disabled={queued}
        />

        <ComposerActions
          disabledUpload={disableActions || !sessionId}
          onPickFile={pickFile}
          showStop={loading || queued}
          onStop={handleStopClick}
          showSend={showSend}
          onSend={handleSendClick}
        />
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatContainer.tsx =====

// frontend/src/file_read/components/ChatContainer.tsx
import { useState, useRef, useEffect, useMemo } from "react";
import ChatView from "./ChatView/ChatView";
import ChatComposer from "./ChatComposer";
import BudgetBar from "./Budget/BudgetBar";
import type { ChatMsg } from "../types/chat";
import type { GenMetrics, RunJson } from "../shared/lib/runjson";
import type { Attachment } from "../types/chat";

interface Props {
  messages: ChatMsg[];
  input: string;
  setInput: (s: string) => void;
  loading: boolean;
  queued?: boolean;
  send: (text?: string, attachments?: Attachment[]) => Promise<void>;
  stop: () => Promise<void> | void;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onRefreshChats?: () => void;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
  sessionId?: string;
}

export default function ChatContainer({
  messages,
  input,
  setInput,
  loading,
  queued = false,
  send,
  stop,
  runMetrics,
  runJson,
  onRefreshChats,
  onDeleteMessages,
  autoFollow = true,
  sessionId,
}: Props) {
  const [composerH, setComposerH] = useState(0);
  const containerRef = useRef<HTMLDivElement>(null);
  const [pinned, setPinned] = useState(false);

  useEffect(() => {
    const el = containerRef.current;
    if (!el) return;
    const threshold = 120;
    const isNearBottom = () => el.scrollHeight - el.scrollTop - el.clientHeight <= threshold;
    const onScroll = () => setPinned(!isNearBottom());
    el.addEventListener("scroll", onScroll, { passive: true });
    setPinned(!isNearBottom());
    return () => el.removeEventListener("scroll", onScroll);
  }, []);

  const forceScrollToBottom = (behavior: ScrollBehavior = "smooth") => {
    const el = containerRef.current;
    if (!el) return;
    el.scrollTo({ top: el.scrollHeight, behavior });
  };

  const handleSend = async (text?: string, attachments?: Attachment[]) => {
    if (!pinned) forceScrollToBottom("auto");
    await send(text, attachments);
    onRefreshChats?.();
    if (!pinned) requestAnimationFrame(() => forceScrollToBottom("smooth"));
  };

  const runJsonForBar = useMemo<RunJson | null>(() => {
    if (runJson) return runJson;
    for (let i = messages.length - 1; i >= 0; i--) {
      const m: any = messages[i];
      if (m?.role === "assistant" && m?.meta?.runJson) return m.meta.runJson as RunJson;
    }
    return null;
  }, [runJson, messages]);

  return (
    <div className="flex flex-col h-full border rounded-lg overflow-hidden bg-white">
      <div ref={containerRef} data-chat-scroll className="flex-1 overflow-y-auto min-w-0">
        <ChatView
          messages={messages}
          loading={loading}
          queued={queued}
          bottomPad={composerH}
          runMetrics={runMetrics}
          runJson={runJson}
          onDeleteMessages={onDeleteMessages}
          autoFollow={autoFollow}
        />
      </div>

      <BudgetBar runJson={runJsonForBar ?? null} />

      <ChatComposer
        input={input}
        setInput={setInput}
        loading={loading}
        queued={queued}
        onSend={handleSend}
        onStop={stop}
        onHeightChange={setComposerH}
        onRefreshChats={onRefreshChats}
        sessionId={sessionId}
      />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatItem.tsx =====

// frontend/src/file_read/components/chat/ChatItem.tsx
import ChatBubble from "./ChatBubble";
import AssistantMetrics from "./AssistantMetrics";
import { buildStatus } from "./ChatView/StatusLine";
import type { ChatMsg } from "../types/chat";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function ChatItem({
  m,
  idx,
  loading,
  lastAssistantIndex,
  runJsonLive,
  runMetricsLive,
  onDelete,
}: {
  m: ChatMsg;
  idx: number;
  loading: boolean;
  lastAssistantIndex: number;
  runJsonLive?: RunJson | null;
  runMetricsLive?: GenMetrics | null;
  onDelete?: (id: string) => void;
}) {
  const isAssistant = m.role === "assistant";
  const isCurrentStreamingAssistant = isAssistant && loading && idx === lastAssistantIndex;

  let jsonForThis: RunJson | null = null;
  let flatForThis: GenMetrics | null = null;

  if (isAssistant) {
    // @ts-ignore meta bag
    const meta = m.meta as { runJson?: RunJson | null; flat?: GenMetrics | null } | undefined;
    jsonForThis = meta?.runJson ?? null;
    flatForThis = meta?.flat ?? null;

    if (isCurrentStreamingAssistant) {
      if (runJsonLive) jsonForThis = runJsonLive;
      if (runMetricsLive) flatForThis = runMetricsLive;
    }
  }

  const status = isAssistant ? buildStatus(jsonForThis, flatForThis) : "";
  const showMetrics = isAssistant && (jsonForThis || flatForThis);

  return (
    <div>
      <ChatBubble
        role={m.role}
        text={m.text}
        attachments={m.attachments} 
        showActions={m.role === "user" || (m.role === "assistant" && !isCurrentStreamingAssistant)}
        onDelete={onDelete ? () => onDelete(m.id) : undefined}
      />
      {showMetrics && <AssistantMetrics status={status} runJson={jsonForThis} flat={flatForThis} />}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/AccountPanel.tsx =====

// frontend/src/file_read/components/ChatSidebar/AccountPanel.tsx
import { useEffect, useMemo, useState } from "react";
import { ChevronDown, LogOut, Settings, HelpCircle, Stars, BookOpen, Wand2, Save } from "lucide-react";
import { useAuth } from "../../auth/AuthContext";

function initials(s: string) {
  const parts = (s || "").trim().split(/\s+/);
  if (!parts[0]) return "AC";
  if (parts.length === 1) return parts[0].slice(0, 2).toUpperCase();
  return (parts[0][0] + parts[1][0]).toUpperCase();
}

export default function AccountPanel() {
  const { user, setToken } = useAuth(); // local auth context
  const [open, setOpen] = useState(false);

  // --- identity display (prefer context; fallback to localStorage) ---
  const storedEmail = typeof localStorage !== "undefined" ? localStorage.getItem("profile_email") || "" : "";
  const display = user?.email || storedEmail || "Account";
  const tier = "Pro";
  const avatarText = useMemo(() => initials(display), [display]);

  // --- license key management (used by proxy / app limits) ---
  const [license, setLicense] = useState<string>("");
  const [saved, setSaved] = useState<null | "ok" | "err">(null);

  useEffect(() => {
    try {
      setLicense(localStorage.getItem("license_key") || "");
    } catch {
      /* ignore */
    }
  }, []);

  function saveLicense() {
    try {
      if (license.trim()) {
        localStorage.setItem("license_key", license.trim());
      } else {
        localStorage.removeItem("license_key");
      }
      setSaved("ok");
      setTimeout(() => setSaved(null), 1500);
    } catch {
      setSaved("err");
      setTimeout(() => setSaved(null), 2000);
    }
  }

  // --- actions ---
  function openSettings() {
    try { window.dispatchEvent(new CustomEvent("open:settings")); } catch {}
    setOpen(false);
  }
  function openKnowledge() {
    try { window.dispatchEvent(new CustomEvent("open:knowledge")); } catch {}
    setOpen(false);
  }
  function openCustomize() {
    try { window.dispatchEvent(new CustomEvent("open:customize")); } catch {}
    setOpen(false);
  }
  function openHelp() {
    // Swap with your docs/help URL
    window.open("https://yourdocs.example.com", "_blank", "noopener,noreferrer");
    setOpen(false);
  }
  function logout() {
    try {
      localStorage.removeItem("local_jwt");
      // keep profile_email / license_key unless you want to clear them too
      setToken?.(null);
    } finally {
      setOpen(false);
      // Simple UX: reload to reset state everywhere
      location.reload();
    }
  }

  return (
    <div className="p-2">
      <button
        onClick={() => setOpen((v) => !v)}
        className="w-full flex items-center gap-3 bg-gray-100 hover:bg-gray-200 active:bg-gray-300 transition rounded-xl px-3 py-2 text-left"
        aria-haspopup="menu"
        aria-expanded={open}
      >
        <div className="w-8 h-8 rounded-lg bg-slate-800 text-white grid place-items-center text-xs font-semibold">
          {avatarText}
        </div>
        <div className="min-w-0 flex-1">
          <div className="text-sm font-medium truncate">{display}</div>
          <div className="text-[11px] text-gray-600">
            <span className="inline-flex items-center gap-1 rounded-full border px-2 py-0.5 text-[10px]">
              {tier}
            </span>
          </div>
        </div>
        <ChevronDown className={`w-4 h-4 transition ${open ? "rotate-180" : ""}`} />
      </button>

      {open && (
        <div className="hidden md:block relative">
          <div className="fixed inset-0 z-30" onClick={() => setOpen(false)} aria-hidden />
          <div role="menu" className="absolute z-40 bottom-14 left-2 right-2 rounded-xl border bg-white shadow-xl overflow-hidden">
            <div className="px-3 py-2 text-xs text-gray-600 border-b truncate">{display}</div>

            {/* License Key (for proxy/app rate-limits) */}
            <div className="px-3 py-3 border-b bg-gray-50/60">
              <div className="text-[11px] font-medium text-gray-600 mb-1">License key</div>
              <div className="flex items-center gap-2">
                <input
                  value={license}
                  onChange={(e) => setLicense(e.target.value)}
                  placeholder="paste-your-key"
                  className="flex-1 rounded-lg border px-3 py-2 text-sm outline-none focus:ring-2 focus:ring-black"
                  spellCheck={false}
                />
                <button
                  onClick={saveLicense}
                  className="inline-flex items-center gap-1 rounded-lg border px-3 py-2 text-sm hover:bg-gray-50"
                >
                  <Save size={16} /> Save
                </button>
              </div>
              {saved === "ok" && <div className="mt-1 text-[11px] text-green-600">Saved</div>}
              {saved === "err" && <div className="mt-1 text-[11px] text-red-600">Couldn’t save</div>}
            </div>

            <button className="w-full flex items-center gap-2 px-3 py-2 text-sm hover:bg-gray-50" onClick={() => { setOpen(false); }}>
              <Stars className="w-4 h-4" /> Upgrade plan
            </button>
            <button className="w-full flex items-center gap-2 px-3 py-2 text-sm hover:bg-gray-50" onClick={openCustomize}>
              <Wand2 className="w-4 h-4" /> Customize
            </button>
            <button className="w-full flex items-center gap-2 px-3 py-2 text-sm hover:bg-gray-50" onClick={openKnowledge}>
              <BookOpen className="w-4 h-4" /> Knowledge
            </button>
            <button className="w-full flex items-center gap-2 px-3 py-2 text-sm hover:bg-gray-50" onClick={openSettings}>
              <Settings className="w-4 h-4" /> Settings
            </button>
            <button className="w-full flex items-center gap-2 px-3 py-2 text-sm hover:bg-gray-50" onClick={openHelp}>
              <HelpCircle className="w-4 h-4" /> Help
            </button>
            <button className="w-full flex items-center gap-2 px-3 py-2 text-sm hover:bg-gray-50 text-red-600" onClick={logout}>
              <LogOut className="w-4 h-4" /> Log out
            </button>
          </div>
        </div>
      )}

      {open && (
        <div className="md:hidden">
          <div className="fixed inset-0 z-40 bg-black/30" onClick={() => setOpen(false)} />
          <div className="fixed inset-x-0 bottom-0 z-50 rounded-t-2xl bg-white shadow-2xl">
            <div className="px-4 pt-4 pb-2 text-sm text-gray-600 truncate border-b">{display}</div>

            {/* License Key (mobile) */}
            <div className="p-4 border-b bg-gray-50/60">
              <div className="text-[11px] font-medium text-gray-600 mb-1">License key</div>
              <div className="flex items-center gap-2">
                <input
                  value={license}
                  onChange={(e) => setLicense(e.target.value)}
                  placeholder="paste-your-key"
                  className="flex-1 rounded-lg border px-3 py-2 text-sm outline-none focus:ring-2 focus:ring-black"
                  spellCheck={false}
                />
                <button
                  onClick={saveLicense}
                  className="inline-flex items-center gap-1 rounded-lg border px-3 py-2 text-sm hover:bg-gray-50"
                >
                  <Save size={16} /> Save
                </button>
              </div>
              {saved === "ok" && <div className="mt-1 text-[11px] text-green-600">Saved</div>}
              {saved === "err" && <div className="mt-1 text-[11px] text-red-600">Couldn’t save</div>}
            </div>

            <div className="p-2">
              <button className="w-full flex items-center gap-2 px-3 py-3 rounded-lg hover:bg-gray-50" onClick={() => { setOpen(false); }}>
                <Stars className="w-4 h-4" /> Upgrade plan
              </button>
              <button className="w-full flex items-center gap-2 px-3 py-3 rounded-lg hover:bg-gray-50" onClick={openCustomize}>
                <Wand2 className="w-4 h-4" /> Customize
              </button>
              <button className="w-full flex items-center gap-2 px-3 py-3 rounded-lg hover:bg-gray-50" onClick={openKnowledge}>
                <BookOpen className="w-4 h-4" /> Knowledge
              </button>
              <button className="w-full flex items-center gap-2 px-3 py-3 rounded-lg hover:bg-gray-50" onClick={openSettings}>
                <Settings className="w-4 h-4" /> Settings
              </button>
              <button className="w-full flex items-center gap-2 px-3 py-3 rounded-lg hover:bg-gray-50 text-red-600" onClick={logout}>
                <LogOut className="w-4 h-4" /> Log out
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/ChatSidebar.tsx =====

import { useState } from "react";
import { deleteChatsBatch } from "../../data/chatApi";
import type { ChatRow } from "../../types/chat";
import { useMultiSelect } from "../../hooks/useMultiSelect";
import { useChatsPager } from "../../hooks/useChatsPager";
import SidebarHeader from "./SidebarHeader";
import SidebarListItem from "./SidebarListItem";
import AccountPanel from "./AccountPanel";

const PAGE_SIZE = 10;

type Props = {
  onOpen: (id: string) => Promise<void>;
  onNew: () => Promise<void>;
  refreshKey?: number;
  activeId?: string;
  onHideSidebar?: () => void;
  onCancelSessions?: (ids: string[]) => Promise<void>;
};

export default function ChatSidebar({
  onOpen, onNew, refreshKey, activeId, onHideSidebar, onCancelSessions,
}: Props) {
  const {
    chats, page, hasMore, total, totalPages,
    initialLoading, loadingMore,
    scrollRef, sentinelRef, loadMore, setChats, decTotal,
  } = useChatsPager(PAGE_SIZE, refreshKey);

  const [isEditing, setIsEditing] = useState(false);
  const [deleting, setDeleting] = useState(false);
  const [newPending, setNewPending] = useState(false);

  const allIds = chats.map(c => c.sessionId);
  const { selected, setSelected, allSelected, toggleOne, toggleAll } = useMultiSelect(allIds);

  async function handleNew() {
    if (newPending) return;
    setNewPending(true);
    try { await onNew(); } finally { setNewPending(false); }
  }

  async function onDeleteSelected(): Promise<void> {
    const count = selected.size;
    if (!count || deleting) return;

    const isAll = count === chats.length;
    const ok = window.confirm(
      isAll
        ? `Delete ALL ${count} chats? This cannot be undone.`
        : `Delete ${count} selected chat${count > 1 ? "s" : ""}?`
    );
    if (!ok) return;

    const ids = [...selected];
    try { await onCancelSessions?.(ids); await Promise.resolve(); } catch {}

    const deletingActive = activeId ? selected.has(activeId) : false;
    const fallback = chats.find(c => !selected.has(c.sessionId))?.sessionId;

    setDeleting(true);
    try {
      const deleted = await deleteChatsBatch(ids);
      if (!deleted.length) return;
      setChats(prev => prev.filter(c => !deleted.includes(c.sessionId)));
      decTotal(deleted.length);
      setSelected(new Set());
      setIsEditing(false);

      if (deletingActive && fallback) {
        void onOpen(fallback);
      }
    } finally {
      setDeleting(false);
    }
  }

  return (
    <aside className="w-full md:w-72 h-full border-r bg-white p-0 flex flex-col">
      <SidebarHeader
        isEditing={isEditing}
        setIsEditing={(v) => { setIsEditing(v); setSelected(new Set()); }}
        newPending={newPending}
        onNew={handleNew}
        onHideSidebar={onHideSidebar}
        selectedCount={selected.size}
        deleting={deleting}
        onDelete={onDeleteSelected}
      />

      <div
        ref={scrollRef}
        className="flex-1 overflow-y-auto p-2 overscroll-contain"
        style={{ WebkitOverflowScrolling: "touch" }}
      >
        {initialLoading && (
          <div className="px-2 py-1 text-xs text-gray-500">Loading…</div>
        )}

        <ul className="space-y-1">
          {chats.map((c: ChatRow) => (
            <SidebarListItem
              key={c.sessionId}
              c={c}
              isActive={activeId === c.sessionId}
              isEditing={isEditing}
              isChecked={selected.has(c.sessionId)}
              onToggle={() => toggleOne(c.sessionId)}
              onOpen={() => void onOpen(c.sessionId)}
            />
          ))}
        </ul>

        <div className="h-6" ref={sentinelRef} />

        {hasMore && (
          <div className="px-2 pb-2">
            <button
              className={`w-full text-xs px-3 py-1 rounded border ${loadingMore ? "opacity-50 cursor-wait" : ""}`}
              onClick={() => void loadMore()}
              disabled={loadingMore}
              title="Load next page"
            >
              {loadingMore ? "Loading…" : `Load more (${chats.length}/${total || "?"})`}
            </button>
          </div>
        )}

        {!hasMore && chats.length > 0 && (
          <div className="px-2 py-2 text-[11px] text-gray-400 text-center">
            End of list • showing {chats.length} of {total || chats.length}
          </div>
        )}
      </div>

      <div className="border-t px-3 py-2 text-[11px] text-gray-500">
        <span className="uppercase tracking-wide">Chats</span>{" "}
        <span className="text-gray-400">
          ({chats.length}{total ? `/${total}` : ""} • page {Math.max(page, 1)} of {Math.max(totalPages || 1, 1)})
        </span>
        {isEditing && (
          <label className="ml-2 text-[11px]">
            <input
              type="checkbox"
              className="mr-1 align-middle"
              checked={allSelected}
              onChange={toggleAll}
            />
            Select all
          </label>
        )}
      </div>

      <div className="border-t">
        <AccountPanel />
      </div>
    </aside>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/SidebarHeader.tsx =====

import { PanelLeftClose, Plus, Pencil, Trash2 } from "lucide-react";

export default function SidebarHeader({
  isEditing, setIsEditing, newPending, onNew, onHideSidebar,
  selectedCount, deleting, onDelete,
}: {
  isEditing: boolean;
  setIsEditing: (v: boolean) => void;
  newPending: boolean;
  onNew: () => Promise<void>;
  onHideSidebar?: () => void;
  selectedCount: number;
  deleting: boolean;
  onDelete: () => void;
}) {
  return (
    <div className="sticky top-0 z-10 bg-white border-b">
      <div className="flex items-center justify-between px-3 py-2">
        <div className="text-[11px] md:text-xs uppercase text-gray-500">Chats</div>
        <div className="flex items-center gap-2">
          <button
            className={`h-9 px-3 inline-flex items-center gap-2 justify-center rounded border ${
              newPending ? "opacity-50 cursor-not-allowed" : ""
            }`}
            onClick={async () => { if (!newPending) await onNew(); }}
            title="New chat"
            disabled={newPending}
          >
            <Plus className="w-4 h-4" />
            <span className="text-xs md:text-[11px] leading-none">New</span>
          </button>

          <button
            className="h-9 px-3 inline-flex items-center gap-2 justify-center rounded border"
            onClick={() => setIsEditing(!isEditing)}
            aria-pressed={isEditing}
            title={isEditing ? "Exit edit mode" : "Edit chats"}
          >
            <Pencil className="w-4 h-4" />
            <span className="text-xs md:text-[11px] leading-none">
              {isEditing ? "Done" : "Edit"}
            </span>
          </button>

          {onHideSidebar && (
            <button
              className="hidden md:inline-flex h-9 w-9 items-center justify-center rounded border"
              onClick={onHideSidebar}
              title="Hide sidebar"
              aria-label="Hide sidebar"
            >
              <PanelLeftClose className="w-4 h-4" />
            </button>
          )}
        </div>
      </div>

      {isEditing && (
        <div className="px-3 py-2 border-t bg-white flex items-center gap-3">
          <div className="text-sm text-gray-600">{selectedCount} selected</div>
          <button
            className={`ml-auto inline-flex items-center gap-2 text-sm px-3 py-1 rounded ${
              selectedCount && !deleting
                ? "bg-red-600 text-white"
                : "bg-gray-200 text-gray-500 cursor-not-allowed"
            }`}
            disabled={!selectedCount || deleting}
            onClick={onDelete}
            title={selectedCount ? "Delete selected chats" : "Select chats to delete"}
          >
            <Trash2 className="w-4 h-4" />
            {deleting ? "Deleting…" : `Delete (${selectedCount})`}
          </button>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/SidebarListItem.tsx =====

import { firstLineSmart } from "../../shared/lib/text";
import type { ChatRow } from "../../types/chat";

export default function SidebarListItem({
  c, isActive, isEditing, isChecked, onToggle, onOpen,
}: {
  c: ChatRow;
  isActive: boolean;
  isEditing: boolean;
  isChecked: boolean;
  onToggle: () => void;
  onOpen: () => void;
}) {
  const displayTitle =
    (c.title && c.title.trim()) ||
    firstLineSmart(c.lastMessage || "", 48) ||
    "New Chat";
  const preview = c.lastMessage ? firstLineSmart(c.lastMessage, 120) : "";

  return (
    <li>
      <div
        className={`w-full flex items-start gap-2 px-2 py-2 rounded ${
          isActive ? "bg-black text-white" : "hover:bg-gray-50"
        }`}
      >
        {isEditing && (
          <input
            type="checkbox"
            checked={isChecked}
            onChange={onToggle}
            className="mt-1"
            aria-label={`Select chat ${displayTitle}`}
          />
        )}
        <button
          className="text-left flex-1"
          aria-current={isActive ? "true" : undefined}
          onClick={() => { if (!isEditing) onOpen(); }}
          title={displayTitle}
        >
          <div className="font-medium truncate">{displayTitle}</div>
          {preview && (
            <div className={`text-xs line-clamp-2 ${isActive ? "text-white/80" : "text-gray-500"}`}>
              {preview}
            </div>
          )}
          <div className="text-[10px] mt-1 opacity-60">
            {new Date(c.updatedAt).toLocaleString()}
          </div>
        </button>
      </div>
    </li>
  );
}

# ===== frontend/src/file_read/components/ChatView/ChatView.tsx =====

// frontend/src/file_read/components/ChatView/ChatView.tsx
import type { ChatMsg } from "../../types/chat";
import type { GenMetrics, RunJson } from "../../hooks/useChatStream";
import ChatItem from "../ChatItem";
import TypingIndicator from "../../shared/ui/TypingIndicator";
import { useChatAutofollow } from "../../hooks/useChatAutoFollow";

export default function ChatView({
  messages,
  loading,
  queued = false,
  bottomPad,
  runMetrics,
  runJson,
  onDeleteMessages,
  autoFollow = true,
}: {
  messages: ChatMsg[];
  loading: boolean;
  queued?: boolean;
  bottomPad: number;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
}) {
  const { listRef, bottomRef, lastAssistantIndex } = useChatAutofollow({
    messages,
    loading,
    autoFollow,
    bottomPad,
  });

  const lastMsg = messages[messages.length - 1];

  return (
    <div
      ref={listRef}
      className="p-4 space-y-3 bg-gray-50 min-w-0"
      style={{ paddingBottom: bottomPad }}
    >
      {messages.map((m, idx) => (
        <ChatItem
          key={m.id}
          m={m}
          idx={idx}
          loading={loading}
          lastAssistantIndex={lastAssistantIndex}
          runJsonLive={runJson ?? null}
          runMetricsLive={runMetrics ?? null}
          onDelete={onDeleteMessages ? (id) => onDeleteMessages([id]) : undefined}
        />
      ))}

      {(loading || queued) &&
        !(lastMsg?.role === "assistant" && (lastMsg.text?.trim().length ?? 0) > 0) && (
          <TypingIndicator />
        )}

      <div ref={bottomRef} className="h-0" />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatView/StatusLine.ts =====

// frontend/src/file_read/components/chat/StatusLine.ts
import type { RunJson, GenMetrics } from "../../shared/lib/runjson";

const oneDec = (n?: number | null) =>
  typeof n === "number" && Number.isFinite(n) ? n.toFixed(1) : undefined;

export function buildStatus(json?: RunJson | null, flat?: GenMetrics | null) {
  const st = json?.stats;
  if (st) {
    const parts: string[] = [];
    if (st.predictedTokensCount != null) parts.push(`${st.predictedTokensCount} tok`);
    if (st.tokensPerSecond != null) parts.push(`${oneDec(st.tokensPerSecond) ?? st.tokensPerSecond} tok/s`);
    if (st.timeToFirstTokenSec != null) parts.push(`TTFT ${Math.round(st.timeToFirstTokenSec * 1000)} ms`);
    if (st.stopReason) parts.push(`stop: ${st.stopReason}`);
    return parts.join(" • ");
  }
  if (flat) {
    const parts: string[] = [];
    if (flat.output_tokens != null) parts.push(`${flat.output_tokens} tok`);
    if (flat.tok_per_sec != null) parts.push(`${oneDec(flat.tok_per_sec) ?? flat.tok_per_sec} tok/s`);
    if (flat.ttft_ms != null) parts.push(`TTFT ${Math.round(flat.ttft_ms)} ms`);
    if (flat.stop_reason) parts.push(`stop: ${flat.stop_reason}`);
    return parts.join(" • ");
  }
  return "";
}

# ===== frontend/src/file_read/components/Composer/AttachmentChip.tsx =====

import { X, Check } from "lucide-react";
import ProgressBar from "./ProgressBar";
import type { Att } from "../../hooks/useAttachmentUploads";

export default function AttachmentChip({
  a,
  onRemove,
}: {
  a: Att;
  onRemove: (a: Att) => void;
}) {
  return (
    <div className="min-w-[160px] max-w-[280px] border rounded-lg px-2 py-2">
      <div className="flex items-center justify-between gap-2">
        <div className="truncate text-sm" title={a.name}>
          {a.name}
        </div>
        <button
          className="p-1 rounded hover:bg-gray-100"
          aria-label="Remove file"
          onClick={() => onRemove(a)}
        >
          <X size={14} />
        </button>
      </div>
      <ProgressBar pct={a.pct} error={a.status === "error"} />
      <div className="mt-1 text-xs text-gray-500 flex items-center gap-1">
        {a.status === "uploading" && <span>Uploading… {a.pct}%</span>}
        {a.status === "ready" && (
          <>
            <Check size={14} /> Ready
          </>
        )}
        {a.status === "error" && <span>Error</span>}
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Composer/ComposerActions.tsx =====

import { Paperclip, Square, SendHorizonal } from "lucide-react";

type Props = {
  disabledUpload: boolean;
  onPickFile: () => void;
  showStop: boolean;
  onStop: () => void;
  showSend: boolean;
  onSend: () => void;
};

export default function ComposerActions({
  disabledUpload,
  onPickFile,
  showStop,
  onStop,
  showSend,
  onSend,
}: Props) {
  return (
    <div className="flex items-end gap-2">
      <button
        className={`p-2 rounded-lg border hover:bg-gray-50 ${disabledUpload ? "opacity-60 cursor-not-allowed" : ""}`}
        onClick={onPickFile}
        title="Upload to this chat"
        aria-label="Upload to this chat"
        disabled={disabledUpload}
      >
        <Paperclip size={18} />
      </button>

      {showStop ? (
        <button
          className="p-2 rounded-lg border hover:bg-gray-50"
          onClick={onStop}
          title="Stop generating"
          aria-label="Stop generating"
        >
          <Square size={18} />
        </button>
      ) : showSend ? (
        <button
          className="p-2 rounded-lg bg-black text-white hover:bg-black/90 active:translate-y-px"
          onClick={onSend}
          title="Send"
          aria-label="Send"
        >
          <SendHorizonal size={18} />
        </button>
      ) : null}
    </div>
  );
}

# ===== frontend/src/file_read/components/Composer/ProgressBar.tsx =====

export default function ProgressBar({ pct, error }: { pct: number; error?: boolean }) {
  return (
    <div className="mt-2 h-1.5 w-full bg-gray-200 rounded">
      <div
        className={`h-1.5 rounded ${error ? "bg-red-500" : "bg-black"}`}
        style={{ width: `${pct}%` }}
      />
    </div>
  );
}

# ===== frontend/src/file_read/components/DesktopHeader.tsx =====

import { PanelLeftOpen } from "lucide-react";

export default function DesktopHeader({
  sidebarOpen,
  onShowSidebar,
  title = "Local AI Model",
}: {
  sidebarOpen: boolean;
  onShowSidebar: () => void;
  title?: string;
}) {
  return (
    <div className="hidden md:flex h-14 shrink-0 items-center justify-between px-4 border-b bg-white">
      <div className="flex items-center gap-2">
        {!sidebarOpen && (
          <button
            className="h-9 w-9 inline-flex items-center justify-center rounded-lg border hover:bg-gray-50"
            onClick={onShowSidebar}
            aria-label="Show sidebar"
            title="Show sidebar"
          >
            <PanelLeftOpen className="w-4 h-4" />
          </button>
        )}
        <div className="font-semibold">{title}</div>
      </div>
      <div />
    </div>
  );
}

# ===== frontend/src/file_read/components/KnowledgePanel.tsx =====

import { useState, useEffect } from "react";
import {
  uploadRag,
  searchRag,
  listUploads,
  deleteUploadHard,
  type UploadRow,
} from "../data/ragApi";

export default function KnowledgePanel({
  sessionId,
  onClose,
  toast,
}: {
  sessionId?: string;
  onClose?: () => void;
  toast?: (msg: string) => void;
}) {
  const [files, setFiles] = useState<FileList | null>(null);
  const [busy, setBusy] = useState(false);
  const [query, setQuery] = useState("");
  const [hits, setHits] = useState<{ text: string; source?: string; score: number }[]>([]);
  const [searching, setSearching] = useState(false);

  const [scope, setScope] = useState<"all" | "session">("all");
  const [uploads, setUploads] = useState<UploadRow[]>([]);
  const [loadingUploads, setLoadingUploads] = useState(false);

  useEffect(() => {
    void refreshUploads();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [scope, sessionId]);

  async function refreshUploads() {
    setLoadingUploads(true);
    try {
      const out = await listUploads(sessionId, scope);
      setUploads(out.uploads || []);
    } catch (e: any) {
      toast?.(e?.message || "Failed to load uploads");
    } finally {
      setLoadingUploads(false);
    }
  }

  async function handleDeleteHard(source: string, ns?: string | null) {
    try {
      const res = await deleteUploadHard(source, ns ?? undefined);
      toast?.(`Removed ${res.removed} chunk${res.removed === 1 ? "" : "s"}. Remaining: ${res.remaining}`);
      await refreshUploads();
    } catch (e: any) {
      toast?.(e?.message || "Delete failed");
    }
  }

  async function doUpload() {
    if (!files || !files.length) return;
    setBusy(true);
    try {
      let total = 0;
      for (const f of Array.from(files)) {
        const out = await uploadRag(f, undefined);
        total += (out as any)?.added || 0;
      }
      toast?.(`Added ${total} chunk${total === 1 ? "" : "s"}`);
      setFiles(null);
      await refreshUploads();
    } catch (e: any) {
      toast?.(e?.message || "Upload failed");
    } finally {
      setBusy(false);
    }
  }

  async function doSearch() {
    const q = query.trim();
    if (!q) return;
    setSearching(true);
    try {
      const out = await searchRag(q, { sessionId, kChat: 6, kGlobal: 4, alpha: 0.5 });
      setHits(out.hits || []);
    } catch (e: any) {
      toast?.(e?.message || "Search failed");
    } finally {
      setSearching(false);
    }
  }

  return (
    <div className="fixed inset-0 z-50 bg-black/40 flex items-center justify-center p-3">
      <div className="w-full max-w-5xl rounded-2xl bg-white shadow-xl border overflow-hidden">
        <div className="px-4 py-3 border-b flex items-center gap-2">
          <div className="font-semibold">Knowledge</div>
          <div className="ml-auto flex items-center gap-2">
            <button className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50" onClick={onClose}>
              Close
            </button>
          </div>
        </div>

        <div className="p-4 grid gap-6 md:grid-cols-2">
          {/* Upload */}
          <div>
            <div className="font-medium mb-2">Upload documents</div>
            <input type="file" multiple className="block w-full text-sm" onChange={(e) => setFiles(e.target.files)} />
            <button
              className={`mt-2 text-sm px-3 py-1.5 rounded ${busy ? "opacity-60 cursor-not-allowed" : "bg-black text-white"}`}
              disabled={busy || !files || files.length === 0}
              onClick={doUpload}
            >
              {busy ? "Uploading…" : "Upload"}
            </button>
            <div className="text-[11px] text-gray-500 mt-2">
              Tip: CSV, TXT, MD, PDF (text extracted). Uploads can be global or per chat.
            </div>

            <div className="mt-6">
              <div className="flex items-center gap-2 mb-2">
                <div className="font-medium">Your uploads</div>
                <select
                  className="ml-auto border rounded px-2 py-1 text-xs"
                  value={scope}
                  onChange={(e) => setScope(e.target.value as "all" | "session")}
                  title="Scope"
                >
                  <option value="all">All (global + this chat)</option>
                  <option value="session">This chat only</option>
                </select>
                <button
                  className="text-xs px-2 py-1 rounded border hover:bg-gray-50"
                  onClick={refreshUploads}
                  disabled={loadingUploads}
                >
                  {loadingUploads ? "Refreshing…" : "Refresh"}
                </button>
              </div>

              <ul className="space-y-2 max-h-64 overflow-auto">
                {uploads.length === 0 && (
                  <li className="text-xs text-gray-500">No uploads yet.</li>
                )}
                {uploads.map((u, i) => (
                  <li key={`${u.source}-${u.sessionId ?? "global"}-${i}`} className="p-2 border rounded bg-gray-50">
                    <div className="flex items-center gap-2">
                      <div className="font-mono text-xs break-all">{u.source}</div>
                      <span className="text-[11px] text-gray-500">
                        {u.sessionId ? "session" : "global"} • {u.chunks} chunk{u.chunks === 1 ? "" : "s"}
                      </span>
                      <button
                        className="ml-auto text-xs px-2 py-1 rounded border hover:bg-gray-100"
                        title="Delete (hard delete by Source)"
                        onClick={() => handleDeleteHard(u.source, u.sessionId ?? undefined)}
                      >
                        Delete
                      </button>
                    </div>
                  </li>
                ))}
              </ul>
            </div>
          </div>

          {/* Search */}
          <div>
            <div className="font-medium mb-2">Quick search</div>
            <div className="flex gap-2">
              <input
                value={query}
                onChange={(e) => setQuery(e.target.value)}
                placeholder="Find in your knowledge…"
                className="flex-1 border rounded px-2 py-1.5 text-sm"
              />
              <button
                className={`text-sm px-3 py-1.5 rounded ${searching ? "opacity-60 cursor-wait" : "border hover:bg-gray-50"}`}
                onClick={doSearch}
                disabled={searching}
              >
                {searching ? "Searching…" : "Search"}
              </button>
            </div>

            <ul className="mt-3 space-y-2 max-h-64 overflow-auto">
              {hits.map((h, i) => (
                <li key={i} className="p-2 border rounded bg-gray-50">
                  <div className="text-[11px] text-gray-500 mb-1">
                    {h.source || "uploaded"} • score {Number.isFinite(h.score) ? h.score.toFixed(3) : "—"}
                  </div>
                  <div className="text-sm whitespace-pre-wrap">{h.text}</div>
                </li>
              ))}
              {!hits.length && <li className="text-xs text-gray-500">No results yet.</li>}
            </ul>
          </div>
        </div>

        <div className="px-4 py-3 border-t text-[11px] text-gray-500">
          “Delete” performs a hard delete: removes chunks for that Source and rebuilds the index.
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Markdown/MarkdownMessage.tsx =====

// frontend/src/file_read/components/MarkdownMessage.tsx
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import rehypeHighlight from "rehype-highlight";
import "highlight.js/styles/github.css";
import CodeCopyButton from "../../shared/ui/CodeCopyButton";

type Props = { text: string };

export default function MarkdownMessage({ text }: Props) {
  return (
    <>
      {/* keep pre spacing at zero; don't overwrite token colors */}
      <style>{`
        pre { margin: 0 !important; padding: 0 !important; background: transparent !important; }
        pre code { display: block; margin: 0 !important; padding: 0 !important; }
        .hljs { background: transparent !important; }
      `}</style>

      <ReactMarkdown
        remarkPlugins={[remarkGfm]}
        rehypePlugins={[[rehypeHighlight, { detect: true, ignoreMissing: true }]]}
        components={{
          code({
            inline,
            className,
            children,
            ...props
          }: {
            inline?: boolean;
            className?: string;
            children?: React.ReactNode;
          }) {
            const raw = String(children ?? "");
            const lang = (className || "").replace("language-", "");

            if (inline) {
              return (
                <code
                  className="px-1.5 py-0.5 rounded bg-gray-100 text-gray-900 font-mono text-[14px]"
                  {...props}
                >
                  {children}
                </code>
              );
            }

            return (
              <div className="relative w-full">
                <pre className="m-0 p-0 w-full overflow-x-auto rounded-md border border-gray-300">
                  {/* Let hljs theme color tokens; no text color override here */}
                  <code className={`${className ?? ""} hljs font-mono text-sm`} {...props}>
                    {children}
                  </code>
                </pre>

                <div className="absolute top-2 right-2 flex items-center gap-1">
                  {lang && (
                    <span className="text-[11px] px-1.5 py-0.5 rounded bg-gray-200 text-gray-700">
                      {lang}
                    </span>
                  )}
                  <CodeCopyButton text={raw} />
                </div>
              </div>
            );
          },
        }}
      >
        {text}
      </ReactMarkdown>
    </>
  );
}

# ===== frontend/src/file_read/components/MetricsHoverCard.tsx =====

// frontend/src/file_read/components/MetricsHoverCard.tsx
import { useEffect, useLayoutEffect, useRef, useState } from "react";
import { Info } from "lucide-react";
import MetricsHoverCardPanel from "./MetricsHoverCardPanel";

type Props = {
  data: unknown;
  title?: string;
  align?: "left" | "right";
  maxWidthPx?: number;
  compact?: boolean;
};

export default function MetricsHoverCard({
  data,
  title = "Run details",
  align = "right",
  maxWidthPx = 460,
  compact = true,
}: Props) {
  const [open, setOpen] = useState(false);
  const btnRef = useRef<HTMLButtonElement>(null);
  const panelRef = useRef<HTMLDivElement>(null);
  const [panelStyle, setPanelStyle] = useState<{ top: number; left: number; width: number } | null>(null);

  useLayoutEffect(() => {
    function place() {
      if (!open || !btnRef.current) return;
      const margin = 8;
      const vw = window.innerWidth;
      const vh = window.innerHeight;
      const width = Math.min(maxWidthPx, vw - margin * 2);
      const btnBox = btnRef.current.getBoundingClientRect();
      let left = align === "right" ? btnBox.right - width : btnBox.left;
      left = Math.max(margin, Math.min(left, vw - margin - width));
      let top = btnBox.bottom + margin;
      let panelH = panelRef.current?.offsetHeight || 0;
      if (!panelH) panelH = 360 + 44;
      if (top + panelH > vh - margin) top = Math.max(margin, btnBox.top - margin - panelH);
      setPanelStyle({ top, left, width });
    }
    place();
    if (!open) return;
    const onReflow = () => place();
    window.addEventListener("resize", onReflow);
    window.addEventListener("scroll", onReflow, true);
    return () => {
      window.removeEventListener("resize", onReflow);
      window.removeEventListener("scroll", onReflow, true);
    };
  }, [open, align, maxWidthPx]);

  useEffect(() => {
    if (!open) return;
    const onKey = (e: KeyboardEvent) => {
      if (e.key === "Escape") {
        e.preventDefault();
        setOpen(false);
        btnRef.current?.focus();
      }
    };
    const onDown = (e: MouseEvent) => {
      const t = e.target as Node;
      if (panelRef.current?.contains(t)) return;
      if (btnRef.current?.contains(t)) return;
      setOpen(false);
    };
    window.addEventListener("keydown", onKey);
    document.addEventListener("mousedown", onDown);
    return () => {
      window.removeEventListener("keydown", onKey);
      document.removeEventListener("mousedown", onDown);
    };
  }, [open]);

  return (
    <div className="relative inline-block">
      <button
        ref={btnRef}
        type="button"
        className={`inline-flex items-center justify-center rounded border bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition ${compact ? "h-7 w-7" : "h-8 w-8"}`}
        title="Show run JSON"
        aria-haspopup="dialog"
        aria-expanded={open ? "true" : "false"}
        onClick={() => setOpen((v) => !v)}
        onMouseEnter={() => setOpen(true)}
      >
        <Info className={compact ? "w-4 h-4" : "w-5 h-5"} />
      </button>
      {open && panelStyle && (
        <div
          ref={panelRef}
          role="dialog"
          aria-label={title}
          className="fixed z-50"
          style={{ top: panelStyle.top, left: panelStyle.left, width: panelStyle.width }}
          onMouseLeave={() => setOpen(false)}
        >
          <MetricsHoverCardPanel data={data} title={title} />
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/MetricsHoverCardPanel.tsx =====

// frontend/src/file_read/components/MetricsHoverCardPanel.tsx
import { useMemo, useState } from "react";
import { Copy, Check, X } from "lucide-react";
import type { RunJson } from "../shared/lib/runjson";
import {
  getNormalizedBudget,
  getRagTelemetry,
  getWebTelemetry,
  getTimingMetrics,
  getPackTelemetry,
} from "../shared/lib/runjson";

type PanelProps = {
  data: unknown;
  title: string;
};

const num0 = (v: unknown) => (typeof v === "number" && Number.isFinite(v) ? v : 0);

export default function MetricsHoverCardPanel({ data, title }: PanelProps) {
  const [copied, setCopied] = useState(false);

  const json = useMemo(() => {
    try {
      const r = (data ?? null) as RunJson | null;
      if (!r || typeof r !== "object") return JSON.stringify(data, null, 2);

      const nb = getNormalizedBudget(r);
      const rag = getRagTelemetry(r) as any | null;
      const web = getWebTelemetry(r) as any | null;
      const pack = getPackTelemetry(r) as any | null;
      const timing = getTimingMetrics(r) as any | null;

      const modelCtx = nb ? num0(nb.modelCtx) : null;
      const clampMargin = nb ? num0(nb.clampMargin) : null;
      const inputTokensEst = nb ? num0(nb.inputTokensEst) : null;
      const outBudgetChosen = nb ? num0(nb.outBudgetChosen) : null;
      const outActual = num0((r as any)?.stats?.predictedTokensCount);
      const outShown = outActual || (outBudgetChosen ?? 0);

      const webRouteSec = web?.elapsedSec;
      const webFetchSec = web?.fetchElapsedSec;
      const webInjectSec = web?.injectElapsedSec;
      const webPre =
        num0(web?.breakdown?.totalWebPreTtftSec) ||
        (num0(webRouteSec) + num0(webFetchSec) + num0(webInjectSec));

      const ragDelta = Math.max(
        0,
        num0((rag as any)?.ragTokensAdded) ||
          num0((rag as any)?.blockTokens) ||
          num0((rag as any)?.blockTokensApprox) ||
          num0((rag as any)?.sessionOnlyTokensApprox)
      );
      const ragPctOfInput =
        inputTokensEst && inputTokensEst > 0 ? Math.round((ragDelta / inputTokensEst) * 100) : 0;

      const packPackSec = num0(pack?.packSec);
      const packSummarySec = num0(pack?.summarySec);
      const packFinalTrimSec = num0(pack?.finalTrimSec);
      const packCompressSec = num0(pack?.compressSec);

      const preModelSec = num0(timing?.preModelSec);
      const modelQueueSec = num0(timing?.modelQueueSec);
      const genSec = num0(timing?.genSec);
      const ttftSec = num0(timing?.ttftSec);

      const breakdown = (r as any)?.budget_view?.breakdown || (r as any)?.stats?.budget?.breakdown || null;

      const preAccountedFromBackend = breakdown?.preTtftAccountedSec;
      const accountedFallback =
        webPre +
        num0((rag as any)?.routerDecideSec) +
        num0((rag as any)?.injectBuildSec ?? (rag as any)?.blockBuildSec ?? (rag as any)?.sessionOnlyBuildSec) +
        packPackSec +
        packSummarySec +
        packFinalTrimSec +
        packCompressSec +
        preModelSec +
        modelQueueSec;

      const accounted = Number.isFinite(preAccountedFromBackend) ? preAccountedFromBackend : accountedFallback;

      const unattributed =
        breakdown && Number.isFinite(breakdown.unattributedTtftSec)
          ? breakdown.unattributedTtftSec
          : Math.max(0, ttftSec - accounted);

      const promptTok = num0((r as any)?.stats?.promptTokensCount) || (inputTokensEst ?? 0);
      const decodeTok = num0((r as any)?.stats?.predictedTokensCount);
      const encodeTps = modelQueueSec > 0 ? promptTok / modelQueueSec : null;
      const decodeTps = genSec > 0 ? decodeTok / genSec : null;

      const totalTok =
        typeof (r as any)?.stats?.totalTokensCount === "number"
          ? ((r as any).stats.totalTokensCount as number)
          : promptTok + decodeTok;
      const totalSecForOverall =
        typeof (r as any)?.stats?.totalTimeSec === "number"
          ? ((r as any).stats.totalTimeSec as number)
          : num0(timing?.totalSec);
      const overallTps = totalSecForOverall > 0 ? totalTok / totalSecForOverall : null;

      const usedCtx = (inputTokensEst ?? 0) + outShown + (clampMargin ?? 0);
      const ctxPct = modelCtx && modelCtx > 0 ? Math.max(0, Math.min(100, (usedCtx / modelCtx) * 100)) : null;

      const augmented = {
        ...r,
        _derived: {
          context: { modelCtx, clampMargin, inputTokensEst, outBudgetChosen, outActual, outShown, usedCtx, ctxPct },
          rag: { ragDelta, ragPctOfInput },
          web: { webPre },
          timing: {
            accountedPreTtftSec: accounted,
            unattributedPreTtftSec: unattributed,
            preModelSec,
            modelQueueSec,
            genSec,
            ttftSec,
          },
          throughput: { encodeTokPerSec: encodeTps, decodeTokPerSec: decodeTps, overallTokPerSec: overallTps },
        },
      };

      return JSON.stringify(augmented, null, 2);
    } catch {
      return String(data ?? "");
    }
  }, [data]);

  return (
    <div className="rounded-xl border bg-white shadow-xl overflow-hidden">
      <div className="px-3 py-2 border-b flex items-center justify-between bg-gray-50">
        <div className="text-xs font-semibold text-gray-700">{title}</div>
        <div className="flex items-center gap-1">
          <button
            className="inline-flex items-center justify-center h-7 w-7 rounded border bg-white text-gray-700 hover:bg-gray-50"
            onClick={() => {
              navigator.clipboard.writeText(json);
              setCopied(true);
              window.setTimeout(() => setCopied(false), 1500);
            }}
            title="Copy JSON"
          >
            {copied ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
          </button>
          <button
            className="inline-flex items-center justify-center h-7 w-7 rounded border bg-white text-gray-700 hover:bg-gray-50"
            onClick={() => {}}
            title="Close"
          >
            <X className="w-4 h-4" />
          </button>
        </div>
      </div>

      {/* JSON only; badges removed */}
      <div className="p-3">
        <pre className="m-0 p-0 text-xs leading-relaxed overflow-auto" style={{ maxHeight: 360 }}>
          <code>{json}</code>
        </pre>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/MobileDrawer.tsx =====

import { useState } from "react";
import { PanelLeftOpen } from "lucide-react";
import ChatSidebar from "./ChatSidebar/ChatSidebar";

export default function MobileDrawer({ ...props }) {
  const {
    onOpenSession, onNewChat, refreshKey, activeId,
    openMobileDrawer, closeMobileDrawer,
  } = props;

  const [mounted, setMounted] = useState(false);

  return (
    <>
      {/* Mobile top bar */}
      <div className="md:hidden fixed top-0 left-0 right-0 z-40 bg-white border-b">
        <div className="h-14 flex items-center justify-between px-3">
          <button
            className="inline-flex items-center justify-center h-9 w-9 rounded-lg border hover:bg-gray-50"
            onClick={() => { setMounted(true); openMobileDrawer(); }}  // ⬅️ mount on open
            aria-label="Open sidebar"
            title="Open sidebar"
          >
            <PanelLeftOpen className="w-4 h-4" />
          </button>
          <div className="font-semibold">Local AI Model</div>
          <div className="w-9" />
        </div>
      </div>

      {/* Backdrop */}
      <div
        id="mobile-backdrop"
        className="md:hidden fixed inset-0 z-40 bg-black/40 hidden"
        onClick={() => { setMounted(false); closeMobileDrawer(); }}  // ⬅️ unmount on close
      />

      {/* Drawer */}
      <aside
        id="mobile-drawer"
        role="dialog"
        aria-modal="true"
        className="md:hidden fixed inset-y-0 left-0 z-50 w-80 max-w-[85vw] bg-white border-r shadow-xl hidden animate-[slideIn_.2s_ease-out]"
      >
        <div className="h-14 flex items-center justify-between px-3 border-b">
          <div className="font-medium">Chats</div>
          <button
            className="h-9 w-9 inline-flex items-center justify-center rounded-lg border hover:bg-gray-50"
            onClick={() => { setMounted(false); closeMobileDrawer(); }}  // ⬅️ unmount on close
            aria-label="Close sidebar"
          >
            <span className="rotate-45 text-xl leading-none">+</span>
          </button>
        </div>

        {mounted && (                                             // ⬅️ only mount when open
          <ChatSidebar
            onOpen={async (id) => { await onOpenSession(id); setMounted(false); closeMobileDrawer(); }}
            onNew={async () => { await onNewChat(); setMounted(false); closeMobileDrawer(); }}
            refreshKey={refreshKey}
            activeId={activeId}
          />
        )}
      </aside>

      <style>{`@keyframes slideIn{from{transform:translateX(-12px);opacity:.0}to{transform:translateX(0);opacity:1}}`}</style>
    </>
  );
}

# ===== frontend/src/file_read/components/SearchTester.tsx =====

import { useState } from "react";
import ChatContainer from "../components/ChatContainer";
import ChatSidebar from "../components/ChatSidebar/ChatSidebar";
import { useChatStream } from "../hooks/useChatStream";
import { useSidebar } from "../hooks/useSidebar";
import { useToast } from "../hooks/useToast";
import DesktopHeader from "../components/DesktopHeader";
import MobileDrawer from "../components/MobileDrawer";
import Toast from "../shared/ui/Toast";
import { createChat, deleteMessagesBatch } from "../data/chatApi";

// NEW: settings panel
import SettingsPanel from "../components/SettingsPanel";
import KnowledgePanel from "../components/KnowledgePanel";
import SearchTester from "../components/SearchTester";

const LS_KEY = "lastSessionId";

export default function AgentRunner() {
  const chat = useChatStream();
  const [showKnowledge, setShowKnowledge] = useState(false);
  const [refreshKey, setRefreshKey] = useState(0);
  const [autoFollow, setAutoFollow] = useState(true);
  const { toast, show } = useToast();
  const { sidebarOpen, setSidebarOpen, openMobileDrawer, closeMobileDrawer } = useSidebar();

  // NEW: settings modal
  const [showSettings, setShowSettings] = useState(false);

  // ⛔ Removed the mount-time bootstrap that fetched chats and loaded history.

  async function newChat(): Promise<void> {
    const id = crypto.randomUUID();
    chat.setSessionId(id);
    try { await createChat(id, "New Chat"); } catch {}
    localStorage.setItem(LS_KEY, id);
    setRefreshKey((k) => k + 1);
    chat.setInput("");
    chat.clearMetrics?.();
    await refreshFollow();
  }

  async function openSession(id: string): Promise<void> {
    if (!id) return;
    await chat.loadHistory(id);
    localStorage.setItem(LS_KEY, id);
    chat.setInput("");
    chat.clearMetrics?.();
    await refreshFollow();
  }

  // --- TWO REFRESH HELPERS ---

  // 1) Follow-to-bottom refresh (normal)
  async function refreshFollow() {
    const sid = chat.sessionIdRef.current;
    if (!sid) return;
    setAutoFollow(true);
    await chat.loadHistory(sid);
    const el = document.getElementById("chat-scroll-container");
    if (el) el.scrollTop = el.scrollHeight;
  }

async function handleCancelSessions(ids: string[]) {
  if (!ids?.length) return;

  const currentId = chat.sessionIdRef.current || "";
  const deletingActive = currentId && ids.includes(currentId);

  if (deletingActive) {
    // clear out current session instead of making a new one
    chat.setSessionId("");
    chat.setInput("");
    chat.clearMetrics?.();
    // optional: clear messages too
    chat.reset();
    localStorage.removeItem(LS_KEY);
  }

  setRefreshKey((k) => k + 1);
  try { window.dispatchEvent(new CustomEvent("chats:refresh")); } catch {}
}


  // 2) Preserve-scroll refresh (use for deletions, etc.)
  async function refreshPreserve() {
    const sid = chat.sessionIdRef.current;
    if (!sid) return;
    const el = document.getElementById("chat-scroll-container");
    const prevTop = el?.scrollTop ?? 0;
    const prevHeight = el?.scrollHeight ?? 0;

    setAutoFollow(false);
    await chat.loadHistory(sid);

    requestAnimationFrame(() => {
      if (el) {
        const newHeight = el.scrollHeight;
        el.scrollTop = prevTop + (newHeight - prevHeight);
      }
      setAutoFollow(true);
    });
  }

  // Delete by clientId(s). Immediate UI remove; API delete only for server-backed msgs.
  async function handleDeleteMessages(clientIds: string[]) {
    const sid = chat.sessionIdRef.current;
    if (!sid || !clientIds?.length) return;

    const current = chat.messages;
    const toDelete = new Set(clientIds);

    const serverIds = current
      .filter((m: any) => toDelete.has(m.id) && m.serverId != null)
      .map((m: any) => m.serverId as number);

    const remaining = current.filter((m) => !toDelete.has(m.id));

    // Optimistic local state
    if ((chat as any).setMessagesForSession) {
      (chat as any).setMessagesForSession(sid, () => remaining);
    }

    try {
      if (serverIds.length) {
        await deleteMessagesBatch(sid, serverIds);
      }

      await refreshPreserve();

      setRefreshKey((k) => k + 1);
      try { window.dispatchEvent(new CustomEvent("chats:refresh")); } catch {}

      show("Message deleted");
    } catch {
      show("Failed to delete message");
      await chat.loadHistory(sid);
      setRefreshKey((k) => k + 1);
    }
  }

  return (
    <div className="h-screen w-full flex bg-gray-50">
      {sidebarOpen && (
        <div className="hidden md:flex h-full">
          <ChatSidebar
            onOpen={openSession}
            onNew={newChat}
            refreshKey={refreshKey}
            activeId={chat.sessionIdRef.current}
            onHideSidebar={() => setSidebarOpen(false)}
            onCancelSessions={handleCancelSessions}
          />
        </div>
      )}

      <MobileDrawer
        onOpenSession={openSession}
        onNewChat={newChat}
        refreshKey={refreshKey}
        activeId={chat.sessionIdRef.current}
        openMobileDrawer={openMobileDrawer}
        closeMobileDrawer={closeMobileDrawer}
      />
      <div className="md:hidden h-14 shrink-0" />

      <div className="flex-1 min-w-0 flex flex-col">
        <DesktopHeader
          sidebarOpen={sidebarOpen}
          onShowSidebar={() => setSidebarOpen(true)}
        />

        <div className="px-3 md:px-6 pt-2">
          <div className="mx-auto max-w-3xl md:max-w-4xl flex justify-end gap-2">
            <button
              className="text-xs px-3 py-1.5 rounded border bg-white hover:bg-gray-50"
              onClick={() => setShowKnowledge(true)}
              title="Open Knowledge"
            >
              Knowledge
            </button>
            <button
              className="text-xs px-3 py-1.5 rounded border bg-white hover:bg-gray-50"
              onClick={() => setShowSettings(true)}
              title="Open Settings"
            >
              Settings
            </button>
          </div>
          <div className="mx-auto max-w-3xl md:max-w-4xl mt-2">
            <SearchTester />
          </div>
        </div>

        <div className="flex-1 min-h-0">
          <div className="h-full px-3 md:px-6">
            <div className="h-full w-full mx-auto max-w-3xl md:max-w-4xl relative">
              {/* Note: add id on the scroll container wrapper for refresh helpers */}
              <div id="chat-scroll-container" className="h-full">
                <ChatContainer
                  messages={chat.messages}
                  input={chat.input}
                  setInput={chat.setInput}
                  loading={chat.loading}
                  queued={chat.queued}
                  send={chat.send}
                  stop={chat.stop}
                  runMetrics={chat.runMetrics}
                  runJson={chat.runJson}
                  onRefreshChats={() => {}}
                  onDeleteMessages={handleDeleteMessages}
                  autoFollow={autoFollow}
                  sessionId={chat.sessionIdRef.current} // enables per-chat uploads
                />
              </div>
              <Toast message={toast} />
            </div>
          </div>
        </div>
      </div>

      {/* NEW: settings modal */}
      {showSettings && (
        <SettingsPanel
          sessionId={chat.sessionIdRef.current}
          onClose={() => setShowSettings(false)}
        />
      )}
      {showKnowledge && (
        <KnowledgePanel
          sessionId={chat.sessionIdRef.current}
          onClose={() => setShowKnowledge(false)}
          toast={show}
        />
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/Settings/BraveSearchCard.tsx =====

import { useEffect, useState } from "react";
import { getEffective, patchOverrides } from "../../data/settingsApi";

export default function BraveSearchCard() {
  const [loading, setLoading] = useState(true);
  const [, setProvider] = useState("brave");
  const [useWorker, setUseWorker] = useState(false);
  const [workerUrl, setWorkerUrl] = useState("");

  // we don’t prefill the API key for security; we only show a “present” flag
  const [hasKey, setHasKey] = useState<boolean>(false);
  const [apiKey, setApiKey] = useState("");

  useEffect(() => {
    (async () => {
      try {
        const eff = await getEffective();
        setProvider(String(eff.web_search_provider ?? "brave"));
        setWorkerUrl(String(eff.brave_worker_url ?? ""));
        setUseWorker(Boolean(eff.brave_worker_url));
        // backend will mask the key and expose only boolean presence
        setHasKey(Boolean(eff.brave_api_key_present));
      } finally {
        setLoading(false);
      }
    })();
  }, []);

  async function save() {
    const patch: Record<string, any> = {
      web_search_provider: "brave",
      // if worker is used, clear server-side key (we’ll route via worker instead)
      brave_api_key: useWorker ? "" : apiKey,
      brave_worker_url: useWorker ? workerUrl : "",
    };
    await patchOverrides(patch);
    // don’t keep key in memory after save
    setApiKey("");
    setHasKey(!useWorker && !!patch.brave_api_key);
  }

  if (loading) return null;

  return (
    <div className="rounded-2xl p-4 border">
      <h3 className="font-semibold mb-2">Web Search (Brave)</h3>

      <div className="mb-3">
        <label className="block text-sm mb-1">Route via Cloudflare Worker</label>
        <input
          type="checkbox"
          checked={useWorker}
          onChange={(e) => setUseWorker(e.target.checked)}
        />
      </div>

      {useWorker ? (
        <div className="mb-3">
          <label className="block text-sm mb-1">Worker URL</label>
          <input
            className="w-full border rounded p-2"
            placeholder="https://your-worker.workers.dev/brave"
            value={workerUrl}
            onChange={(e) => setWorkerUrl(e.target.value)}
          />
          <p className="text-xs text-muted-foreground mt-1">
            Your Worker holds the Brave API key. The app doesn’t store it.
          </p>
        </div>
      ) : (
        <div className="mb-3">
          <label className="block text-sm mb-1">Brave API Key</label>
          <input
            className="w-full border rounded p-2"
            placeholder="X-Subscription-Token"
            type="password"
            value={apiKey}
            onChange={(e) => setApiKey(e.target.value)}
            spellCheck={false}
          />
          <p className="text-xs text-muted-foreground mt-1">
            {hasKey ? "Key is stored on the server." : "No key stored yet."}
          </p>
        </div>
      )}

      <button className="mt-2 px-3 py-2 rounded bg-black text-white" onClick={save}>
        Save
      </button>
    </div>
  );
}

# ===== frontend/src/file_read/components/Settings/UpgradeSection.tsx =====

// frontend/src/file_read/components/UpgradeSection.tsx
import { useEffect, useMemo, useState } from "react";
import { getBillingStatus, openPortal, startCheckout } from "../../api/billing";
import { buildUrl } from "../../services/http";

type Props = { token: string | null | undefined; userEmail?: string | null };

function openExternal(url: string) {
  // @ts-ignore
  if (window?.electron?.openExternal) return window.electron.openExternal(url);
  window.open(url, "_blank", "noopener,noreferrer");
}

export default function UpgradeSection({ token, userEmail }: Props) {
  const [billing, setBilling] = useState<null | { status: string; current_period_end: number }>(null);
  const [license, setLicense] = useState<{ plan: string; valid: boolean; exp?: number } | null>(null);
  const [loading, setLoading] = useState(false);
  const [err, setErr] = useState<string | null>(null);

  const isPro = useMemo(() => {
    const s = (billing?.status || "").toLowerCase();
    return s === "active" || s === "trialing" || s === "past_due";
  }, [billing]);

  useEffect(() => {
    let mounted = true;
    (async () => {
      if (!token) return;
      setLoading(true);
      setErr(null);
      try {
          const [b, l] = await Promise.all([
            getBillingStatus(token),
            fetch(buildUrl("/license/status"), {
              headers: { Authorization: `Bearer ${token}` }
            }).then(r => r.json()),
          ]);
        if (mounted) { setBilling(b); setLicense(l); }
      } catch (e:any) {
        if (mounted) setErr(e?.message || "Failed to load billing/license");
      } finally {
        if (mounted) setLoading(false);
      }
    })();
    return () => { mounted = false; };
  }, [token]);

  async function handleUpgrade() {
    try {
      if (!token) { setErr("Please sign in to upgrade."); return; }
      setErr(null);
      // Optionally pick a price id from env:
      const priceId = (import.meta.env.VITE_STRIPE_PRICE_PRO_MONTHLY as string) || undefined;
      const { url } = await startCheckout(token, priceId);
      openExternal(url);
    } catch (e:any) {
      setErr(e?.message || "Could not start checkout");
    }
  }

  async function handleManage() {
    try {
      if (!token) { setErr("Please sign in to manage billing."); return; }
      setErr(null);
      const { url } = await openPortal(token);
      openExternal(url);
    } catch (e:any) {
      setErr(e?.message || "Could not open billing portal");
    }
  }

  return (
    <div className="rounded-2xl border p-4">
      <div className="mb-3 flex items-center justify-between">
        <div className="font-semibold">Upgrade</div>
        <span className={
          "inline-flex items-center gap-2 rounded-full px-3 py-1 text-xs " +
          (isPro ? "bg-green-600 text-white" : "bg-gray-200 text-gray-800")
        }>
          {isPro ? "Pro (active)" : loading ? "Loading…" : "Free"}
        </span>
      </div>

      {err && <div className="mb-4 rounded-lg border border-red-200 bg-red-50 px-3 py-2 text-xs text-red-700">{err}</div>}

      <div className="grid gap-6 md:grid-cols-2">
        <PlanCard title="Free" price="$0" features={["Local model", "Basic settings"]} ctaLabel="Current Plan" ctaDisabled />
        <PlanCard
          title="Pro"
          price="$12/mo"
          subtitle="or $96/year"
          features={["Pro features via license", "Priority updates", "Manage billing in portal"]}
          ctaLabel={isPro ? "Manage Billing" : "Go Pro"}
          onClick={isPro ? handleManage : handleUpgrade}
          highlight
        />
      </div>

      <div className="mt-6 rounded-xl border bg-gray-50 p-3 text-xs text-gray-700">
        <div className="font-medium mb-1">License</div>
        <div>Plan: <strong>{license?.plan ?? "free"}</strong> {license?.valid === false && "(invalid/expired)"}</div>
        {license?.exp ? <div>Exp: {new Date(license.exp * 1000).toLocaleDateString()}</div> : null}
        {userEmail && <div className="mt-2 text-gray-600">Signed in as <strong>{userEmail}</strong></div>}
      </div>
    </div>
  );
}

function PlanCard({ title, price, subtitle, features, ctaLabel, ctaDisabled, onClick, highlight }:{
  title: string; price: string; subtitle?: string; features: string[]; ctaLabel: string;
  ctaDisabled?: boolean; onClick?: () => void; highlight?: boolean;
}) {
  return (
    <div className={"rounded-2xl border bg-white p-6 shadow-sm " + (highlight ? "ring-2 ring-indigo-500" : "")}>
      <div className="mb-2 flex items-center justify-between">
        <div className="text-lg font-semibold">{title}</div>
        {highlight && <span className="rounded-full bg-indigo-100 px-2 py-1 text-xs text-indigo-700">Best Value</span>}
      </div>
      <div className="mb-1 text-3xl font-bold">{price}</div>
      {subtitle && <div className="mb-4 text-sm text-gray-500">{subtitle}</div>}
      <ul className="mb-6 space-y-2 text-sm text-gray-700">{features.map(f => <li key={f}>• {f}</li>)}</ul>
      <button disabled={ctaDisabled} onClick={onClick} className={
        "w-full rounded-xl px-4 py-2 font-medium " +
        (ctaDisabled ? "cursor-not-allowed border text-gray-500" :
         highlight ? "bg-indigo-600 text-white hover:bg-indigo-700" : "border hover:bg-gray-50")
      }>{ctaLabel}</button>
    </div>
  );
}

# ===== frontend/src/file_read/components/Settings/WebSearchSection.tsx =====

import { useEffect, useState } from "react";
import { getEffective, patchOverrides } from "../../data/settingsApi";

export default function WebSearchSection({ onSaved }: { onSaved?: () => void }) {
  const [apiKey, setApiKey] = useState("");
  const [masked, setMasked] = useState(true);
  const [status, setStatus] = useState<null | { ok: boolean; msg: string }>(null);

  useEffect(() => {
    (async () => {
      try {
        const eff = await getEffective();
        setApiKey(eff.brave_api_key || "");
      } catch {
        // fallback if no settings
      }
    })();
  }, []);

  async function save() {
    try {
      await patchOverrides({ brave_api_key: apiKey.trim() });
      setStatus({ ok: true, msg: "Saved" });
      onSaved?.();
    } catch (e: any) {
      setStatus({ ok: false, msg: e?.message || "Failed to save" });
    }
  }

  function clearKey() {
    patchOverrides({ brave_api_key: "" });
    setApiKey("");
    setStatus({ ok: true, msg: "Key cleared" });
    onSaved?.();
  }

  const displayVal = masked && apiKey ? "•".repeat(Math.min(apiKey.length, 24)) : apiKey;

  return (
    <div className="space-y-4">
      <div className="text-sm text-gray-600">
        Enable web search via your own <b>Brave Search API</b> key. Stored in settings, not in local storage.
      </div>

      <div className="space-y-2">
        <label className="block text-sm">Brave API key</label>
        <div className="flex items-stretch gap-2">
          <input
            className="flex-1 border rounded px-3 py-2 text-sm"
            placeholder="X-Subscription-Token"
            value={displayVal}
            onChange={(e) => setApiKey(e.target.value)}
            onFocus={() => setMasked(false)}
            onBlur={() => setMasked(true)}
          />
          <button
            className="px-3 py-2 rounded border text-sm hover:bg-gray-50"
            onClick={() => setMasked((m) => !m)}
            title={masked ? "Show" : "Hide"}
          >
            {masked ? "Show" : "Hide"}
          </button>
          <button
            className="px-3 py-2 rounded border text-sm hover:bg-gray-50"
            onClick={clearKey}
            title="Clear key"
          >
            Clear
          </button>
          <button className="px-3 py-2 rounded bg-black text-white text-sm" onClick={save}>
            Save
          </button>
        </div>
        {status && (
          <div className={`text-xs ${status.ok ? "text-green-600" : "text-red-600"}`}>{status.msg}</div>
        )}
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/SettingsPanel.tsx =====

import { useEffect, useMemo, useState } from "react";
import { useSettings } from "../hooks/useSettings";
import WebSearchSection from "../components/Settings/WebSearchSection";
import { useAuth } from "../auth/AuthContext";                      // ⬅️ NEW
import UpgradeSection from "../components/Settings/UpgradeSection"; // ⬅️ NEW

type Tab =
  | "general"
  | "notifications"
  | "personalization"
  | "connectors"
  | "schedules"
  | "datacontrols"
  | "security"
  | "account"
  | "developer";

const NAV: { key: Tab; label: string }[] = [
  { key: "general", label: "General" },
  { key: "notifications", label: "Notifications" },
  { key: "personalization", label: "Personalization" },
  { key: "connectors", label: "Connectors" },
  { key: "schedules", label: "Schedules" },
  { key: "datacontrols", label: "Data controls" },
  { key: "security", label: "Security" },
  { key: "account", label: "Account" },
  { key: "developer", label: "Developer" },
];

export default function SettingsPanel({
  sessionId,
  onClose,
}: {
  sessionId?: string;
  onClose?: () => void;
}) {
  const {
    loading,
    error,
    effective,
    overrides,
    defaults,
    adaptive,
    saveOverrides,
    runAdaptive,
    reload,
  } = useSettings(sessionId);

  const { user, token } = useAuth(); // ⬅️ NEW

  const [tab, setTab] = useState<Tab>("general");

  // Developer JSON views
  const [devSubtab, setDevSubtab] = useState<"effective" | "overrides" | "adaptive" | "defaults">("effective");
  const [draft, setDraft] = useState(() => JSON.stringify(overrides ?? {}, null, 2));
  const [saveBusy, setSaveBusy] = useState(false);
  const [saveErr, setSaveErr] = useState<string | null>(null);

  useEffect(() => {
    setDraft(JSON.stringify(overrides ?? {}, null, 2));
  }, [overrides]);

  const devView = useMemo(() => {
    switch (devSubtab) {
      case "effective":
        return effective;
      case "adaptive":
        return adaptive;
      case "defaults":
        return defaults;
      case "overrides":
        return null;
    }
  }, [devSubtab, effective, adaptive, defaults]);

  async function onSaveDev(method: "patch" | "put") {
    setSaveErr(null);
    setSaveBusy(true);
    try {
      const parsed = draft.trim() ? JSON.parse(draft) : {};
      await saveOverrides(parsed, method);
    } catch (e: any) {
      setSaveErr(e?.message || "Invalid JSON or save failed");
    } finally {
      setSaveBusy(false);
    }
  }

  return (
    <div className="fixed inset-0 z-50 bg-black/40 flex items-center justify-center p-3">
      <div className="w-full max-w-5xl h-[90vh] rounded-2xl bg-white shadow-xl border overflow-hidden flex">
        {/* Sidebar */}
        <aside className="w-60 border-r bg-gray-50/60">
          <div className="px-4 py-3 text-sm font-semibold">Settings</div>
          <nav className="px-2 pb-3 space-y-1">
            {NAV.map((item) => (
              <button
                key={item.key}
                onClick={() => setTab(item.key)}
                className={`w-full text-left px-3 py-2 rounded-lg text-sm ${
                  tab === item.key ? "bg-black text-white" : "hover:bg-gray-100"
                }`}
              >
                {item.label}
              </button>
            ))}
          </nav>
          <div className="px-3 pt-2 mt-auto hidden md:block">
            <button
              className="w-full text-xs px-3 py-2 rounded border hover:bg-gray-100"
              onClick={() => reload()}
              title="Reload settings"
            >
              Reload
            </button>
            <button
              className="w-full mt-2 text-xs px-3 py-2 rounded border hover:bg-gray-100"
              onClick={() => runAdaptive()}
              title="Recompute adaptive"
            >
              Recompute Adaptive
            </button>
            <button
              className="w-full mt-2 text-xs px-3 py-2 rounded border hover:bg-gray-100"
              onClick={onClose}
              title="Close"
            >
              Close
            </button>
          </div>
        </aside>

        {/* Main */}
        <section className="flex-1 min-w-0 flex flex-col">
          <header className="px-5 py-4 border-b">
            <div className="text-base font-semibold capitalize">
              {tab.replace(/([a-z])([A-Z])/g, "$1 $2")}
            </div>
            {loading && <div className="text-xs text-gray-500 mt-1">Loading…</div>}
            {error && <div className="text-xs text-red-600 mt-1">{error}</div>}
          </header>

          <div className="flex-1 overflow-auto p-5">
            {tab === "general" && (
              <div className="space-y-6">
                <Section title="Appearance">
                  <Row label="Theme">
                    <select className="border rounded px-2 py-1 text-sm">
                      <option>System</option>
                      <option>Light</option>
                      <option>Dark</option>
                    </select>
                  </Row>
                  <Row label="Accent color">
                    <select className="border rounded px-2 py-1 text-sm">
                      <option>Default</option>
                      <option>Blue</option>
                      <option>Green</option>
                      <option>Purple</option>
                    </select>
                  </Row>
                  <Row label="Language">
                    <select className="border rounded px-2 py-1 text-sm">
                      <option>Auto-detect</option>
                      <option>English</option>
                      <option>Spanish</option>
                      <option>French</option>
                    </select>
                  </Row>
                </Section>

                <Section title="Web & Integrations">
                  <WebSearchSection
                    onSaved={() => {
                      // reflect to backend that web search is enabled (no key transmitted)
                      saveOverrides({
                        web_search_provider: "brave",
                        brave_worker_url: "",
                        brave_api_key_present: true,
                      });
                    }}
                  />
                </Section>
              </div>
            )}

            {tab === "account" && ( // ⬅️ NEW: Upgrade lives here
              <div className="space-y-6">
                <UpgradeSection token={token} userEmail={user?.email ?? ""} />
                {/* Add other account/profile sections here as needed */}
              </div>
            )}

            {tab === "developer" && (
              <div className="space-y-4">
                <div className="flex items-center gap-2">
                  {(["effective", "overrides", "adaptive", "defaults"] as const).map((k) => (
                    <button
                      key={k}
                      onClick={() => setDevSubtab(k)}
                      className={`text-xs mr-2 px-3 py-1.5 rounded ${
                        devSubtab === k ? "bg-black text-white" : "border hover:bg-gray-50"
                      }`}
                    >
                      {k}
                    </button>
                  ))}
                </div>

                {devSubtab !== "overrides" && (
                  <pre className="text-xs bg-gray-50 border rounded p-3 overflow-auto max-h-[60vh]">
                    {JSON.stringify(devView ?? {}, null, 2)}
                  </pre>
                )}

                {devSubtab === "overrides" && (
                  <div className="space-y-2">
                    <div className="text-xs text-gray-600">
                      Edit <code>user_overrides</code> JSON. Use <b>Patch</b> to merge or <b>Replace</b> to overwrite.
                    </div>
                    <textarea
                      value={draft}
                      onChange={(e) => setDraft(e.target.value)}
                      className="w-full h-[50vh] border rounded p-2 font-mono text-xs"
                      spellCheck={false}
                    />
                    <div className="flex items-center gap-2">
                      <button
                        className={`text-xs px-3 py-1.5 rounded ${
                          saveBusy ? "opacity-60 cursor-not-allowed" : "bg-black text-white"
                        }`}
                        disabled={saveBusy}
                        onClick={() => onSaveDev("patch")}
                        title="Deep-merge with existing overrides"
                      >
                        Save (Patch)
                      </button>
                      <button
                        className={`text-xs px-3 py-1.5 rounded border ${
                          saveBusy ? "opacity-60 cursor-not-allowed" : "hover:bg-gray-50"
                        }`}
                        disabled={saveBusy}
                        onClick={() => onSaveDev("put")}
                        title="Replace overrides entirely"
                      >
                        Save (Replace)
                      </button>
                      {saveErr && <div className="text-xs text-red-600 ml-2">{saveErr}</div>}
                    </div>
                  </div>
                )}
              </div>
            )}

            {/* Stubs for other tabs (expand later) */}
            {tab !== "general" && tab !== "developer" && tab !== "account" && (
              <div className="text-sm text-gray-500">This section is coming soon.</div>
            )}
          </div>
        </section>
      </div>
    </div>
  );
}

function Section({ title, children }: { title: string; children: React.ReactNode }) {
  return (
    <div className="rounded-2xl border p-4">
      <div className="font-semibold mb-3">{title}</div>
      {children}
    </div>
  );
}

function Row({ label, children }: { label: string; children: React.ReactNode }) {
  return (
    <div className="flex items-center gap-3 py-2">
      <div className="w-44 text-sm text-gray-600">{label}</div>
      <div className="flex-1">{children}</div>
    </div>
  );
}

# ===== frontend/src/file_read/data/chatApi.ts =====

import type { Attachment, ChatRow, ChatMessageRow} from "../types/chat";
import { request } from "../services/http";

// Spring Page<T> type
export type PageResp<T> = {
  content: T[];
  totalElements: number;
  totalPages: number;
  size: number;
  number: number;      // current page index (0-based)
  first: boolean;
  last: boolean;
  empty: boolean;
};

export function createChat(sessionId: string, title: string) {
  return request<ChatRow>("/api/chats", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ sessionId, title }),
  });
}

export function updateChatLast(sessionId: string, lastMessage: string, title?: string) {
  return request<ChatRow>(`/api/chats/${encodeURIComponent(sessionId)}/last`, {
    method: "PUT",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ lastMessage, title: title || "" }),
  });
}

// Legacy (unused after pagination in sidebar)
export function listChats() {
  return request<ChatRow[]>("/api/chats");
}

// NEW: paginated list
export function listChatsPage(page = 0, size = 30, ceiling?: string) {
  const qs = new URLSearchParams({ page: String(page), size: String(size) });
  if (ceiling) qs.set("ceiling", ceiling);
  return request<PageResp<ChatRow>>(`/api/chats/paged?${qs.toString()}`);
}

export function listMessages(sessionId: string) {
  return request<ChatMessageRow[]>(`/api/chats/${encodeURIComponent(sessionId)}/messages`);
}

export async function appendMessage(
  sessionId: string,
  role: "user" | "assistant",
  content: string,
  attachments?: Attachment[]
) {
  const body: any = { role, content };
  if (attachments && attachments.length) body.attachments = attachments;

  return request<ChatMessageRow>(`/api/chats/${sessionId}/messages`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
  });
}

export async function deleteChatsBatch(sessionIds: string[]) {
  const data = await request<{ deleted: string[] }>("/api/chats/batch", {
    method: "DELETE",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ sessionIds }),
  });
  return data.deleted;
}

export function deleteMessage(sessionId: string, messageId: string | number) {
  return request<{ deleted: number }>(
    `/api/chats/${encodeURIComponent(sessionId)}/messages/${encodeURIComponent(String(messageId))}`,
    { method: "DELETE" }
  );
}

/** Delete a batch of messages. Backend returns { deleted: number[] } */
export function deleteMessagesBatch(sessionId: string, messageIds: (number | string)[]) {
  const ids = messageIds
    .map((id) => (typeof id === "string" ? Number(id) : id))
    .filter((n) => Number.isFinite(n)) as number[];

  return request<{ deleted: number[] }>(  // <-- number[]
    `/api/chats/${encodeURIComponent(sessionId)}/messages/batch`,
    {
      method: "DELETE",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ messageIds: ids }),
    }
  );
}

# ===== frontend/src/file_read/data/ragApi.ts =====

// frontend/src/file_read/data/ragApi.ts
import { request, API_BASE } from "../services/http";

export function uploadRag(file: File, sessionId?: string, forceGlobal = false) {
  const form = new FormData();
  form.append("file", file);
  if (sessionId && !forceGlobal) form.append("sessionId", sessionId);

  return request<{ ok: boolean; added: number }>(
    "/api/rag/upload",
    { method: "POST", body: form }
  );
}

export function uploadRagWithProgress(
  file: File,
  sessionId: string,
  onProgress: (pct: number) => void,
  signal?: AbortSignal
): Promise<{ ok: boolean; added: number }> {
  return new Promise((resolve, reject) => {
    const form = new FormData();
    form.append("file", file);
    form.append("sessionId", sessionId);

    const xhr = new XMLHttpRequest();
    const url = `${API_BASE}/api/rag/upload`.replace(/([^:]\/)\/+/g, "$1");
    xhr.open("POST", url);

    xhr.upload.onprogress = (e) => {
      if (e.lengthComputable) onProgress(Math.round((e.loaded / e.total) * 100));
    };

    xhr.onload = () => {
      if (xhr.status >= 200 && xhr.status < 300) {
        try { resolve(JSON.parse(xhr.responseText)); }
        catch { resolve({ ok: true, added: 0 }); }
      } else {
        reject(new Error(`Upload failed (${xhr.status})`));
      }
    };

    // treat abort as a silent resolution, not an error
    xhr.onabort = () => resolve({ ok: false, added: 0 });
    xhr.onerror = () => reject(new Error("Network error"));

    if (signal) {
      if (signal.aborted) { xhr.abort(); return resolve({ ok: false, added: 0 }); }
      signal.addEventListener("abort", () => xhr.abort(), { once: true });
    }

    xhr.send(form);
  });
}


export function searchRag(query: string, opts?: {
  sessionId?: string;
  kChat?: number;
  kGlobal?: number;
  alpha?: number; // hybrid_alpha
}) {
  const body = {
    query,
    sessionId: opts?.sessionId ?? undefined,
    kChat: opts?.kChat ?? 6,
    kGlobal: opts?.kGlobal ?? 4,
    hybrid_alpha: opts?.alpha ?? 0.5,
  };

  return request<{
    hits: Array<{
      id?: string;
      score: number;
      source?: string;
      title?: string;
      text: string;
      sessionId?: string | null;
    }>;
  }>(
    "/api/rag/search",   // ✅ relative path, request() adds API_BASE
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(body),
    }
  );
}

export type UploadRow = {
  source: string;
  sessionId?: string | null;
  chunks: number;
};

export async function listUploads(sessionId?: string, scope: "all" | "session" = "all") {
  const p = new URLSearchParams();
  if (sessionId) p.set("sessionId", sessionId);
  if (scope) p.set("scope", scope);

  return request<{ uploads: UploadRow[] }>(
    `/api/rag/uploads?${p.toString()}`,
    { method: "GET" }
  );
}

export async function deleteUploadHard(source: string, sessionId?: string) {
  return request<{ ok: boolean; removed: number; remaining: number }>(
    `/api/rag/uploads/delete-hard`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ source, sessionId }),
    }
  );
}

# ===== frontend/src/file_read/data/settingsApi.ts =====

// frontend/src/file_read/data/settingsApi.ts
import { request } from "../services/http";

export function getDefaults() {
  return request<Record<string, any>>("/api/settings/defaults");
}

export function getAdaptive(sessionId?: string) {
  const qs = sessionId ? `?sessionId=${encodeURIComponent(sessionId)}` : "";
  return request<Record<string, any>>(`/api/settings/adaptive${qs}`);
}

export function getOverrides() {
  return request<Record<string, any>>("/api/settings/overrides");
}

export function getEffective(sessionId?: string) {
  const qs = sessionId ? `?sessionId=${encodeURIComponent(sessionId)}` : "";
  return request<Record<string, any>>(`/api/settings/effective${qs}`);
}

export function putOverrides(overrides: Record<string, any>) {
  return request<{ ok: boolean; overrides: any }>("/api/settings/overrides", {
    method: "PUT",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(overrides), // ← send raw object
  });
}

export function patchOverrides(patch: Record<string, any>) {
  return request<{ ok: boolean; overrides: any }>("/api/settings/overrides", {
    method: "PATCH",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(patch), // ← send raw object
  });
}

export function recomputeAdaptive(sessionId?: string) {
  const qs = sessionId ? `?sessionId=${encodeURIComponent(sessionId)}` : "";
  return request<{ ok: boolean; adaptive: any }>(
    `/api/settings/adaptive/recompute${qs}`,
    { method: "POST" }
  );
}

# ===== frontend/src/file_read/hooks/stream/core/buffer.ts =====

// frontend/src/file_read/hooks/stream/core/buffer.ts
import type { GenMetrics, RunJson } from "../../../shared/lib/runjson";
import { extractRunJsonFromBuffer } from "../../../shared/lib/runjson";
import { STOP_SENTINEL_AT_END } from "./constants";

export type BufferStep = {
  cleanText: string;
  delta: string;
  metrics?: { json?: RunJson; flat?: GenMetrics };
};

export function processChunk(prevClean: string, rawBufIn: string): BufferStep {
  let rawBuf = rawBufIn;
  if (STOP_SENTINEL_AT_END.test(rawBuf)) {
    rawBuf = rawBuf.replace(STOP_SENTINEL_AT_END, "");
  }
  const { clean, json, flat } = extractRunJsonFromBuffer(rawBuf);
  const delta = clean.slice(prevClean.length);
  return { cleanText: clean, delta, metrics: json || flat ? { json, flat } : undefined };
}

# ===== frontend/src/file_read/hooks/stream/core/cancel.ts =====

import { postCancel } from "./network";
import { STOP_FLUSH_TIMEOUT_MS } from "./constants";

type Deps = {
  getVisibleSid: () => string;
  setLoadingFor: (sid: string, v: boolean) => void;
  setQueuedFor: (sid: string, v: boolean) => void;
  getController: () => AbortController | null;
  getReader: () => ReadableStreamDefaultReader<Uint8Array> | null;
  setCancelForSid: (sid: string | null) => void;
  isActiveSid: (sid: string) => boolean;
  dropJobsForSid: (sid: string) => void;
};

export function createCanceller(d: Deps) {
  async function cancelBySessionId(id: string) {
    // Mark as canceled so read loop can react, but DO NOT abort fetch yet.
    d.setCancelForSid(id);

    // Tell the backend to stop gracefully (flush metrics + close).
    postCancel(id).catch(() => {});

    if (d.isActiveSid(id)) {
      // Drop any queued jobs for this session, but keep loading true —
      // runStreamOnce will turn loading off in its finally after the stream ends.
      d.dropJobsForSid(id);
      d.setQueuedFor(id, false);

      // Safety net: if server doesn’t flush within timeout, hard-abort.
      window.setTimeout(() => {
        if (d.isActiveSid(id)) {
          try { d.getReader()?.cancel(); } catch {}
          try { d.getController()?.abort(); } catch {}
        }
      }, STOP_FLUSH_TIMEOUT_MS + 500);
    } else {
      // Not active: just clear queued jobs and cancel flag.
      d.dropJobsForSid(id);
      d.setQueuedFor(id, false);
      d.setCancelForSid(null);
    }
  }

  async function stopVisible() {
    const id = d.getVisibleSid();
    await cancelBySessionId(id);
  }

  return { cancelBySessionId, stopVisible };
}

# ===== frontend/src/file_read/hooks/stream/core/constants.ts =====

// frontend/src/file_read/hooks/stream/core/constants.ts
export const STOP_SENTINEL_AT_END = /(?:\r?\n)?\u23F9 stopped(?:\r?\n)?$/u;
export const STOP_FLUSH_TIMEOUT_MS = 3500;

# ===== frontend/src/file_read/hooks/stream/core/controller.ts =====

import { appendMessage } from "../../../data/chatApi";
import type { StreamController, StreamCoreOpts } from "./types";
import { createScheduler, type QueueItem } from "./queue";
import { createCanceller } from "./cancel";
import { runStreamOnce } from "./runner";
import type { Attachment } from "../../../types/chat";

export function createStreamController(opts: StreamCoreOpts): StreamController {
  let cancelForSid: string | null = null;
  let controllerRef: AbortController | null = null;
  let readerRef: ReadableStreamDefaultReader<Uint8Array> | null = null;

  const scheduler = createScheduler(async (job: QueueItem) => {
    try { opts.setQueuedFor(job.sid, false); } catch {}
    await runStreamOnce(job, {
      opts,
      getCancelForSid: () => cancelForSid,
      clearCancelIf: (sid) => { if (cancelForSid === sid) cancelForSid = null; },
      setController: (c) => { controllerRef = c; },
      setReader: (r) => { readerRef = r; },
    });
  });

  const canceller = createCanceller({
    getVisibleSid: opts.getSessionId,
    setLoadingFor: opts.setLoadingFor,
    setQueuedFor: opts.setQueuedFor,
    getController: () => controllerRef,
    getReader: () => readerRef,
    setCancelForSid: (sid) => { cancelForSid = sid; },
    isActiveSid: scheduler.isActiveSid,
    dropJobsForSid: scheduler.dropJobsForSid,
  });

  async function send(override?: string, attachments?: Attachment[]) {
    const prompt = (override ?? "").trim();
    const atts = (attachments ?? []).filter(Boolean);
    if (!prompt && atts.length === 0) return; // allow attachments-only, but not truly empty

    await opts.ensureChatCreated();
    const sid = opts.getSessionId();

    const userCid = crypto.randomUUID();
    const asstCid = crypto.randomUUID();

    // optimistic bubbles
    opts.setMessagesFor(sid, (prev) => [
      ...prev,
      { id: userCid, serverId: null, role: "user", text: prompt, attachments: atts.length ? atts : undefined },
      { id: asstCid, serverId: null, role: "assistant", text: "" },
    ]);
    opts.setInput("");

    // persist user
    appendMessage(sid, "user", prompt, atts.length ? atts : undefined)
      .then((row) => {
        if (row?.id != null) {
          opts.setServerIdFor(sid, userCid, Number(row.id));
          try {
            window.dispatchEvent(
              new CustomEvent("chats:refresh", {
                detail: {
                  sessionId: sid,
                  lastMessage: prompt,
                  updatedAt: new Date().toISOString(),
                },
              })
            );
          } catch {}
        }
      })
      .catch(() => {});

    // enqueue generation with attachments
    try { opts.setQueuedFor(sid, true); } catch {}
    scheduler.enqueue({ sid, prompt, asstId: asstCid, attachments: atts.length ? atts : undefined });
  }

  async function stop() { await canceller.stopVisible(); }
  async function cancelBySessionId(id: string) { await canceller.cancelBySessionId(id); }
  function dispose() {
    try { controllerRef?.abort(); } catch {}
    try { readerRef?.cancel(); } catch {}
    controllerRef = null; readerRef = null;
  }

  return { send, stop, cancelBySessionId, dispose };
}

# ===== frontend/src/file_read/hooks/stream/core/network.ts =====

// frontend/src/file_read/hooks/stream/core/network.ts
// (replace the existing file contents with this version)

import { buildUrl, requestRaw } from "../../../services/http";

function bearerHeaderFromLocal() {
  const h: Record<string, string> = {};
  const jwt = localStorage.getItem("local_jwt");
  if (jwt) h["Authorization"] = `Bearer ${jwt}`;
  const lic = localStorage.getItem("license_key");
  if (lic) h["x-license"] = lic;
  return h;
}

export async function postStream(body: unknown, signal: AbortSignal) {
  const url = buildUrl("/ai/generate/stream");

  const headers: Record<string, string> = {
    "Content-Type": "application/json",
    Accept: "text/event-stream",
    ...bearerHeaderFromLocal(),
  };

  console.debug("[SSE fetch]", { url, hasAuth: !!headers.Authorization }); 

  const res = await requestRaw(url, {
    method: "POST",
    headers,
    body: JSON.stringify(body),
    signal,
  });

  if (!res.ok || !res.body) {
    const t = await res.text().catch(() => "");
    throw new Error(`HTTP ${res.status} ${res.statusText} ${t}`);
  }
  return res.body.getReader();
}

export async function postCancel(sessionId: string) {
  try {
    const url = buildUrl(`/ai/cancel/${encodeURIComponent(sessionId)}`);
    await requestRaw(url, { method: "POST", headers: bearerHeaderFromLocal() });
  } catch {
  }
}

# ===== frontend/src/file_read/hooks/stream/core/queue.ts =====

// frontend/src/file_read/hooks/stream/core/queue.ts
import type { Attachment } from "../../../types/chat";

export type QueueItem = {
  sid: string;
  prompt: string;              // may be empty when attachments-only
  asstId: string;              // client id of the assistant bubble to stream into
  attachments?: Attachment[];  // optional attachments for this turn
};

export type RunJob = (job: QueueItem) => Promise<void>;

export function createScheduler(runJob: RunJob) {
  const q: QueueItem[] = [];
  let active: { sid: string } | null = null;

  async function startNext() {
    if (active || q.length === 0) return;
    const job = q.shift()!;
    active = { sid: job.sid };
    try {
      await runJob(job);
    } finally {
      active = null;
      if (q.length) void startNext();
    }
  }

  return {
    enqueue(job: QueueItem) {
      q.push(job);
      void startNext();
    },
    isActiveSid: (sid: string) => active?.sid === sid,
    dropJobsForSid(sid: string) {
      for (let i = q.length - 1; i >= 0; i--) {
        if (q[i].sid === sid) q.splice(i, 1);
      }
    },
    getActiveSid: () => active?.sid ?? null,
  };
}

# ===== frontend/src/file_read/hooks/stream/core/runner.ts =====

// frontend/src/file_read/hooks/stream/core/runner.ts
import { postStream } from "./network";
import { ensureAssistantPlaceholder, snapshotPendingAssistant } from "./updater";
import type { ChatMsg } from "../../../types/chat";
import type { RunJson, GenMetrics } from "../../../shared/lib/runjson";
import { readStreamLoop } from "./runner_stream";
import {
  pinLiveMetricsToSession,
  pinLiveMetricsToBubble,
  pinFallbackToSessionAndBubble,
} from "./runner_metrics";
import { persistAssistantTurn } from "./runner_persist";
import type { QueueItem } from "./queue";
import { listChatsPage } from "../../../data/chatApi";


export type RunnerDeps = {
  opts: {
    ensureChatCreated: () => Promise<void>;
    getSessionId: () => string;
    getMessagesFor: (sid: string) => ChatMsg[];
    setMessagesFor: (sid: string, fn: (prev: ChatMsg[]) => ChatMsg[]) => void;
    setInput: (v: string) => void;
    setLoadingFor: (sid: string, v: boolean) => void;
    setQueuedFor: (sid: string, v: boolean) => void;
    resetMetricsFor: (sid: string) => void;
    setMetricsFor: (sid: string, json?: RunJson, flat?: GenMetrics) => void;
    setMetricsFallbackFor: (sid: string, reason: string, text: string) => void;
    onRetitle: (sid: string, finalText: string) => Promise<void>;
    setServerIdFor: (sid: string, clientId: string, serverId: number) => void;
  };
  getCancelForSid: () => string | null;
  clearCancelIf: (sid: string) => void;
  setController: (c: AbortController | null) => void;
  setReader: (r: ReadableStreamDefaultReader<Uint8Array> | null) => void;
};

export async function runStreamOnce(job: QueueItem, d: RunnerDeps) {
  const { sid, prompt, asstId, attachments } = job;
  const { opts } = d;
  const wasCanceled = () => d.getCancelForSid() === sid;

  opts.resetMetricsFor(sid);
  opts.setLoadingFor(sid, true);

  ensureAssistantPlaceholder(
    { getMessagesFor: opts.getMessagesFor, setMessagesFor: opts.setMessagesFor },
    sid,
    asstId
  );

  const MAX_HISTORY = 10;
  const history = opts
    .getMessagesFor(sid)
    .slice(-MAX_HISTORY)
    .map((m) => ({
      role: m.role,
      content: m.text || "",
      attachments: m.attachments && m.attachments.length ? m.attachments : undefined,
    }))
    .filter(
      (m) =>
        (m.content && m.content.trim().length > 0) ||
        (m.attachments && m.attachments.length > 0)
    );

  const userTurn = {
    role: "user" as const,
    content: prompt,
    attachments: attachments && attachments.length ? attachments : undefined,
  };

  const controller = new AbortController();
  d.setController(controller);
  let reader: ReadableStreamDefaultReader<Uint8Array> | null = null;

  try {
    reader = await postStream(
      { sessionId: sid, messages: [...history, userTurn] },
      controller.signal
    );
    d.setReader(reader);

    const { finalText, gotMetrics, lastRunJson } = await readStreamLoop(reader, {
      wasCanceled,
      onDelta: (delta) => {
        opts.setMessagesFor(sid, (prev) => {
          const idx = prev.findIndex((m) => m.id === asstId);
          if (idx === -1) return prev;
          const next = [...prev];
          next[idx] = { ...next[idx], text: (next[idx].text || "") + delta };
          return next;
        });
      },
      onMetrics: (json, flat) => {
        pinLiveMetricsToSession(opts, sid, json, flat);
        pinLiveMetricsToBubble(opts, sid, asstId, json, flat);
      },
      onCancelTimeout: (cleanSoFar) => {
        opts.setMetricsFallbackFor(sid, "user_cancel_timeout", cleanSoFar);
      },
    });

    let persistJson: RunJson | null = gotMetrics ? lastRunJson : null;

    if (!gotMetrics) {
      const reason = wasCanceled() ? "user_cancel" : "end_of_stream_no_metrics";
      const fallback = pinFallbackToSessionAndBubble(opts, sid, asstId, reason, finalText);
      if (!wasCanceled()) persistJson = fallback;
    }

    if (!wasCanceled() && finalText.trim()) {
      const newServerId = await persistAssistantTurn(sid, finalText, persistJson);
      if (newServerId != null) {
        opts.setServerIdFor(sid, asstId, newServerId);
      }

      try { await opts.onRetitle(sid, finalText); } catch {}
      try {
        window.dispatchEvent(
          new CustomEvent("chats:refresh", {
            detail: {
              sessionId: sid,
              lastMessage: finalText,
              updatedAt: new Date().toISOString(),
            },
          })
        );
      } catch {}

      const pokeForTitle = (delayMs: number) => {
        window.setTimeout(async () => {
          try {
            const ceil = new Date().toISOString();
            const page = await listChatsPage(0, 30, ceil);
            const row = page.content.find((r: any) => r.sessionId === sid);
            if (row?.title) {
              window.dispatchEvent(
                new CustomEvent("chats:refresh", {
                  detail: {
                    sessionId: sid,
                    title: row.title,
                    updatedAt: row.updatedAt,
                  },
                })
              );
            }
          } catch {}
        }, delayMs);
      };
      pokeForTitle(3000);
      pokeForTitle(8000);
    }
  } catch (e: any) {
    const localAbort =
      e?.name === "AbortError" && (wasCanceled() || controller.signal.aborted);
    const reason = localAbort ? "client_abort_after_stop" : e?.name || "client_error";

    const last = snapshotPendingAssistant(opts.getMessagesFor(sid));
    opts.setMetricsFallbackFor(sid, reason, last);
    pinFallbackToSessionAndBubble(opts, sid, asstId, reason, last);

    opts.setMessagesFor(sid, (prev) => {
      const end = prev[prev.length - 1];
      if (end?.role === "assistant" && !end.text.trim()) {
        return prev.map((m, i) =>
          i === prev.length - 1 ? { ...m, text: "[stream error]" } : m
        );
      }
      return prev;
    });
  } finally {
    if (d.getCancelForSid() === sid) d.clearCancelIf(sid);
    opts.setLoadingFor(sid, false);
    d.setController(null);
    d.setReader(null);
  }
}

# ===== frontend/src/file_read/hooks/stream/core/runner_metrics.ts =====

import type { RunJson, GenMetrics } from "../../../shared/lib/runjson";
import type { ChatMsg } from "../../../types/chat";

type Opts = {
  setMetricsFor: (sid: string, json?: RunJson, flat?: GenMetrics) => void;
  setMessagesFor: (sid: string, fn: (prev: ChatMsg[]) => ChatMsg[]) => void;
  setMetricsFallbackFor: (sid: string, reason: string, text: string) => void;
};

export function pinLiveMetricsToSession(
  opts: Opts,
  sid: string,
  json?: RunJson,
  flat?: GenMetrics
) {
  opts.setMetricsFor(sid, json, flat);
}

export function pinLiveMetricsToBubble(
  opts: Opts,
  sid: string,
  asstId: string,
  json?: RunJson,
  flat?: GenMetrics
) {
  opts.setMessagesFor(sid, (prev) =>
    prev.map((m) =>
      m.id === asstId
        ? { ...m, meta: { ...(m.meta ?? {}), runJson: json ?? m.meta?.runJson, flat: flat ?? m.meta?.flat } }
        : m
    )
  );
}

/** Also returns the synthesized RunJson so caller can persist if needed. */
export function pinFallbackToSessionAndBubble(
  opts: Opts,
  sid: string,
  asstId: string,
  reason: string,
  finalText: string
): RunJson {
  const json: RunJson = {
    stats: {
      stopReason: reason,
      tokensPerSecond: null,
      timeToFirstTokenSec: null,
      totalTimeSec: null,
      promptTokensCount: null,
      predictedTokensCount: finalText ? finalText.length : 0,
      totalTokensCount: null,
    },
  };
  const flat: GenMetrics = {
    stop_reason: reason,
    tok_per_sec: null,
    ttft_ms: null,
    output_tokens: null,
    input_tokens_est: null,
    total_tokens_est: null,
  };

  opts.setMetricsFallbackFor(sid, reason, finalText);
  opts.setMessagesFor(sid, (prev) =>
    prev.map((m) =>
      m.id === asstId ? { ...m, meta: { ...(m.meta ?? {}), runJson: json, flat } } : m
    )
  );
  return json;
}

# ===== frontend/src/file_read/hooks/stream/core/runner_persist.ts =====

import { appendMessage, updateChatLast } from "../../../data/chatApi";
import type { RunJson } from "../../../shared/lib/runjson";
import { MET_START, MET_END } from "../../../shared/lib/runjson";

/**
 * Persist assistant turn; returns the new server message id (or null).
 */
export async function persistAssistantTurn(
  sid: string,
  finalText: string,
  json: RunJson | null
): Promise<number | null> {
  let toPersist = finalText;
  if (json) {
    toPersist = `${finalText}\n${MET_START}\n${JSON.stringify(json)}\n${MET_END}\n`;
  }
  try {
    const row = await appendMessage(sid, "assistant", toPersist);
    await updateChatLast(sid, finalText, "").catch(() => {});
    return row?.id != null ? Number(row.id) : null;
  } catch {
    return null;
  }
}

# ===== frontend/src/file_read/hooks/stream/core/runner_stream.ts =====

import { STOP_FLUSH_TIMEOUT_MS } from "./constants";
import type { RunJson, GenMetrics } from "../../../shared/lib/runjson";
import { processChunk } from "./buffer";

type LoopDeps = {
  wasCanceled: () => boolean;
  onDelta: (delta: string, cleanSoFar: string) => void;
  onMetrics: (json?: RunJson, flat?: GenMetrics) => void;
  onCancelTimeout: (cleanSoFar: string) => void;
};

export async function readStreamLoop(
  reader: ReadableStreamDefaultReader<Uint8Array>,
  d: LoopDeps
): Promise<{ finalText: string; gotMetrics: boolean; lastRunJson: RunJson | null }> {
  const decoder = new TextDecoder();
  let rawBuf = "";
  let cleanSoFar = "";
  let gotMetrics = false;
  let lastRunJson: RunJson | null = null;
  let stopTimeout: number | null = null;

  while (true) {
    const { value, done } = await reader.read();
    if (done) break;

    if (value) {
      rawBuf += decoder.decode(value, { stream: true });
      const step = processChunk(cleanSoFar, rawBuf);

      if (step.metrics) {
        gotMetrics = true;
        if (step.metrics.json) lastRunJson = step.metrics.json;
        d.onMetrics(step.metrics.json, step.metrics.flat);
      }
      if (step.delta) {
        cleanSoFar = step.cleanText;
        d.onDelta(step.delta, cleanSoFar);
      }
    }

    // If user canceled, schedule a final flush check, but DO NOT break early
    if (d.wasCanceled() && stopTimeout === null) {
      stopTimeout = window.setTimeout(() => {
        if (!gotMetrics) d.onCancelTimeout(cleanSoFar);
      }, STOP_FLUSH_TIMEOUT_MS) as unknown as number;
    }
  }

  if (stopTimeout !== null) {
    // If we scheduled a timeout but finished before it fired, synthesize now.
    if (!gotMetrics && d.wasCanceled()) {
      d.onCancelTimeout(cleanSoFar);
    }
    window.clearTimeout(stopTimeout);
  }

  return { finalText: cleanSoFar, gotMetrics, lastRunJson };
}

# ===== frontend/src/file_read/hooks/stream/core/types.ts =====

// frontend/src/file_read/hooks/stream/core/types.ts
import type { ChatMsg, Attachment } from "../../../types/chat";
import type { GenMetrics, RunJson } from "../../../shared/lib/runjson";

export type MsgAccessor = {
  getMessagesFor: (sessionId: string) => ChatMsg[];
  setMessagesFor: (sessionId: string, updater: (prev: ChatMsg[]) => ChatMsg[]) => void;
};

export type UiHooks = {
  setInput: (v: string) => void;
  setLoadingFor: (sessionId: string, v: boolean) => void;
  setQueuedFor: (sessionId: string, v: boolean) => void;
  setMetricsFor: (sessionId: string, json?: RunJson, flat?: GenMetrics) => void;
  setMetricsFallbackFor: (sessionId: string, reason: string, partialOut: string) => void;

  /** patch the server id for a bubble identified by clientId */
  setServerIdFor: (sid: string, clientId: string, serverId: number) => void;
};

export type SessionPlumbing = {
  getSessionId: () => string;
  ensureChatCreated: () => Promise<void>;
  onRetitle: (sessionId: string, latestAssistant: string) => Promise<void>;
  resetMetricsFor: (sessionId: string) => void;
};

export type StreamCoreOpts = MsgAccessor & UiHooks & SessionPlumbing;

export type StreamController = {
  /** text can be empty if attachments are present */
  send: (override?: string, attachments?: Attachment[]) => Promise<void>;
  stop: () => Promise<void>;
  cancelBySessionId: (sid: string) => Promise<void>;
  dispose: () => void;
};

# ===== frontend/src/file_read/hooks/stream/core/updater.ts =====

import type { ChatMsg } from "../../../types/chat";
import type { MsgAccessor } from "./types";

export function appendAssistantDelta(
  access: MsgAccessor,
  sessionId: string,
  asstId: string,
  delta: string
) {
  if (!delta) return;
  access.setMessagesFor(sessionId, (prev) => {
    const idx = prev.findIndex((m) => m.id === asstId);
    if (idx === -1) return prev;
    const next = [...prev];
    const cur = next[idx];
    next[idx] = { ...cur, text: (cur.text || "") + delta };
    return next;
  });
}

export function ensureAssistantPlaceholder(
  access: MsgAccessor,
  sessionId: string,
  asstId: string
) {
  access.setMessagesFor(sessionId, (prev) => {
    const idx = prev.findIndex((m) => m.id === asstId);
    if (idx !== -1) return prev;
    return [...prev, { id: asstId, serverId: null, role: "assistant", text: "" } as ChatMsg];
  });
}

export function snapshotPendingAssistant(msgs: ChatMsg[]): string {
  if (!msgs.length) return "";
  const last = msgs[msgs.length - 1];
  return last.role === "assistant" ? last.text : "";
}

# ===== frontend/src/file_read/hooks/useAttachmentUploads.ts =====

import { useCallback, useMemo, useRef, useState } from "react";
import type { Attachment } from "../types/chat";
import { requestRaw } from "../services/http";

// Local UI id for keys & removal
const makeUiId = () =>
  Math.random().toString(36).slice(2) + "-" + Date.now().toString(36);

export type UIAttachment = {
  uiId: string;
  name: string;

  // UI state
  status: "uploading" | "ready" | "error";
  pct: number;           // 0..100
  error?: string;

  // Optional extras
  size?: number;
  mime?: string;
  url?: string;
  serverId?: string;     // if backend returns an id
};

type ReturnShape = {
  atts: UIAttachment[];
  addFiles: (files: FileList | File[]) => Promise<void>;
  removeAtt: (arg: string | UIAttachment) => void; // accepts uiId or whole object
  anyUploading: boolean;
  anyReady: boolean;
  attachmentsForPost: () => Attachment[];          // what your API expects
  reset: () => void;
};

export function useAttachmentUploads(
  sessionId?: string,
  onRefreshChats?: () => void
): ReturnShape {
  const [atts, setAtts] = useState<UIAttachment[]>([]);
  const triedEndpoints = useRef<string[] | null>(null);

  const detectAndUpload = useCallback(
    async (file: File): Promise<UIAttachment> => {
      const endpoints =
        triedEndpoints.current ??
        ["/api/rag/upload", "/api/rag/uploads", "/api/uploads"];

      let lastErr: unknown = null;

      for (const ep of endpoints) {
        try {
          const fd = new FormData();
          fd.append("file", file);
          if (sessionId) fd.append("sessionId", sessionId);

          const res = await requestRaw(ep, { method: "POST", body: fd });
          const text = await res.text();
          if (!res.ok) throw new Error(`Upload failed (${res.status}) ${res.statusText} ${text || ""}`.trim());

          let data: any = {};
          try { data = text ? JSON.parse(text) : {}; } catch {}

          const ui: UIAttachment = {
            uiId: makeUiId(),
            name: (data.name ?? file.name) as string,
            status: "ready",
            pct: 100,
            size: Number(data.size ?? file.size) || file.size,
            mime: (data.contentType ?? data.mime ?? file.type) as string,
            url: (data.url ?? data.location ?? undefined) as string | undefined,
            serverId: (data.id ?? data.fileId ?? data.uploadId)?.toString(),
          };

          if (!triedEndpoints.current) {
            triedEndpoints.current = [ep, ...endpoints.filter((e) => e !== ep)];
          }
          return ui;
        } catch (e) {
          lastErr = e;
        }
      }
      throw lastErr ?? new Error("No working upload endpoint found");
    },
    [sessionId]
  );

  const addFiles = useCallback(
    async (files: FileList | File[]) => {
      const arr = Array.from(files);
      if (arr.length === 0) return;

      // optimistic rows
      const optimistic: UIAttachment[] = arr.map((f) => ({
        uiId: makeUiId(),
        name: f.name,
        status: "uploading",
        pct: 0,
        size: f.size,
        mime: f.type,
      }));
      setAtts((cur) => [...cur, ...optimistic]);

      await Promise.all(
        arr.map(async (file, i) => {
          const tempUiId = optimistic[i].uiId;
          try {
            const finalUi: UIAttachment = sessionId
              ? await detectAndUpload(file)
              : {
                  uiId: makeUiId(),
                  name: file.name,
                  status: "ready",
                  pct: 100,
                  size: file.size,
                  mime: file.type,
                };

            setAtts((cur) => cur.map((a) => (a.uiId === tempUiId ? finalUi : a)));
          } catch (e: any) {
            setAtts((cur) =>
              cur.map((a) =>
                a.uiId === tempUiId
                  ? { ...a, status: "error", pct: 0, error: e?.message ?? "Upload failed." }
                  : a
              )
            );
          }
        })
      );

      onRefreshChats?.();
    },
    [detectAndUpload, onRefreshChats, sessionId]
  );

  const removeAtt = useCallback((arg: string | UIAttachment) => {
    const uiId = typeof arg === "string" ? arg : arg.uiId;
    setAtts((cur) => cur.filter((a) => a.uiId !== uiId));
  }, []);

  const anyUploading = useMemo(() => atts.some((a) => a.status === "uploading"), [atts]);
  const anyReady = useMemo(() => atts.some((a) => a.status === "ready"), [atts]);

  const attachmentsForPost = useCallback((): Attachment[] => {
    return atts
      .filter((a) => a.status === "ready")
      .map<Attachment>((a) => {
        const out = { name: a.name } as Attachment;
        // If your Attachment type supports these, you can add them:
        // (out as any).url = a.url;
        // (out as any).contentType = a.mime;
        // (out as any).bytes = a.size;
        // (out as any).id = a.serverId;
        return out;
      });
  }, [atts]);

  const reset = useCallback(() => setAtts([]), []);

  return { atts, addFiles, removeAtt, anyUploading, anyReady, attachmentsForPost, reset };
}

// Back-compat alias if you want to import as Att
export type Att = UIAttachment;

# ===== frontend/src/file_read/hooks/useChatAutoFollow.ts =====

import { useEffect, useMemo, useRef } from "react";
import type { ChatMsg } from "../types/chat";

const SCROLL_THRESHOLD_VH = 0.75;
const MIN_THRESHOLD_PX = 24;
const FORCE_SCROLL_EVT = "chat:force-scroll-bottom";

export function useChatAutofollow({
  messages,
  loading,
  autoFollow = true,
  bottomPad = 0,
}: {
  messages: ChatMsg[];
  loading: boolean;
  autoFollow?: boolean;
  bottomPad?: number;
}) {
  const listRef = useRef<HTMLDivElement>(null);
  const bottomRef = useRef<HTMLDivElement>(null);

  const prevLoadingRef = useRef<boolean>(loading);
  const prevAsstLenRef = useRef<number>(0);
  const didInitialAutoscrollRef = useRef(false);

  function getScrollEl(): HTMLElement | null {
    const el = listRef.current;
    if (!el) return null;
    return el.closest<HTMLElement>("[data-chat-scroll]") ?? el;
  }

  function isNearBottom(ratio = SCROLL_THRESHOLD_VH): boolean {
    const el = getScrollEl();
    if (!el) return true;
    const threshold = Math.max(MIN_THRESHOLD_PX, el.clientHeight * ratio);
    const dist = el.scrollHeight - el.scrollTop - el.clientHeight;
    return dist <= threshold;
  }

  const scrollToBottom = (behavior: ScrollBehavior = "smooth") => {
    bottomRef.current?.scrollIntoView({ behavior, block: "end" });
  };

  useEffect(() => {
    if (didInitialAutoscrollRef.current) return;
    if (messages.length === 0) return;
    didInitialAutoscrollRef.current = true;
    scrollToBottom("auto");
    requestAnimationFrame(() => scrollToBottom("auto"));
  }, [messages.length]);

  useEffect(() => {
    const handler = (evt: Event) => {
      const behavior =
        (evt as CustomEvent<{ behavior?: ScrollBehavior }>).detail?.behavior ?? "smooth";
      bottomRef.current?.scrollIntoView({ behavior, block: "end" });
    };
    window.addEventListener(FORCE_SCROLL_EVT, handler as EventListener);
    return () => window.removeEventListener(FORCE_SCROLL_EVT, handler as EventListener);
  }, []);

  const lastAssistantIndex = useMemo(() => {
    for (let i = messages.length - 1; i >= 0; i--) {
      if (messages[i].role === "assistant") return i;
    }
    return -1;
  }, [messages]);

  const asstText = lastAssistantIndex >= 0 ? (messages[lastAssistantIndex]?.text ?? "") : "";

  useEffect(() => {
    if (lastAssistantIndex < 0) return;
    const len = asstText.length;
    const prev = prevAsstLenRef.current || 0;
    prevAsstLenRef.current = len;
    if (!autoFollow) return;
    if (len > prev && isNearBottom()) scrollToBottom("auto");
  }, [lastAssistantIndex, asstText, autoFollow]);

  useEffect(() => {
    const prev = prevLoadingRef.current;
    const cur = loading;
    prevLoadingRef.current = cur;
    if (prev && !cur && autoFollow && isNearBottom()) scrollToBottom("smooth");
  }, [loading, autoFollow]);

  useEffect(() => {
    if (autoFollow && isNearBottom()) {
      bottomRef.current?.scrollIntoView({ behavior: "auto", block: "end" });
    }
  }, [bottomPad, autoFollow]);

  return { listRef, bottomRef, lastAssistantIndex };
}

# ===== frontend/src/file_read/hooks/useChatsPager.ts =====

import { useEffect, useMemo, useRef, useState } from "react";
import { listChatsPage } from "../data/chatApi";
import type { ChatRow } from "../types/chat";

export function useChatsPager(pageSize = 10, refreshKey?: number) {
  const [chats, setChats] = useState<ChatRow[]>([]);
  const [page, setPage] = useState(0);
  const [hasMore, setHasMore] = useState(true);
  const [total, setTotal] = useState(0);
  const [totalPages, setTotalPages] = useState(0);
  const [ceiling, setCeiling] = useState<string | null>(null);
  const [initialLoading, setInitialLoading] = useState(false);
  const [loadingMore, setLoadingMore] = useState(false);

  const scrollRef = useRef<HTMLDivElement>(null);
  const sentinelRef = useRef<HTMLDivElement>(null);
  const loadingMoreRef = useRef(false);

  const seenIds = useMemo(() => new Set(chats.map(c => c.sessionId)), [chats]);

  async function loadFirst() {
    setInitialLoading(true);
    try {
      const ceil = new Date().toISOString();
      setCeiling(ceil);
      const res = await listChatsPage(0, pageSize, ceil);
      setChats(res.content);
      setPage(1);
      setHasMore(!res.last);
      setTotal(res.totalElements ?? 0);
      setTotalPages(res.totalPages ?? 0);
    } catch {
      setChats([]); setPage(0); setHasMore(false); setTotal(0); setTotalPages(0);
    } finally {
      setInitialLoading(false);
    }
  }

  async function loadMore() {
    if (loadingMoreRef.current || loadingMore || !hasMore || !ceiling) return;
    loadingMoreRef.current = true;
    setLoadingMore(true);
    try {
      const res = await listChatsPage(page, pageSize, ceiling);
      const next = res.content.filter(c => !seenIds.has(c.sessionId));
      setChats(prev => [...prev, ...next]);
      setPage(p => p + 1);
      setHasMore(!res.last);
      setTotal(res.totalElements ?? total);
      setTotalPages(res.totalPages ?? totalPages);
    } catch {
      setHasMore(false);
    } finally {
      loadingMoreRef.current = false;
      setLoadingMore(false);
    }
  }

  async function refreshFirst() {
    setChats([]); setPage(0); setHasMore(true);
    setTotal(0); setTotalPages(0); setCeiling(null);
    await loadFirst();
  }

  const didMountRef = useRef(false);
  useEffect(() => {
    if (!didMountRef.current) { didMountRef.current = true; return; }
    if (typeof refreshKey !== "undefined") void refreshFirst();
  }, [refreshKey]);

  useEffect(() => {
    const handler = (evt: Event) => {
      const detail = (evt as CustomEvent<any>).detail;
      if (!detail?.sessionId) return;
      const sid = String(detail.sessionId);
      setChats(prev => {
        const ix = prev.findIndex(c => c.sessionId === sid);
        const shouldMoveToTop = typeof detail.lastMessage === "string";
        if (ix === -1) {
          if (!shouldMoveToTop) return prev;
          const injected: ChatRow = {
            sessionId: sid,
            id: -1,
            title: detail.title ?? "New Chat",
            lastMessage: detail.lastMessage ?? "",
            createdAt: new Date().toISOString(),
            updatedAt: detail.updatedAt ?? new Date().toISOString(),
          };
          return [injected, ...prev];
        }
        const cur = prev[ix];
        const patched: ChatRow = {
          ...cur,
          lastMessage: detail.lastMessage ?? cur.lastMessage,
          title: detail.title ?? cur.title,
          updatedAt: detail.updatedAt ?? cur.updatedAt,
        };
        const rest = prev.filter((_, i) => i !== ix);
        return shouldMoveToTop ? [patched, ...rest] : [...prev.slice(0, ix), patched, ...prev.slice(ix + 1)];
      });
    };
    window.addEventListener("chats:refresh", handler as EventListener);
    return () => window.removeEventListener("chats:refresh", handler as EventListener);
  }, []);

  useEffect(() => {
    const rootEl = scrollRef.current, sentinel = sentinelRef.current;
    if (!rootEl || !sentinel) return;
    const hasOverflow = rootEl.scrollHeight - rootEl.clientHeight > 8;
    if (!hasOverflow) return;
    const io = new IntersectionObserver((entries) => {
      const entry = entries[0];
      if (entry?.isIntersecting) void loadMore();
    }, { root: rootEl, rootMargin: "96px 0px", threshold: 0.01 });
    io.observe(sentinel);
    return () => io.disconnect();
  }, [chats.length, page, hasMore, ceiling]);

  function decTotal(count: number) {
    setTotal(prev => {
      const next = Math.max(0, prev - count);
      setTotalPages(Math.max(1, Math.ceil(next / pageSize)));
      return next;
    });
  }

  return {
    chats, page, hasMore, total, totalPages,
    initialLoading, loadingMore,
    scrollRef, sentinelRef,
    loadMore, refreshFirst, setChats,
    decTotal,
  };
}

# ===== frontend/src/file_read/hooks/useChatState.ts =====

import { useState } from "react";
import type { ChatMsg } from "../types/chat";

type BySession<T> = Record<string, T>;

export function useChatState() {
  const [bySession, setBySession] = useState<BySession<ChatMsg[]>>({});
  const [input, setInput] = useState("");

  const getMsgs = (sid: string) => bySession[sid] ?? [];
  const setMsgs = (
    sid: string,
    upd: ((prev: ChatMsg[]) => ChatMsg[]) | ChatMsg[]
  ) => {
    setBySession((prev) => {
      const cur = prev[sid] ?? [];
      const next = Array.isArray(upd) ? upd : upd(cur);
      return next === cur ? prev : { ...prev, [sid]: next };
    });
  };

  const resetMsgs = (sid: string) =>
    setBySession((prev) => (prev[sid] ? { ...prev, [sid]: [] } : prev));

  return { bySession, input, setInput, getMsgs, setMsgs, resetMsgs };
}

# ===== frontend/src/file_read/hooks/useChatStream.ts =====

import { useMemo } from "react";
import type { ChatMsg } from "../types/chat";
import { useSession } from "./useSession";
import { useStream } from "./useStream";
import { useChatState } from "./useChatState";
import { useRunState } from "./useRunState";
import { useMetrics } from "./useMetrics";
// Retitle disabled for now
// import { useRetitle } from "./useRetitle";

export type { GenMetrics, RunJson } from "../shared/lib/runjson";

export function useChatStream() {
  // state slices
  const chat = useChatState();
  const run = useRunState();
  const met = useMetrics();
  // Retitle disabled
  // const { retitleNow } = useRetitle(true);

  // session plumbing
  const { sessionIdRef, ensureChatCreated, loadHistory, setSessionId, resetSession } = useSession({
    setMessagesForSession: (sid: string, msgs: ChatMsg[]) => chat.setMsgs(sid, msgs),
    getMessagesForSession: (sid: string) => chat.getMsgs(sid),
    isStreaming: (sid: string) => !!run.loadingBy[sid],
  });

  // stream controller
  const { send, stop, cancelBySessionId } = useStream({
    messages: chat.getMsgs(sessionIdRef.current), // keeps deps stable
    setMessagesForSession: (sid, updater) => chat.setMsgs(sid, updater),
    getMessagesForSession: chat.getMsgs,
    setInput: chat.setInput,
    sessionId: () => sessionIdRef.current,
    ensureChatCreated,

    // 🔕 Retitle disabled: provide a no-op with the expected signature
    onRetitle: async (_sessionId: string, _latestAssistant: string) => { /* no-op */ },

    setLoadingForSession: run.setLoadingFor,
    setQueuedForSession: run.setQueuedFor,
    setMetricsForSession: met.setMetricsFor,
    setMetricsFallbackForSession: met.setMetricsFallbackFor,
    resetMetricsForSession: met.resetMetricsFor,
  });

  async function cancelSessions(ids: string[]) {
    await Promise.all(ids.map(cancelBySessionId));
  }

  // derived (active session)
  const activeId = sessionIdRef.current;
  const messages = useMemo(() => chat.getMsgs(activeId), [chat.bySession, activeId]);
  const loading = !!run.loadingBy[activeId];
  const queued = !!run.queuedBy[activeId];
  const runJson = met.metricsBy[activeId]?.runJson ?? null;
  const runMetrics = met.metricsBy[activeId]?.flat ?? null;

  function reset() {
    chat.setInput("");
    met.resetMetricsFor(activeId);
    resetSession();
  }

  function snapshotPendingAssistant(): string {
    const msgs = chat.getMsgs(activeId);
    const last = msgs[msgs.length - 1];
    return last?.role === "assistant" ? (last.text ?? "") : "";
  }

  return {
    // chat state
    messages,
    input: chat.input,
    setInput: chat.setInput,
    loading,
    queued,

    // actions
    send,
    stop,
    cancelSessions,

    // session ctl
    setSessionId,
    sessionIdRef,
    loadHistory,
    reset,
    snapshotPendingAssistant,

    // metrics (active only)
    runMetrics,
    runJson,
    clearMetrics: () => met.resetMetricsFor(activeId),
  };
}

# ===== frontend/src/file_read/hooks/useMetrics.ts =====

import { useState } from "react";
import type { GenMetrics, RunJson } from "../shared/lib/runjson";

type BySession<T> = Record<string, T>;
type Pair = { runJson: RunJson | null; flat: GenMetrics | null };

export function useMetrics() {
  const [metricsBy, setMetricsBy] = useState<BySession<Pair>>({});

  function resetMetricsFor(sid: string) {
    setMetricsBy((prev) => ({ ...prev, [sid]: { runJson: null, flat: null } }));
  }

  function setMetricsFor(sid: string, json?: RunJson, flat?: GenMetrics) {
    setMetricsBy((prev) => {
      const cur = prev[sid] ?? { runJson: null, flat: null };
      return { ...prev, [sid]: { runJson: json ?? cur.runJson, flat: flat ?? cur.flat } };
    });
  }

  function setMetricsFallbackFor(sid: string, reason: string, partialOut: string) {
    const json: RunJson = {
      stats: {
        stopReason: reason,
        tokensPerSecond: null,
        timeToFirstTokenSec: null,
        totalTimeSec: null,
        promptTokensCount: null,
        predictedTokensCount: partialOut ? partialOut.length : 0,
        totalTokensCount: null,
      },
    };
    const flat: GenMetrics = {
      stop_reason: reason,
      tok_per_sec: null,
      ttft_ms: null,
      output_tokens: null,
      input_tokens_est: null,
      total_tokens_est: null,
    };
    setMetricsFor(sid, json, flat);
  }

  return { metricsBy, resetMetricsFor, setMetricsFor, setMetricsFallbackFor };
}

# ===== frontend/src/file_read/hooks/useMultiSelect.ts =====

// frontend/src/file_read/hooks/useMultiSelect.ts
import { useEffect, useMemo, useState } from "react";

export function useMultiSelect(allIds: string[]) {
  const [selected, setSelected] = useState<Set<string>>(new Set());

  // keep selection pruned when the list of ids changes
  useEffect(() => {
    setSelected(prev => {
      const next = new Set([...prev].filter(id => allIds.includes(id)));
      // only update if it actually changed
      return next.size === prev.size ? prev : next;
    });
  }, [allIds]);

  const allSelected = useMemo(
    () => selected.size > 0 && selected.size === allIds.length,
    [selected, allIds.length]
  );

  const toggleOne = (id: string) => {
    setSelected(prev => {
      const next = new Set(prev);
      next.has(id) ? next.delete(id) : next.add(id);
      return next;
    });
  };

  const toggleAll = () => {
    setSelected(prev =>
      prev.size === allIds.length ? new Set() : new Set(allIds)
    );
  };

  return { selected, setSelected, allSelected, toggleOne, toggleAll };
}

# ===== frontend/src/file_read/hooks/useRetitle.ts =====

// frontend/src/file_read/hooks/useRetitle.ts
import { API_BASE } from "../services/http";
import { listMessages, updateChatLast } from "../data/chatApi";
import { stripRunJson } from "../shared/lib/runjson";

const TITLE_MAX_WORDS = 6;

function sanitizeTitle(raw: string, maxWords = TITLE_MAX_WORDS) {
  let t = (raw.split(/\r?\n/)[0] ?? "");
  t = t.replace(/[^\p{L}\p{N} ]+/gu, " ");
  t = t.replace(/\s+/g, " ").trim();
  t = t.split(" ").slice(0, maxWords).join(" ").trim();
  return t;
}

export function useRetitle(enabled = true) {
  async function retitleNow(sessionId: string, latestAssistant: string) {
    console.log("1")
    if (!enabled) return;

    try {
      // Build a minimal transcript to title on
      const rows = await listMessages(sessionId);
      const textDump = rows
        .map((r) => `${r.role === "user" ? "User" : "Assistant"}: ${r.content}`)
        .join("\n");

      // Ask the local model for a very short title
      const res = await fetch(`${API_BASE}/api/ai/generate/stream`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          sessionId: `${sessionId}::titlebot`,
          messages: [
            {
              role: "system",
              content: "You output ultra-concise, neutral chat titles and nothing else.",
            },
            {
              role: "user",
              content:
                "Write ONE short title summarizing the conversation below.\n" +
                "Rules: at most 6 words — sentence or title case — no punctuation — no emojis — no quotes — return ONLY the title\n\n" +
                textDump +
                "\n\nTitle:",
            },
          ],
          max_tokens: 24,
          temperature: 0.2,
          top_p: 0.9,
        }),
      });

      if (!res.ok || !res.body) return;

      // Gather the streamed text + strip runjson
      const reader = res.body.getReader();
      const decoder = new TextDecoder();
      let raw = "";
      while (true) {
        const { value, done } = await reader.read();
        if (done) break;
        if (value) raw += decoder.decode(value, { stream: true });
      }

      const { text } = stripRunJson(raw);
      const title = sanitizeTitle(text);
      if (!title) return;

      // Save title + lastMessage (lastMessage stays the full assistant text)
      await updateChatLast(sessionId, latestAssistant, title).catch(() => {});
      // Nudge sidebar so it shows new title immediately
      try {
        window.dispatchEvent(new CustomEvent("chats:refresh"));
      } catch {}
    } catch (e) {
      console.warn("retitleNow failed:", e);
    }
  }

  return { retitleNow };
}

# ===== frontend/src/file_read/hooks/useRunState.ts =====

import { useState } from "react";

type BySession<T> = Record<string, T>;

export function useRunState() {
  const [loadingBy, setLoadingBy] = useState<BySession<boolean>>({});
  const [queuedBy, setQueuedBy] = useState<BySession<boolean>>({});

  const setBool =
    (setter: React.Dispatch<React.SetStateAction<BySession<boolean>>>) =>
    (sid: string, v: boolean) =>
      setter((prev) => (prev[sid] === v ? prev : { ...prev, [sid]: v }));

  return {
    loadingBy,
    queuedBy,
    setLoadingFor: setBool(setLoadingBy),
    setQueuedFor: setBool(setQueuedBy),
  };
}

# ===== frontend/src/file_read/hooks/useSession.ts =====

// frontend/src/file_read/hooks/useSession.ts
import { useRef } from "react";
import type { ChatMsg, ChatMessageRow } from "../types/chat";
import { createChat, listMessages } from "../data/chatApi";
import { extractRunJsonFromBuffer } from "../shared/lib/runjson";

function rowToMsg(r: ChatMessageRow): ChatMsg {
  if (r.role === "assistant") {
    const { clean, json, flat } = extractRunJsonFromBuffer(r.content);
    const base: ChatMsg = {
      id: `cid-${r.id}`,        
      serverId: r.id,
      role: r.role,
      text: clean,
      attachments: r.attachments ?? [],
    };
    if (json || flat) base.meta = { runJson: json ?? null, flat: flat ?? null };
    return base;
  }

  // user
  return {
    id: `cid-${r.id}`,
    serverId: r.id,
    role: r.role,
    text: r.content,
    attachments: r.attachments ?? [],
  };
}

export function useSession(opts: {
  setMessagesForSession: (sid: string, msgs: ChatMsg[]) => void;
  getMessagesForSession: (sid: string) => ChatMsg[];
  isStreaming: (sid: string) => boolean;
}) {
  const { setMessagesForSession, getMessagesForSession, isStreaming } = opts;
  const sessionIdRef = useRef<string>(crypto.randomUUID());
  const hasCreatedRef = useRef(false);

  async function ensureChatCreated() {
    if (!sessionIdRef.current) {
      sessionIdRef.current = crypto.randomUUID();
      hasCreatedRef.current = false;
    }
    if (hasCreatedRef.current) return;
    try {
      await createChat(sessionIdRef.current, "New Chat");
      hasCreatedRef.current = true;
    } catch (e) {
      console.warn("createChat failed:", e);
    }
  }

  async function loadHistory(sessionId: string): Promise<void> {
    sessionIdRef.current = sessionId;
    hasCreatedRef.current = true;

    try {
      const rows = await listMessages(sessionId);
      const serverMsgs = rows.map(rowToMsg);

      const prevClient = getMessagesForSession(sessionId) ?? [];

      if (isStreaming(sessionId)) {
        const byServer = new Map<number, ChatMsg>(
          prevClient.filter(m => m.serverId != null).map(m => [m.serverId as number, m])
        );
        const merged = serverMsgs.map(s => {
          const prev = byServer.get(s.serverId!);
          if (!prev) return s;
          const meta = s.meta ?? prev.meta ?? undefined;
          return { ...prev, ...s, meta };
        });

        const tail: ChatMsg[] = [];
        const last = prevClient[prevClient.length - 1];
        if (last?.role === "assistant" && (last.text?.length ?? 0) > 0 && last.serverId == null) {
          tail.push(last);
        }

        setMessagesForSession(sessionId, [...merged, ...tail]);
      } else {
        setMessagesForSession(sessionId, serverMsgs);
      }
    } catch (e) {
      console.warn("listMessages failed:", e);
      if (!isStreaming(sessionId)) setMessagesForSession(sessionId, []);
    }
  }

  function setSessionId(newId: string) {
    sessionIdRef.current = newId;
    hasCreatedRef.current = false;
    setMessagesForSession(newId, []);
  }

  function resetSession() {
    const id = sessionIdRef.current;
    if (id) setMessagesForSession(id, []);
  }

  return {
    sessionIdRef,
    ensureChatCreated,
    loadHistory,
    setSessionId,
    resetSession,
  };
}

# ===== frontend/src/file_read/hooks/useSettings.ts =====

import { useCallback, useEffect, useMemo, useState } from "react";
import {
  getEffective, getOverrides, getDefaults, getAdaptive,
  patchOverrides, putOverrides, recomputeAdaptive,
} from "../data/settingsApi";

type State = {
  loading: boolean;
  error: string | null;
  effective: any | null;
  overrides: any | null;
  defaults: any | null;
  adaptive: any | null;
};

export function useSettings(sessionId?: string) {
  const [state, setState] = useState<State>({
    loading: false,
    error: null,
    effective: null,
    overrides: null,
    defaults: null,
    adaptive: null,
  });

  const load = useCallback(async () => {
    setState(s => ({ ...s, loading: true, error: null }));
    try {
      const [effective, overrides, defaults, adaptive] = await Promise.all([
        getEffective(sessionId),
        getOverrides(),
        getDefaults(),
        getAdaptive(),
      ]);
      setState({
        loading: false,
        error: null,
        effective,
        overrides,
        defaults,
        adaptive,
      });
    } catch (e: any) {
      setState(s => ({
        ...s,
        loading: false,
        error: e?.message || "Failed to load settings",
      }));
    }
  }, [sessionId]);

  useEffect(() => { void load(); }, [load]);

  const saveOverrides = useCallback(
    async (data: Record<string, any>, method: "patch" | "put" = "patch") => {
      if (method === "put") await putOverrides(data);
      else await patchOverrides(data);
      await load();
    },
    [load]
  );

  const runAdaptive = useCallback(async () => {
    await recomputeAdaptive(sessionId);
    await load();
  }, [load, sessionId]);

  return useMemo(
    () => ({
      ...state,
      reload: load,
      saveOverrides,
      runAdaptive,
    }),
    [state, load, saveOverrides, runAdaptive]
  );
}

# ===== frontend/src/file_read/hooks/useSidebar.ts =====

// frontend/src/file_read/hooks/useSidebar.ts
import { useEffect, useState } from "react";

export function useSidebar() {
  const [sidebarOpen, setSidebarOpen] = useState(() => window.innerWidth >= 768);

  // pin on desktop
  useEffect(() => {
    const onResize = () => {
      if (window.innerWidth >= 768) setSidebarOpen(true);
    };
    window.addEventListener("resize", onResize);
    return () => window.removeEventListener("resize", onResize);
  }, []);

  // Ctrl+B toggle
  useEffect(() => {
    const onKey = (e: KeyboardEvent) => {
      if (e.ctrlKey && (e.key === "b" || e.key === "B")) {
        e.preventDefault();
        setSidebarOpen((v) => !v);
      }
    };
    window.addEventListener("keydown", onKey);
    return () => window.removeEventListener("keydown", onKey);
  }, []);

  // mobile drawer helpers (DOM-level; safe here)
  const openMobileDrawer = () => {
    document.getElementById("mobile-drawer")?.classList.remove("hidden");
    document.getElementById("mobile-backdrop")?.classList.remove("hidden");
    document.body.style.overflow = "hidden";
  };
  const closeMobileDrawer = () => {
    document.getElementById("mobile-drawer")?.classList.add("hidden");
    document.getElementById("mobile-backdrop")?.classList.add("hidden");
    document.body.style.overflow = "";
  };

  return {
    sidebarOpen,
    setSidebarOpen,
    openMobileDrawer,
    closeMobileDrawer,
  };
}

# ===== frontend/src/file_read/hooks/useStream.ts =====

// frontend/src/file_read/hooks/useStream.ts
import { useEffect, useMemo, useRef, useState } from "react";
import type { ChatMsg } from "../types/chat";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";
import { createStreamController } from "./stream/core/controller";
import type { Attachment } from "../types/chat";

type UseStreamDeps = {
  // not read directly; keeps memo deps stable
  messages: ChatMsg[];

  // message state per session
  setMessagesForSession: (
    sid: string,
    updater: (prev: ChatMsg[]) => ChatMsg[]
  ) => void;
  getMessagesForSession: (sid: string) => ChatMsg[];

  // ui
  setInput: (v: string) => void;

  // session plumbing
  sessionId: () => string;
  ensureChatCreated: () => Promise<void>;
  onRetitle: (sessionId: string, latestAssistant: string) => Promise<void>;

  // per-session flags/metrics
  setLoadingForSession: (sid: string, v: boolean) => void;
  setQueuedForSession?: (sid: string, v: boolean) => void;
  setMetricsForSession: (sid: string, json?: RunJson, flat?: GenMetrics) => void;
  setMetricsFallbackForSession: (
    sid: string,
    reason: string,
    partialOut: string
  ) => void;
  resetMetricsForSession: (sid: string) => void;
};

export function useStream({
  setMessagesForSession,
  getMessagesForSession,
  setInput,
  sessionId,
  ensureChatCreated,
  onRetitle,
  setLoadingForSession,
  setQueuedForSession,
  setMetricsForSession,
  setMetricsFallbackForSession,
  resetMetricsForSession,
}: UseStreamDeps) {
  const [loading, setLoading] = useState(false);
  const controllerRef = useRef<ReturnType<typeof createStreamController> | null>(null);

  const controller = useMemo(() => {
    return createStreamController({
      // message access
      getMessagesFor: (sid) => getMessagesForSession(sid),
      setMessagesFor: (sid, updater) => setMessagesForSession(sid, updater),

      // ui hooks
      setInput: () => setInput(""),
      setLoadingFor: (sid, v) => {
        if (sid === sessionId()) setLoading(v);
        setLoadingForSession(sid, v);
      },
      setQueuedFor: setQueuedForSession ?? (() => {}),

      // metrics
      setMetricsFor: (sid, json, flat) => setMetricsForSession(sid, json, flat),
      setMetricsFallbackFor: (sid, reason, out) =>
        setMetricsFallbackForSession(sid, reason, out),

      // patch server id onto a bubble identified by clientId
      setServerIdFor: (sid, clientId, serverId) => {
        setMessagesForSession(sid, (prev) =>
          prev.map((m) => (m.id === clientId ? { ...m, serverId } : m))
        );
      },

      // session plumbing
      getSessionId: sessionId,
      ensureChatCreated,
      onRetitle,
      resetMetricsFor: (sid) => resetMetricsForSession(sid),
    });
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  useEffect(() => {
    controllerRef.current = controller;
    return () => controllerRef.current?.dispose();
  }, [controller]);

  async function send(override?: string, attachments: Attachment[] = []) {
    const text = (override ?? "").trim();
    if (!text && attachments.length === 0) return;
    await controller.send(text, attachments);
  }

  async function stop() {
    await controller.stop();
  }

  async function cancelBySessionId(sid: string) {
    await controller.cancelBySessionId(sid);
  }

  return { loading, send, stop, cancelBySessionId };
}

# ===== frontend/src/file_read/hooks/useToast.ts =====

import { useState } from "react";

export function useToast() {
  const [toast, setToast] = useState<string | null>(null);
  function show(msg: string, ms = 1800) {
    setToast(msg);
    window.clearTimeout((show as any)._t);
    (show as any)._t = window.setTimeout(() => setToast(null), ms);
  }
  return { toast, show };
}

# ===== frontend/src/file_read/index.css =====

@tailwind base;
@tailwind components;
@tailwind utilities;

# ===== frontend/src/file_read/main.tsx =====

import { StrictMode } from "react";
import "./index.css";
import { createRoot } from "react-dom/client";
import App from "./App";
import { AuthProvider } from "./auth/AuthContext"; // ⬅️ add

createRoot(document.getElementById("root")!).render(
  <StrictMode>
    <AuthProvider>
      <App />
    </AuthProvider>
  </StrictMode>
);

# ===== frontend/src/file_read/pages/AgentRunner.tsx =====

//frontend/src/file_read/pages/AgentRunner.tsx
import { useEffect, useState } from "react";
import ChatContainer from "../components/ChatContainer";
import ChatSidebar from "../components/ChatSidebar/ChatSidebar";
import { useChatStream } from "../hooks/useChatStream";
import { useSidebar } from "../hooks/useSidebar";
import { useToast } from "../hooks/useToast";
import DesktopHeader from "../components/DesktopHeader";
import MobileDrawer from "../components/MobileDrawer";
import Toast from "../shared/ui/Toast";
import { createChat, deleteMessagesBatch } from "../data/chatApi";
import SettingsPanel from "../components/SettingsPanel";
import KnowledgePanel from "../components/KnowledgePanel";

const LS_KEY = "lastSessionId";

export default function AgentRunner() {
  const chat = useChatStream();
  const [showKnowledge, setShowKnowledge] = useState(false);
  const [refreshKey, setRefreshKey] = useState(0);
  const [autoFollow, setAutoFollow] = useState(true);
  const { toast, show } = useToast();
  const { sidebarOpen, setSidebarOpen, openMobileDrawer, closeMobileDrawer } = useSidebar();
  const [showSettings, setShowSettings] = useState(false);

useEffect(() => {
  const openSettings = () => setShowSettings(true);
  const openKnowledge = () => setShowKnowledge(true);
  const openCustomize = () => setShowKnowledge(true);
  window.addEventListener("open:settings", openSettings);
  window.addEventListener("open:knowledge", openKnowledge);
  window.addEventListener("open:customize", openCustomize);
  return () => {
    window.removeEventListener("open:settings", openSettings);
    window.removeEventListener("open:knowledge", openKnowledge);
    window.removeEventListener("open:customize", openCustomize);
  };
}, []);

  async function newChat(): Promise<void> {
    const id = crypto.randomUUID();
    chat.setSessionId(id);
    try { await createChat(id, "New Chat"); } catch {}
    localStorage.setItem(LS_KEY, id);
    setRefreshKey((k) => k + 1);
    chat.setInput("");
    chat.clearMetrics?.();
    await refreshFollow();
  }

  async function openSession(id: string): Promise<void> {
    if (!id) return;
    await chat.loadHistory(id);
    localStorage.setItem(LS_KEY, id);
    chat.setInput("");
    chat.clearMetrics?.();
    await refreshFollow();
  }

  async function refreshFollow() {
    const sid = chat.sessionIdRef.current;
    if (!sid) return;
    setAutoFollow(true);
    await chat.loadHistory(sid);
    const el = document.getElementById("chat-scroll-container");
    if (el) el.scrollTop = el.scrollHeight;
  }

  async function handleCancelSessions(ids: string[]) {
    if (!ids?.length) return;
    const currentId = chat.sessionIdRef.current || "";
    const deletingActive = currentId && ids.includes(currentId);
    if (deletingActive) {
      chat.setSessionId("");
      chat.setInput("");
      chat.clearMetrics?.();
      chat.reset();
      localStorage.removeItem(LS_KEY);
    }
    setRefreshKey((k) => k + 1);
    try { window.dispatchEvent(new CustomEvent("chats:refresh")); } catch {}
  }

  async function refreshPreserve() {
    const sid = chat.sessionIdRef.current;
    if (!sid) return;
    const el = document.getElementById("chat-scroll-container");
    const prevTop = el?.scrollTop ?? 0;
    const prevHeight = el?.scrollHeight ?? 0;
    setAutoFollow(false);
    await chat.loadHistory(sid);
    requestAnimationFrame(() => {
      if (el) {
        const newHeight = el.scrollHeight;
        el.scrollTop = prevTop + (newHeight - prevHeight);
      }
      setAutoFollow(true);
    });
  }

  async function handleDeleteMessages(clientIds: string[]) {
    const sid = chat.sessionIdRef.current;
    if (!sid || !clientIds?.length) return;
    const current = chat.messages;
    const toDelete = new Set(clientIds);
    const serverIds = current
      .filter((m: any) => toDelete.has(m.id) && m.serverId != null)
      .map((m: any) => m.serverId as number);
    const remaining = current.filter((m) => !toDelete.has(m.id));
    if ((chat as any).setMessagesForSession) {
      (chat as any).setMessagesForSession(sid, () => remaining);
    }
    try {
      if (serverIds.length) {
        await deleteMessagesBatch(sid, serverIds);
      }
      await refreshPreserve();
      setRefreshKey((k) => k + 1);
      try { window.dispatchEvent(new CustomEvent("chats:refresh")); } catch {}
      show("Message deleted");
    } catch {
      show("Failed to delete message");
      await chat.loadHistory(sid);
      setRefreshKey((k) => k + 1);
    }
  }

  return (
    <div className="h-screen w-full flex bg-gray-50">
      {sidebarOpen && (
        <div className="hidden md:flex h-full">
          <ChatSidebar
            onOpen={openSession}
            onNew={newChat}
            refreshKey={refreshKey}
            activeId={chat.sessionIdRef.current}
            onHideSidebar={() => setSidebarOpen(false)}
            onCancelSessions={handleCancelSessions}
          />
        </div>
      )}

      <MobileDrawer
        onOpenSession={openSession}
        onNewChat={newChat}
        refreshKey={refreshKey}
        activeId={chat.sessionIdRef.current}
        openMobileDrawer={openMobileDrawer}
        closeMobileDrawer={closeMobileDrawer}
      />
      <div className="md:hidden h-14 shrink-0" />

      <div className="flex-1 min-w-0 flex flex-col">
        <DesktopHeader
          sidebarOpen={sidebarOpen}
          onShowSidebar={() => setSidebarOpen(true)}
        />

        <div className="flex-1 min-h-0">
          <div className="h-full px-3 md:px-6">
            <div className="h-full w-full mx-auto max-w-3xl md:max-w-4xl relative">
              <div id="chat-scroll-container" className="h-full">
                <ChatContainer
                  messages={chat.messages}
                  input={chat.input}
                  setInput={chat.setInput}
                  loading={chat.loading}
                  queued={chat.queued}
                  send={chat.send}
                  stop={chat.stop}
                  runMetrics={chat.runMetrics}
                  runJson={chat.runJson}
                  onRefreshChats={() => {}}
                  onDeleteMessages={handleDeleteMessages}
                  autoFollow={autoFollow}
                  sessionId={chat.sessionIdRef.current}
                />
              </div>
              <Toast message={toast} />
            </div>
          </div>
        </div>
      </div>

      {showSettings && (
        <SettingsPanel
          sessionId={chat.sessionIdRef.current}
          onClose={() => setShowSettings(false)}
        />
      )}
      {showKnowledge && (
        <KnowledgePanel
          sessionId={chat.sessionIdRef.current}
          onClose={() => setShowKnowledge(false)}
          toast={show}
        />
      )}
    </div>
  );
}

# ===== frontend/src/file_read/services/http.ts =====

// frontend/src/file_read/services/http.ts
// Minimal HTTP helpers: local JWT + optional x-license (no Brave key on client)

export const API_BASE = (import.meta.env.VITE_API_URL || "/api").trim();

// ✅ Exported so every caller uses the same URL logic (prevents "/api/api/...")
export function buildUrl(path: string) {
  if (/^https?:\/\//i.test(path)) return path;

  const base = API_BASE.replace(/\/+$/, ""); // e.g. "/api"
  const p = (path.startsWith("/") ? path : `/${path}`).replace(/\/{2,}/g, "/");

  // If caller already included the base ("/api/..."), don't prepend it again
  if (base && p.startsWith(base + "/")) return p;

  return `${base}${p}`;
}

type JSONValue = unknown;

export class HttpError extends Error {
  status: number;
  body?: string;
  constructor(status: number, message: string, body?: string) {
    super(message);
    this.status = status;
    this.body = body;
  }
}

// Attach local JWT + optional license header (no Brave key here)
function authHeaders(): Record<string, string> {
  const h: Record<string, string> = {};
  try {
    const jwt = localStorage.getItem("local_jwt");
    if (jwt) h["Authorization"] = `Bearer ${jwt}`;
    const lic = localStorage.getItem("license_key");
    if (lic) h["x-license"] = lic;
  } catch {
    /* ignore storage errors */
  }
  return h;
}

async function buildHeaders(init: RequestInit = {}) {
  const headers = new Headers(init.headers || {});

  // merge auth headers
  const a = authHeaders();
  for (const [k, v] of Object.entries(a)) headers.set(k, v);

  if (!headers.has("Accept")) headers.set("Accept", "application/json");

  // If body looks like JSON string and Content-Type not set, set it
  if (!headers.has("Content-Type") && typeof init.body === "string") {
    try {
      JSON.parse(init.body);
      headers.set("Content-Type", "application/json");
    } catch {
      /* not JSON; ignore */
    }
  }
  return headers;
}

async function doFetch(path: string, init: RequestInit = {}, timeoutMs = 30000) {
  const controller = new AbortController();
  const t = setTimeout(() => controller.abort(), timeoutMs);
  try {
    const headers = await buildHeaders(init);
    const res = await fetch(buildUrl(path), { ...init, headers, signal: controller.signal });
    return res;
  } finally {
    clearTimeout(t);
  }
}

export async function requestRaw(path: string, init: RequestInit = {}, timeoutMs = 30000): Promise<Response> {
  // single attempt (no Firebase retry)
  return await doFetch(path, init, timeoutMs);
}

export async function request<T = JSONValue>(path: string, init: RequestInit = {}, timeoutMs = 30000): Promise<T> {
  const res = await requestRaw(path, init, timeoutMs);
  const text = await res.text().catch(() => "");

  if (!res.ok) {
    let msg = res.statusText || "Request failed";
    try {
      const parsed = text ? JSON.parse(text) : undefined;
      msg = (parsed?.detail || parsed?.message || msg) as string;
    } catch {
      /* leave msg */
    }
    throw new HttpError(res.status, `HTTP ${res.status} – ${msg}`, text);
  }

  if (!text) return undefined as unknown as T;
  try {
    return JSON.parse(text) as T;
  } catch {
    return text as unknown as T; // allow text endpoints
  }
}

export const getJSON = <T = JSONValue>(path: string, init: RequestInit = {}) =>
  request<T>(path, { ...init, method: "GET" });

export const delJSON = <T = JSONValue>(path: string, init: RequestInit = {}) =>
  request<T>(path, { ...init, method: "DELETE" });

export const postJSON = <T = JSONValue>(path: string, body: unknown, init: RequestInit = {}) =>
  request<T>(path, {
    ...init,
    method: "POST",
    headers: { ...(init.headers || {}), "Content-Type": "application/json" },
    body: JSON.stringify(body ?? {}),
  });

export const putJSON = <T = JSONValue>(path: string, body: unknown, init: RequestInit = {}) =>
  request<T>(path, {
    ...init,
    method: "PUT",
    headers: { ...(init.headers || {}), "Content-Type": "application/json" },
    body: JSON.stringify(body ?? {}),
  });

# ===== frontend/src/file_read/shared/lib/runjson.helpers.ts =====

// frontend/src/file_read/components/shared/lib/runjson.helpers.ts
import {
  MET_START,
  MET_END,
  type RunJson,
  type GenMetrics,
  type BudgetViewJson,
  type TurnBudgetJson,
  type NormalizedBudget,
  type RagTelemetry,
  type WebTelemetry,
  type PackTelemetry,
  type BudgetBreakdown,
} from "./runjson.types";

export function extractRunJsonFromBuffer(
  buf: string
): { clean: string; json?: RunJson; flat?: GenMetrics } {
  const re = /(?:\s*)\[\[RUNJSON\]\]\s*([\s\S]*?)\s*\[\[\/RUNJSON\]\](?:\s*)/g;
  let match: RegExpExecArray | null = null;
  let last: RegExpExecArray | null = null;
  while ((match = re.exec(buf)) !== null) last = match;
  if (!last) return { clean: buf };
  const payload = last[1] ?? "";
  let parsed: RunJson | undefined;
  try {
    parsed = JSON.parse(payload) as RunJson;
  } catch {}
  const start = last.index as number;
  const end = start + last[0].length;
  const clean = buf.slice(0, start) + buf.slice(end);
  let flat: GenMetrics | undefined;
  const s = parsed?.stats;
  if (s) {
    flat = {
      ttft_ms: s.timeToFirstTokenSec != null ? Math.max(0, s.timeToFirstTokenSec) * 1000 : null,
      tok_per_sec: s.tokensPerSecond ?? null,
      output_tokens: s.predictedTokensCount ?? null,
      input_tokens_est: s.promptTokensCount ?? null,
      total_tokens_est: s.totalTokensCount ?? null,
      stop_reason: s.stopReason ?? null,
    };
  }
  return { clean, json: parsed, flat };
}

export function stripRunJson(raw: string): {
  text: string;
  json?: RunJson;
  flat?: GenMetrics;
} {
  const { clean, json, flat } = extractRunJsonFromBuffer(raw);
  return { text: clean, json, flat };
}

export { MET_START as MET_START_TAG, MET_END as MET_END_TAG };

export function getNormalizedBudget(r?: RunJson | null): NormalizedBudget | null {
  if (!r) return null;
  const bv = r.budget_view as BudgetViewJson | null | undefined;
  if (bv && bv.modelCtx != null) {
    return {
      modelCtx: bv.modelCtx as number,
      clampMargin: (bv.clampMargin as number) ?? 0,
      inputTokensEst: (bv.inputTokensEst as number) ?? 0,
      outBudgetChosen: (bv.outBudgetChosen as number) ?? 0,
      outBudgetMaxAllowed: (bv.outBudgetMaxAllowed as number) ?? 0,
      overByTokens: (bv.overByTokens as number) ?? 0,
    };
  }
  const tb = r.stats?.budget as TurnBudgetJson | null | undefined;
  if (tb && tb.n_ctx != null) {
    const nctx = tb.n_ctx ?? 0;
    const margin = tb.clamp_margin ?? 0;
    const inp = tb.input_tokens_est ?? 0;
    const chosen = tb.clamped_out_tokens ?? 0;
    const avail = Math.max(0, nctx - inp - margin);
    return {
      modelCtx: nctx,
      clampMargin: margin,
      inputTokensEst: inp,
      outBudgetChosen: chosen,
      outBudgetMaxAllowed: Math.max(0, avail),
      overByTokens: Math.max(0, (tb.requested_out_tokens ?? chosen) - avail),
    };
  }
  return null;
}

export function getRagTelemetry(r?: RunJson | null): RagTelemetry | null {
  if (!r) return null;
  const bv = r.budget_view as BudgetViewJson | null | undefined;
  if (bv?.rag) return bv.rag || null;
  const tb = r.stats?.budget as TurnBudgetJson | null | undefined;
  if (tb?.rag) return tb.rag || null;
  return null;
}

export function getWebTelemetry(r?: RunJson | null): WebTelemetry | null {
  if (!r) return null;
  const bv = r.budget_view as BudgetViewJson | null | undefined;
  if (bv?.web) return bv.web || null;
  const tb = r.stats?.budget as TurnBudgetJson | null | undefined;
  if (tb?.web) return tb.web || null;
  return null;
}

export function getPackTelemetry(r?: RunJson | null): PackTelemetry | null {
  if (!r) return null;
  const bv = r.budget_view as BudgetViewJson | null | undefined;
  if (bv?.pack) return bv.pack || null;
  const tb = r.stats?.budget as TurnBudgetJson | null | undefined;
  if (tb?.pack) return tb.pack || null;
  return null;
}

export function getBudgetBreakdown(r?: RunJson | null): BudgetBreakdown | null {
  if (!r) return null;
  const bv = r.budget_view as BudgetViewJson | null | undefined;
  if (bv?.breakdown) return bv.breakdown || null;
  const tb = (r.stats?.budget as any) || null;
  if (tb?.breakdown) return (tb.breakdown as BudgetBreakdown) || null;
  return null;
}

export function getTimingMetrics(
  r?: RunJson | null
): {
  ttftSec: number | null;
  totalSec: number | null;
  genSec: number | null;
  queueWaitSec: number | null;
  preModelSec: number | null;
  modelQueueSec: number | null;
  engine?: {
    loadSec?: number | null;
    promptSec?: number | null;
    evalSec?: number | null;
    promptN?: number | null;
    evalN?: number | null;
  } | null;
} | null {
  if (!r?.stats) return null;
  const timings: any = (r.stats as any).timings || null;
  const ttftSec =
    typeof timings?.ttftSec === "number"
      ? timings.ttftSec
      : typeof r.stats.timeToFirstTokenSec === "number"
      ? r.stats.timeToFirstTokenSec
      : null;
  const totalSec =
    typeof timings?.totalSec === "number"
      ? timings.totalSec
      : typeof r.stats.totalTimeSec === "number"
      ? r.stats.totalTimeSec
      : null;
  const genSec = typeof timings?.genSec === "number" ? timings.genSec : null;

  const qvFromBV = (r.budget_view as any)?.queueWaitSec;
  const qvFromStats = (r.stats as any)?.budget?.queueWaitSec;
  const queueWaitSec =
    typeof qvFromBV === "number"
      ? qvFromBV
      : typeof qvFromStats === "number"
      ? qvFromStats
      : typeof timings?.queueWaitSec === "number"
      ? timings.queueWaitSec
      : null;

  const preModelSec =
    typeof timings?.preModelSec === "number" ? timings.preModelSec : null;
  const modelQueueSec =
    typeof timings?.modelQueueSec === "number" ? timings.modelQueueSec : null;
  const engine = timings?.engine ?? null;
  return { ttftSec, totalSec, genSec, queueWaitSec, preModelSec, modelQueueSec, engine };
}

export function getThroughput(r?: RunJson | null): {
  encodeTps: number | null;
  decodeTps: number | null;
  overallTps: number | null;
  promptN: number | null;
  evalN: number | null;
} | null {
  if (!r?.stats) return null;
  const promptN =
    typeof r.stats.promptTokensCount === "number"
      ? r.stats.promptTokensCount
      : typeof r.stats.timings?.engine?.promptN === "number"
      ? r.stats.timings!.engine!.promptN!
      : null;
  const evalN =
    typeof r.stats.predictedTokensCount === "number"
      ? r.stats.predictedTokensCount
      : typeof r.stats.timings?.engine?.evalN === "number"
      ? r.stats.timings!.engine!.evalN!
      : null;
  const modelQueueSec =
    typeof r.stats.timings?.modelQueueSec === "number"
      ? r.stats.timings!.modelQueueSec!
      : null;
  const genSec =
    typeof r.stats.timings?.genSec === "number"
      ? r.stats.timings!.genSec!
      : null;
  const totalTokens =
    typeof r.stats.totalTokensCount === "number"
      ? r.stats.totalTokensCount
      : null;
  const totalSec =
    typeof r.stats.totalTimeSec === "number"
      ? r.stats.totalTimeSec
      : typeof r.stats.timings?.totalSec === "number"
      ? r.stats.timings!.totalSec!
      : null;
  const safeDiv = (n: number | null, d: number | null) =>
    n != null && d != null && d > 0 ? n / d : null;
  const encodeTps = safeDiv(promptN, modelQueueSec);
  const decodeTps = safeDiv(evalN, genSec);
  const overallTps = safeDiv(totalTokens, totalSec);
  return { encodeTps, decodeTps, overallTps, promptN, evalN };
}

# ===== frontend/src/file_read/shared/lib/runjson.ts =====

// frontend/src/file_read/components/shared/lib/runjson.ts
export * from "./runjson.types";
export * from "./runjson.helpers";

# ===== frontend/src/file_read/shared/lib/runjson.types.ts =====

// frontend/src/file_read/components/shared/lib/runjson.types.ts
export const MET_START = "[[RUNJSON]]";
export const MET_END = "[[/RUNJSON]]";

export type GenMetrics = {
  ttft_ms?: number | null;
  tok_per_sec?: number | null;
  output_tokens?: number | null;
  input_tokens_est?: number | null;
  total_tokens_est?: number | null;
  stop_reason?: string | null;
};

export type RagTelemetry = {
  routerDecideSec?: number;
  routerNeeded?: boolean;
  routerQuery?: string;
  embedSec?: number;
  searchChatSec?: number;
  searchGlobalSec?: number;
  hitsChat?: number;
  hitsGlobal?: number;
  dedupeSec?: number;
  blockBuildSec?: number;
  injectBuildSec?: number;
  sessionOnlyBuildSec?: number;
  topKRequested?: number;
  blockChars?: number;
  injected?: boolean;
  mode?: string;
  blockTokens?: number;
  blockTokensApprox?: number;
  packedTokensBefore?: number;
  packedTokensAfter?: number;
  ragTokensAdded?: number;
  sessionOnlyTokensApprox?: number;
  sessionOnly?: boolean;
  routerSkipped?: boolean;
  routerSkippedReason?: string;
};


export type WebBreakdown = {
  routerSec?: number;
  summarizeSec?: number;
  searchSec?: number;
  fetchSec?: number;
  jsFetchSec?: number;
  assembleSec?: number;
  orchestratorSec?: number;
  injectSec?: number;
  totalWebPreTtftSec?: number;
  unattributedWebSec?: number;
  prepSec?: number;
};

export type WebTelemetry = {
  needed?: boolean;
  summarizedQuery?: string;
  fetchElapsedSec?: number;
  blockChars?: number;
  injectElapsedSec?: number;
  ephemeralBlocks?: number;
  summarizer?: Record<string, unknown> | null;
  orchestrator?: Record<string, unknown> | null;
  elapsedSec?: number;
  breakdown?: WebBreakdown | null;
};


export type PackTelemetry = {
  packSec?: number;
  summarySec?: number;
  finalTrimSec?: number;
  compressSec?: number;
  summaryTokensApprox?: number;
  summaryUsedLLM?: boolean;
  packedChars?: number;
  messages?: number;


  packInputTokensApprox?: number;
  packMsgs?: number;

  finalTrimTokensBefore?: number;
  finalTrimTokensAfter?: number;
  finalTrimDroppedMsgs?: number;
  finalTrimDroppedApproxTokens?: number;
  finalTrimSummaryShrunkFromChars?: number;
  finalTrimSummaryShrunkToChars?: number;
  finalTrimSummaryDroppedChars?: number;

  rollStartTokens?: number;
  rollOverageTokens?: number;
  rollPeeledMsgs?: number;
  rollNewSummaryChars?: number;
  rollNewSummaryTokensApprox?: number;
};

export type TurnBudgetJson = {
  n_ctx?: number;
  input_tokens_est?: number | null;
  requested_out_tokens?: number;
  clamped_out_tokens?: number;
  clamp_margin?: number;
  reserved_system_tokens?: number | null;
  available_for_out_tokens?: number | null;
  headroom_tokens?: number | null;
  overage_tokens?: number | null;
  reason?: string;
  rag?: RagTelemetry | null;
  web?: WebTelemetry | null;
  pack?: PackTelemetry | null;
  ephemeral?: boolean;
  droppedFromSummary?: boolean;
};

export type BudgetView = {
  modelCtx: number;
  clampMargin: number;
  usableCtx: number;
  inputTokensEst: number;
  outBudgetChosen: number;
  outBudgetDefault: number;
  outBudgetRequested: number;
  outBudgetMaxAllowed: number;
  overByTokens: number;
  minOutTokens: number;
  queueWaitSec: number | null;
};

export type BudgetBreakdown = {
  ttftSec?: number;
  preTtftAccountedSec?: number;
  unattributedTtftSec?: number;
};

export type BudgetViewJson = Partial<BudgetView> & {
  rag?: RagTelemetry | null;
  web?: WebTelemetry | null;
  pack?: PackTelemetry | null;
  breakdown?: BudgetBreakdown | null;
};

export type RunJson = {
  stats?: {
    stopReason?: string | null;
    tokensPerSecond?: number | null;
    timeToFirstTokenSec?: number | null;
    totalTimeSec?: number | null;
    promptTokensCount?: number | null;
    predictedTokensCount?: number | null;
    totalTokensCount?: number | null;
    budget?: TurnBudgetJson | null;
    timings?: {
      queueWaitSec?: number | null;
      genSec?: number | null;
      ttftSec?: number | null;
      totalSec?: number | null;
      preModelSec?: number | null;
      modelQueueSec?: number | null;
      engine?: {
        loadSec?: number | null;
        promptSec?: number | null;
        evalSec?: number | null;
        promptN?: number | null;
        evalN?: number | null;
      } | null;
    } | null;
  };
  budget_view?: BudgetViewJson | null;
  [k: string]: unknown;
};

export type NormalizedBudget = {
  modelCtx: number;
  clampMargin: number;
  inputTokensEst: number;
  outBudgetChosen: number;
  outBudgetMaxAllowed: number;
  overByTokens: number;
};

# ===== frontend/src/file_read/shared/lib/text.ts =====

export function firstLineSmart(s: string, max = 48): string {
  const one = s.replace(/\s+/g, " ").trim();
  return one.length <= max ? one : one.slice(0, max - 1).trimEnd() + "…";
}

# ===== frontend/src/file_read/shared/ui/CodeCopyButton.tsx =====

// frontend/src/file_read/components/CodeCopyButton.tsx
import { Copy, Check } from "lucide-react";
import { useState } from "react";

export default function CodeCopyButton({ text }: { text: string }) {
  const [copied, setCopied] = useState(false);
  async function onCopy() {
    try { await navigator.clipboard.writeText(text); setCopied(true); setTimeout(() => setCopied(false), 1500); } catch {}
  }
  return (
    <button
      type="button"
      onClick={onCopy}
      title={copied ? "Copied!" : "Copy"}
      className="inline-flex items-center justify-center w-7 h-7 rounded bg-gray-200 text-gray-600 hover:bg-gray-300 transition"
    >
      {copied ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
    </button>
  );
}

# ===== frontend/src/file_read/shared/ui/Toast.tsx =====

export default function Toast({ message }: { message: string | null }) {
  if (!message) return null;
  return (
    <div className="pointer-events-none fixed bottom-4 left-1/2 -translate-x-1/2 z-50">
      <div className="px-3 py-2 rounded-lg bg-black text-white text-xs shadow-lg">
        {message}
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/shared/ui/TypingIndicator.tsx =====

// components/TypingIndicator.tsx
export default function TypingIndicator() {
  return (
    <div className="flex items-start gap-2">
      {/* Optional avatar spot */}
      <div className="h-8 w-8 rounded-full bg-gray-200 shrink-0" />
      <div className="px-3 py-2 rounded-lg bg-gray-100 text-gray-600">
        <span className="inline-flex gap-1">
          <span className="h-2 w-2 rounded-full bg-gray-400 animate-bounce [animation-delay:-0.2s]" />
          <span className="h-2 w-2 rounded-full bg-gray-400 animate-bounce [animation-delay:-0.1s]" />
          <span className="h-2 w-2 rounded-full bg-gray-400 animate-bounce" />
        </span>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/types/chat.ts =====

//aimodel/file_read/types/chat.ts
export type Role = "user" | "assistant";

import type { GenMetrics, RunJson } from "../shared/lib/runjson";

export type Attachment = {
  name: string;
  source?: string;
  sessionId?: string | null;
};

export type ChatMsg = {
  id: string;                // clientId
  serverId: number | null;
  role: Role;
  text: string;

  // NEW
  attachments?: Attachment[];

  meta?: {
    runJson?: RunJson | null;
    flat?: GenMetrics | null;
  };
};

export type ChatRow = {
  id: number;
  sessionId: string;
  title: string;
  lastMessage: string | null;
  createdAt: string;
  updatedAt: string;
};

export type ChatMessageRow = {
  id: number;
  sessionId: string;
  role: Role;
  content: string;
  createdAt: string;
  attachments?: Attachment[]; // ✅ this must exist
};

# ===== frontend/src/file_read/vite-env.d.ts =====

/// <reference types="vite/client" />
