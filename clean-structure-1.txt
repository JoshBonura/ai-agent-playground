
# ===== aimodel/file_read/__init__.py =====

from .adaptive.config.paths import app_data_dir, read_settings, write_settings
from .runtime.model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/adaptive/config/adaptive_config.py =====

# aimodel/file_read/runtime/adaptive_config.py
from __future__ import annotations
import os, shutil, subprocess, platform
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

try:
    import psutil
except Exception:
    psutil = None
try:
    import torch
except Exception:
    torch = None

from .paths import read_settings

def _env_bool(k:str, default:bool)->bool:
    v = os.getenv(k)
    if v is None: return default
    return v.strip().lower() in ("1","true","yes","on")

def _cpu_count()->int:
    try:
        import multiprocessing as mp
        return max(1, mp.cpu_count() or os.cpu_count() or 1)
    except Exception:
        return os.cpu_count() or 1

def _avail_ram()->Optional[int]:
    if not psutil: return None
    try: return int(psutil.virtual_memory().available)
    except Exception: return None

def _cuda_vram()->Optional[int]:
    if torch and torch.cuda.is_available():
        try:
            dev = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(dev)
            return int(props.total_memory)
        except Exception:
            pass
    if shutil.which("nvidia-smi"):
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-gpu=memory.total","--format=csv,noheader,nounits"],
                text=True, stderr=subprocess.DEVNULL, timeout=2.0
            )
            mb = max(int(x.strip()) for x in out.strip().splitlines() if x.strip())
            return mb * 1024 * 1024
        except Exception:
            return None
    return None

def _gpu_kind()->str:
    if _cuda_vram(): return "cuda"
    if torch and getattr(torch.backends,"mps",None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

def _safe_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default

def _pick_dtype_quant(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> tuple[Optional[str], Optional[str]]:
    dq = a.get("dtype_quant", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = dq.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype"), best.get("quant")
        return dq.get("cuda_default_dtype"), dq.get("cuda_default_quant")
    if device == "mps":
        return dq.get("mps_default_dtype"), None
    return dq.get("cpu_default_dtype"), dq.get("cpu_default_quant")

def _pick_kv(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> Optional[str]:
    kv = a.get("kv_cache", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = kv.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype")
        return kv.get("cuda_default")
    if device == "mps":
        return kv.get("mps_default")
    return kv.get("cpu_default")

def _pick_capacity(device: str, a: Dict[str, Any], vram_bytes: Optional[int], threads:int) -> tuple[int,int,Optional[int]]:
    cap = a.get("capacity", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = cap.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return int(best.get("seq_len") or 0), int(best.get("batch") or 1), int(best.get("n_gpu_layers") or 0)
        return 0, 1, 0
    if device == "mps":
        m = cap.get("mps", {})
        return int(m.get("seq_len") or 0), int(m.get("batch") or 1), 0
    cpu = cap.get("cpu", {})
    seq_len = int(cpu.get("seq_len") or 0)
    batch = 1
    by = cpu.get("batch_by_threads") or []
    best = None
    for t in sorted(by, key=lambda x: int(x.get("min_threads", 0)), reverse=True):
        if threads >= int(t.get("min_threads") or 0):
            best = t
            break
    if best:
        batch = int(best.get("batch") or 1)
    return seq_len, batch, 0

def _gpu_mem_fraction(device:str, a: Dict[str, Any]) -> float:
    table = a.get("gpu_fraction", {}) if isinstance(a, dict) else {}
    v = table.get(device)
    return _safe_float(v, 0.0)

def _torch_flags(device:str, a: Dict[str, Any]) -> tuple[bool,bool]:
    flags = a.get("flags", {}) if isinstance(a, dict) else {}
    flash = bool(flags.get("enable_flash_attn_cuda")) if device == "cuda" else False
    tc = bool(flags.get("use_torch_compile_on_cuda_linux")) if (device == "cuda" and platform.system().lower()=="linux") else False
    return flash, tc

def _threads(a: Dict[str, Any]) -> tuple[int,int,int,int]:
    policy = a.get("cpu_threads_policy", {}) if isinstance(a, dict) else {}
    mode = str(policy.get("mode") or "").lower()
    ncpu = _cpu_count()
    if mode == "fixed":
        v = int(policy.get("value") or max(1, ncpu-1))
        t = max(1, min(v, ncpu))
    elif mode == "percent":
        pct = _safe_float(policy.get("value"), 0.0)
        t = max(1, min(ncpu, int(round(ncpu*pct/100.0))))
        if t < 1: t = 1
    else:
        t = max(1, ncpu-1)
    intra = t
    inter = max(1, ncpu//2)
    return ncpu, t, intra, inter

@dataclass
class AdaptiveConfig:
    device: str
    dtype: Optional[str]
    quant: Optional[str]
    kv_cache_dtype: Optional[str]
    max_seq_len: int
    max_batch_size: int
    gpu_memory_fraction: float
    cpu_threads: int
    torch_intraop_threads: int
    torch_interop_threads: int
    enable_flash_attn: bool
    use_torch_compile: bool
    total_vram_bytes: Optional[int]
    avail_ram_bytes: Optional[int]
    cpu_count: int
    def as_dict(self)->Dict[str,Any]:
        return asdict(self)

def compute_adaptive_config()->AdaptiveConfig:
    settings = read_settings()
    a = settings.get("adaptive", {}) if isinstance(settings, dict) else {}
    device = _gpu_kind()
    vram = _cuda_vram() if device=="cuda" else None
    ram = _avail_ram()
    ncpu, threads, intra, inter = _threads(a)
    dtype, quant = _pick_dtype_quant(device, a, vram)
    kv = _pick_kv(device, a, vram)
    seq_len, batch, n_gpu_layers = _pick_capacity(device, a, vram, threads)
    frac = _gpu_mem_fraction(device, a)
    flash, tcompile = _torch_flags(device, a)
    return AdaptiveConfig(
        device=device,
        dtype=dtype,
        quant=quant,
        kv_cache_dtype=kv,
        max_seq_len=int(seq_len or 0),
        max_batch_size=int(batch or 1),
        gpu_memory_fraction=frac,
        cpu_threads=threads,
        torch_intraop_threads=intra,
        torch_interop_threads=inter,
        enable_flash_attn=flash,
        use_torch_compile=tcompile,
        total_vram_bytes=vram,
        avail_ram_bytes=ram,
        cpu_count=ncpu
    )

# ===== aimodel/file_read/adaptive/config/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/adaptive/config/settings.json =====

{
  "adaptiveEnabled": false,
  "adaptive": {
    "enabled": true,
    "cpu_threads_policy": {
      "mode": "leave_one"
    },
    "dtype_quant": {
      "cuda_default_dtype": "float16",
      "cuda_default_quant": null,
      "mps_default_dtype": "float16",
      "cpu_default_dtype": "int8",
      "cpu_default_quant": "q4_K_M",
      "cuda_tiers": [
        { "min_vram_gb": 24, "dtype": "bfloat16", "quant": null },
        { "min_vram_gb": 12, "dtype": "float16", "quant": null },
        { "min_vram_gb": 6,  "dtype": "float16", "quant": "bnb-int8" },
        { "min_vram_gb": 4,  "dtype": "float16", "quant": "bnb-int8" }
      ]
    },
    "kv_cache": {
      "cuda_default": "fp8",
      "mps_default": "fp16",
      "cpu_default": "fp32",
      "cuda_tiers": [
        { "min_vram_gb": 16, "dtype": "fp16" },
        { "min_vram_gb": 0,  "dtype": "fp8" }
      ]
    },
    "capacity": {
      "cuda_tiers": [
        { "min_vram_gb": 24, "seq_len": 8192, "batch": 8, "n_gpu_layers": 9999 },
        { "min_vram_gb": 12, "seq_len": 4096, "batch": 4, "n_gpu_layers": 48 },
        { "min_vram_gb": 8,  "seq_len": 3072, "batch": 2, "n_gpu_layers": 40 },
        { "min_vram_gb": 6,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 32 },
        { "min_vram_gb": 4,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 28 }
      ],
      "mps": { "seq_len": 2048, "batch": 1 },
      "cpu": {
        "seq_len": 2048,
        "batch_by_threads": [
          { "min_threads": 16, "batch": 8 },
          { "min_threads": 8,  "batch": 4 },
          { "min_threads": 1,  "batch": 2 }
        ]
      }
    },
    "gpu_fraction": {
      "cuda": 0.8,
      "mps": 0.7,
      "cpu": 0.0
    },
    "flags": {
      "enable_flash_attn_cuda": true,
      "use_torch_compile_on_cuda_linux": true
    },
    "batchTokenMap": [
      { "minConcurrency": 8, "n_batch": 512 },
      { "minConcurrency": 4, "n_batch": 384 },
      { "minConcurrency": 2, "n_batch": 256 },
      { "minConcurrency": 1, "n_batch": 192 }
    ]
  },
  "modelPath": "",
  "nCtx": null,
  "nThreads": null,
  "nGpuLayers": null,
  "nBatch": null
}

# ===== aimodel/file_read/adaptive/controller.py =====



# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/auth_router.py =====

# backend/api/auth_router.py
from __future__ import annotations
import os
from typing import Dict
from fastapi import APIRouter, HTTPException, Response, Depends

from ..services.auth_service import (
    firebase_sign_in_with_password,
    verify_jwt_with_google,firebase_sign_up_with_password,
)
from ..services.licensing_service import (
    recover_by_email,
    license_status_local,
)
from ..deps.auth_deps import require_auth

router = APIRouter(prefix="/api")

# --- Config ---
AUTH_REQUIRE_VERIFIED = os.getenv("AUTH_REQUIRE_VERIFIED", "false").lower() == "true"
ID_COOKIE_NAME       = os.getenv("AUTH_IDTOKEN_COOKIE", "fb_id")
LEGACY_COOKIE_NAME   = os.getenv("AUTH_SESSION_COOKIE", "fb_session")
SESSION_DAYS         = int(os.getenv("AUTH_SESSION_DAYS", "7"))
COOKIE_SECURE        = os.getenv("AUTH_COOKIE_SECURE", "false").lower() == "true"
COOKIE_SAMESITE      = (os.getenv("AUTH_COOKIE_SAMESITE", "lax") or "lax").lower()  # 'lax'|'strict'|'none'
COOKIE_DOMAIN        = os.getenv("AUTH_COOKIE_DOMAIN", "").strip() or None
COOKIE_PATH          = "/"

# Enforce browser rule: SameSite=None requires Secure=true
if COOKIE_SAMESITE == "none" and not COOKIE_SECURE:
    # You can switch this to a warning + force true if you prefer:
    # print("[auth] WARNING: Forcing Secure=True because SameSite=None")
    # COOKIE_SECURE = True
    raise RuntimeError("AUTH_COOKIE_SAMESITE=none requires AUTH_COOKIE_SECURE=true")

def _set_cookie(resp: Response, name: str, value: str, max_age_s: int):
    resp.set_cookie(
        key=name,
        value=value,
        max_age=max_age_s,
        httponly=True,
        secure=COOKIE_SECURE,
        samesite=COOKIE_SAMESITE,
        domain=COOKIE_DOMAIN,
        path=COOKIE_PATH,
    )

def _clear_cookie(resp: Response, name: str):
    resp.delete_cookie(key=name, domain=COOKIE_DOMAIN, path=COOKIE_PATH)

# --- Routes ---

@router.post("/auth/login")
async def login(body: Dict[str, str], response: Response):
    email = (body.get("email") or "").strip().lower()
    password = body.get("password") or ""
    if not email or not password:
        raise HTTPException(400, "Email and password required")

    # 1) Firebase REST sign-in
    data = await firebase_sign_in_with_password(email, password)
    id_token = data.get("idToken")
    if not id_token:
        raise HTTPException(401, "Login failed")

    # 2) Verify token with Google certs
    claims = verify_jwt_with_google(id_token)

    # 3) Optional email verification gate
    if AUTH_REQUIRE_VERIFIED and not bool(claims.get("email_verified")):
        raise HTTPException(401, "Email not verified")

    # 4) Set session cookies (both current + legacy for compatibility)
    max_age = SESSION_DAYS * 86400
    _set_cookie(response, ID_COOKIE_NAME, id_token, max_age)
    _set_cookie(response, LEGACY_COOKIE_NAME, id_token, max_age)

    # 5) Try to recover the license by email so backend knows tier immediately
    lic_snapshot = {"plan": "free", "valid": False, "exp": None}
    try:
        await recover_by_email(email)          # fetches LM1 by customer email & saves locally
        lic_snapshot = license_status_local()  # reads & verifies local license file
    except Exception as e:
        print(f"[auth] license recover after login failed: {e!r}")

    return {
        "ok": True,
        "email": email,
        "uid": claims.get("user_id") or claims.get("sub"),
        "emailVerified": bool(claims.get("email_verified")),
        "expiresInDays": SESSION_DAYS,
        "license": lic_snapshot,  # {"plan": "...", "valid": bool, "exp": int|None}
    }

@router.post("/auth/logout")
def logout(response: Response):
    _clear_cookie(response, ID_COOKIE_NAME)
    _clear_cookie(response, LEGACY_COOKIE_NAME)
    return {"ok": True}

@router.get("/auth/me")
def me(user=Depends(require_auth)):
    # Include license snapshot so the client can show the tier without another call
    try:
        lic = license_status_local()
    except Exception:
        lic = {"plan": "free", "valid": False, "exp": None}
    return {
        "email": (user.get("email") or "").lower(),
        "uid": user.get("user_id") or user.get("sub"),
        "emailVerified": bool(user.get("email_verified")),
        "name": user.get("name"),
        "picture": user.get("picture"),
        "iat": user.get("iat"),
        "exp": user.get("exp"),
        "license": lic,
    }

@router.post("/auth/register")
async def register(body: Dict[str, str]):
    email = (body.get("email") or "").strip().lower()
    password = body.get("password") or ""
    if not email or not password:
        raise HTTPException(400, "Email and password required")
    # Create account in Firebase; do NOT set cookies here (frontend will call /auth/login next)
    await firebase_sign_up_with_password(email, password)
    return {"ok": True}

# ===== aimodel/file_read/api/billing.py =====

from __future__ import annotations
import os
from fastapi import APIRouter, HTTPException, Depends
import httpx

from ..deps.auth_deps import require_auth as decode_bearer
from ..services.licensing_service import license_status_local  

router = APIRouter(prefix="/api", tags=["billing"])

LIC_SERVER = (os.getenv("LIC_SERVER_BASE") or "").rstrip("/")
if not LIC_SERVER:
    raise RuntimeError(
        "LIC_SERVER_BASE env var is required (e.g. https://lic-server.localmind.workers.dev)"
    )

@router.get("/billing/status")
async def billing_status(auth=Depends(decode_bearer)):
    email = (auth.get("email") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")

    try:
        lic = license_status_local(expected_email=email)  # <-- pass expected_email
        active = bool(lic.get("valid"))
        return {
            "status": "active" if active else "inactive",
            "current_period_end": int(lic.get("exp") or 0),
        }
    except Exception:
        return {"status": "inactive", "current_period_end": 0}

@router.post("/billing/checkout")
async def start_checkout(auth=Depends(decode_bearer)):
    email = (auth.get("email") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")
    url = f"{LIC_SERVER}/api/checkout/session"
    async with httpx.AsyncClient(timeout=20.0) as client:
        r = await client.post(url, json={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    if not isinstance(data, dict) or "url" not in data:
        raise HTTPException(502, "Bad response from licensing server")
    return {"url": data["url"]}

@router.post("/billing/portal")
async def open_portal(auth=Depends(decode_bearer)):
    email = (auth.get("email") or "").strip().lower()
    if not email:
        raise HTTPException(401, "No email in token")
    url = f"{LIC_SERVER}/api/portal/session"
    async with httpx.AsyncClient(timeout=20.0) as client:
        r = await client.post(url, json={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    data = r.json()
    if not isinstance(data, dict) or "url" not in data:
        raise HTTPException(502, "Bad response from licensing server")
    return {"url": data["url"]}

@router.get("/license/by-session")
async def license_by_session(session_id: str):
    if not session_id:
        raise HTTPException(400, "Missing session_id")
    url = f"{LIC_SERVER}/api/license/by-session"
    async with httpx.AsyncClient(timeout=15.0) as client:
        r = await client.get(url, params={"session_id": session_id}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    return r.json()

# ===== aimodel/file_read/api/chats.py =====

# ===== aimodel/file_read/api/chats.py =====
from __future__ import annotations

from ..core.schemas import ChatMessage
from dataclasses import asdict
from typing import List, Optional, Dict
from ..utils.streaming import strip_runjson
from fastapi import APIRouter
from pydantic import BaseModel
from ..services.cancel import GEN_SEMAPHORE
from ..workers.retitle_worker import enqueue as enqueue_retitle  # ✅ import the enqueuer

from ..core.schemas import (
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    EditMessageReq,
)

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    edit_message as edit_message,
)

router = APIRouter()

@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)

@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]


@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, msg: ChatMessage):
    role = msg.role
    content = (msg.content or "").rstrip()
    attachments = msg.attachments or []

    row = store_append(session_id, role, content, attachments=attachments)

    if role == "assistant":
        try:
            msgs = store_list_messages(session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            msgs_clean = []
            for m in msgs:
                dm = asdict(m)
                dm["content"] = strip_runjson(dm.get("content") or "")
                msgs_clean.append(dm)
            enqueue_retitle(session_id, msgs_clean, job_seq=last_seq)
        except Exception:
            pass

    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}


@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

from fastapi import APIRouter, Body, Request
from ..core.schemas import ChatBody
from ..services.generate_flow import generate_stream_flow, cancel_session_alias

router = APIRouter()

@router.post("/api/ai/generate/stream")
async def generate_stream_alias(request: Request, data: ChatBody = Body(...)):
    return await generate_stream_flow(data, request)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/licensing_router.py =====

from __future__ import annotations
from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel
from ..deps.auth_deps import require_auth
from ..services.licensing_service import (
    apply_license_string,
    license_status_local,
    recover_by_email,
    refresh_license,
    fetch_license_by_session,
    install_from_session as svc_install_from_session,
    remove_license_file,
    email_from_auth,
    read_license_claims
)

router = APIRouter(prefix="/api/license", tags=["license"])

class ApplyReq(BaseModel):
    license: str

@router.post("/apply")
def apply_license(body: ApplyReq):
    print("[license] POST /apply")
    return apply_license_string(body.license)

@router.get("/apply")
def apply_license_get(license: str = Query(..., min_length=10)):
    print("[license] GET /apply")
    return apply_license_string(license)

@router.get("/status")
def status(user=Depends(require_auth)):  # <-- remove decorator dependencies=[]
    email = (user.get("email") or "").strip().lower()
    st = license_status_local(expected_email=email)   # <-- pass expected_email
    return st if st else {"plan": "free", "valid": False}

@router.get("/claims", dependencies=[Depends(require_auth)])
def claims():
    print("[license] GET /claims")
    st = license_status_local()
    if not st.get("valid"):
        raise HTTPException(404, "No license installed")
    return st

@router.delete("/", dependencies=[Depends(require_auth)])
def remove_license():
    print("[license] DELETE /api/license")
    return remove_license_file()

@router.get("/by-session", dependencies=[Depends(require_auth)])
async def license_by_session(session_id: str = Query(..., min_length=6)):
    print(f"[license] GET /by-session session_id={session_id}")
    return await fetch_license_by_session(session_id)

@router.post("/install-from-session", dependencies=[Depends(require_auth)])
async def install_from_session(session_id: str = Query(..., min_length=6)):
    print(f"[license] POST /install-from-session session_id={session_id}")
    return await svc_install_from_session(session_id)

@router.post("/recover")
async def recover(auth=Depends(require_auth)):
    email = email_from_auth(auth)
    print(f"[license] POST /recover email={email or 'MISSING'}")
    if not email:
        raise HTTPException(400, "Email required")
    return await recover_by_email(email)

@router.post("/refresh")
async def refresh(auth=Depends(require_auth), force: bool = Query(False)):
    print(f"[license] POST /refresh force={force}")
    email = email_from_auth(auth)
    return await refresh_license(email, force)

@router.get("/claims", dependencies=[Depends(require_auth)])
def claims():
    # Now returns the raw license claims (license_id, sub, entitlements, issued_at, exp, plan, etc.)
    return read_license_claims()

# ===== aimodel/file_read/api/metrics.py =====

# aimodel/file_read/api/metrics.py
from __future__ import annotations
from fastapi import APIRouter, HTTPException, Query
from typing import Optional
from ..runtime.model_runtime import get_llm
from ..services.packing import build_system_text, pack_with_rollup
from ..services.budget import analyze_budget
from ..core.settings import SETTINGS
from ..store import list_messages, get_summary, set_summary

router = APIRouter(prefix="/metrics", tags=["metrics"])

@router.get("/budget")
def get_budget(sessionId: Optional[str] = Query(default=None), maxTokens: Optional[int] = None):
    eff0 = SETTINGS.effective()
    sid = sessionId or eff0["default_session_id"]
    llm = get_llm()
    eff = SETTINGS.effective(session_id=sid)

    msgs = [{"role": m.role, "content": m.content} for m in list_messages(sid)]
    summary = get_summary(sid)
    system_text = build_system_text()
    packed, new_summary, _ = pack_with_rollup(
        system_text=system_text,
        summary=summary,
        recent=msgs,
        max_ctx=int(eff["model_ctx"]),
        out_budget=int(eff["default_max_tokens"]),
    )
    if new_summary != summary:
        set_summary(sid, new_summary)

    requested_out = int(maxTokens or eff["default_max_tokens"])
    budget = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=requested_out,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()
    return {"sessionId": sid, "budget": budget}

# ===== aimodel/file_read/api/models.py =====

from __future__ import annotations
from typing import Optional, Dict
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..adaptive.config.paths import read_settings, write_settings
from ..runtime.model_runtime import (
    list_local_models, current_model_info,
    load_model, unload_model
)

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/api/rag.py =====

# aimodel/file_read/api/rag.py
from __future__ import annotations
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from typing import Optional, List, Dict
import numpy as np
from sentence_transformers import SentenceTransformer
from threading import RLock
from ..rag.uploads import list_sources as rag_list_sources, hard_delete_source
from ..rag.schemas import SearchReq, SearchHit
from ..rag.ingest import sniff_and_extract, chunk_text, build_metas
from ..rag.store import add_vectors, search_vectors

router = APIRouter(prefix="/api/rag", tags=["rag"])

_st_model: SentenceTransformer | None = None
_st_lock = RLock()

def _get_st_model() -> SentenceTransformer:
    global _st_model
    if _st_model is None:
        with _st_lock:
            if _st_model is None:
                print("[RAG EMBED] loading e5-small-v2… (one-time)")
                _st_model = SentenceTransformer("intfloat/e5-small-v2")
                print("[RAG EMBED] model ready")
    return _st_model

def _embed(texts: List[str]) -> np.ndarray:
    model = _get_st_model()
    arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
    return arr.astype("float32")

@router.post("/upload")
async def upload_doc(sessionId: Optional[str] = Form(default=None), file: UploadFile = File(...)):
    print(f"[RAG UPLOAD] sessionId={sessionId}, filename={file.filename}, content_type={file.content_type}")

    data = await file.read()
    print(f"[RAG UPLOAD] file size={len(data)} bytes")

    text, mime = sniff_and_extract(file.filename, data)
    print(f"[RAG UPLOAD] extracted mime={mime}, text_len={len(text)}")

    if not text.strip():
        raise HTTPException(status_code=400, detail="Empty/unsupported file")

    chunks = chunk_text(text, {"mime": mime})
    print(f"[RAG UPLOAD] chunk_count={len(chunks)}")

    metas = build_metas(sessionId, file.filename, chunks, size=len(data))
    embeds = _embed([c.text for c in chunks])
    print(f"[RAG UPLOAD] embed_shape={embeds.shape}")

    add_vectors(sessionId, embeds, metas, dim=embeds.shape[1])
    return {"ok": True, "added": len(chunks)}

@router.post("/search")
async def search(req: SearchReq):
    q = (req.query or "").strip()
    if not q:
        return {"hits": []}

    qv = _embed([q])[0]
    chat_hits = search_vectors(req.sessionId, qv, req.kChat, dim=qv.shape[0]) if req.kChat else []
    global_hits = search_vectors(None, qv, req.kGlobal, dim=qv.shape[0]) if req.kGlobal else []

    fused = sorted(chat_hits + global_hits, key=lambda r: r["score"], reverse=True)

    out: List[SearchHit] = []
    for r in fused:
        out.append(SearchHit(
            id=r["id"],
            text=r["text"],
            score=float(r["score"]),
            source=r.get("source"),
            title=r.get("title"),
            sessionId=r.get("sessionId"),
        ))
    return {"hits": [h.model_dump() for h in out]}

@router.get("/list")
async def list_items(sessionId: Optional[str] = None, k: int = 20):
    qv = _embed(["list"])[0]
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    items = []
    for h in hits:
        txt = (h.get("text") or "")
        items.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": txt,
        })
    print(f"[RAG LIST] sessionId={sessionId} k={k} -> {len(items)} items")
    return {"items": items}

@router.get("/dump")
async def dump_items(sessionId: Optional[str] = None, k: int = 50):
    qv = _embed(["dump"])[0]
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    chunks = []
    for h in hits:
        chunks.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": h.get("text") or "",
        })
    print(f"[RAG DUMP] sessionId={sessionId} k={k} -> {len(chunks)} items")
    return {"chunks": chunks}

@router.get("/uploads")
async def api_list_uploads(sessionId: Optional[str] = None, scope: str = "all"):
    include_global = scope != "session"
    return {"uploads": rag_list_sources(sessionId, include_global=include_global)}

@router.post("/uploads/delete-hard")
async def api_delete_upload_hard(body: dict[str, str]):
    source = (body.get("source") or "").strip()
    session_id = (body.get("sessionId") or None)
    if not source:
        return {"error": "source required"}

    out = hard_delete_source(source, session_id=session_id, embedder=_embed)
    return out

# ===== aimodel/file_read/api/settings.py =====

from fastapi import APIRouter, Query, Body
from typing import Dict, Any, Optional
from ..core.settings import SETTINGS

router = APIRouter(prefix="/api/settings", tags=["settings"])

@router.get("/defaults")
def get_defaults():
    return SETTINGS.defaults

@router.get("/overrides")
def get_overrides():
    return SETTINGS.overrides

@router.patch("/overrides")
def patch_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.patch_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.put("/overrides")
def put_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.replace_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.get("/adaptive")
def get_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.adaptive(session_id=session_id)

@router.post("/adaptive/recompute")
def recompute_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    SETTINGS.recompute_adaptive(session_id=session_id)
    return {"ok": True, "adaptive": SETTINGS.adaptive(session_id=session_id)}

@router.get("/effective")
def get_effective(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.effective(session_id=session_id)

# ===== aimodel/file_read/app.py =====

from __future__ import annotations
import os, asyncio
from pathlib import Path

try:
    from dotenv import load_dotenv
    _ENV_PATH = Path(__file__).resolve().parent / ".env"
    load_dotenv(dotenv_path=_ENV_PATH, override=False)
except Exception as _e:
    print(f"[env] NOTE: could not load .env: {_e}")

from fastapi import FastAPI, Depends, Request
from fastapi.middleware.cors import CORSMiddleware

from .core import request_ctx
from .adaptive.config.paths import bootstrap
from .workers.retitle_worker import start_worker
from .runtime.model_runtime import load_model

from .api.models import router as models_router
from .api.chats import router as chats_router
from .api.generate_router import router as generate_router
from .api.metrics import router as metrics_router
from .api.rag import router as rag_router
from .api import settings as settings_router
from .api.billing import router as billing_router
from .api.licensing_router import router as licensing_router
from .api.auth_router import router as auth_router, require_auth

bootstrap()
app = FastAPI()

# --- CORS setup ---
origins = [
    o.strip()
    for o in os.getenv("APP_CORS_ORIGIN", "http://localhost:5173").split(",")
    if o.strip()
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Routers ---
deps = [Depends(require_auth)]

# Public
app.include_router(models_router)

# Protected
app.include_router(chats_router,           dependencies=deps)
app.include_router(generate_router,        dependencies=deps)
app.include_router(settings_router.router, dependencies=deps)
app.include_router(rag_router,             dependencies=deps)
app.include_router(metrics_router,         dependencies=deps)
app.include_router(billing_router,         dependencies=deps)
app.include_router(licensing_router,       dependencies=deps)

# Auth endpoints (login/logout/me)
app.include_router(auth_router)

# --- Startup tasks ---
@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")
    asyncio.create_task(start_worker(), name="retitle_worker")

# --- Per-request header capture ---
@app.middleware("http")
async def _capture_auth_headers(request: Request, call_next):
    auth = (request.headers.get("authorization") or "").strip()
    if auth.lower().startswith("bearer "):
        request_ctx.set_id_token(auth.split(None, 1)[1])
    else:
        request_ctx.set_id_token("")
    request_ctx.set_x_id((request.headers.get("x-id") or "").strip())
    return await call_next(request)

# ===== aimodel/file_read/backend/.runtime/ports.json =====

{"api_port": 8001}

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/files.py =====

# aimodel/file_read/core/files.py
from __future__ import annotations
from pathlib import Path
import json, os
from typing import Any


CORE_DIR = Path(__file__).resolve().parent
STORE_DIR = CORE_DIR.parent / "store"

EFFECTIVE_SETTINGS_FILE = Path(
    os.getenv("EFFECTIVE_SETTINGS_PATH", str(STORE_DIR / "effective_settings.json"))
)
OVERRIDES_SETTINGS_FILE = Path(
    os.getenv("OVERRIDES_SETTINGS_PATH", str(STORE_DIR / "override_settings.json"))
)
DEFAULTS_SETTINGS_FILE = Path(
    os.getenv("DEFAULT_SETTINGS_PATH", str(STORE_DIR / "default_settings.json"))
)

def load_json_file(path: Path, default: Any = None) -> Any:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {} if default is None else default

def save_json_file(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ===== aimodel/file_read/core/packing_memory_core.py =====

from __future__ import annotations
import math, time
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque

from ..runtime.model_runtime import get_llm
from .style import get_style_sys
from ..store import get_summary as store_get_summary
from ..store import list_messages as store_list_messages
from ..utils.streaming import strip_runjson
from ..core.files import EFFECTIVE_SETTINGS_FILE, load_json_file

SESSIONS: Dict[str, Dict] = {}
PACK_TELEMETRY: Dict[str, object] = {
    "packSec": 0.0,
    "summarySec": 0.0,
    "finalTrimSec": 0.0,
    "compressSec": 0.0,
    "summaryTokensApprox": 0,
    "summaryUsedLLM": False,
    "summaryBullets": 0,
    "summaryAddedChars": 0,
    "summaryOutTokensApprox": 0,
    "summaryCompressedFromChars": 0,
    "summaryCompressedToChars": 0,
    "summaryCompressedDroppedChars": 0,
}
SUMMARY_TEL = PACK_TELEMETRY

class _SettingsCache:
    def __init__(self) -> None:
        self.path: Path = EFFECTIVE_SETTINGS_FILE
        self._mtime: float | None = None
        self._data: Dict = {}

    def get(self) -> Dict:
        try:
            m = self.path.stat().st_mtime
        except FileNotFoundError:
            m = None
        if self._mtime != m or not self._data:
            self._data = load_json_file(self.path, default={})
            self._mtime = m
        return self._data

_SETTINGS = _SettingsCache()

def approx_tokens(text: str) -> int:
    cfg = _SETTINGS.get()
    return max(1, math.ceil(len(text) / int(cfg["chars_per_token"])))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    cfg = _SETTINGS.get()
    overhead = int(cfg["prompt_per_message_overhead"])
    return sum(approx_tokens(m["content"]) + overhead for m in msgs)

def get_session(session_id: str):
    cfg = _SETTINGS.get()
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=int(cfg["recent_maxlen"])),
        "style": get_style_sys(),
        "short": False,
        "bullets": False,
    })
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
        except Exception:
            pass
    if not st["recent"]:
        try:
            rows = store_list_messages(session_id)
            tail = rows[-st["recent"].maxlen:]
            for m in tail:
                st["recent"].append({"role": m.role, "content": strip_runjson(m.content)})
        except Exception:
            pass
    return st

def _heuristic_bullets(chunks: List[Dict[str,str]], cfg: Dict) -> str:
    max_bullets = int(cfg["heuristic_max_bullets"])
    max_words = int(cfg["heuristic_max_words"])
    prefix = cfg["bullet_prefix"]
    bullets = []
    for m in chunks:
        txt = " ".join((m.get("content") or "").split())
        if not txt:
            continue
        words = txt.replace("\n", " ").split()
        snippet = " ".join(words[:max_words]) if words else ""
        bullets.append(f"{prefix}{snippet}" if snippet else prefix.strip())
        if len(bullets) >= max_bullets:
            break
    return "\n".join(bullets) if bullets else prefix.strip()

def summarize_chunks(chunks: List[Dict[str,str]]) -> Tuple[str, bool]:
    cfg = _SETTINGS.get()
    t0 = time.time()
    PACK_TELEMETRY["summarySec"] = 0.0
    PACK_TELEMETRY["summaryTokensApprox"] = 0
    PACK_TELEMETRY["summaryUsedLLM"] = False
    PACK_TELEMETRY["summaryBullets"] = 0
    PACK_TELEMETRY["summaryAddedChars"] = 0
    PACK_TELEMETRY["summaryOutTokensApprox"] = 0
    use_fast = bool(cfg["use_fast_summary"])
    if use_fast:
        txt = _heuristic_bullets(chunks, cfg)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = False
        PACK_TELEMETRY["summaryBullets"] = len([l for l in txt.splitlines() if l.strip()])
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        return txt, False
    text = "\n".join(f'{m.get("role","")}: {m.get("content","")}' for m in chunks)
    sys_inst = cfg["summary_sys_inst"]
    user_prompt = cfg["summary_user_prefix"] + text + cfg["summary_user_suffix"]
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": sys_inst},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=int(cfg["llm_summary_max_tokens"]),
        temperature=float(cfg["llm_summary_temperature"]),
        top_p=float(cfg["llm_summary_top_p"]),
        stream=False,
        stop=list(cfg["llm_summary_stop"]),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip()
    lines = [ln.strip() for ln in raw.splitlines()]
    bullets: List[str] = []
    seen = set()
    max_words = int(cfg["heuristic_max_words"])
    max_bullets = int(cfg["heuristic_max_bullets"])
    for ln in lines:
        if not ln.startswith(cfg["bullet_prefix"]):
            continue
        norm = " ".join(ln[len(cfg["bullet_prefix"]):].lower().split())
        if not norm or norm in seen:
            continue
        seen.add(norm)
        words = ln[len(cfg["bullet_prefix"]):].split()
        if len(words) > max_words:
            ln = cfg["bullet_prefix"] + " ".join(words[:max_words])
        bullets.append(ln)
        if len(bullets) >= max_bullets:
            break
    if bullets:
        txt = "\n".join(bullets)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = True
        PACK_TELEMETRY["summaryBullets"] = len(bullets)
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        return txt, True
    s = " ".join(raw.split())[:160]
    fallback = (cfg["bullet_prefix"] + s) if s else cfg["bullet_prefix"].strip()
    dt = time.time() - t0
    PACK_TELEMETRY["summarySec"] = float(dt)
    PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(fallback))
    PACK_TELEMETRY["summaryUsedLLM"] = True
    PACK_TELEMETRY["summaryBullets"] = len([l for l in fallback.splitlines() if l.strip()])
    PACK_TELEMETRY["summaryAddedChars"] = len(fallback)
    PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(fallback))
    return fallback, True

def _compress_summary_block(s: str) -> str:
    cfg = _SETTINGS.get()
    max_chars = int(cfg["summary_max_chars"])
    prefix = cfg["bullet_prefix"]
    lines = [ln.strip() for ln in (s or "").splitlines()]
    out, seen = [], set()
    for ln in lines:
        if not ln.startswith(prefix):
            continue
        norm = " ".join(ln[len(prefix):].lower().split())
        if norm in seen:
            continue
        seen.add(norm)
        out.append(ln)
    text = "\n".join(out)
    PACK_TELEMETRY["summaryCompressedFromChars"] = int(len(s or ""))
    if len(text) > max_chars:
        last, total = [], 0
        for ln in reversed(out):
            if total + len(ln) + 1 > max_chars:
                break
            last.append(ln)
            total += len(ln) + 1
        text = "\n".join(reversed(last))
    PACK_TELEMETRY["summaryCompressedToChars"] = int(len(text))
    PACK_TELEMETRY["summaryCompressedDroppedChars"] = int(max(0, int(PACK_TELEMETRY["summaryCompressedFromChars"]) - int(PACK_TELEMETRY["summaryCompressedToChars"])))
    return text

# ===== aimodel/file_read/core/packing_ops.py =====

from __future__ import annotations
import time
from typing import Dict, List

from .packing_memory_core import (
    _SETTINGS,
    count_prompt_tokens,
    approx_tokens,
    summarize_chunks,
    _compress_summary_block,
    PACK_TELEMETRY,
)
from .style import get_style_sys

def build_system(style: str, short: bool, bullets: bool) -> str:
    cfg = _SETTINGS.get()
    base = get_style_sys()
    parts = [base]
    if style and style != base:
        parts.append(style)
    if short:
        parts.append(cfg["system_brief_directive"])
    if bullets:
        parts.append(cfg["system_bullets_directive"])
    parts.append(cfg["system_follow_user_style_directive"])
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    t0_pack = time.time()
    cfg = _SETTINGS.get()
    model_ctx = int(max_ctx or cfg["model_ctx"])
    gen_budget = int(out_budget or cfg["out_budget"])
    reserved = int(cfg["reserved_system_tokens"])
    input_budget = model_ctx - gen_budget - reserved
    if input_budget < int(cfg["min_input_budget"]):
        input_budget = int(cfg["min_input_budget"])
    sys_text = build_system(style, short, bullets)
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": cfg["summary_header_prefix"] + summary})
    packed = prologue + list(recent)
    try:
        PACK_TELEMETRY["packInputTokensApprox"] = int(count_prompt_tokens(packed))
        PACK_TELEMETRY["packMsgs"] = int(len(packed))
    except Exception:
        pass
    PACK_TELEMETRY["packSec"] += float(time.time() - t0_pack)
    return packed, input_budget

def _final_safety_trim(packed: List[Dict[str,str]], input_budget: int) -> List[Dict[str,str]]:
    t0 = time.time()
    cfg = _SETTINGS.get()
    keep_ratio = float(cfg["final_shrink_summary_keep_ratio"])
    min_keep = int(cfg["final_shrink_summary_min_chars"])
    def toks() -> int:
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    t_before = toks()
    PACK_TELEMETRY["finalTrimTokensBefore"] = int(t_before)
    dropped_msgs = 0
    dropped_tokens = 0
    keep_head = 2 if len(packed) >= 2 and isinstance(packed[1].get("content"), str) and packed[1]["content"].startswith(cfg["summary_header_prefix"]) else 1
    while toks() > input_budget and len(packed) > keep_head + 1:
        dropped = packed.pop(keep_head)
        try:
            dropped_tokens += int(approx_tokens(dropped["content"]))
            dropped_msgs += 1
        except Exception:
            pass
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        summary_msg = packed[1]
        txt = summary_msg["content"]
        n = max(min_keep, int(len(txt) * keep_ratio))
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"] = int(len(txt))
        except Exception:
            pass
        summary_msg["content"] = txt[-n:]
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkToChars"] = int(len(summary_msg["content"]))
            PACK_TELEMETRY["finalTrimSummaryDroppedChars"] = int(max(0, int(PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"]) - int(PACK_TELEMETRY["finalTrimSummaryShrunkToChars"])))
        except Exception:
            pass
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        removed = packed.pop(1)
        try:
            dropped_tokens += int(approx_tokens(removed["content"]))
            dropped_msgs += 1
        except Exception:
            pass
    while toks() > input_budget and len(packed) > 2:
        removed = packed.pop(2 if len(packed) > 3 else 1)
        try:
            dropped_tokens += int(approx_tokens(removed["content"]))
            dropped_msgs += 1
        except Exception:
            pass
    t_after = toks()
    PACK_TELEMETRY["finalTrimTokensAfter"] = int(t_after)
    PACK_TELEMETRY["finalTrimDroppedMsgs"] = int(dropped_msgs)
    PACK_TELEMETRY["finalTrimDroppedApproxTokens"] = int(max(0, dropped_tokens))
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0)
    return packed

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    cfg = _SETTINGS.get()
    def _tok():
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    start_tokens = _tok()
    overage = start_tokens - input_budget
    PACK_TELEMETRY["rollStartTokens"] = int(start_tokens)
    PACK_TELEMETRY["rollOverageTokens"] = int(overage)
    if overage <= int(cfg["skip_overage_lt"]):
        packed = _final_safety_trim(packed, input_budget)
        PACK_TELEMETRY["rollEndTokens"] = int(count_prompt_tokens(packed))
        return packed, summary
    peels_done = 0
    peeled_n = 0
    if len(recent) > 6 and peels_done < int(cfg["max_peel_per_turn"]):
        peel_min = int(cfg["peel_min"])
        peel_frac = float(cfg["peel_frac"])
        peel_max = int(cfg["peel_max"])
        target = max(peel_min, min(peel_max, int(len(recent) * peel_frac)))
        peel = []
        for _ in range(min(target, len(recent))):
            peel.append(recent.popleft())
        peeled_n = len(peel)
        t0_sum = time.time()
        new_sum, _used_llm = summarize_chunks(peel)
        PACK_TELEMETRY["summarySec"] += float(time.time() - t0_sum)
        if new_sum.startswith(cfg["bullet_prefix"]):
            summary = (summary + "\n" + new_sum).strip() if summary else new_sum
        else:
            summary = new_sum
        t0_comp = time.time()
        summary = _compress_summary_block(summary)
        PACK_TELEMETRY["compressSec"] += float(time.time() - t0_comp)
        try:
            PACK_TELEMETRY["rollPeeledMsgs"] = int(peeled_n)
            PACK_TELEMETRY["rollNewSummaryChars"] = int(len(summary))
            PACK_TELEMETRY["rollNewSummaryTokensApprox"] = int(approx_tokens(summary))
        except Exception:
            pass
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": cfg["summary_header_prefix"] + summary},
            *list(recent),
        ]
    t0_trim = time.time()
    packed = _final_safety_trim(packed, input_budget)
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0_trim)
    end_tokens = count_prompt_tokens(packed)
    PACK_TELEMETRY["rollEndTokens"] = int(end_tokens)
    return packed, summary

# ===== aimodel/file_read/core/request_ctx.py =====

# aimodel/file_read/core/request_ctx.py
from contextvars import ContextVar

x_id_ctx: ContextVar[str] = ContextVar("x_id_ctx", default="")
id_token_ctx: ContextVar[str] = ContextVar("id_token_ctx", default="")
user_email_ctx: ContextVar[str] = ContextVar("user_email_ctx", default="")

def get_x_id() -> str: return x_id_ctx.get()
def set_x_id(val: str): x_id_ctx.set((val or "").strip())

def get_id_token() -> str: return id_token_ctx.get()
def set_id_token(val: str): id_token_ctx.set((val or "").strip())

def get_user_email() -> str: return user_email_ctx.get()
def set_user_email(val: str): user_email_ctx.set((val or "").strip())

# ===== aimodel/file_read/core/schemas.py =====

# core/schemas.py
from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel


class Attachment(BaseModel):
    name: str
    source: Optional[str] = None
    sessionId: Optional[str] = None

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str
    attachments: Optional[List[Attachment]] = None  

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None

    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None

    autoWeb: Optional[bool] = None
    webK: Optional[int] = None
    autoRag: Optional[bool] = None   

# ===== aimodel/file_read/core/settings.py =====

from __future__ import annotations
import json
from threading import RLock
from typing import Any, Dict, Optional

from .files import (
    DEFAULTS_SETTINGS_FILE,
    OVERRIDES_SETTINGS_FILE,
    load_json_file,
    save_json_file,
)


def _deep_merge(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(dst)
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)
        else:
            out[k] = v
    return out


class _SettingsManager:
    """
    Sources of truth:
      - defaults:    read-only from DEFAULTS_SETTINGS_FILE
      - overrides:   persisted to OVERRIDES_SETTINGS_FILE
      - adaptive:    in-memory (per-session or global)
    Effective settings are computed on the fly: defaults <- adaptive <- overrides.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._defaults: Dict[str, Any] = self._load_defaults()
        self._overrides: Dict[str, Any] = self._load_overrides()
        self._adaptive_by_session: Dict[str, Dict[str, Any]] = {}

    # ----- I/O -----
    def _load_defaults(self) -> Dict[str, Any]:
        return load_json_file(DEFAULTS_SETTINGS_FILE, default={})

    def _load_overrides(self) -> Dict[str, Any]:
        return load_json_file(OVERRIDES_SETTINGS_FILE, default={})

    def _save_overrides_unlocked(self) -> None:
        save_json_file(OVERRIDES_SETTINGS_FILE, self._overrides)

    # ----- Effective -----
    def _effective_unlocked(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get(session_id or "_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        return eff

    def _get_unlocked(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        eff = self._effective_unlocked(session_id)
        if key in eff:
            return eff[key]
        if default is not None:
            return default
        raise AttributeError(f"_SettingsManager has no key '{key}'")

    # ----- Public API -----
    @property
    def defaults(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._defaults))

    @property
    def overrides(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._overrides))

    def adaptive(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        key = session_id or "_global_"
        with self._lock:
            return json.loads(json.dumps(self._adaptive_by_session.get(key, {})))

    def effective(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        with self._lock:
            return self._effective_unlocked(session_id)

    def __getattr__(self, name: str) -> Any:
        with self._lock:
            return self._get_unlocked(name)

    def __getitem__(self, key: str) -> Any:
        with self._lock:
            return self._get_unlocked(key)

    def get(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        with self._lock:
            try:
                return self._get_unlocked(key, default=default, session_id=session_id)
            except AttributeError:
                return default

    # ----- Mutations -----
    def patch_overrides(self, patch: Dict[str, Any]) -> None:
        if not isinstance(patch, dict):
            return
        with self._lock:
            self._overrides = _deep_merge(self._overrides, patch)
            self._save_overrides_unlocked()

    def replace_overrides(self, new_overrides: Dict[str, Any]) -> None:
        if not isinstance(new_overrides, dict):
            new_overrides = {}
        with self._lock:
            self._overrides = json.loads(json.dumps(new_overrides))
            self._save_overrides_unlocked()

    def reload_overrides(self) -> None:
        with self._lock:
            self._overrides = self._load_overrides()

    def set_adaptive_for_session(self, session_id: Optional[str], values: Dict[str, Any]) -> None:
        key = session_id or "_global_"
        if not isinstance(values, dict):
            values = {}
        with self._lock:
            self._adaptive_by_session[key] = json.loads(json.dumps(values))

    def recompute_adaptive(self, session_id: Optional[str] = None) -> None:
        # Kept for API compatibility; effective is always computed on demand.
        return None


SETTINGS = _SettingsManager()

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple
from .settings import SETTINGS

def get_style_sys() -> str:
    return SETTINGS.get("style_sys", "")

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    S = SETTINGS.effective()
    pats = S.get("style_patterns", {})
    template = S.get("style_template", "You must talk like {style}.")

    compiled = []
    for key in ["talk_like", "respond_like", "from_now", "be"]:
        if key in pats:
            compiled.append(re.compile(pats[key], re.I))

    t = user_text.strip()
    style_match = None
    for pat in compiled:
        style_match = pat.search(t)
        if style_match:
            break

    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = template.format(style=raw)

    return style_inst, False, False

# ===== aimodel/file_read/deps/auth_deps.py =====

from __future__ import annotations
from typing import Optional, Dict, Any
from fastapi import Header, HTTPException, Cookie, Request
from ..services.auth_service import verify_jwt_with_google  # adjust import path to your layout

ID_COOKIE_NAME = "fb_id"
LEGACY_COOKIE_NAME = "fb_session"

def require_auth(
    request: Request,
    authorization: Optional[str] = Header(None),
    fb_id_cookie: Optional[str] = Cookie(None, alias=ID_COOKIE_NAME),
    fb_session_cookie: Optional[str] = Cookie(None, alias=LEGACY_COOKIE_NAME),
) -> Dict[str, Any]:
    if fb_id_cookie:
        try:
            claims = verify_jwt_with_google(fb_id_cookie)
            if not claims.get("email"):
                raise HTTPException(401, "Email missing")
            return claims
        except HTTPException as e:
            print(f"[auth] cookie verify error: {e!r}")
        except Exception as e:
            print(f"[auth] cookie verify error: {e!r}")

    if fb_session_cookie:
        try:
            claims = verify_jwt_with_google(fb_session_cookie)
            if not claims.get("email"):
                raise HTTPException(401, "Email missing")
            return claims
        except HTTPException as e:
            print(f"[auth] legacy cookie verify error: {e!r}")
        except Exception as e:
            print(f"[auth] legacy cookie verify error: {e!r}")

    if authorization and authorization.lower().startswith("bearer "):
        token = authorization.split(None, 1)[1]
        try:
            claims = verify_jwt_with_google(token)
            if not claims.get("email"):
                raise HTTPException(401, "Email missing")
            return claims
        except HTTPException as e:
            print(f"[auth] bearer verify error: {e!r}")
            raise
        except Exception as e:
            print(f"[auth] bearer verify error: {e!r}")
            raise HTTPException(401, "Invalid token")

    raise HTTPException(401, "Not authenticated")

# ===== aimodel/file_read/rag/__init__.py =====



# ===== aimodel/file_read/rag/ingest/__init__.py =====

# aimodel/file_read/rag/ingest/__init__.py

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import io, re, csv, hashlib, json
from datetime import datetime, date, time

import pytesseract
from PIL import Image
import pypdfium2 as pdfium

from ...core.settings import SETTINGS

from .ocr import ocr_image_bytes, is_bad_text, ocr_pdf
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas
from .excel_ingest_core import (
    scan_blocks_by_blank_rows,
    rightmost_nonempty_header,
    select_indices,
)
from .xls_ingest import extract_xls
from .excel_ingest import extract_excel
from .csv_ingest import extract_csv
from .docx_ingest import extract_docx
from .doc_binary_ingest import extract_doc_binary
from .ppt_ingest import extract_pptx, extract_ppt
from .pdf_ingest import extract_pdf
from .main import sniff_and_extract

__all__ = [
    "dataclass",
    "List", "Dict", "Optional", "Tuple",
    "io", "re", "csv", "hashlib", "json",
    "datetime", "date", "time",
    "pytesseract", "Image", "pdfium",
    "SETTINGS",
    "ocr_image_bytes", "is_bad_text", "ocr_pdf",
    "_utf8", "_strip_html", "Chunk", "chunk_text", "build_metas",
    "scan_blocks_by_blank_rows", "rightmost_nonempty_header", "select_indices",
    "extract_xls", "extract_excel", "extract_csv",
    "extract_docx", "extract_doc_binary", "extract_pptx", "extract_ppt", "extract_pdf",
    "sniff_and_extract",
]

# ===== aimodel/file_read/rag/ingest/common.py =====

# ===== aimodel/file_read/rag/ingest/common.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional
import re
from ...core.settings import SETTINGS

@dataclass
class Chunk:
    text: str
    meta: Dict[str, str]


def _utf8(data: bytes) -> str:

    return (data or b"").decode("utf-8", errors="ignore")

def _strip_html(txt: str) -> str:

    if not txt:
        return ""

    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", txt)
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p>", "\n\n", txt)
    txt = re.sub(r"(?is)<.*?>", " ", txt)
    txt = re.sub(r"[ \t]+", " ", txt)
    return txt.strip()

_HDR_RE = re.compile(r"^(#{1,3})\s+.*$", flags=re.MULTILINE)
_PARA_SPLIT_RE = re.compile(r"\n\s*\n+")

def _split_sections(text: str) -> List[str]:

    text = (text or "").strip()
    if not text:
        return []
    starts = [m.start() for m in _HDR_RE.finditer(text)]
    if not starts:
        return [text]
    if 0 not in starts:
        starts = [0] + starts
    sections: List[str] = []
    for i, s in enumerate(starts):
        e = starts[i + 1] if i + 1 < len(starts) else len(text)
        block = text[s:e].strip()
        if block:
            sections.append(block)
    return sections

def _split_paragraphs(block: str) -> List[str]:
    paras = [p.strip() for p in _PARA_SPLIT_RE.split(block or "")]
    return [p for p in paras if p]

def _hard_split(text: str, max_len: int) -> List[str]:

    approx = re.split(r"(?<=[\.\!\?\;])\s+", text or "")
    out: List[str] = []
    buf = ""
    for s in approx:
        if not s:
            continue
        if len(buf) + (1 if buf else 0) + len(s) <= max_len:
            buf = s if not buf else (buf + " " + s)
        else:
            if buf:
                out.append(buf)
            if len(s) <= max_len:
                out.append(s)
            else:
                words = re.split(r"\s+", s)
                cur = ""
                for w in words:
                    if not w:
                        continue
                    if len(cur) + (1 if cur else 0) + len(w) <= max_len:
                        cur = w if not cur else (cur + " " + w)
                    else:
                        if cur:
                            out.append(cur)
                        cur = w
                if cur:
                    out.append(cur)
            buf = ""
    if buf:
        out.append(buf)
    return out

def _pack_with_budget(pieces: List[str], *, max_chars: int) -> List[str]:
    chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for p in pieces:
        plen = len(p)
        if plen > max_chars:
            chunks.extend(_hard_split(p, max_chars))
            continue
        if cur_len == 0:
            cur, cur_len = [p], plen
            continue
        if cur_len + 2 + plen <= max_chars:  
            cur.append(p)
            cur_len += 2 + plen
        else:
            chunks.append("\n\n".join(cur).strip())
            cur, cur_len = [p], plen
    if cur_len:
        chunks.append("\n\n".join(cur).strip())
    return chunks

def chunk_text(
    text: str,
    meta: Optional[Dict[str, str]] = None,
    *,
    max_chars: int = int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    overlap: int = int(SETTINGS.get("rag_chunk_overlap_chars", 150)),
) -> List[Chunk]:

    base_meta = (meta or {}).copy()
    text = (text or "").strip()
    if not text:
        return []

    if len(text) <= max_chars:
        return [Chunk(text=text, meta=base_meta)]

    sections = _split_sections(text)
    if not sections:
        sections = [text]

    chunks: List[Chunk] = []
    last_tail: Optional[str] = None

    for sec in sections:
        paras = _split_paragraphs(sec)
        if not paras:
            continue
        packed = _pack_with_budget(paras, max_chars=max_chars)
        for ch in packed:
            if last_tail and overlap > 0:
                tail = last_tail[-overlap:] if len(last_tail) > overlap else last_tail
                candidate = f"{tail}\n{ch}"
                chunks.append(Chunk(text=candidate if len(candidate) <= max_chars else ch, meta=base_meta))
            else:
                chunks.append(Chunk(text=ch, meta=base_meta))
            last_tail = ch

    return chunks

def build_metas(session_id: Optional[str], filename: str, chunks: List[Chunk], *, size: int = 0) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for i, c in enumerate(chunks):
        out.append({
            "id": f"{filename}:{i}",
            "sessionId": session_id or "",
            "source": filename,
            "title": filename,
            "mime": "text/plain",
            "size": str(size),
            "chunkIndex": str(i),
            "text": c.text,  
        })
    return out

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
import io, re, csv
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_csv(data: bytes) -> Tuple[str, str]:
    S = SETTINGS.effective
    max_chars = int(S().get("csv_value_max_chars"))
    quote_strings = bool(S().get("csv_quote_strings"))
    header_normalize = bool(S().get("csv_header_normalize"))
    max_rows = int(S().get("csv_infer_max_rows"))
    max_cols = int(S().get("csv_infer_max_cols"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_val(v) -> str:
        if v is None:
            return ""
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not header_normalize:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def rightmost_nonempty_header(headers: List[str]) -> int:
        for i in range(len(headers) - 1, -1, -1):
            h = headers[i]
            if h and not h.isspace():
                return i
        return -1

    def keep_headers(headers: List[str]) -> List[int]:
        keep = []
        for i, h in enumerate(headers):
            hn = (h or "").strip().lower()
            if not hn:
                continue
            if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
                continue
            keep.append(i)
        return keep or list(range(len(headers)))

    def _row_blank_csv(row: List[str]) -> bool:
        if row is None:
            return True
        for c in row:
            if c is None:
                continue
            if str(c).strip():
                return False
        return True

    txt = io.StringIO(data.decode("utf-8", errors="ignore"))
    sample = txt.read(2048)
    txt.seek(0)
    try:
        dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
    except Exception:
        dialect = csv.excel
    reader = csv.reader(txt, dialect)
    rows = list(reader)
    if not rows:
        return "", "text/plain"

    n = len(rows)
    lines: List[str] = []
    lines.append("# Sheet: CSV")

    i = 0
    while i < n:
        while i < n and _row_blank_csv(rows[i]):
            i += 1
        if i >= n:
            break

        start = i
        while i < n and not _row_blank_csv(rows[i]):
            i += 1
        end = i - 1
        if start > end:
            continue

        headers_raw = (rows[start] if start < n else [])[:max_cols]
        norm_headers = [normalize_header(fmt_val(h)) for h in headers_raw]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[: rmax + 1]
        norm_headers = norm_headers[:max_cols]
        keep_idx = keep_headers(norm_headers)
        kept_headers = [norm_headers[j] for j in keep_idx]

        total_rows_block = (end - start + 1)
        use_rows = total_rows_block if max_rows <= 0 else min(total_rows_block, max_rows + 1)
        total_cols_block = len(kept_headers)
        if max_cols > 0:
            total_cols_block = min(total_cols_block, max_cols)

        lines.append(f"## Table: CSV!R{start+1}-{start+use_rows},C1-{max(total_cols_block,1)}")
        if any(kept_headers):
            lines.append("headers: " + ", ".join(h for h in kept_headers if h))
        lines.append("")

        data_start = start + 1
        data_end = min(end, start + use_rows - 1)
        usable_cols_for_slice = min(len(norm_headers), max_cols if max_cols > 0 else len(norm_headers))
        for r in range(data_start, data_end + 1):
            row_vals_raw = rows[r][:usable_cols_for_slice] if r < n else []
            vals = [fmt_val(c) for c in row_vals_raw]
            vals = [vals[j] if j < len(vals) else "" for j in keep_idx]
            while vals and (vals[-1] == "" or vals[-1] is None):
                vals.pop()
            if not any(vals):
                continue

            pairs: List[str] = []
            for h, v in zip(kept_headers, vals):
                if h and v:
                    pairs.append(f"{h}={v}")

            excel_row_num = r + 1
            lines.append(f"### Row {excel_row_num} — CSV")
            lines.append(", ".join(pairs) if pairs else ", ".join(vals))
            lines.append("")

    return "\n".join(lines).strip() + "\n", "text/plain"

# ===== aimodel/file_read/rag/ingest/doc_binary_ingest.py =====

# RTF and legacy .doc/.ole helpers and extraction (no DOCX here)
from __future__ import annotations
from typing import Tuple, List
import re
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _is_ole(b: bytes) -> bool:
    return len(b) >= 8 and b[:8] == b"\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1"

def _dbg(msg: str):
    try:
        S = SETTINGS.effective
        if bool(S().get("doc_debug", False)):
            print(f"[doc_ingest] {msg}")
    except Exception:
        pass

_RTF_CTRL_RE = re.compile(r"\\[a-zA-Z]+-?\d* ?")
_RTF_GROUP_RE = re.compile(r"[{}]")
_RTF_UNICODE_RE = re.compile(r"\\u(-?\d+)\??")
_RTF_HEX_RE = re.compile(r"\\'[0-9a-fA-F]{2}")
_HEX_BLOCK_RE = re.compile(r"(?:\s*[0-9A-Fa-f]{2}){120,}")

def _rtf_to_text_simple(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    def _hex_sub(m):
        try:
            return bytes.fromhex(m.group(0)[2:]).decode("latin-1", errors="ignore")
        except Exception:
            return ""
    s = _RTF_HEX_RE.sub(_hex_sub, s)
    def _uni_sub(m):
        try:
            cp = int(m.group(1))
            if cp < 0:
                cp = 65536 + cp
            return chr(cp)
        except Exception:
            return ""
    s = _RTF_UNICODE_RE.sub(_uni_sub, s)
    s = s.replace(r"\par", "\n").replace(r"\line", "\n")
    s = _RTF_CTRL_RE.sub("", s)
    s = _RTF_GROUP_RE.sub("", s)
    s = _HEX_BLOCK_RE.sub("", s)
    s = s.replace("\r", "\n")
    s = re.sub(r"\n\s*\n\s*\n+", "\n\n", s)
    s = _squeeze_spaces(s)
    return s if keep_newlines else s.replace("\n", " ")

def _rtf_to_text_via_lib(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        from striprtf.striprtf import rtf_to_text
    except Exception:
        return _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    try:
        txt = rtf_to_text(s)
    except Exception:
        txt = _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    txt = _squeeze_spaces(txt)
    return txt if keep_newlines else txt.replace("\n", " ")

def _generic_ole_text(data: bytes) -> str:
    S = SETTINGS.effective
    MIN_RUN = int(S().get("doc_ole_min_run_chars", 8))
    MAX_LINE = int(S().get("doc_ole_max_line_chars", 600))
    MIN_ALPHA_RATIO = float(S().get("doc_ole_min_alpha_ratio", 0.25))
    DROP_XMLISH = bool(S().get("doc_ole_drop_xmlish", True))
    DROP_PATHISH = bool(S().get("doc_ole_drop_pathish", True))
    DROP_SYMBOL_LINES = bool(S().get("doc_ole_drop_symbol_lines", True))
    DEDUPE_SHORT_REPEATS = bool(S().get("doc_ole_dedupe_short_repeats", True))
    XMLISH = re.compile(r"^\s*<[^>]+>", re.I)
    PATHISH = re.compile(r"[\\/].+\.(?:xml|rels|png|jpg|jpeg|gif|bmp|bin|dat)\b", re.I)
    SYMBOLLINE = re.compile(r"^[\W_]{6,}$")
    s = data.replace(b"\x00", b"")
    runs = re.findall(rb"[\t\r\n\x20-\x7E]{%d,}" % MIN_RUN, s)
    if not runs:
        return ""
    def _dec(b: bytes) -> str:
        try:
            return b.decode("cp1252", errors="ignore")
        except Exception:
            return b.decode("latin-1", errors="ignore")
    kept: List[str] = []
    for raw in runs:
        chunk = _dec(raw).replace("\r", "\n")
        for ln in re.split(r"\n+", chunk):
            t = ln.strip()
            if not t:
                continue
            if MAX_LINE > 0 and len(t) > MAX_LINE:
                t = t[:MAX_LINE].rstrip()
            t = _squeeze_spaces(t)
            letters = sum(1 for c in t if c.isalpha())
            if letters / max(1, len(t)) < MIN_ALPHA_RATIO:
                continue
            if DROP_XMLISH and XMLISH.search(t):
                continue
            if DROP_PATHISH and PATHISH.search(t):
                continue
            if DROP_SYMBOL_LINES and SYMBOLLINE.fullmatch(t):
                continue
            if DEDUPE_SHORT_REPEATS:
                t = re.sub(r"\b(\w{2,4})\1{2,}\b", r"\1\1", t)
            kept.append(t)
    out = "\n".join(kept)
    out = re.sub(r"\n\s*\n\s*\n+", "\n\n", out).strip()
    return out

def extract_doc_binary(data: bytes) -> Tuple[str, str]:
    head = (data[:64] or b"").lstrip()
    is_rtf = head.startswith(b"{\\rtf") or head.startswith(b"{\\RTF}")
    is_ole = _is_ole(data)
    _dbg(f"extract_doc_binary: bytes={len(data)} is_rtf={is_rtf} is_ole={is_ole}")
    if is_rtf:
        txt = _rtf_to_text_via_lib(data, keep_newlines=True).strip()
        return (txt + "\n" if txt else ""), "text/plain"
    if is_ole:
        txt = _generic_ole_text(data)
        return (txt + "\n" if txt else ""), "text/plain"
    try:
        txt = data.decode("utf-8", errors="ignore").strip()
    except Exception:
        txt = data.decode("latin-1", errors="ignore").strip()
    return (txt + ("\n" if txt else "")), "text/plain"

# ===== aimodel/file_read/rag/ingest/docx_ingest.py =====

# DOCX-only extraction (no .doc/RTF here)
from __future__ import annotations
from typing import Tuple, List, Optional
import io, re
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _is_heading(style_name: str) -> Optional[int]:
    if not style_name:
        return None
    m = re.match(r"Heading\s+(\d+)", style_name, flags=re.IGNORECASE)
    if not m:
        return None
    try:
        return max(1, min(6, int(m.group(1))))
    except Exception:
        return None

def _is_list_style(style_name: str) -> bool:
    return bool(style_name) and any(k in style_name.lower() for k in ("list", "bullet", "number"))

def _extract_paragraph_text(p) -> str:
    return _squeeze_spaces(p.text)

def _docx_image_blobs(doc) -> List[bytes]:
    blobs: List[bytes] = []
    seen_rids = set()
    try:
        part = doc.part

        # inline images
        for ish in getattr(doc, "inline_shapes", []) or []:
            try:
                rId = ish._inline.graphic.graphicData.pic.blipFill.blip.embed
                if rId and rId not in seen_rids:
                    blob = part.related_parts[rId].blob
                    if blob:
                        blobs.append(blob)
                        seen_rids.add(rId)
            except Exception:
                pass

        for p in doc.paragraphs:
            for r in p.runs:
                for d in getattr(r._element, "xpath", lambda *_: [])(".//a:blip"):
                    try:
                        rId = d.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed")
                        if rId and rId not in seen_rids:
                            blob = part.related_parts[rId].blob
                            if blob:
                                blobs.append(blob)
                                seen_rids.add(rId)
                    except Exception:
                        pass

    except Exception:
        pass
    return blobs

def extract_docx(data: bytes) -> Tuple[str, str]:
    from docx import Document
    S = SETTINGS.effective
    HEADING_MAX_LEVEL = int(S().get("docx_heading_max_level", 3))
    USE_MARKDOWN_HEADINGS = bool(S().get("docx_use_markdown_headings", True))
    PRESERVE_BULLETS = bool(S().get("docx_preserve_bullets", True))
    INCLUDE_TABLES = bool(S().get("docx_include_tables", True))
    INCLUDE_HEADERS_FOOTERS = bool(S().get("docx_include_headers_footers", False))
    MAX_PARA_CHARS = int(S().get("docx_para_max_chars", 0))
    DROP_EMPTY_LINES = bool(S().get("docx_drop_empty_lines", True))

    doc = Document(io.BytesIO(data))
    lines: List[str] = []

    try:
        title = (getattr(doc, "core_properties", None) or {}).title
        if title:
            lines.append(f"# {title}")
            lines.append("")
    except Exception:
        pass

    def _clip(s: str) -> str:
        if MAX_PARA_CHARS > 0 and len(s) > MAX_PARA_CHARS:
            return s[:MAX_PARA_CHARS] + "…"
        return s

    if INCLUDE_HEADERS_FOOTERS:
        try:
            for i, sec in enumerate(getattr(doc, "sections", []) or []):
                if i > 0:
                    break
                try:
                    hdr_ps = getattr(sec.header, "paragraphs", []) or []
                    hdr_text = "\n".join(_squeeze_spaces(p.text) for p in hdr_ps if _squeeze_spaces(p.text))
                    if hdr_text:
                        lines.append("## Header")
                        lines.append(_clip(hdr_text))
                        lines.append("")
                except Exception:
                    pass
                try:
                    ftr_ps = getattr(sec.footer, "paragraphs", []) or []
                    ftr_text = "\n".join(_squeeze_spaces(p.text) for p in ftr_ps if _squeeze_spaces(p.text))
                    if ftr_text:
                        lines.append("## Footer")
                        lines.append(_clip(ftr_text))
                        lines.append("")
                except Exception:
                    pass
        except Exception:
            pass

    for p in doc.paragraphs:
        txt = _extract_paragraph_text(p)
        if not txt and DROP_EMPTY_LINES:
            continue
        style_name = getattr(p.style, "name", "") or ""
        lvl = _is_heading(style_name)
        if lvl and lvl <= HEADING_MAX_LEVEL and USE_MARKDOWN_HEADINGS:
            prefix = "#" * max(1, min(6, lvl))
            lines.append(f"{prefix} {txt}".strip())
            continue
        if PRESERVE_BULLETS and _is_list_style(style_name):
            if txt:
                lines.append(f"- {_clip(txt)}")
            continue
        if txt:
            lines.append(_clip(txt))
        elif not DROP_EMPTY_LINES:
            lines.append("")

    if INCLUDE_TABLES and getattr(doc, "tables", None):
        for t_idx, tbl in enumerate(doc.tables):
            try:
                non_empty = any(_squeeze_spaces(cell.text) for row in tbl.rows for cell in row.cells)
            except Exception:
                non_empty = True
            if not non_empty:
                continue
            lines.append("")
            lines.append(f"## Table {t_idx + 1}")
            try:
                for row in tbl.rows:
                    cells = [_squeeze_spaces(c.text) for c in row.cells]
                    if any(cells):
                        lines.append(" | ".join(c for c in cells if c))
            except Exception:
                pass

    if bool(S().get("docx_ocr_images", False)):
        min_bytes = int(S().get("ocr_min_image_bytes", 16384))
        seen_ocr_text = set()
        for blob in _docx_image_blobs(doc):
            if len(blob) >= min_bytes:
                t = (ocr_image_bytes(blob) or "").strip()
                if t:
                    key = t.lower()
                    if key not in seen_ocr_text:
                        lines.append(t)
                        seen_ocr_text.add(key)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
import io, re, hashlib
from datetime import datetime, date, time
from ...core.settings import SETTINGS
from .excel_ingest_core import (
    scan_blocks_by_blank_rows,
    rightmost_nonempty_header,
    select_indices,
)

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_excel(data: bytes) -> Tuple[str, str]:
    from openpyxl import load_workbook
    from openpyxl.utils import range_boundaries
    from openpyxl.worksheet.worksheet import Worksheet
    from openpyxl.utils.datetime import from_excel as _from_excel

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def _sheet_used_range(ws: Worksheet):
        from openpyxl.utils import range_boundaries
        if callable(getattr(ws, "calculate_dimension", None)):
            dim_ref = ws.calculate_dimension()
            try:
                min_c, min_r, max_c, max_r = range_boundaries(dim_ref)
                return min_c, min_r, max_c, max_r
            except Exception:
                pass
        return 1, 1, ws.max_column or 1, ws.max_row or 1

    def _cap_rows(min_r: int, max_r: int) -> int:
        return max_r if INFER_MAX_ROWS <= 0 else min(max_r, min_r + INFER_MAX_ROWS - 1)

    def _keep_and_rename_phantom(headers: List[str]) -> tuple[List[int], List[str]]:
        if len(headers) >= 2 and re.fullmatch(r"\d+_\d+$", (headers[1] or "")) and (headers[0] or ""):
            keep_idx = [0, 1]
            new_headers = headers[:]
            new_headers[1] = "value"
            return keep_idx, new_headers
        keep_idx = [i for i, h in enumerate(headers) if h and not re.fullmatch(r"\d+_\d+$", h)]
        new_headers = [headers[i] for i in keep_idx]
        return keep_idx, new_headers

    wb_vals = load_workbook(io.BytesIO(data), data_only=True, read_only=True)

    def _coerce_excel_datetime(cell, v):
        try:
            if getattr(cell, "is_date", False) and isinstance(v, (int, float)):
                return _from_excel(v, wb_vals.epoch)
        except Exception:
            pass
        return v

    lines: List[str] = []
    ingest_id = hashlib.sha1(data).hexdigest()[:16]
    lines.append(f"# Ingest-ID: {ingest_id}")

    def _emit_inferred_table(ws: Worksheet, sheet_name: str, min_c, min_r, max_c, max_r):
        lines.append(f"# Sheet: {sheet_name}")
        max_c_eff = min(max_c, min_c + INFER_MAX_COLS - 1)
        max_r_eff = _cap_rows(min_r, max_r)
        headers: List[str] = []
        header_fill = 0
        for c in range(min_c, max_c_eff + 1):
            val = ws.cell(row=min_r, column=c).value
            s = fmt_val("" if val is None else str(val).strip())
            if s:
                header_fill += 1
            headers.append(s)
        fill_ratio = header_fill / max(1, (max_c_eff - min_c + 1))
        if fill_ratio < INFER_MIN_HEADER_FILL and (min_r + 1) <= max_r_eff:
            headers = []
            hdr_r = min_r + 1
            for c in range(min_c, max_c_eff + 1):
                val = ws.cell(row=hdr_r, column=c).value
                s = fmt_val("" if val is None else str(val).strip())
                headers.append(s)
            min_r = hdr_r
        norm_headers = [normalize_header(h) for h in headers]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[:rmax + 1]
            max_c_eff = min(max_c_eff, min_c + rmax)
        keep_idx, norm_headers = _keep_and_rename_phantom(norm_headers)
        lines.append("## Table: " + f"{sheet_name}!R{min_r}-{max_r_eff},C{min_c}-{max_c_eff}")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))
        lines.append("")
        for r in range(min_r + 1, max_r_eff + 1):
            raw_vals: List[str] = []
            for c in range(min_c, max_c_eff + 1):
                cell = ws.cell(row=r, column=c)
                vv = _coerce_excel_datetime(cell, cell.value)
                raw_vals.append(fmt_val(vv))
            row_vals = select_indices(raw_vals, keep_idx)
            while row_vals and (row_vals[-1] == "" or row_vals[-1] is None):
                row_vals.pop()
            if not any(v for v in row_vals):
                continue
            pairs: List[str] = []
            for h, v in zip(norm_headers, row_vals):
                if h and v:
                    pairs.append(f"{h}={v}")
            lines.append(f"### Row {r} — {sheet_name}")
            lines.append(", ".join(pairs) if pairs else ", ".join(row_vals))
            lines.append("")

    for sheet_name in wb_vals.sheetnames:
        ws_v: Worksheet = wb_vals[sheet_name]
        min_c, min_r, max_c, max_r = _sheet_used_range(ws_v)
        max_c = min(max_c, min_c + INFER_MAX_COLS - 1)
        for b_min_c, b_min_r, b_max_c, b_max_r in scan_blocks_by_blank_rows(ws_v, min_c, min_r, max_c, max_r):
            if b_min_r > b_max_r:
                continue
            _emit_inferred_table(ws_v, sheet_name, b_min_c, b_min_r, b_max_c, b_max_r)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====
from __future__ import annotations
from typing import List
import re

_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def row_blank(ws, r: int, min_c: int, max_c: int) -> bool:
    for c in range(min_c, max_c + 1):
        v = ws.cell(row=r, column=c).value
        if v not in (None, "") and not (isinstance(v, str) and not v.strip()):
            return False
    return True

def scan_blocks_by_blank_rows(ws, min_c: int, min_r: int, max_c: int, max_r: int):
    r = min_r
    while r <= max_r:
        while r <= max_r and row_blank(ws, r, min_c, max_c):
            r += 1
        if r > max_r:
            break
        start = r
        while r <= max_r and not row_blank(ws, r, min_c, max_c):
            r += 1
        end = r - 1
        yield (min_c, start, max_c, end)

def rightmost_nonempty_header(headers: List[str]) -> int:
    for i in range(len(headers) - 1, -1, -1):
        h = headers[i]
        if h and not h.isspace():
            return i
    return -1

def drop_bad_columns(headers: List[str]) -> List[int]:
    keep = []
    for i, h in enumerate(headers):
        hn = (h or "").strip().lower()
        if not hn:
            continue
        if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
            continue
        keep.append(i)
    return keep or list(range(len(headers)))

def select_indices(seq: List[str], idxs: List[int]) -> List[str]:
    out = []
    for i in idxs:
        out.append(seq[i] if i < len(seq) else "")
    return out

# ===== aimodel/file_read/rag/ingest/main.py =====

# aimodel/file_read/rag/ingest/main.py
from __future__ import annotations
from typing import Tuple
import io, json
from .xls_ingest import extract_xls
from .excel_ingest import extract_excel
from .csv_ingest import extract_csv
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas
from .docx_ingest import extract_docx
from .doc_binary_ingest import extract_doc_binary
from .ppt_ingest import extract_pptx, extract_ppt
from .pdf_ingest import extract_pdf   
from ...core.settings import SETTINGS

__all__ = ["sniff_and_extract", "Chunk", "chunk_text", "build_metas"]

def _ing_dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[ingest]", *args)
    except Exception:
        pass

def sniff_and_extract(filename: str, data: bytes) -> Tuple[str, str]:
    name = (filename or "").lower()
    _ing_dbg("route:", name, "bytes=", len(data))

    if name.endswith((".pptx", ".pptm")):
        _ing_dbg("-> pptx/pptm")
        return extract_pptx(data)

    if name.endswith(".ppt"):
        _ing_dbg("-> ppt (ole)")
        return extract_ppt(data)

    if name.endswith((".xlsx", ".xlsm")):
        _ing_dbg("-> excel")
        return extract_excel(data)

    if name.endswith(".xls"):
        _ing_dbg("-> excel (xls via xlrd)")
        return extract_xls(data)

    if name.endswith((".csv", ".tsv")):
        _ing_dbg("-> csv/tsv")
        return extract_csv(data)

    if name.endswith(".docx"):
        _ing_dbg("-> docx")
        try:
            return extract_docx(data)
        except Exception as e:
            _ing_dbg("docx err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".doc"):
        _ing_dbg("-> doc (binary/rtf)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("doc err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".rtf"):
        _ing_dbg("-> rtf (via doc_binary)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("rtf err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".pdf"):
        _ing_dbg("-> pdf (delegating to extract_pdf)") 
        return extract_pdf(data)

    if name.endswith(".json"):
        _ing_dbg("-> json")
        try:
            return json.dumps(json.loads(_utf8(data)), ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("json err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".jsonl", ".jsonlines")):
        _ing_dbg("-> jsonl")
        lines = _utf8(data).splitlines()
        out = []
        for ln in lines:
            ln = ln.strip()
            if not ln:
                co