
# ===== aimodel/file_read/__init__.py =====

from .adaptive.config.paths import app_data_dir, read_settings, write_settings
from .runtime.model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/adaptive/config/adaptive_config.py =====

# aimodel/file_read/runtime/adaptive_config.py
from __future__ import annotations
import os, shutil, subprocess, platform
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

try:
    import psutil
except Exception:
    psutil = None
try:
    import torch
except Exception:
    torch = None

from .paths import read_settings

def _env_bool(k:str, default:bool)->bool:
    v = os.getenv(k)
    if v is None: return default
    return v.strip().lower() in ("1","true","yes","on")

def _cpu_count()->int:
    try:
        import multiprocessing as mp
        return max(1, mp.cpu_count() or os.cpu_count() or 1)
    except Exception:
        return os.cpu_count() or 1

def _avail_ram()->Optional[int]:
    if not psutil: return None
    try: return int(psutil.virtual_memory().available)
    except Exception: return None

def _cuda_vram()->Optional[int]:
    if torch and torch.cuda.is_available():
        try:
            dev = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(dev)
            return int(props.total_memory)
        except Exception:
            pass
    if shutil.which("nvidia-smi"):
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-gpu=memory.total","--format=csv,noheader,nounits"],
                text=True, stderr=subprocess.DEVNULL, timeout=2.0
            )
            mb = max(int(x.strip()) for x in out.strip().splitlines() if x.strip())
            return mb * 1024 * 1024
        except Exception:
            return None
    return None

def _gpu_kind()->str:
    if _cuda_vram(): return "cuda"
    if torch and getattr(torch.backends,"mps",None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

def _safe_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default

def _pick_dtype_quant(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> tuple[Optional[str], Optional[str]]:
    dq = a.get("dtype_quant", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = dq.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype"), best.get("quant")
        return dq.get("cuda_default_dtype"), dq.get("cuda_default_quant")
    if device == "mps":
        return dq.get("mps_default_dtype"), None
    return dq.get("cpu_default_dtype"), dq.get("cpu_default_quant")

def _pick_kv(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> Optional[str]:
    kv = a.get("kv_cache", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = kv.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype")
        return kv.get("cuda_default")
    if device == "mps":
        return kv.get("mps_default")
    return kv.get("cpu_default")

def _pick_capacity(device: str, a: Dict[str, Any], vram_bytes: Optional[int], threads:int) -> tuple[int,int,Optional[int]]:
    cap = a.get("capacity", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = cap.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return int(best.get("seq_len") or 0), int(best.get("batch") or 1), int(best.get("n_gpu_layers") or 0)
        return 0, 1, 0
    if device == "mps":
        m = cap.get("mps", {})
        return int(m.get("seq_len") or 0), int(m.get("batch") or 1), 0
    cpu = cap.get("cpu", {})
    seq_len = int(cpu.get("seq_len") or 0)
    batch = 1
    by = cpu.get("batch_by_threads") or []
    best = None
    for t in sorted(by, key=lambda x: int(x.get("min_threads", 0)), reverse=True):
        if threads >= int(t.get("min_threads") or 0):
            best = t
            break
    if best:
        batch = int(best.get("batch") or 1)
    return seq_len, batch, 0

def _gpu_mem_fraction(device:str, a: Dict[str, Any]) -> float:
    table = a.get("gpu_fraction", {}) if isinstance(a, dict) else {}
    v = table.get(device)
    return _safe_float(v, 0.0)

def _torch_flags(device:str, a: Dict[str, Any]) -> tuple[bool,bool]:
    flags = a.get("flags", {}) if isinstance(a, dict) else {}
    flash = bool(flags.get("enable_flash_attn_cuda")) if device == "cuda" else False
    tc = bool(flags.get("use_torch_compile_on_cuda_linux")) if (device == "cuda" and platform.system().lower()=="linux") else False
    return flash, tc

def _threads(a: Dict[str, Any]) -> tuple[int,int,int,int]:
    policy = a.get("cpu_threads_policy", {}) if isinstance(a, dict) else {}
    mode = str(policy.get("mode") or "").lower()
    ncpu = _cpu_count()
    if mode == "fixed":
        v = int(policy.get("value") or max(1, ncpu-1))
        t = max(1, min(v, ncpu))
    elif mode == "percent":
        pct = _safe_float(policy.get("value"), 0.0)
        t = max(1, min(ncpu, int(round(ncpu*pct/100.0))))
        if t < 1: t = 1
    else:
        t = max(1, ncpu-1)
    intra = t
    inter = max(1, ncpu//2)
    return ncpu, t, intra, inter

@dataclass
class AdaptiveConfig:
    device: str
    dtype: Optional[str]
    quant: Optional[str]
    kv_cache_dtype: Optional[str]
    max_seq_len: int
    max_batch_size: int
    gpu_memory_fraction: float
    cpu_threads: int
    torch_intraop_threads: int
    torch_interop_threads: int
    enable_flash_attn: bool
    use_torch_compile: bool
    total_vram_bytes: Optional[int]
    avail_ram_bytes: Optional[int]
    cpu_count: int
    def as_dict(self)->Dict[str,Any]:
        return asdict(self)

def compute_adaptive_config()->AdaptiveConfig:
    settings = read_settings()
    a = settings.get("adaptive", {}) if isinstance(settings, dict) else {}
    device = _gpu_kind()
    vram = _cuda_vram() if device=="cuda" else None
    ram = _avail_ram()
    ncpu, threads, intra, inter = _threads(a)
    dtype, quant = _pick_dtype_quant(device, a, vram)
    kv = _pick_kv(device, a, vram)
    seq_len, batch, n_gpu_layers = _pick_capacity(device, a, vram, threads)
    frac = _gpu_mem_fraction(device, a)
    flash, tcompile = _torch_flags(device, a)
    return AdaptiveConfig(
        device=device,
        dtype=dtype,
        quant=quant,
        kv_cache_dtype=kv,
        max_seq_len=int(seq_len or 0),
        max_batch_size=int(batch or 1),
        gpu_memory_fraction=frac,
        cpu_threads=threads,
        torch_intraop_threads=intra,
        torch_interop_threads=inter,
        enable_flash_attn=flash,
        use_torch_compile=tcompile,
        total_vram_bytes=vram,
        avail_ram_bytes=ram,
        cpu_count=ncpu
    )

# ===== aimodel/file_read/adaptive/config/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/adaptive/config/settings.json =====

{
  "adaptiveEnabled": false,
  "adaptive": {
    "enabled": true,
    "cpu_threads_policy": {
      "mode": "leave_one"
    },
    "dtype_quant": {
      "cuda_default_dtype": "float16",
      "cuda_default_quant": null,
      "mps_default_dtype": "float16",
      "cpu_default_dtype": "int8",
      "cpu_default_quant": "q4_K_M",
      "cuda_tiers": [
        { "min_vram_gb": 24, "dtype": "bfloat16", "quant": null },
        { "min_vram_gb": 12, "dtype": "float16", "quant": null },
        { "min_vram_gb": 6,  "dtype": "float16", "quant": "bnb-int8" },
        { "min_vram_gb": 4,  "dtype": "float16", "quant": "bnb-int8" }
      ]
    },
    "kv_cache": {
      "cuda_default": "fp8",
      "mps_default": "fp16",
      "cpu_default": "fp32",
      "cuda_tiers": [
        { "min_vram_gb": 16, "dtype": "fp16" },
        { "min_vram_gb": 0,  "dtype": "fp8" }
      ]
    },
    "capacity": {
      "cuda_tiers": [
        { "min_vram_gb": 24, "seq_len": 8192, "batch": 8, "n_gpu_layers": 9999 },
        { "min_vram_gb": 12, "seq_len": 4096, "batch": 4, "n_gpu_layers": 48 },
        { "min_vram_gb": 8,  "seq_len": 3072, "batch": 2, "n_gpu_layers": 40 },
        { "min_vram_gb": 6,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 32 },
        { "min_vram_gb": 4,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 28 }
      ],
      "mps": { "seq_len": 2048, "batch": 1 },
      "cpu": {
        "seq_len": 2048,
        "batch_by_threads": [
          { "min_threads": 16, "batch": 8 },
          { "min_threads": 8,  "batch": 4 },
          { "min_threads": 1,  "batch": 2 }
        ]
      }
    },
    "gpu_fraction": {
      "cuda": 0.8,
      "mps": 0.7,
      "cpu": 0.0
    },
    "flags": {
      "enable_flash_attn_cuda": true,
      "use_torch_compile_on_cuda_linux": true
    },
    "batchTokenMap": [
      { "minConcurrency": 8, "n_batch": 512 },
      { "minConcurrency": 4, "n_batch": 384 },
      { "minConcurrency": 2, "n_batch": 256 },
      { "minConcurrency": 1, "n_batch": 192 }
    ]
  },
  "modelPath": "",
  "nCtx": null,
  "nThreads": null,
  "nGpuLayers": null,
  "nBatch": null
}

# ===== aimodel/file_read/adaptive/controller.py =====



# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/chats.py =====

# ===== aimodel/file_read/api/chats.py =====

from __future__ import annotations
from dataclasses import asdict
from typing import List, Optional, Dict
from ..utils.streaming import strip_runjson
from fastapi import APIRouter
from pydantic import BaseModel
from ..services.cancel import GEN_SEMAPHORE
from ..workers.retitle_worker import enqueue as enqueue_retitle  # ✅ import the enqueuer

from ..core.schemas import (
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    MergeChatReq,
    EditMessageReq,
)

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    merge_chat as store_merge_chat,
    merge_chat_new as store_merge_chat_new,
    edit_message as edit_message,
)

router = APIRouter()

# ---------- Routes ----------
@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)

@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]

# ✅ Single append route; triggers retitle only after assistant turn is persisted
@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, body: Dict[str, str]):
    role = (body.get("role") or "user").strip()
    # 🔴 was: content = strip_runjson(...)
    content = (body.get("content") or "").rstrip()
    row = store_append(session_id, role, content)

    if role == "assistant":
        # Retitle should *not* see RUNJSON blobs
        try:
            msgs = store_list_messages(session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            msgs_clean = []
            for m in msgs:
                dm = asdict(m)
                dm["content"] = strip_runjson(dm.get("content") or "")
                msgs_clean.append(dm)
            enqueue_retitle(session_id, msgs_clean, job_seq=last_seq)
        except Exception:
            pass

    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}

@router.post("/api/chats/merge")
async def api_merge_chat(req: MergeChatReq):
    if req.newChat:
        new_id, merged = store_merge_chat_new(req.sourceId, req.targetId)
        return {"newChatId": new_id, "mergedCount": len(merged)}
    else:
        merged = store_merge_chat(req.sourceId, req.targetId)
        return {"mergedCount": len(merged)}

@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

# aimodel/file_read/api/generate_router.py
from __future__ import annotations
from fastapi import APIRouter, Body, Request
from ..core.schemas import ChatBody
from ..services.generate_flow import generate_stream_flow, cancel_session, cancel_session_alias
from ..services.cancel import is_active  # re-export for back-compat

router = APIRouter()

@router.post("/generate/stream")
async def generate_stream(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

# legacy alias (kept identical)
@router.post("/api/ai/generate/stream")
async def generate_stream_alias(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

@router.post("/cancel/{session_id}")
async def _cancel_session(session_id: str):
    return await cancel_session(session_id)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/models.py =====

from __future__ import annotations
from typing import Optional, Dict
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..adaptive.config.paths import read_settings, write_settings
from ..runtime.model_runtime import (
    list_local_models, current_model_info,
    load_model, unload_model
)

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/api/rag.py =====

# aimodel/file_read/api/rag.py
from __future__ import annotations
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from typing import Optional, List
import numpy as np
from sentence_transformers import SentenceTransformer
from threading import RLock

from ..rag.schemas import SearchReq, SearchHit
from ..rag.ingest import sniff_and_extract, chunk_text, build_metas
from ..rag.store import add_vectors, search_vectors

router = APIRouter(prefix="/api/rag", tags=["rag"])

# ---- embedding model (sync, cached) ----
_st_model: SentenceTransformer | None = None
_st_lock = RLock()

def _get_st_model() -> SentenceTransformer:
    global _st_model
    if _st_model is None:
        with _st_lock:
            if _st_model is None:
                print("[RAG EMBED] loading e5-small-v2… (one-time)")
                _st_model = SentenceTransformer("intfloat/e5-small-v2")
                print("[RAG EMBED] model ready")
    return _st_model

def _embed(texts: List[str]) -> np.ndarray:
    model = _get_st_model()
    arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
    return arr.astype("float32")

@router.post("/upload")
async def upload_doc(sessionId: Optional[str] = Form(default=None), file: UploadFile = File(...)):
    print(f"[RAG UPLOAD] sessionId={sessionId}, filename={file.filename}, content_type={file.content_type}")

    data = await file.read()
    print(f"[RAG UPLOAD] file size={len(data)} bytes")

    text, mime = sniff_and_extract(file.filename, data)
    print(f"[RAG UPLOAD] extracted mime={mime}, text_len={len(text)}")

    if not text.strip():
        raise HTTPException(status_code=400, detail="Empty/unsupported file")

    chunks = chunk_text(text, {"mime": mime})
    print(f"[RAG UPLOAD] chunk_count={len(chunks)}")

    metas = build_metas(sessionId, file.filename, chunks, size=len(data))
    embeds = _embed([c.text for c in chunks])
    print(f"[RAG UPLOAD] embed_shape={embeds.shape}")

    add_vectors(sessionId, embeds, metas, dim=embeds.shape[1])
    return {"ok": True, "added": len(chunks)}

@router.post("/search")
async def search(req: SearchReq):
    q = req.query.strip()
    if not q:
        return {"hits": []}
    qv = np.array(_embed([q])[0], dtype="float32")
    chat_hits = search_vectors(req.sessionId, qv, req.kChat, dim=qv.shape[0])
    global_hits = search_vectors(None, qv, req.kGlobal, dim=qv.shape[0])
    from ..rag.search import merge_chat_first
    fused = merge_chat_first(chat_hits, global_hits, alpha=req.hybrid_alpha)
    out: List[SearchHit] = []
    for r in fused:
        out.append(SearchHit(
            id=r["id"], text=r["text"], score=float(r["score"]),
            source=r.get("source"), title=r.get("title"), sessionId=r.get("sessionId")
        ))
    return {"hits": [h.model_dump() for h in out]}

@router.get("/list")
async def list_items(sessionId: Optional[str] = None, k: int = 20):
    """
    Debug: show a lightweight list of stored chunks for a session (or global if sessionId is None).
    Returns id/source/score and a short text preview so you can verify ingestion.
    """
    qv = np.array(_embed(["list"])[0], dtype="float32")
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    items = []
    for h in hits:
        txt = (h.get("text") or "")
        items.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": txt,
        })
    print(f"[RAG LIST] sessionId={sessionId} k={k} -> {len(items)} items")
    return {"items": items}

@router.get("/dump")
async def dump_items(sessionId: Optional[str] = None, k: int = 50):
    """
    Debug: return full texts of top-k chunks (be careful: can be large).
    Useful to confirm exact strings that were indexed.
    """
    qv = np.array(_embed(["dump"])[0], dtype="float32")
    hits = search_vectors(sessionId, qv, k=k, dim=qv.shape[0])

    chunks = []
    for h in hits:
        chunks.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": h.get("text") or "",
        })
    print(f"[RAG DUMP] sessionId={sessionId} k={k} -> {len(chunks)} items")
    return {"chunks": chunks}

# ===== aimodel/file_read/api/settings.py =====

from fastapi import APIRouter, Query, Body
from typing import Dict, Any, Optional

from ..core.settings import SETTINGS

router = APIRouter(prefix="/api/settings", tags=["settings"])

@router.get("/defaults")
def get_defaults():
    return SETTINGS.defaults

@router.get("/overrides")
def get_overrides():
    return SETTINGS.overrides

@router.patch("/overrides")
def patch_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.patch_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.put("/overrides")
def put_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.replace_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.get("/adaptive")
def get_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.adaptive(session_id=session_id)

@router.post("/adaptive/recompute")
def recompute_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    SETTINGS.recompute_adaptive(session_id=session_id)
    return {"ok": True, "adaptive": SETTINGS.adaptive(session_id=session_id)}

@router.get("/effective")
def get_effective(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.effective(session_id=session_id)

# ===== aimodel/file_read/app.py =====

# aimodel/file_read/app.py
from __future__ import annotations
import os, asyncio, atexit
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .adaptive.config.paths import bootstrap
from .workers.retitle_worker import start_worker
from .api.models import router as models_router
from .api.chats import router as chats_router
from .runtime.model_runtime import load_model
from .api.generate_router import router as generate_router
from .api.rag import router as rag_router
from .services.cancel import is_active
from .api import settings as settings_router
bootstrap()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("APP_CORS_ORIGIN", "http://localhost:5173")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(models_router)
app.include_router(chats_router)
app.include_router(generate_router)
app.include_router(settings_router.router)
app.include_router(rag_router)

@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")

    asyncio.create_task(start_worker(), name="retitle_worker")

# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/files.py =====

# aimodel/file_read/core/files.py
from __future__ import annotations
from pathlib import Path
import json, os
from typing import Any

# Base dirs
CORE_DIR = Path(__file__).resolve().parent
STORE_DIR = CORE_DIR.parent / "store"

# Paths (overridable via env)
EFFECTIVE_SETTINGS_FILE = Path(
    os.getenv("EFFECTIVE_SETTINGS_PATH", str(STORE_DIR / "effective_settings.json"))
)
OVERRIDES_SETTINGS_FILE = Path(
    os.getenv("OVERRIDES_SETTINGS_PATH", str(STORE_DIR / "override_settings.json"))
)
DEFAULTS_SETTINGS_FILE = Path(
    os.getenv("DEFAULT_SETTINGS_PATH", str(STORE_DIR / "default_settings.json"))
)

def load_json_file(path: Path, default: Any = None) -> Any:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {} if default is None else default

def save_json_file(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ===== aimodel/file_read/core/memory.py =====

from __future__ import annotations
import math, time, json
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque

from ..runtime.model_runtime import get_llm
from .style import STYLE_SYS
from ..store import get_summary as store_get_summary
from ..store import list_messages as store_list_messages
from ..utils.streaming import strip_runjson
from ..core.files import EFFECTIVE_SETTINGS_FILE, load_json_file

SESSIONS: Dict[str, Dict] = {}

def _now() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

def _log(msg: str) -> None:
    print(f"[{_now()}] {msg}")

def _snapshot(cfg: Dict) -> str:
    """Short human-readable snapshot of the most important knobs."""
    keys = [
        "model_ctx", "out_budget", "reserved_system_tokens", "min_input_budget",
        "chars_per_token", "prompt_per_message_overhead",
        "summary_max_chars", "use_fast_summary",
    ]
    parts = []
    for k in keys:
        if k in cfg:
            parts.append(f"{k}={cfg[k]}")
    return ", ".join(parts)

class _SettingsCache:
    def __init__(self) -> None:
        self.path: Path = EFFECTIVE_SETTINGS_FILE  # central path
        self._mtime: float | None = None
        self._data: Dict = {}
        _log(f"settings path = {self.path}")

    def get(self) -> Dict:
        """Load EFFECTIVE settings (defaults ⟶ adaptive ⟶ overrides)."""
        try:
            m = self.path.stat().st_mtime
        except FileNotFoundError:
            m = None

        if self._mtime != m or not self._data:
            self._data = load_json_file(self.path, default={})
            self._mtime = m
            _log(f"settings reload ok file={self.path.name} snapshot: {_snapshot(self._data)}")
        return self._data

_SETTINGS = _SettingsCache()

def approx_tokens(text: str) -> int:
    cfg = _SETTINGS.get()
    return max(1, math.ceil(len(text) / int(cfg["chars_per_token"])))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    cfg = _SETTINGS.get()
    overhead = int(cfg["prompt_per_message_overhead"])
    return sum(approx_tokens(m["content"]) + overhead for m in msgs)

def get_session(session_id: str):
    cfg = _SETTINGS.get()
    _log(f"get_session IN session={session_id} (settings: {_snapshot(cfg)})")
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=int(cfg["recent_maxlen"])),
        "style": STYLE_SYS,
        "short": False,
        "bullets": False,
    })
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
            _log(f"get_session loaded summary len={len(st['summary'])}")
        except Exception as e:
            _log(f"get_session summary load error {e}")
    if not st["recent"]:
        try:
            rows = store_list_messages(session_id)
            tail = rows[-st["recent"].maxlen:]
            for m in tail:
                st["recent"].append({"role": m.role, "content": strip_runjson(m.content)})
            _log(f"get_session hydrated recent={len(st['recent'])}")
        except Exception as e:
            _log(f"get_session hydrate error {e}")
    return st

def _heuristic_bullets(chunks: List[Dict[str,str]], cfg: Dict) -> str:
    max_bullets = int(cfg["heuristic_max_bullets"])
    max_words = int(cfg["heuristic_max_words"])
    prefix = cfg["bullet_prefix"]
    bullets = []
    for m in chunks:
        txt = " ".join((m.get("content") or "").split())
        if not txt:
            continue
        words = txt.replace("\n", " ").split()
        snippet = " ".join(words[:max_words]) if words else ""
        bullets.append(f"{prefix}{snippet}" if snippet else prefix.strip())
        if len(bullets) >= max_bullets:
            break
    return "\n".join(bullets) if bullets else prefix.strip()

def summarize_chunks(chunks: List[Dict[str,str]]) -> Tuple[str, bool]:
    cfg = _SETTINGS.get()
    t0 = time.time()
    use_fast = bool(cfg["use_fast_summary"])
    _log(f"summarize_chunks IN chunks={len(chunks)} FAST={use_fast}")
    if use_fast:
        txt = _heuristic_bullets(chunks, cfg)
        dt = time.time() - t0
        _log(f"summarize_chunks OUT (FAST) bullets={len([l for l in txt.splitlines() if l])} chars={len(txt)} dt={dt:.2f}s")
        return txt, False
    text = "\n".join(f'{m.get("role","")}: {m.get("content","")}' for m in chunks)
    sys_inst = cfg["summary_sys_inst"]
    user_prompt = cfg["summary_user_prefix"] + text + cfg["summary_user_suffix"]
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": sys_inst},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=int(cfg["llm_summary_max_tokens"]),
        temperature=float(cfg["llm_summary_temperature"]),
        top_p=float(cfg["llm_summary_top_p"]),
        stream=False,
        stop=list(cfg["llm_summary_stop"]),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip()
    lines = [ln.strip() for ln in raw.splitlines()]
    bullets: List[str] = []
    seen = set()
    max_words = int(cfg["heuristic_max_words"])
    max_bullets = int(cfg["heuristic_max_bullets"])
    for ln in lines:
        if not ln.startswith(cfg["bullet_prefix"]):
            continue
        norm = " ".join(ln[len(cfg["bullet_prefix"]):].lower().split())
        if not norm or norm in seen:
            continue
        seen.add(norm)
        words = ln[len(cfg["bullet_prefix"]):].split()
        if len(words) > max_words:
            ln = cfg["bullet_prefix"] + " ".join(words[:max_words])
        bullets.append(ln)
        if len(bullets) >= max_bullets:
            break
    if bullets:
        txt = "\n".join(bullets)
        dt = time.time() - t0
        _log(f"summarize_chunks OUT bullets={len(bullets)} chars={len(txt)} dt={dt:.2f}s")
        return txt, True
    s = " ".join(raw.split())[:160]
    fallback = (cfg["bullet_prefix"] + s) if s else cfg["bullet_prefix"].strip()
    dt = time.time() - t0
    _log(f"summarize_chunks OUT bullets=0 chars={len(fallback)} dt={dt:.2f}s (FALLBACK)")
    return fallback, True

def _compress_summary_block(s: str) -> str:
    cfg = _SETTINGS.get()
    max_chars = int(cfg["summary_max_chars"])
    prefix = cfg["bullet_prefix"]
    lines = [ln.strip() for ln in (s or "").splitlines()]
    out, seen = [], set()
    for ln in lines:
        if not ln.startswith(prefix):
            continue
        norm = " ".join(ln[len(prefix):].lower().split())
        if norm in seen:
            continue
        seen.add(norm)
        out.append(ln)
    text = "\n".join(out)
    _log(f"compress_summary IN chars={len(s)} kept_lines={len(out)}")
    if len(text) > max_chars:
        last, total = [], 0
        for ln in reversed(out):
            if total + len(ln) + 1 > max_chars:
                break
            last.append(ln)
            total += len(ln) + 1
        text = "\n".join(reversed(last))
    _log(f"compress_summary OUT chars={len(text)} lines={len(text.splitlines())}")
    return text

def build_system(style: str, short: bool, bullets: bool) -> str:
    cfg = _SETTINGS.get()
    _log(f"build_system flags short={short} bullets={bullets}")
    parts = [STYLE_SYS]
    if style and style != STYLE_SYS:
        parts.append(style)
    if short:
        parts.append(cfg["system_brief_directive"])
    if bullets:
        parts.append(cfg["system_bullets_directive"])
    parts.append(cfg["system_follow_user_style_directive"])
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    cfg = _SETTINGS.get()
    model_ctx = int(max_ctx or cfg["model_ctx"])
    gen_budget = int(out_budget or cfg["out_budget"])
    reserved = int(cfg["reserved_system_tokens"])
    input_budget = model_ctx - gen_budget - reserved
    if input_budget < int(cfg["min_input_budget"]):
        input_budget = int(cfg["min_input_budget"])
    sys_text = build_system(style, short, bullets)
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": cfg["summary_header_prefix"] + summary})
    packed = prologue + list(recent)
    _log(f"pack_messages SETTINGS snapshot: {_snapshot(cfg)}")
    _log(f"pack_messages OUT msgs={len(packed)} tokens~{count_prompt_tokens(packed)} "
         f"(model_ctx={model_ctx}, out_budget={gen_budget}, input_budget={input_budget})")
    return packed, input_budget

def _final_safety_trim(packed: List[Dict[str,str]], input_budget: int) -> List[Dict[str,str]]:
    cfg = _SETTINGS.get()
    keep_ratio = float(cfg["final_shrink_summary_keep_ratio"])
    min_keep = int(cfg["final_shrink_summary_min_chars"])
    def toks() -> int:
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    _log(f"final_trim START tokens={toks()} budget={input_budget}")
    keep_head = 2 if len(packed) >= 2 and isinstance(packed[1].get("content"), str) and packed[1]["content"].startswith(cfg["summary_header_prefix"]) else 1
    while toks() > input_budget and len(packed) > keep_head + 1:
        dropped = packed.pop(keep_head)
        _log(f"final_trim DROP msg role={dropped['role']} size~{approx_tokens(dropped['content'])} toks={toks()}")
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        summary_msg = packed[1]
        txt = summary_msg["content"]
        n = max(min_keep, int(len(txt) * keep_ratio))
        summary_msg["content"] = txt[-n:]
        _log(f"final_trim SHRINK summary to {len(summary_msg['content'])} chars toks={toks()}")
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        removed = packed.pop(1)
        _log(f"final_trim REMOVE summary len~{len(removed['content'])} toks={toks()}")
    while toks() > input_budget and len(packed) > 2:
        removed = packed.pop(2 if len(packed) > 3 else 1)
        _log(f"final_trim LAST_RESORT drop size~{approx_tokens(removed['content'])} toks={toks()}")
    _log(f"final_trim END tokens={toks()} msgs={len(packed)}")
    return packed

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    cfg = _SETTINGS.get()

    # --- DEBUG SNAPSHOT ---
    _log("=== roll_summary_if_needed DEBUG START ===")
    _log(f"skip_overage_lt={cfg['skip_overage_lt']}, "
         f"max_peel_per_turn={cfg['max_peel_per_turn']}, "
         f"peel_min={cfg['peel_min']}, "
         f"peel_frac={cfg['peel_frac']}, "
         f"peel_max={cfg['peel_max']}")
    _log(f"len(recent)={len(recent)}, current_summary_len={len(summary) if summary else 0}")
    _log(f"input_budget={input_budget}, reserved_system_tokens={cfg['reserved_system_tokens']}")
    _log(f"model_ctx={cfg['model_ctx']}, out_budget={cfg['out_budget']}")
    # ----------------------

    def _tok():
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999

    start_tokens = _tok()
    overage = start_tokens - input_budget
    _log(f"roll_summary_if_needed START tokens={start_tokens} "
         f"input_budget={input_budget} overage={overage}")

    if overage <= int(cfg["skip_overage_lt"]):
        _log(f"roll_summary_if_needed SKIP (overage {overage} <= {cfg['skip_overage_lt']})")
        packed = _final_safety_trim(packed, input_budget)
        return packed, summary

    peels_done = 0
    if len(recent) > 6 and peels_done < int(cfg["max_peel_per_turn"]):
        peel_min = int(cfg["peel_min"])
        peel_frac = float(cfg["peel_frac"])
        peel_max = int(cfg["peel_max"])
        target = max(peel_min, min(peel_max, int(len(recent) * peel_frac)))
        peel = []
        for _ in range(min(target, len(recent))):
            peel.append(recent.popleft())
        _log(f"roll_summary peeled={len(peel)}")
        new_sum, _used_llm = summarize_chunks(peel)
        if new_sum.startswith(cfg["bullet_prefix"]):
            summary = (summary + "\n" + new_sum).strip() if summary else new_sum
        else:
            summary = new_sum
        summary = _compress_summary_block(summary)
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": cfg["summary_header_prefix"] + summary},
            *list(recent),
        ]
        _log(f"roll_summary updated summary_len={len(summary)} tokens={_tok()}")

    packed = _final_safety_trim(packed, input_budget)
    _log(f"roll_summary_if_needed END tokens={count_prompt_tokens(packed)}")
    _log("=== roll_summary_if_needed DEBUG END ===")
    return packed, summary

# ===== aimodel/file_read/core/schemas.py =====

# core/schemas.py
from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class MergeChatReq(BaseModel):
    sourceId: str
    targetId: Optional[str] = None
    newChat: bool = False

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None

    # ↓ make optional; defaults come from effective settings
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None

    # ↓ also optional; defaults come from settings
    autoWeb: Optional[bool] = None
    webK: Optional[int] = None

# ===== aimodel/file_read/core/settings.py =====

# aimodel/file_read/core/settings.py
from __future__ import annotations

import json
from threading import RLock
from typing import Any, Dict, Optional

from .files import (
    DEFAULTS_SETTINGS_FILE,
    OVERRIDES_SETTINGS_FILE,
    EFFECTIVE_SETTINGS_FILE,
    load_json_file,
    save_json_file,
)


def _deep_merge(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(dst)
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)  # type: ignore[arg-type]
        else:
            out[k] = v
    return out


class _SettingsManager:
    """
    layers: defaults → adaptive(session/_global_) → overrides
    also persists the merged *global* effective to EFFECTIVE_SETTINGS_FILE
    (that’s what memory.py watches/loads)

    Dynamic access:
      - Attribute style: SETTINGS.stream_queue_maxsize
      - Dict style:      SETTINGS["stream_queue_maxsize"]
      - Safe get:        SETTINGS.get("stream_queue_maxsize", 64)

    Only keys present in the merged effective map are exposed dynamically.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._defaults: Dict[str, Any] = self._load_defaults()
        self._overrides: Dict[str, Any] = self._load_overrides()
        self._adaptive_by_session: Dict[str, Dict[str, Any]] = {}
        # write initial effective so memory.py has something on boot
        self._persist_effective_unlocked()

    # ---------- loading ----------
    def _load_defaults(self) -> Dict[str, Any]:
        return load_json_file(DEFAULTS_SETTINGS_FILE, default={})

    def _load_overrides(self) -> Dict[str, Any]:
        return load_json_file(OVERRIDES_SETTINGS_FILE, default={})

    # ---------- saving ----------
    def _save_overrides_unlocked(self) -> None:
        save_json_file(OVERRIDES_SETTINGS_FILE, self._overrides)

    def _persist_effective_unlocked(self) -> None:
        # Persist *global* effective (session-less) for memory.py
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get("_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        save_json_file(EFFECTIVE_SETTINGS_FILE, eff)

    # ---------- helpers ----------
    def _effective_unlocked(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get(session_id or "_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        return eff

    def _get_unlocked(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        eff = self._effective_unlocked(session_id)
        if key in eff:
            return eff[key]
        if default is not None:
            return default
        raise AttributeError(f"_SettingsManager has no key '{key}'")

    # ---------- public read API ----------
    @property
    def defaults(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._defaults))

    @property
    def overrides(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._overrides))

    def adaptive(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        key = session_id or "_global_"
        with self._lock:
            return json.loads(json.dumps(self._adaptive_by_session.get(key, {})))

    def effective(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        with self._lock:
            return self._effective_unlocked(session_id)

    # Dynamic attribute access (e.g., SETTINGS.stream_queue_maxsize)
    def __getattr__(self, name: str) -> Any:
        # Only called if normal attributes/methods aren't found
        with self._lock:
            return self._get_unlocked(name)

    # Dict-style access (e.g., SETTINGS["stream_queue_maxsize"])
    def __getitem__(self, key: str) -> Any:
        with self._lock:
            return self._get_unlocked(key)

    # Safe getter (optional default)
    def get(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        with self._lock:
            try:
                return self._get_unlocked(key, default=default, session_id=session_id)
            except AttributeError:
                return default

    # ---------- public write API ----------
    def patch_overrides(self, patch: Dict[str, Any]) -> None:
        if not isinstance(patch, dict):
            return
        with self._lock:
            self._overrides = _deep_merge(self._overrides, patch)
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def replace_overrides(self, new_overrides: Dict[str, Any]) -> None:
        if not isinstance(new_overrides, dict):
            new_overrides = {}
        with self._lock:
            self._overrides = json.loads(json.dumps(new_overrides))
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def reload_overrides(self) -> None:
        with self._lock:
            self._overrides = self._load_overrides()
            self._persist_effective_unlocked()

    def set_adaptive_for_session(self, session_id: Optional[str], values: Dict[str, Any]) -> None:
        key = session_id or "_global_"
        if not isinstance(values, dict):
            values = {}
        with self._lock:
            self._adaptive_by_session[key] = json.loads(json.dumps(values))
            # If updating the global adaptive layer, refresh persisted effective
            if key == "_global_":
                self._persist_effective_unlocked()

    def recompute_adaptive(self, session_id: Optional[str] = None) -> None:
        # placeholder for your controller logic later
        with self._lock:
            # after recompute, also refresh persisted effective for global
            self._persist_effective_unlocked()


SETTINGS = _SettingsManager()

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple

STYLE_SYS = (
    "You are a helpful assistant. "
    "Always follow the user's explicit instructions carefully and exactly. "
    "Do not repeat yourself. Stay coherent and complete."
)

PAT_TALK_LIKE = re.compile(r"\btalk\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_RESPOND_LIKE = re.compile(r"\brespond\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_BE = re.compile(r"\bbe\s+(?P<style>[^.;\n]+)", re.I)
PAT_FROM_NOW = re.compile(r"\bfrom\s+now\s+on[, ]+\s*(?P<style>[^.;\n]+)", re.I)

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    t = user_text.strip()
    style_match = (
        PAT_TALK_LIKE.search(t)
        or PAT_RESPOND_LIKE.search(t)
        or PAT_FROM_NOW.search(t)
        or PAT_BE.search(t)
    )
    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = (
            f"You must talk like {raw}. "
            f"Stay in character but remain helpful and accurate. "
            f"Follow the user's latest style instructions."
        )
    return style_inst, False, False

# ===== aimodel/file_read/rag/ingest/__init__.py =====

from __future__ import annotations
from typing import Tuple
import io, json

from .excel_ingest import extract_excel
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas

__all__ = ["sniff_and_extract", "Chunk", "chunk_text", "build_metas"]

def sniff_and_extract(filename: str, data: bytes) -> Tuple[str, str]:
    """
    Dispatcher that returns (text, mime) for many file types.
    Excel (.xlsx) is handled by excel_ingest.extract_excel for rich structure.
    """
    name = (filename or "").lower()

    if name.endswith(".xlsx"):
        return extract_excel(data)

    if name.endswith(".docx"):
        try:
            from docx import Document  # python-docx
            doc = Document(io.BytesIO(data))
            paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]
            return "\n".join(paras).strip(), "text/plain"
        except Exception:
            return _utf8(data), "text/plain"

    if name.endswith(".pdf"):
        try:
            from pdfminer.high_level import extract_text
            txt = extract_text(io.BytesIO(data)) or ""
            return txt.strip(), "text/plain"
        except Exception:
            try:
                from PyPDF2 import PdfReader
                r = PdfReader(io.BytesIO(data))
                pages = [(p.extract_text() or "").strip() for p in r.pages]
                return "\n\n".join([p for p in pages if p]).strip(), "text/plain"
            except Exception:
                return _utf8(data), "text/plain"

    if name.endswith(".json"):
        try:
            return json.dumps(json.loads(_utf8(data)), ensure_ascii=False, indent=2), "text/plain"
        except Exception:
            return _utf8(data), "text/plain"

    if name.endswith((".jsonl", ".jsonlines")):
        lines = _utf8(data).splitlines()
        out = []
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            try:
                out.append(json.dumps(json.loads(ln), ensure_ascii=False, indent=2))
            except Exception:
                out.append(ln)
        return "\n".join(out).strip(), "text/plain"

    if name.endswith((".yaml", ".yml")):
        try:
            import yaml
            obj = yaml.safe_load(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception:
            return _utf8(data), "text/plain"

    if name.endswith(".toml"):
        try:
            try:
                import tomllib
                obj = tomllib.loads(_utf8(data))
            except Exception:
                import toml
                obj = toml.loads(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception:
            return _utf8(data), "text/plain"

    if name.endswith((".csv", ".tsv")):
        return _utf8(data), "text/plain"

    if name.endswith((".htm", ".html", ".xml")):
        return _strip_html(_utf8(data)), "text/plain"

    # plaintext / code-ish
    if name.endswith((
        ".txt", ".log", ".md",
        ".c", ".cpp", ".h", ".hpp",
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".sh", ".ps1",
        ".rs", ".java", ".go", ".rb", ".php",
        ".swift", ".kt", ".scala", ".lua", ".perl",
    )):
        return _utf8(data), "text/plain"

    # default
    return _utf8(data), "text/plain"

# ===== aimodel/file_read/rag/ingest/common.py =====

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Optional
import re
from ...core.settings import SETTINGS

@dataclass
class Chunk:
    text: str
    meta: Dict[str, str]

def _utf8(data: bytes) -> str:
    return data.decode("utf-8", errors="ignore")

def _strip_html(txt: str) -> str:
    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", txt)
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p>", "\n\n", txt)
    txt = re.sub(r"(?is)<.*?>", " ", txt)
    txt = re.sub(r"[ \t]+", " ", txt)
    return txt.strip()

_WHITESPACE_RE = re.compile(r"\s+")
_PARA_SPLIT_RE = re.compile(r"\n\s*\n+")

def _split_paragraphs(text: str) -> List[str]:
    paras = [p.strip() for p in _PARA_SPLIT_RE.split(text)]
    return [p for p in paras if p]

def _split_hard(text: str, max_len: int) -> List[str]:
    approx = re.split(r"(?<=[\.\!\?\;])\s+", text)
    out: List[str] = []
    buf = ""
    for s in approx:
        if not s:
            continue
        if len(buf) + 1 + len(s) <= max_len:
            buf = s if not buf else (buf + " " + s)
        else:
            if buf:
                out.append(buf)
            if len(s) <= max_len:
                out.append(s)
            else:
                words = _WHITESPACE_RE.split(s)
                cur = ""
                for w in words:
                    if not w:
                        continue
                    if len(cur) + 1 + len(w) <= max_len:
                        cur = w if not cur else (cur + " " + w)
                    else:
                        if cur:
                            out.append(cur)
                        cur = w
                if cur:
                    out.append(cur)
            buf = ""
    if buf:
        out.append(buf)
    return out

def chunk_text(
    text: str,
    meta: Optional[Dict[str, str]] = None,
    *,
    max_chars=int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    overlap=int(SETTINGS.get("rag_chunk_overlap_chars", 150)),
) -> List[Chunk]:
    meta = meta or {}
    text = text.strip()
    if not text:
        return []

    if len(text) <= max_chars:
        return [Chunk(text=text, meta=meta.copy())]

    paragraphs = _split_paragraphs(text) or [text]

    normalized: List[str] = []
    for p in paragraphs:
        if len(p) <= max_chars:
            normalized.append(p)
        else:
            normalized.extend(_split_hard(p, max_chars))

    chunks: List[Chunk] = []
    cur: List[str] = []
    cur_len = 0

    for piece in normalized:
        plen = len(piece)
        if cur_len == 0:
            cur, cur_len = [piece], plen
            continue
        if cur_len + 1 + plen <= max_chars:
            cur.append(piece)
            cur_len += 1 + plen
        else:
            joined = "\n".join(cur).strip()
            if joined:
                chunks.append(Chunk(text=joined, meta=meta.copy()))
            if overlap > 0 and joined:
                tail = joined[-overlap:]
                cur, cur_len = [tail, piece], len(tail) + 1 + plen
            else:
                cur, cur_len = [piece], plen

    if cur_len:
        joined = "\n".join(cur).strip()
        if joined:
            chunks.append(Chunk(text=joined, meta=meta.copy()))

    return chunks

def build_metas(session_id: Optional[str], filename: str, chunks: List[Chunk], *, size: int = 0) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for i, c in enumerate(chunks):
        out.append({
            "id": f"{filename}:{i}",
            "sessionId": session_id or "",
            "source": filename,
            "title": filename,
            "mime": "text/plain",
            "size": str(size),
            "chunkIndex": str(i),
            "text": c.text,  # stored for RAG block display
        })
    return out

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====

from __future__ import annotations
from typing import Tuple, List
import io, re
from datetime import datetime, date, time
from ...core.settings import SETTINGS

def S(key: str):
    return SETTINGS.effective()[key]

_WS_RE = re.compile(r"[ \t]+")

def _squeeze_spaces(s: str) -> str:
    return _WS_RE.sub(" ", s).strip()

def extract_excel(data: bytes) -> Tuple[str, str]:
    from openpyxl import load_workbook
    from openpyxl.utils import range_boundaries, get_column_letter
    from openpyxl.worksheet.worksheet import Worksheet

    sig = int(S("excel_number_sigfigs"))
    maxp = int(S("excel_decimal_max_places"))
    trim = bool(S("excel_trim_trailing_zeros"))
    drop_midnight = bool(S("excel_dates_drop_time_if_midnight"))
    time_prec = str(S("excel_time_precision"))
    max_chars = int(S("excel_value_max_chars"))
    quote_strings = bool(S("excel_quote_strings"))

    # NEW: feature flag (defaults to True if missing to preserve old behavior)
    try:
        include_formulas = bool(S("excel_include_formulas"))
    except Exception:
        include_formulas = True

    def clip(s: str) -> str:
        return s if (max_chars <= 0 or len(s) <= max_chars) else s[:max_chars] + "…"

    def fmt_number(v) -> str:
        s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        if "e" in s or "E" in s:
            try:
                s = f"{float(s):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(s)
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    wb_vals = load_workbook(io.BytesIO(data), data_only=True, read_only=True)
    # Only load the formulas workbook if we actually plan to read formulas
    wb_form = load_workbook(io.BytesIO(data), data_only=False, read_only=True) if include_formulas else None

    MAX_ROWS_PER_TABLE = int(S("excel_max_rows_per_table"))
    MAX_FORMULAS_PER_SHEET = int(S("excel_max_formulas_per_sheet"))
    MAX_CELLS_PER_SHEET = int(S("excel_max_cells_per_sheet"))
    MAX_NR_PREVIEW = int(S("excel_named_range_preview"))
    EMIT_TABLES = bool(S("excel_emit_tables"))
    EMIT_MERGED = bool(S("excel_emit_merged"))
    EMIT_CELLS = bool(S("excel_emit_cells"))

    lines: List[str] = []

    try:
        if getattr(wb_vals, "defined_names", None):
            nr_out: List[str] = []
            for dn in wb_vals.defined_names.definedName:
                if getattr(dn, "hidden", False):
                    continue
                try:
                    dests = list(dn.destinations)
                except Exception:
                    dests = []
                if not dests:
                    continue
                for sheet_name, ref in dests:
                    try:
                        ws = wb_vals[sheet_name]
                        min_c, min_r, max_c, max_r = range_boundaries(ref)
                        vals: List[str] = []
                        ws_cell = ws.cell
                        gl = get_column_letter
                        for r in range(min_r, max_r + 1):
                            for c in range(min_c, max_c + 1):
                                v = ws_cell(row=r, column=c).value
                                vv = fmt_val(v)
                                if vv:
                                    vals.append(f"{gl(c)}{r}={vv}")
                                if len(vals) >= MAX_NR_PREVIEW:
                                    break
                            if len(vals) >= MAX_NR_PREVIEW:
                                break
                        vals_str = "; ".join(vals)
                        if vals_str:
                            nr_out.append(f"- {dn.name}: {sheet_name}!{ref} = {vals_str}")
                        else:
                            nr_out.append(f"- {dn.name}: {sheet_name}!{ref}")
                    except Exception:
                        nr_out.append(f"- {dn.name}: {sheet_name}!{ref}")
            if nr_out:
                lines.append("## Named Ranges")
                lines.extend(nr_out)
                lines.append("")
    except Exception:
        pass

    col_letter_cache = {}

    for sheet_name in wb_vals.sheetnames:
        ws_v: Worksheet = wb_vals[sheet_name]
        ws_f: Worksheet | None = wb_form[sheet_name] if wb_form else None
        lines.append(f"# Sheet: {sheet_name}")

        formulas_emitted = 0

        if EMIT_TABLES:
            try:
                tables = dict(getattr(ws_v, "tables", {}) or {})
                if tables:
                    lines.append("## Tables")
                    for tname, tobj in tables.items():
                        ref = getattr(tobj, "ref", "")
                        lines.append(f"- Table {tname} ({ref})")
                        try:
                            min_c, min_r, max_c, max_r = range_boundaries(ref)
                        except Exception:
                            min_r = 1; min_c = 1
                            max_r = ws_v.max_row or 1
                            max_c = ws_v.max_column or 1
                        headers: List[str] = []
                        ws_v_cell = ws_v.cell
                        ws_f_cell = ws_f.cell if ws_f else None
                        gl = get_column_letter
                        for c in range(min_c, max_c + 1):
                            v = ws_v_cell(row=min_r, column=c).value
                            headers.append(fmt_val("" if v is None else str(v).strip()))
                        if any(h for h in headers):
                            lines.append(f"  headers: [{', '.join(h for h in headers if h)}]")
                        shown = 0
                        for r in range(min_r + 1, max_r + 1):
                            cells: List[str] = []
                            need_formula = include_formulas and (formulas_emitted < MAX_FORMULAS_PER_SHEET)
                            for c in range(min_c, max_c + 1):
                                cl = col_letter_cache.get(c)
                                if cl is None:
                                    cl = gl(c)
                                    col_letter_cache[c] = cl
                                addr = f"{cl}{r}"
                                vv = ws_v_cell(row=r, column=c).value
                                val_str = fmt_val(vv)
                                fv = ws_f_cell(row=r, column=c).value if (ws_f_cell and need_formula) else None
                                if include_formulas and isinstance(fv, str) and fv.startswith("=") and need_formula:
                                    if val_str:
                                        cells.append(f"{addr}={val_str} (formula:={fv[1:]})")
                                    else:
                                        cells.append(f"{addr} (formula:={fv[1:]})")
                                    formulas_emitted += 1
                                    need_formula = include_formulas and (formulas_emitted < MAX_FORMULAS_PER_SHEET)
                                elif val_str:
                                    cells.append(f"{addr}={val_str}")
                            if not cells:
                                continue
                            lines.append(f"  row {r}: " + ", ".join(cells))
                            shown += 1
                            if shown >= MAX_ROWS_PER_TABLE:
                                lines.append("  - … (rows truncated)")
                                break
                    lines.append("")
            except Exception:
                pass

        if EMIT_MERGED:
            try:
                merges = getattr(ws_v, "merged_cells", None)
                if merges and getattr(merges, "ranges", None):
                    lines.append("## Merged Ranges")
                    for mr in merges.ranges:
                        ref = str(getattr(mr, "coord", getattr(mr, "bounds", mr)))
                        lines.append(f"- {ref}")
                    lines.append("")
            except Exception:
                pass

        if EMIT_CELLS:
            try:
                if callable(getattr(ws_v, "calculate_dimension", None)):
                    dim_ref = ws_v.calculate_dimension()
                    min_c, min_r, max_c, max_r = range_boundaries(dim_ref)
                else:
                    min_r = 1; min_c = 1
                    max_r = ws_v.max_row or 1
                    max_c = ws_v.max_column or 1
                lines.append(f"## Cells (non-empty, first {MAX_CELLS_PER_SHEET})")
                emitted = 0
                ws_v_cell = ws_v.cell
                ws_f_cell = ws_f.cell if ws_f else None
                gl = get_column_letter
                for r in range(min_r, max_r + 1):
                    for c in range(min_c, max_c + 1):
                        need_formula = include_formulas and (formulas_emitted < MAX_FORMULAS_PER_SHEET)
                        vv = ws_v_cell(row=r, column=c).value
                        fv = ws_f_cell(row=r, column=c).value if (ws_f_cell and need_formula or (ws_f_cell and include_formulas and vv is None)) else None
                        # If formulas are off, skip formula-only cells (vv is None)
                        if vv is None and not (include_formulas and isinstance(fv, str) and fv.startswith("=")):
                            continue
                        cl = col_letter_cache.get(c)
                        if cl is None:
                            cl = gl(c)
                            col_letter_cache[c] = cl
                        addr = f"{cl}{r}"
                        val_str = fmt_val(vv)
                        if include_formulas and isinstance(fv, str) and fv.startswith("=") and need_formula:
                            if val_str:
                                lines.append(f"- {addr}={val_str} (formula:={fv[1:]})")
                            else:
                                lines.append(f"- {addr} (formula:={fv[1:]})")
                            formulas_emitted += 1
                        else:
                            if val_str:
                                lines.append(f"- {addr}={val_str}")
                            else:
                                continue
                        emitted += 1
                        if emitted >= MAX_CELLS_PER_SHEET:
                            lines.append("… (cells truncated)")
                            break
                    if emitted >= MAX_CELLS_PER_SHEET:
                        break
                lines.append("")
            except Exception:
                lines.append("")

    txt = _squeeze_spaces("\n".join(lines).strip())
    return txt, "text/plain"

# ===== aimodel/file_read/rag/retrieve.py =====

from __future__ import annotations
from typing import List, Optional, Tuple
from ..core.settings import SETTINGS
from .store import search_vectors

def _embed_query(q: str) -> List[float]:
    q = (q or "").strip()
    if not q:
        return []
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer("intfloat/e5-small-v2")
        arr = model.encode([q], normalize_embeddings=True, convert_to_numpy=True)
        return arr[0].tolist()
    except Exception as e:
        print(f"[RAG] embedding backend failed: {e}")
        return []

def _dedupe_and_sort(hits: List[dict], *, k: int) -> List[dict]:
    # sort by score desc, then stable de-dupe by (source, chunkIndex)
    hits_sorted = sorted(hits, key=lambda h: float(h.get("score", 0.0)), reverse=True)
    seen: set[Tuple[str, str]] = set()
    out: List[dict] = []
    for h in hits_sorted:
        key = (str(h.get("source") or ""), str(h.get("chunkIndex") or ""))
        if key in seen:
            continue
        seen.add(key)
        out.append(h)
        if len(out) >= k:
            break
    return out

def make_rag_block(hits: List[dict], *, max_chars: int = 800) -> str:
    lines = ["Local knowledge:"]
    total_budget = int(SETTINGS.get("rag_total_char_budget", 2200))  # NEW
    used = 0

    for h in hits:
        src = h.get("source") or ""
        idx = h.get("chunkIndex")
        head = f"- {src} — chunk {idx}" if idx is not None else f"- {src}"
        body = (h.get("text") or "").strip()

        # header + newline cost
        head_cost = len(head) + 1
        if used + head_cost >= total_budget:
            break
        lines.append(head)
        used += head_cost

        if body:
            snippet = body[:max_chars]
            snippet_cost = len(snippet) + 1 + 2  # indent + newline
            if used + snippet_cost > total_budget:
                remain = max(0, total_budget - used - 3)
                if remain > 0:
                    lines.append("  " + snippet[:remain])
                    used += 2 + remain
                break
            lines.append("  " + snippet)
            used += snippet_cost

    return "\n".join(lines)

def build_rag_block(query: str, session_id: str | None = None) -> str | None:
    if not bool(SETTINGS.get("rag_enabled", True)):
        return None
    k = int(SETTINGS.get("rag_top_k", 4))

    q = (query or "").strip()
    print(f"[RAG SEARCH] q={q!r} session={session_id} k={k}")
    qvec = _embed_query(q)
    if not qvec:
        print("[RAG SEARCH] no qvec")
        return None

    # Pull hits from session scope and global scope
    d = len(qvec)
    hits_chat = search_vectors(session_id, qvec, k, dim=d) or []
    hits_glob = search_vectors(None,       qvec, k, dim=d) or []
    hits = hits_chat + hits_glob

    print(f"[RAG SEARCH] hits={len(hits)}")
    if not hits:
        return None

    hits_top = _dedupe_and_sort(hits, k=k)
    block = make_rag_block(
    hits_top,
    max_chars=int(SETTINGS.get("rag_max_chars_per_chunk", 800))  # ← use RAG knob
)
    print(f"[RAG BLOCK] chars={len(block)}")
    return block



# ===== aimodel/file_read/rag/router_ai.py =====

from __future__ import annotations
from typing import Tuple, Optional, Any
import json, re, traceback
from ..core.settings import SETTINGS

# --------- Prompt (brace-safe; only {text} is formatted) ----------
_DECIDE_PROMPT = (
    "You are a router deciding whether the user message should query the app's LOCAL knowledge "
    "(uploaded files, chat/session documents) via RAG.\n"
    "Respond with JSON only in exactly this schema:\n"
    "{{\"need\": true|false, \"query\": \"<text or empty>\"}}\n\n"
    "Decision principle:\n"
    "- Set need=true if answering would materially benefit from the user's LOCAL knowledge base "
    "(e.g., their files, prior session uploads, or internal notes).\n"
    "- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n"
    "- Do NOT consider the public web here.\n"
    "- If you set need=true and you can succinctly restate the search intent for the local KB, "
    "  put that in \"query\". Otherwise leave \"query\" empty.\n\n"
    "Text:\n{text}\n"
    "JSON:"
)

def _dbg(msg: str):
    print(f"[RAG ROUTER] {msg}")

def _force_json(s: str) -> dict:
    if not s:
        _dbg("empty LLM output")
        return {}
    # try direct
    try:
        v = json.loads(s)
        if isinstance(v, dict):
            _dbg(f"JSON direct OK keys={list(v.keys())}")
            return v
        _dbg(f"JSON direct produced non-dict type={type(v).__name__}")
    except Exception as e:
        _dbg(f"direct loads failed: {e} | text={s!r}")

    # settings regex
    rgx = SETTINGS.get("router_json_extract_regex")
    cand = None
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
                _dbg(f"regex cand via settings len={len(cand)}")
        except Exception as e:
            _dbg(f"settings regex error: {e}")

    # fallback: first {...}
    if not cand:
        m2 = re.search(r"\{.*?\}", s, re.DOTALL)
        cand = m2.group(0) if m2 else None
        if cand:
            _dbg(f"fallback cand len={len(cand)}")

    if not cand:
        _dbg("no JSON candidate found")
        return {}

    try:
        v = json.loads(cand)
        if isinstance(v, dict):
            _dbg(f"cand loads OK keys={list(v.keys())}")
            return v
        _dbg(f"cand loads non-dict type={type(v).__name__}")
    except Exception as e:
        _dbg(f"cand loads failed: {e} | cand={cand!r}")
    return {}

def _strip_wrappers(text: str) -> str:
    t = (text or "")
    if SETTINGS.get("router_trim_whitespace") is True:
        t = t.strip()
    if SETTINGS.get("router_strip_wrappers_enabled") is not True:
        return t
    head = t
    if SETTINGS.get("router_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]
    pat = SETTINGS.get("router_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception as e:
            _dbg(f"strip_wrappers regex error: {e}")
            return head
    return head

def _normalize_keys(d: dict) -> dict:
    """Make keys robust: strip quotes/spaces and lowercase."""
    norm = {}
    for k, v in d.items():
        ks = str(k).strip().strip('"').strip("'").strip().lower()
        norm[ks] = v
    return norm

def _as_bool(v) -> Optional[bool]:
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        s = v.strip().strip('"').strip("'").lower()
        if s in ("true", "yes", "y", "1"):
            return True
        if s in ("false", "no", "n", "0"):
            return False
    return None

def decide_rag(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    try:
        if not user_text or not user_text.strip():
            _dbg("SKIP empty user_text")
            return (False, None)

        core_text = _strip_wrappers(user_text.strip())
        the_prompt = _DECIDE_PROMPT.format(text=core_text)
        _dbg(f"INPUT core_text={core_text!r}")
        _dbg(f"PROMPT >>>\n{the_prompt}\n<<< PROMPT")

        params = {
            "max_tokens": SETTINGS.get("router_decide_max_tokens"),
            "temperature": SETTINGS.get("router_decide_temperature"),
            "top_p": SETTINGS.get("router_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}
        _dbg(f"PARAMS={params}")

        raw = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        _dbg(f"RAW OBJ keys={list(raw.keys())}")
        text_out = (raw.get("choices", [{}])[0].get("message", {}).get("content") or "").strip()
        _dbg(f"RAW OUT str={text_out!r}")

        data = _force_json(text_out) or {}
        _dbg(f"PARSED JSON raw={data}")

        data = _normalize_keys(data)
        _dbg(f"PARSED JSON norm_keys={data}")

        need_raw = data.get("need")
        need_bool = _as_bool(need_raw) if not isinstance(need_raw, bool) else need_raw
        if need_bool is None:
            need_default = SETTINGS.get("rag_default_need_when_invalid")
            need = bool(need_default) if isinstance(need_default, bool) else False
            _dbg(f"'need' missing/invalid -> default={need}")
        else:
            need = bool(need_bool)

        query_field = data.get("query", "")
        try:
            query = _strip_wrappers(str(query_field or "").strip())
        except Exception as e:
            _dbg(f"query strip error: {e}")
            query = ""

        if not need:
            query = None
        _dbg(f"DECISION need={need} query={query!r}")
        return (need, query)

    except Exception as e:
        _dbg(f"FATAL {type(e).__name__}: {e}")
        traceback.print_exc()
        return (bool(SETTINGS.get("rag_default_need_when_invalid", False)), None)

# ===== aimodel/file_read/rag/schemas.py =====

from pydantic import BaseModel, Field
from typing import Optional, List, Literal, Dict

class SearchReq(BaseModel):
    query: str
    sessionId: Optional[str] = None
    kChat: int = 6
    kGlobal: int = 4
    hybrid_alpha: float = 0.5  # 0..1, higher = semantic weight

class ItemRow(BaseModel):
    id: str
    sessionId: Optional[str]
    source: str
    title: Optional[str]
    mime: Optional[str]
    size: Optional[int]
    createdAt: str
    meta: Dict[str, str] = Field(default_factory=dict)

class SearchHit(BaseModel):
    id: str
    text: str
    score: float
    source: Optional[str] = None   # <-- safer
    title: Optional[str] = None
    sessionId: Optional[str] = None
    url: Optional[str] = None

# ===== aimodel/file_read/rag/search.py =====

from __future__ import annotations
from typing import List, Dict, Optional
import numpy as np
from .store import search_vectors

def reciprocal_rank_fusion(results: List[List[Dict]], k: int = 60) -> List[Dict]:
    # results = [list_from_chat, list_from_global]
    scores: Dict[str, float] = {}
    lookup: Dict[str, Dict] = {}
    for lst in results:
        for rank, r in enumerate(lst, start=1):
            rid = r["id"]
            scores[rid] = scores.get(rid, 0.0) + 1.0 / (k + rank)
            lookup[rid] = r
    fused = [{"score": s, **lookup[rid]} for rid, s in scores.items()]
    fused.sort(key=lambda x: x["score"], reverse=True)
    return fused

def merge_chat_first(chat_hits: List[Dict], global_hits: List[Dict], alpha: float = 0.5) -> List[Dict]:
    # alpha weights semantic scores; RRF stabilizes positions
    fused = reciprocal_rank_fusion([chat_hits, global_hits])
    return fused

# ===== aimodel/file_read/rag/store.py =====

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import faiss, json, os, time, hashlib
import numpy as np

BASE = Path(__file__).resolve().parents[1] / "store" / "rag"

def _ns_dir(session_id: Optional[str]) -> Path:
    if session_id:
        return BASE / "by_session" / session_id
    return BASE / "global"

def _paths(session_id: Optional[str]) -> Tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / "index.faiss", d / "meta.jsonl"

def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x

def _load_index(dim: int, p: Path) -> faiss.Index:
    if p.exists():
        return faiss.read_index(str(p))
    return faiss.IndexFlatIP(dim)

def _save_index(idx: faiss.Index, p: Path) -> None:
    faiss.write_index(idx, str(p))

def add_vectors(session_id: Optional[str], embeds: np.ndarray, metas: List[Dict], dim: int):
    idx_path, meta_path = _paths(session_id)
    idx = _load_index(dim, idx_path)

    # ensure index type
    if not isinstance(idx, faiss.IndexFlatIP):
        idx = faiss.IndexFlatIP(dim) if idx.ntotal == 0 else idx

    # normalize vectors
    embeds = _norm(embeds)

    # ✅ read existing ids to avoid duplicates
    existing_ids = set()
    if meta_path.exists():
        with meta_path.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    existing_ids.add(j["id"])
                except:
                    pass

    start = idx.ntotal
    new_embeds = []
    new_metas = []

    for i, m in enumerate(metas):
        if m["id"] in existing_ids:
            continue
        m["row"] = start + len(new_embeds)
        new_embeds.append(embeds[i])
        new_metas.append(m)

    if new_embeds:
        idx.add(np.vstack(new_embeds))
        _save_index(idx, idx_path)
        with meta_path.open("a", encoding="utf-8") as f:
            for m in new_metas:
                f.write(json.dumps(m, ensure_ascii=False) + "\n")

def search_vectors(session_id: Optional[str], query_vec: np.ndarray, topk: int, dim: int) -> List[Dict]:
    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return []

    idx = _load_index(dim, idx_path)

    # ✅ ensure numpy array for reshape
    query_vec = np.asarray(query_vec, dtype="float32")
    q = _norm(query_vec.reshape(1, -1))

    D, I = idx.search(q, topk)
    out: List[Dict] = []
    rows: Dict[int, Dict] = {}
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                j = json.loads(line)
                rows[int(j["row"])] = j
            except:
                pass
    for score, row in zip(D[0].tolist(), I[0].tolist()):
        if row < 0:
            continue
        m = rows.get(row)
        if not m:
            continue
        m = dict(m)
        m["score"] = float(score)
        out.append(m)
    return out

def search_similar(qvec: List[float] | np.ndarray, *, k: int = 5, session_id: Optional[str] = None) -> List[Dict]:
    """
    Compatibility wrapper used by retrieve.py.
    qvec: a single embedding vector (list or np.ndarray)
    """
    arr = np.asarray(qvec, dtype="float32")
    dim = int(arr.shape[-1])
    return search_vectors(session_id, arr, k, dim)

def add_texts(
    texts: List[str],
    metas: List[Dict],
    *,
    session_id: Optional[str],
    embed_fn,  # callable: List[str] -> np.ndarray[float32]
) -> int:
    if not texts:
        return 0
    vecs = embed_fn(texts)  # should return (n, d) float32
    if not isinstance(vecs, np.ndarray):
        vecs = np.asarray(vecs, dtype="float32")
    dim = int(vecs.shape[-1])
    add_vectors(session_id, vecs, metas, dim)
    return len(texts)

# ===== aimodel/file_read/rag/worker.py =====



# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs==9.5.4
httpx==0.27.0
# HTML parsing / readability (permissive)
lxml==6.0.1
readability-lxml==0.8.1
selectolax==0.3.21
beautifulsoup4==4.12.3

# RAG
faiss-cpu==1.8.0.post1
numpy==1.26.4
sentence-transformers==3.0.1
tzlocal==5.2
openpyxl==3.1.5 
python-multipart==0.0.20

python-docx==1.1.2
PyYAML==6.0.2
toml==0.10.2
pdfminer.six==20240706

PyPDF2==3.0.1
pandas==2.2.2




# ===== aimodel/file_read/runtime/__init__.py =====



# ===== aimodel/file_read/runtime/model_runtime.py =====

# aimodel/file_read/model_runtime.py
from __future__ import annotations
import os
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import RLock
from typing import Any, Dict, Optional, List

from ..adaptive.config.paths import read_settings, write_settings

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

    @staticmethod
    def from_dict(d: Dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath","")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None,"") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None,"") else None),
        )

_runtime_lock = RLock()
_llm: Optional[Llama] = None
_cfg: Optional[ModelConfig] = None

def _build_kwargs(cfg: ModelConfig) -> Dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    # advanced optional tuning
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            # llama_cpp doesn't expose explicit close; allow GC
            _llm = None
    except Exception:
        _llm = None

def current_model_info() -> Dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
        }

def ensure_ready() -> None:
    """
    Lazily load a model based on current settings if nothing is loaded.
    """
    global _llm, _cfg
    with _runtime_lock:
        if _llm is not None:
            return
        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected. Load one via /models/load or set LOCALAI_MODEL_PATH.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")
        _llm = Llama(**_build_kwargs(cfg))
        _cfg = cfg

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: Dict[str, Any]) -> Dict[str, Any]:
    """
    Load/swap to a new model with given config fields (any subset).
    Persists to settings.json so the choice survives restarts.
    """
    global _llm, _cfg
    with _runtime_lock:
        s = read_settings()
        s.update({k:v for k,v in config_patch.items() if v is not None})
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")

        # swap
        _close_llm()
        _llm = Llama(**_build_kwargs(cfg))
        _cfg = cfg
        write_settings(asdict(cfg))
        return current_model_info()

def unload_model() -> None:
    """
    Unload current model, keep settings as-is.
    """
    global _llm
    with _runtime_lock:
        _close_llm()

def list_local_models() -> List[Dict[str, Any]]:
    """
    Scan modelsDir for .gguf files and return a lightweight listing.
    """
    s = read_settings()
    root = Path(s.get("modelsDir") or "")
    root.mkdir(parents=True, exist_ok=True)
    out: List[Dict[str, Any]] = []
    for p in root.rglob("*.gguf"):
        try:
            out.append({
                "path": str(p.resolve()),
                "sizeBytes": p.stat().st_size,
                "name": p.name,
                "rel": str(p.relative_to(root)),
            })
        except Exception:
            pass
    # sort largest first (usually higher quant or full precision)
    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/services/cancel.py =====

from __future__ import annotations
import asyncio
from threading import Event
from typing import Dict
from ..core.settings import SETTINGS

eff = SETTINGS.effective()
GEN_SEMAPHORE = asyncio.Semaphore(int(eff["gen_semaphore_permits"]))
_ACTIVE: Dict[str, int] = {}
_CANCELS: Dict[str, Event] = {}

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ===== aimodel/file_read/services/context_window.py =====

from __future__ import annotations
from typing import List, Dict, Optional, Tuple
from ..utils.streaming import safe_token_count_messages
from ..runtime.model_runtime import current_model_info
from ..core.settings import SETTINGS

def estimate_tokens(llm, messages: List[Dict[str,str]]) -> Optional[int]:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None

def current_n_ctx() -> int:
    eff = SETTINGS.effective()
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or eff["nctx_fallback"])
    except Exception:
        return int(eff["nctx_fallback"])

def clamp_out_budget(
    *, llm, messages: List[Dict[str,str]], requested_out: int, margin: int = 32
) -> Tuple[int, int]:
    eff = SETTINGS.effective()
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = int(eff["token_estimate_fallback"])
    n_ctx = current_n_ctx()
    available = max(int(eff["min_out_tokens"]), n_ctx - prompt_est - margin)
    safe_out = max(int(eff["min_out_tokens"]), min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations
import asyncio, time, json
from typing import AsyncGenerator, Dict, List, Optional
from fastapi.responses import StreamingResponse
from datetime import datetime
from dataclasses import asdict

from ..core.settings import SETTINGS
from ..runtime.model_runtime import ensure_ready, get_llm
from ..core.schemas import ChatBody

from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .session_io import handle_incoming, persist_summary
from .packing import build_system_text, pack_with_rollup, maybe_inject_rag_block
from .context_window import clamp_out_budget

from ..web.router_ai import decide_web_and_fetch
from ..rag.router_ai import decide_rag  # <-- NEW: LLM JSON router for RAG
from ..utils.streaming import RUNJSON_START, RUNJSON_END

from .streaming_worker import run_stream as _run_stream
from typing import AsyncIterator
run_stream: (callable[..., AsyncIterator[bytes]]) = _run_stream  # type: ignore[assignment]


def _now() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def _chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total


def _dump_full_prompt(messages: List[Dict[str, object]], *, params: Dict[str, object], session_id: str) -> None:
    try:
        print(f"[{_now()}] PROMPT DUMP BEGIN session={session_id} msgs={len(messages)}")
        print(json.dumps({"messages": messages, "params": params}, ensure_ascii=False, indent=2))
        print(f"[{_now()}] PROMPT DUMP END   session={session_id}")
    except Exception as e:
        print(f"[{_now()}] PROMPT DUMP ERROR session={session_id} err={type(e).__name__}: {e}")


def _compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: Optional[int] = None,
    summary_chars: Optional[int] = None,
    max_chars: Optional[int] = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: List[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out


async def generate_stream_flow(data: ChatBody, request) -> StreamingResponse:
    ensure_ready()
    llm = get_llm()

    eff0 = SETTINGS.effective()
    session_id = data.sessionId or eff0["default_session_id"]
    eff = SETTINGS.effective(session_id=session_id)

    if not data.messages:
        return StreamingResponse(iter([eff["empty_messages_response"].encode("utf-8")]), media_type="text/plain")

    temperature = data.temperature if getattr(data, "temperature", None) is not None else float(eff["default_temperature"])
    top_p = data.top_p if getattr(data, "top_p", None) is not None else float(eff["default_top_p"])
    out_budget_req = int(data.max_tokens) if getattr(data, "max_tokens", None) is not None else int(eff["default_max_tokens"])

    auto_web = data.autoWeb if getattr(data, "autoWeb", None) is not None else bool(eff["default_auto_web"])
    web_k = int(data.webK) if getattr(data, "webK", None) is not None else int(eff["default_web_k"])
    web_k = max(int(eff["web_k_min"]), min(web_k, int(eff["web_k_max"])))

    model_ctx = int(eff["model_ctx"])

    incoming = [{"role": m.role, "content": m.content} for m in data.messages]
    print(f"[{_now()}] GEN request START session={session_id} msgs_in={len(incoming)}")

    st = handle_incoming(session_id, incoming)

    latest_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")
    lut_chars = len(latest_user_text) if isinstance(latest_user_text, str) else len(str(latest_user_text) or "")

    # ---------------------- WEB INJECT (optional) ----------------------
    t0 = time.perf_counter()
    print(f"[{_now()}] GEN web_inject START session={session_id} latest_user_text_chars={lut_chars}")
    try:
        block = None
        router_text = _compose_router_text(
            st.get("recent", []),
            str(latest_user_text or ""),
            st.get("summary", "") or "",
            tail_turns=int(eff["router_tail_turns"]),
            summary_chars=int(eff["router_summary_chars"]),
            max_chars=int(eff["router_max_chars"]),
        )

        if auto_web:
            block = await decide_web_and_fetch(llm, router_text, k=web_k)

        print(f"[{_now()}] ORCH build done has_block={bool(block)} block_len={(len(block) if block else 0)}")

        if block:
            st["_ephemeral_web"] = (st.get("_ephemeral_web") or []) + [
                {
                    "role": "assistant",
                    "content": eff["web_block_preamble"] + "\n\n" + block,
                }
            ]
            types_preview = [type(x).__name__ for x in (st.get("_ephemeral_web") or [])]
            print(f"[{_now()}] EPHEMERAL attached count={len(st['_ephemeral_web'])} types={types_preview}")

        dt = time.perf_counter() - t0
        eph_cnt = len(st.get("_ephemeral_web") or [])
        print(f"[{_now()}] GEN web_inject END   session={session_id} elapsed={dt:.3f}s ephemeral_blocks={eph_cnt}")
    except Exception as e:
        dt = time.perf_counter() - t0
        print(f"[{_now()}] GEN web_inject ERROR session={session_id} elapsed={dt:.3f}s err={type(e).__name__}: {e}")

    # ---------------------- PACK (with ephemeral web) ----------------------
    system_text = build_system_text()

    ephemeral_once = st.pop("_ephemeral_web", [])
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )

    # ---------------------- RAG ROUTER (LLM JSON) ----------------------
    rag_need = False
    rag_query: Optional[str] = None
    try:
        rag_need, rag_query = decide_rag(llm, router_text)
        print(f"[{_now()}] RAG ROUTER need={rag_need} query={rag_query!r}")
    except Exception as e:
        print(f"[{_now()}] RAG ROUTER ERROR {type(e).__name__}: {e}")
        rag_need = bool(SETTINGS.effective().get("rag_default_need_when_invalid", False))
        rag_query = None

    # If a web block was injected OR RAG router said no, skip RAG
    skip_rag = bool(ephemeral_once) or (not rag_need)

    # Optionally pass LLM-refined query to RAG
    packed = maybe_inject_rag_block(
        packed,
        session_id=session_id,
        skip_rag=skip_rag,
        rag_query=rag_query,
    )

    # ---------------------- STREAM SETUP ----------------------
    packed_chars = _chars_len(packed)
    print(f"[{_now()}] GEN pack READY       session={session_id} msgs={len(packed)} chars={packed_chars} out_budget_req={out_budget_req}")

    _dump_full_prompt(
        packed,
        params={
            "requested_out": out_budget_req,
            "temperature": temperature,
            "top_p": top_p,
        },
        session_id=session_id,
    )

    persist_summary(session_id, st["summary"])

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_req, margin=int(eff["clamp_margin"])
    )
    print(f"[{_now()}] GEN clamp_out_budget  session={session_id} out_budget={out_budget} input_tokens_est={input_tokens_est}")

    stop_ev = cancel_event(session_id)
    stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        async with GEN_SEMAPHORE:
            mark_active(session_id, +1)
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
      