
# ===== aimodel/file_read/__init__.py =====

from .adaptive.config.paths import app_data_dir, read_settings, write_settings
from .runtime.model_runtime import ensure_ready, get_llm, current_model_info

__all__ = [
    "app_data_dir", "read_settings", "write_settings",
    "ensure_ready", "get_llm", "current_model_info",
]

# ===== aimodel/file_read/adaptive/config/adaptive_config.py =====

# aimodel/file_read/runtime/adaptive_config.py
from __future__ import annotations
import os, shutil, subprocess, platform
from dataclasses import dataclass, asdict
from typing import Optional, Dict, Any

try:
    import psutil
except Exception:
    psutil = None
try:
    import torch
except Exception:
    torch = None

from .paths import read_settings

def _env_bool(k:str, default:bool)->bool:
    v = os.getenv(k)
    if v is None: return default
    return v.strip().lower() in ("1","true","yes","on")

def _cpu_count()->int:
    try:
        import multiprocessing as mp
        return max(1, mp.cpu_count() or os.cpu_count() or 1)
    except Exception:
        return os.cpu_count() or 1

def _avail_ram()->Optional[int]:
    if not psutil: return None
    try: return int(psutil.virtual_memory().available)
    except Exception: return None

def _cuda_vram()->Optional[int]:
    if torch and torch.cuda.is_available():
        try:
            dev = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(dev)
            return int(props.total_memory)
        except Exception:
            pass
    if shutil.which("nvidia-smi"):
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-gpu=memory.total","--format=csv,noheader,nounits"],
                text=True, stderr=subprocess.DEVNULL, timeout=2.0
            )
            mb = max(int(x.strip()) for x in out.strip().splitlines() if x.strip())
            return mb * 1024 * 1024
        except Exception:
            return None
    return None

def _gpu_kind()->str:
    if _cuda_vram(): return "cuda"
    if torch and getattr(torch.backends,"mps",None) and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

def _safe_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default

def _pick_dtype_quant(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> tuple[Optional[str], Optional[str]]:
    dq = a.get("dtype_quant", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = dq.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype"), best.get("quant")
        return dq.get("cuda_default_dtype"), dq.get("cuda_default_quant")
    if device == "mps":
        return dq.get("mps_default_dtype"), None
    return dq.get("cpu_default_dtype"), dq.get("cpu_default_quant")

def _pick_kv(device: str, a: Dict[str, Any], vram_bytes: Optional[int]) -> Optional[str]:
    kv = a.get("kv_cache", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = kv.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return best.get("dtype")
        return kv.get("cuda_default")
    if device == "mps":
        return kv.get("mps_default")
    return kv.get("cpu_default")

def _pick_capacity(device: str, a: Dict[str, Any], vram_bytes: Optional[int], threads:int) -> tuple[int,int,Optional[int]]:
    cap = a.get("capacity", {}) if isinstance(a, dict) else {}
    if device == "cuda":
        tiers = cap.get("cuda_tiers") or []
        vram_gb = (vram_bytes or 0) / (1024**3)
        best = None
        for t in sorted(tiers, key=lambda x: float(x.get("min_vram_gb", 0)), reverse=True):
            if vram_gb >= _safe_float(t.get("min_vram_gb"), 0.0):
                best = t
                break
        if best:
            return int(best.get("seq_len") or 0), int(best.get("batch") or 1), int(best.get("n_gpu_layers") or 0)
        return 0, 1, 0
    if device == "mps":
        m = cap.get("mps", {})
        return int(m.get("seq_len") or 0), int(m.get("batch") or 1), 0
    cpu = cap.get("cpu", {})
    seq_len = int(cpu.get("seq_len") or 0)
    batch = 1
    by = cpu.get("batch_by_threads") or []
    best = None
    for t in sorted(by, key=lambda x: int(x.get("min_threads", 0)), reverse=True):
        if threads >= int(t.get("min_threads") or 0):
            best = t
            break
    if best:
        batch = int(best.get("batch") or 1)
    return seq_len, batch, 0

def _gpu_mem_fraction(device:str, a: Dict[str, Any]) -> float:
    table = a.get("gpu_fraction", {}) if isinstance(a, dict) else {}
    v = table.get(device)
    return _safe_float(v, 0.0)

def _torch_flags(device:str, a: Dict[str, Any]) -> tuple[bool,bool]:
    flags = a.get("flags", {}) if isinstance(a, dict) else {}
    flash = bool(flags.get("enable_flash_attn_cuda")) if device == "cuda" else False
    tc = bool(flags.get("use_torch_compile_on_cuda_linux")) if (device == "cuda" and platform.system().lower()=="linux") else False
    return flash, tc

def _threads(a: Dict[str, Any]) -> tuple[int,int,int,int]:
    policy = a.get("cpu_threads_policy", {}) if isinstance(a, dict) else {}
    mode = str(policy.get("mode") or "").lower()
    ncpu = _cpu_count()
    if mode == "fixed":
        v = int(policy.get("value") or max(1, ncpu-1))
        t = max(1, min(v, ncpu))
    elif mode == "percent":
        pct = _safe_float(policy.get("value"), 0.0)
        t = max(1, min(ncpu, int(round(ncpu*pct/100.0))))
        if t < 1: t = 1
    else:
        t = max(1, ncpu-1)
    intra = t
    inter = max(1, ncpu//2)
    return ncpu, t, intra, inter

@dataclass
class AdaptiveConfig:
    device: str
    dtype: Optional[str]
    quant: Optional[str]
    kv_cache_dtype: Optional[str]
    max_seq_len: int
    max_batch_size: int
    gpu_memory_fraction: float
    cpu_threads: int
    torch_intraop_threads: int
    torch_interop_threads: int
    enable_flash_attn: bool
    use_torch_compile: bool
    total_vram_bytes: Optional[int]
    avail_ram_bytes: Optional[int]
    cpu_count: int
    def as_dict(self)->Dict[str,Any]:
        return asdict(self)

def compute_adaptive_config()->AdaptiveConfig:
    settings = read_settings()
    a = settings.get("adaptive", {}) if isinstance(settings, dict) else {}
    device = _gpu_kind()
    vram = _cuda_vram() if device=="cuda" else None
    ram = _avail_ram()
    ncpu, threads, intra, inter = _threads(a)
    dtype, quant = _pick_dtype_quant(device, a, vram)
    kv = _pick_kv(device, a, vram)
    seq_len, batch, n_gpu_layers = _pick_capacity(device, a, vram, threads)
    frac = _gpu_mem_fraction(device, a)
    flash, tcompile = _torch_flags(device, a)
    return AdaptiveConfig(
        device=device,
        dtype=dtype,
        quant=quant,
        kv_cache_dtype=kv,
        max_seq_len=int(seq_len or 0),
        max_batch_size=int(batch or 1),
        gpu_memory_fraction=frac,
        cpu_threads=threads,
        torch_intraop_threads=intra,
        torch_interop_threads=inter,
        enable_flash_attn=flash,
        use_torch_compile=tcompile,
        total_vram_bytes=vram,
        avail_ram_bytes=ram,
        cpu_count=ncpu
    )

# ===== aimodel/file_read/adaptive/config/paths.py =====

# aimodel/file_read/paths.py
from __future__ import annotations
import json, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, Optional
import sys

# App data dir (override with LOCALAI_DATA_DIR for dev/electron)
def app_data_dir() -> Path:
    override = os.getenv("LOCALAI_DATA_DIR")
    if override:
        return Path(override)

    if os.name == "nt":
        base = os.environ.get("APPDATA") or (Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"

    if sys.platform == "darwin":  # macOS
        return Path.home() / "Library" / "Application Support" / "LocalAI"

    if os.name == "posix":  # Linux/other UNIX
        return Path.home() / ".local" / "share" / "LocalAI"

    return Path.home() / ".localai"

SETTINGS_PATH = app_data_dir() / "settings.json"

DEFAULTS = {
    "modelsDir": str((app_data_dir() / "models").resolve()),
    "modelPath": "",            # empty = none selected
    "nCtx": 4096,
    "nThreads": 8,
    "nGpuLayers": 40,
    "nBatch": 256,
    "ropeFreqBase": None,       # advanced (optional)
    "ropeFreqScale": None,      # advanced (optional)
}

def bootstrap() -> None:
    ad = app_data_dir()
    ad.mkdir(parents=True, exist_ok=True)
    md = Path(DEFAULTS["modelsDir"])
    md.mkdir(parents=True, exist_ok=True)
    if not SETTINGS_PATH.exists():
        SETTINGS_PATH.write_text(json.dumps(DEFAULTS, indent=2), encoding="utf-8")

def _read_json(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def read_settings() -> Dict[str, Any]:
    # precedence: ENV > settings.json > defaults
    bootstrap()
    cfg = DEFAULTS | _read_json(SETTINGS_PATH)

    # ENV overrides (optional)
    env_model = os.getenv("LOCALAI_MODEL_PATH")
    if env_model:
        cfg["modelPath"] = env_model

    for key, env in [
        ("modelsDir", "LOCALAI_MODELS_DIR"),
        ("nCtx", "LOCALAI_CTX"),
        ("nThreads", "LOCALAI_THREADS"),
        ("nGpuLayers", "LOCALAI_GPU_LAYERS"),
        ("nBatch", "LOCALAI_BATCH"),
        ("ropeFreqBase", "LOCALAI_ROPE_BASE"),
        ("ropeFreqScale", "LOCALAI_ROPE_SCALE"),
    ]:
        v = os.getenv(env)
        if v is not None and v != "":
            try:
                cfg[key] = int(v) if key in {"nCtx","nThreads","nGpuLayers","nBatch"} else float(v) if key in {"ropeFreqBase","ropeFreqScale"} else v
            except Exception:
                cfg[key] = v

    return cfg

def write_settings(patch: Dict[str, Any]) -> Dict[str, Any]:
    cfg = read_settings()
    cfg.update({k:v for k,v in patch.items() if v is not None})
    SETTINGS_PATH.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
    return cfg

# ===== aimodel/file_read/adaptive/config/settings.json =====

{
  "adaptiveEnabled": false,
  "adaptive": {
    "enabled": true,
    "cpu_threads_policy": {
      "mode": "leave_one"
    },
    "dtype_quant": {
      "cuda_default_dtype": "float16",
      "cuda_default_quant": null,
      "mps_default_dtype": "float16",
      "cpu_default_dtype": "int8",
      "cpu_default_quant": "q4_K_M",
      "cuda_tiers": [
        { "min_vram_gb": 24, "dtype": "bfloat16", "quant": null },
        { "min_vram_gb": 12, "dtype": "float16", "quant": null },
        { "min_vram_gb": 6,  "dtype": "float16", "quant": "bnb-int8" },
        { "min_vram_gb": 4,  "dtype": "float16", "quant": "bnb-int8" }
      ]
    },
    "kv_cache": {
      "cuda_default": "fp8",
      "mps_default": "fp16",
      "cpu_default": "fp32",
      "cuda_tiers": [
        { "min_vram_gb": 16, "dtype": "fp16" },
        { "min_vram_gb": 0,  "dtype": "fp8" }
      ]
    },
    "capacity": {
      "cuda_tiers": [
        { "min_vram_gb": 24, "seq_len": 8192, "batch": 8, "n_gpu_layers": 9999 },
        { "min_vram_gb": 12, "seq_len": 4096, "batch": 4, "n_gpu_layers": 48 },
        { "min_vram_gb": 8,  "seq_len": 3072, "batch": 2, "n_gpu_layers": 40 },
        { "min_vram_gb": 6,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 32 },
        { "min_vram_gb": 4,  "seq_len": 2048, "batch": 1, "n_gpu_layers": 28 }
      ],
      "mps": { "seq_len": 2048, "batch": 1 },
      "cpu": {
        "seq_len": 2048,
        "batch_by_threads": [
          { "min_threads": 16, "batch": 8 },
          { "min_threads": 8,  "batch": 4 },
          { "min_threads": 1,  "batch": 2 }
        ]
      }
    },
    "gpu_fraction": {
      "cuda": 0.8,
      "mps": 0.7,
      "cpu": 0.0
    },
    "flags": {
      "enable_flash_attn_cuda": true,
      "use_torch_compile_on_cuda_linux": true
    },
    "batchTokenMap": [
      { "minConcurrency": 8, "n_batch": 512 },
      { "minConcurrency": 4, "n_batch": 384 },
      { "minConcurrency": 2, "n_batch": 256 },
      { "minConcurrency": 1, "n_batch": 192 }
    ]
  },
  "modelPath": "",
  "nCtx": null,
  "nThreads": null,
  "nGpuLayers": null,
  "nBatch": null
}

# ===== aimodel/file_read/api/__init__.py =====



# ===== aimodel/file_read/api/barrel.py =====

from __future__ import annotations

from dataclasses import asdict
from threading import RLock
from typing import Any, Dict, List, Optional

import numpy as np
from fastapi import APIRouter, Body, File, Form, HTTPException, Query, Request, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

from ..core.settings import SETTINGS
from ..core.schemas import (
    ChatBody,
    ChatMessage,
    ChatMetaModel,
    PageResp,
    BatchMsgDeleteReq,
    BatchDeleteReq,
    MergeChatReq,
    EditMessageReq,
)

from ..utils.streaming import strip_runjson

from ..services.cancel import GEN_SEMAPHORE, is_active
from ..services.generate_flow import generate_stream_flow, cancel_session, cancel_session_alias
from ..services.packing import build_system_text, pack_with_rollup
from ..services.budget import analyze_budget

from ..runtime.model_runtime import (
    get_llm,
    list_local_models,
    current_model_info,
    load_model,
    unload_model,
)
from ..adaptive.config.paths import read_settings, write_settings

from ..store import (
    upsert_on_first_message,
    update_last as store_update_last,
    list_messages as store_list_messages,
    list_paged as store_list_paged,
    append_message as store_append,
    delete_batch as store_delete_batch,
    delete_message as store_delete_message,
    delete_messages_batch as store_delete_messages_batch,
    merge_chat as store_merge_chat,
    merge_chat_new as store_merge_chat_new,
    edit_message as edit_message,
    get_summary,
    set_summary,
)

from ..rag.uploads import list_sources as rag_list_sources, hard_delete_source
from ..rag.schemas import SearchReq, SearchHit
from ..rag.ingest import sniff_and_extract, chunk_text, build_metas
from ..rag.store import add_vectors, search_vectors

from ..workers.retitle_worker import enqueue as enqueue_retitle

__all__ = [
    "asdict", "RLock", "Any", "Dict", "List", "Optional",
    "np", "APIRouter", "Body", "File", "Form", "HTTPException", "Query", "Request",
    "UploadFile", "JSONResponse", "BaseModel", "SentenceTransformer",
    "SETTINGS", "ChatBody", "ChatMessage", "ChatMetaModel", "PageResp",
    "BatchMsgDeleteReq", "BatchDeleteReq", "MergeChatReq", "EditMessageReq",
    "strip_runjson",
    "GEN_SEMAPHORE", "is_active",
    "generate_stream_flow", "cancel_session", "cancel_session_alias",
    "build_system_text", "pack_with_rollup", "analyze_budget",
    "get_llm", "list_local_models", "current_model_info", "load_model", "unload_model",
    "read_settings", "write_settings",
    "upsert_on_first_message", "store_update_last", "store_list_messages", "store_list_paged",
    "store_append", "store_delete_batch", "store_delete_message", "store_delete_messages_batch",
    "store_merge_chat", "store_merge_chat_new", "edit_message", "get_summary", "set_summary",
    "rag_list_sources", "hard_delete_source", "SearchReq", "SearchHit",
    "sniff_and_extract", "chunk_text", "build_metas", "add_vectors", "search_vectors",
    "enqueue_retitle",
]

# ===== aimodel/file_read/api/chats.py =====

#aimodel/file_read/api/chats.py
from .barrel import *

router = APIRouter()

@router.post("/api/chats")
async def api_create_chat(body: Dict[str, str]):
    session_id = (body.get("sessionId") or "").strip()
    title = (body.get("title") or "").strip()
    if not session_id:
        return {"error": "sessionId required"}
    row = upsert_on_first_message(session_id, title or "New Chat")
    return asdict(row)

@router.put("/api/chats/{session_id}/last")
async def api_update_last(session_id: str, body: Dict[str, str]):
    last_message = body.get("lastMessage")
    title = body.get("title")
    row = store_update_last(session_id, last_message, title)
    return asdict(row)

@router.delete("/api/chats/{session_id}/messages/batch")
async def api_delete_messages_batch(session_id: str, req: BatchMsgDeleteReq):
    deleted = store_delete_messages_batch(session_id, req.messageIds or [])
    return {"deleted": deleted}

@router.delete("/api/chats/{session_id}/messages/{message_id}")
async def api_delete_message(session_id: str, message_id: int):
    deleted = store_delete_message(session_id, int(message_id))
    return {"deleted": deleted}

@router.get("/api/chats/paged", response_model=PageResp)
async def api_list_paged(page: int = 0, size: int = 30, ceiling: Optional[str] = None):
    rows, total, total_pages, last_flag = store_list_paged(page, size, ceiling)
    content = [ChatMetaModel(**asdict(r)) for r in rows]
    return PageResp(
        content=content,
        totalElements=total,
        totalPages=total_pages,
        size=size,
        number=page,
        first=(page == 0),
        last=last_flag,
        empty=(len(content) == 0),
    )

@router.get("/api/chats/{session_id}/messages")
async def api_list_messages(session_id: str):
    rows = store_list_messages(session_id)
    return [asdict(r) for r in rows]


@router.post("/api/chats/{session_id}/messages")
async def api_append_message(session_id: str, msg: ChatMessage):
    role = msg.role
    content = (msg.content or "").rstrip()
    attachments = msg.attachments or []

    row = store_append(session_id, role, content, attachments=attachments)

    if role == "assistant":
        try:
            msgs = store_list_messages(session_id)
            last_seq = max((int(m.id) for m in msgs), default=0)
            msgs_clean = []
            for m in msgs:
                dm = asdict(m)
                dm["content"] = strip_runjson(dm.get("content") or "")
                msgs_clean.append(dm)
            enqueue_retitle(session_id, msgs_clean, job_seq=last_seq)
        except Exception:
            pass

    return asdict(row)

@router.delete("/api/chats/batch")
async def api_delete_batch(req: BatchDeleteReq):
    deleted = store_delete_batch(req.sessionIds or [])
    return {"deleted": deleted}


@router.put("/api/chats/{session_id}/messages/{message_id}")
async def api_edit_message(session_id: str, message_id: int, req: EditMessageReq):
    row = edit_message(session_id, message_id, req.content)
    if not row:
        return {"error": "Message not found"}
    return asdict(row)

# ===== aimodel/file_read/api/generate_router.py =====

# aimodel/file_read/api/generate_router.py
from .barrel import *

router = APIRouter()

@router.post("/generate/stream")
async def generate_stream(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

# legacy alias (kept identical)
@router.post("/api/ai/generate/stream")
async def generate_stream_alias(data: ChatBody = Body(...), request: Request = None):
    return await generate_stream_flow(data, request)

@router.post("/cancel/{session_id}")
async def _cancel_session(session_id: str):
    return await cancel_session(session_id)

@router.post("/api/ai/cancel/{session_id}")
async def _cancel_session_alias(session_id: str):
    return await cancel_session_alias(session_id)

# ===== aimodel/file_read/api/metrics.py =====

# aimodel/file_read/api/metrics.py
from .barrel import *

router = APIRouter(prefix="/metrics", tags=["metrics"])

@router.get("/budget")
def get_budget(sessionId: Optional[str] = Query(default=None), maxTokens: Optional[int] = None):
    eff0 = SETTINGS.effective()
    sid = sessionId or eff0["default_session_id"]
    llm = get_llm()
    eff = SETTINGS.effective(session_id=sid)

    msgs = [{"role": m.role, "content": m.content} for m in store_list_messages(sid)]
    summary = get_summary(sid)
    system_text = build_system_text()
    packed, new_summary, _ = pack_with_rollup(
        system_text=system_text,
        summary=summary,
        recent=msgs,
        max_ctx=int(eff["model_ctx"]),
        out_budget=int(eff["default_max_tokens"]),
    )
    if new_summary != summary:
        set_summary(sid, new_summary)

    requested_out = int(maxTokens or eff["default_max_tokens"])
    budget = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=requested_out,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()
    return {"sessionId": sid, "budget": budget}

# ===== aimodel/file_read/api/models.py =====

#aimodel/file_read/api/models.py
from .barrel import *

router = APIRouter()

class LoadReq(BaseModel):
    modelPath: str
    nCtx: Optional[int] = None
    nThreads: Optional[int] = None
    nGpuLayers: Optional[int] = None
    nBatch: Optional[int] = None
    ropeFreqBase: Optional[float] = None
    ropeFreqScale: Optional[float] = None

@router.get("/models")
async def api_list_models():
    return {
        "available": list_local_models(),
        "current": current_model_info(),
        "settings": read_settings(),
    }

@router.post("/models/load")
async def api_load_model(req: LoadReq):
    try:
        info = load_model(req.model_dump(exclude_none=True))
        return info
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=400)

@router.post("/models/unload")
async def api_unload_model():
    unload_model()
    return {"ok": True, "current": current_model_info()}

@router.post("/settings")
async def api_update_settings(patch: Dict[str, object]):
    s = write_settings(patch)
    return s

# ===== aimodel/file_read/api/rag.py =====

# aimodel/file_read/api/rag.py
from .barrel import *

router = APIRouter(prefix="/api/rag", tags=["rag"])

# ---- embedding model (sync, cached) ----
_st_model: SentenceTransformer | None = None
_st_lock = RLock()

def _get_st_model() -> SentenceTransformer:
    global _st_model
    if _st_model is None:
        with _st_lock:
            if _st_model is None:
                print("[RAG EMBED] loading e5-small-v2… (one-time)")
                _st_model = SentenceTransformer("intfloat/e5-small-v2")
                print("[RAG EMBED] model ready")
    return _st_model

def _embed(texts: List[str]) -> np.ndarray:
    model = _get_st_model()
    arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
    return arr.astype("float32")

@router.post("/upload")
async def upload_doc(sessionId: Optional[str] = Form(default=None), file: UploadFile = File(...)):
    print(f"[RAG UPLOAD] sessionId={sessionId}, filename={file.filename}, content_type={file.content_type}")

    data = await file.read()
    print(f"[RAG UPLOAD] file size={len(data)} bytes")

    text, mime = sniff_and_extract(file.filename, data)
    print(f"[RAG UPLOAD] extracted mime={mime}, text_len={len(text)}")

    if not text.strip():
        raise HTTPException(status_code=400, detail="Empty/unsupported file")

    chunks = chunk_text(text, {"mime": mime})
    print(f"[RAG UPLOAD] chunk_count={len(chunks)}")

    metas = build_metas(sessionId, file.filename, chunks, size=len(data))
    embeds = _embed([c.text for c in chunks])
    print(f"[RAG UPLOAD] embed_shape={embeds.shape}")

    add_vectors(sessionId, embeds, metas, dim=embeds.shape[1])
    return {"ok": True, "added": len(chunks)}

@router.post("/search")
async def search(req: SearchReq):
    q = req.query.strip()
    if not q:
        return {"hits": []}
    qv = np.array(_embed([q])[0], dtype="float32")
    chat_hits = search_vectors(req.sessionId, qv, req.kChat, dim=qv.shape[0])
    global_hits = search_vectors(None, qv, req.kGlobal, dim=qv.shape[0])
    from ..rag.search import merge_chat_first
    fused = merge_chat_first(chat_hits, global_hits, alpha=req.hybrid_alpha)
    out: List[SearchHit] = []
    for r in fused:
        out.append(SearchHit(
            id=r["id"], text=r["text"], score=float(r["score"]),
            source=r.get("source"), title=r.get("title"), sessionId=r.get("sessionId")
        ))
    return {"hits": [h.model_dump() for h in out]}

@router.get("/list")
async def list_items(sessionId: Optional[str] = None, k: int = 20):

    qv = np.array(_embed(["list"])[0], dtype="float32")
    hits = search_vectors(sessionId, qv, topk=k, dim=qv.shape[0])

    items = []
    for h in hits:
        txt = (h.get("text") or "")
        items.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": txt,
        })
    print(f"[RAG LIST] sessionId={sessionId} k={k} -> {len(items)} items")
    return {"items": items}

@router.get("/dump")
async def dump_items(sessionId: Optional[str] = None, k: int = 50):
    """
    Debug: return full texts of top-k chunks (be careful: can be large).
    Useful to confirm exact strings that were indexed.
    """
    qv = np.array(_embed(["dump"])[0], dtype="float32")
    hits = search_vectors(sessionId, qv, k=k, dim=qv.shape[0])

    chunks = []
    for h in hits:
        chunks.append({
            "id": h.get("id"),
            "sessionId": h.get("sessionId"),
            "source": h.get("source"),
            "title": h.get("title"),
            "score": float(h.get("score", 0.0)),
            "text": h.get("text") or "",
        })
    print(f"[RAG DUMP] sessionId={sessionId} k={k} -> {len(chunks)} items")
    return {"chunks": chunks}

@router.get("/uploads")
async def api_list_uploads(sessionId: Optional[str] = None, scope: str = "all"):
    include_global = scope != "session"
    return {"uploads": rag_list_sources(sessionId, include_global=include_global)}

@router.post("/uploads/delete-hard")
async def api_delete_upload_hard(body: dict[str, str]):
    source = (body.get("source") or "").strip()
    session_id = (body.get("sessionId") or None)
    if not source:
        return {"error": "source required"}

    # use the same model as ingest/search
    model = _get_st_model()  # you already have this in rag.py
    def _embed(texts: List[str]):
        arr = model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)
        return arr.astype("float32")

    out = hard_delete_source(source, session_id=session_id, embedder=_embed)
    return out

# ===== aimodel/file_read/api/settings.py =====

#aimodel/file_read/api/settings.py
from .barrel import *

from ..core.settings import SETTINGS

router = APIRouter(prefix="/api/settings", tags=["settings"])

@router.get("/defaults")
def get_defaults():
    return SETTINGS.defaults

@router.get("/overrides")
def get_overrides():
    return SETTINGS.overrides

@router.patch("/overrides")
def patch_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.patch_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.put("/overrides")
def put_overrides(payload: Dict[str, Any] = Body(...)):
    SETTINGS.replace_overrides(payload)
    return {"ok": True, "overrides": SETTINGS.overrides}

@router.get("/adaptive")
def get_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.adaptive(session_id=session_id)

@router.post("/adaptive/recompute")
def recompute_adaptive(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    SETTINGS.recompute_adaptive(session_id=session_id)
    return {"ok": True, "adaptive": SETTINGS.adaptive(session_id=session_id)}

@router.get("/effective")
def get_effective(session_id: Optional[str] = Query(default=None, alias="sessionId")):
    return SETTINGS.effective(session_id=session_id)

# ===== aimodel/file_read/app.py =====

from __future__ import annotations
import os, asyncio, atexit
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .adaptive.config.paths import bootstrap
from .workers.retitle_worker import start_worker
from .api.models import router as models_router
from .api.chats import router as chats_router
from .runtime.model_runtime import load_model
from .api.generate_router import router as generate_router
from .api import metrics as metrics_api
from .api.rag import router as rag_router
from .services.cancel import is_active
from .api import settings as settings_router
from .core.settings import SETTINGS

bootstrap()
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("APP_CORS_ORIGIN", "http://localhost:5173")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(models_router)
app.include_router(chats_router)
app.include_router(generate_router)
app.include_router(settings_router.router)
app.include_router(rag_router)
app.include_router(metrics_api.router)

_warmup_stop_ev = None

@app.on_event("startup")
async def _startup():
    try:
        load_model(config_patch={})
        print("✅ llama model loaded at startup")
    except Exception as e:
        print(f"❌ llama failed to load at startup: {e}")

    asyncio.create_task(start_worker(), name="retitle_worker")


# ===== aimodel/file_read/core/__init__.py =====



# ===== aimodel/file_read/core/files.py =====

# aimodel/file_read/core/files.py
from __future__ import annotations
from pathlib import Path
import json, os
from typing import Any

# Base dirs
CORE_DIR = Path(__file__).resolve().parent
STORE_DIR = CORE_DIR.parent / "store"

# Paths (overridable via env)
EFFECTIVE_SETTINGS_FILE = Path(
    os.getenv("EFFECTIVE_SETTINGS_PATH", str(STORE_DIR / "effective_settings.json"))
)
OVERRIDES_SETTINGS_FILE = Path(
    os.getenv("OVERRIDES_SETTINGS_PATH", str(STORE_DIR / "override_settings.json"))
)
DEFAULTS_SETTINGS_FILE = Path(
    os.getenv("DEFAULT_SETTINGS_PATH", str(STORE_DIR / "default_settings.json"))
)

def load_json_file(path: Path, default: Any = None) -> Any:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return {} if default is None else default

def save_json_file(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# ===== aimodel/file_read/core/logging_init.py =====

# aimodel/file_read/core/logging_init.py
from __future__ import annotations
import logging, sys

def init_logging(level: int = logging.INFO) -> None:
    # Root + uvicorn
    logging.basicConfig(
        level=level,
        format="%(asctime)s %(levelname).1s %(name)s: %(message)s",
    )
    # Make sure our module loggers show up
    logging.getLogger("aimodel").setLevel(level)
    logging.getLogger("aimodel.api.generate").setLevel(level)

    # Capture C++ backend prints (llama_perf_context_print) from stderr
    class _StderrToLogger:
        def __init__(self, logger: logging.Logger) -> None:
            self._logger = logger
        def write(self, msg: str) -> None:
            msg = (msg or "").rstrip()
            if msg:
                # Use INFO to avoid dropping at default Uvicorn levels
                self._logger.info(msg)
        def flush(self) -> None:
            pass

    sys.stderr = _StderrToLogger(logging.getLogger("llama_cpp.stderr"))

# ===== aimodel/file_read/core/packing_memory_core.py =====

from __future__ import annotations
import math, time
from pathlib import Path
from typing import Dict, List, Tuple
from collections import deque

from ..runtime.model_runtime import get_llm
from .style import STYLE_SYS
from ..store import get_summary as store_get_summary
from ..store import list_messages as store_list_messages
from ..utils.streaming import strip_runjson
from ..core.files import EFFECTIVE_SETTINGS_FILE, load_json_file

SESSIONS: Dict[str, Dict] = {}
PACK_TELEMETRY: Dict[str, object] = {
    "packSec": 0.0,
    "summarySec": 0.0,
    "finalTrimSec": 0.0,
    "compressSec": 0.0,
    "summaryTokensApprox": 0,
    "summaryUsedLLM": False,
    "summaryBullets": 0,
    "summaryAddedChars": 0,
    "summaryOutTokensApprox": 0,
    "summaryCompressedFromChars": 0,
    "summaryCompressedToChars": 0,
    "summaryCompressedDroppedChars": 0,
}
SUMMARY_TEL = PACK_TELEMETRY

def _now() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

def _log(msg: str) -> None:
    print(f"[{_now()}] {msg}")

def _snapshot(cfg: Dict) -> str:
    keys = [
        "model_ctx", "out_budget", "reserved_system_tokens", "min_input_budget",
        "chars_per_token", "prompt_per_message_overhead",
        "summary_max_chars", "use_fast_summary",
    ]
    parts = []
    for k in keys:
        if k in cfg:
            parts.append(f"{k}={cfg[k]}")
    return ", ".join(parts)

class _SettingsCache:
    def __init__(self) -> None:
        self.path: Path = EFFECTIVE_SETTINGS_FILE
        self._mtime: float | None = None
        self._data: Dict = {}
        _log(f"settings path = {self.path}")

    def get(self) -> Dict:
        try:
            m = self.path.stat().st_mtime
        except FileNotFoundError:
            m = None
        if self._mtime != m or not self._data:
            self._data = load_json_file(self.path, default={})
            self._mtime = m
            _log(f"settings reload ok file={self.path.name} snapshot: {_snapshot(self._data)}")
        return self._data

_SETTINGS = _SettingsCache()

def approx_tokens(text: str) -> int:
    cfg = _SETTINGS.get()
    return max(1, math.ceil(len(text) / int(cfg["chars_per_token"])))

def count_prompt_tokens(msgs: List[Dict[str, str]]) -> int:
    cfg = _SETTINGS.get()
    overhead = int(cfg["prompt_per_message_overhead"])
    return sum(approx_tokens(m["content"]) + overhead for m in msgs)

def get_session(session_id: str):
    cfg = _SETTINGS.get()
    _log(f"get_session IN session={session_id} (settings: {_snapshot(cfg)})")
    st = SESSIONS.setdefault(session_id, {
        "summary": "",
        "recent": deque(maxlen=int(cfg["recent_maxlen"])),
        "style": STYLE_SYS,
        "short": False,
        "bullets": False,
    })
    if not st["summary"]:
        try:
            st["summary"] = store_get_summary(session_id) or ""
            _log(f"get_session loaded summary len={len(st['summary'])}")
        except Exception as e:
            _log(f"get_session summary load error {e}")
    if not st["recent"]:
        try:
            rows = store_list_messages(session_id)
            tail = rows[-st["recent"].maxlen:]
            for m in tail:
                st["recent"].append({"role": m.role, "content": strip_runjson(m.content)})
            _log(f"get_session hydrated recent={len(st['recent'])}")
        except Exception as e:
            _log(f"get_session hydrate error {e}")
    return st

def _heuristic_bullets(chunks: List[Dict[str,str]], cfg: Dict) -> str:
    max_bullets = int(cfg["heuristic_max_bullets"])
    max_words = int(cfg["heuristic_max_words"])
    prefix = cfg["bullet_prefix"]
    bullets = []
    for m in chunks:
        txt = " ".join((m.get("content") or "").split())
        if not txt:
            continue
        words = txt.replace("\n", " ").split()
        snippet = " ".join(words[:max_words]) if words else ""
        bullets.append(f"{prefix}{snippet}" if snippet else prefix.strip())
        if len(bullets) >= max_bullets:
            break
    return "\n".join(bullets) if bullets else prefix.strip()

def summarize_chunks(chunks: List[Dict[str,str]]) -> Tuple[str, bool]:
    cfg = _SETTINGS.get()
    t0 = time.time()
    PACK_TELEMETRY["summarySec"] = 0.0
    PACK_TELEMETRY["summaryTokensApprox"] = 0
    PACK_TELEMETRY["summaryUsedLLM"] = False
    PACK_TELEMETRY["summaryBullets"] = 0
    PACK_TELEMETRY["summaryAddedChars"] = 0
    PACK_TELEMETRY["summaryOutTokensApprox"] = 0
    use_fast = bool(cfg["use_fast_summary"])
    _log(f"summarize_chunks IN chunks={len(chunks)} FAST={use_fast}")
    if use_fast:
        txt = _heuristic_bullets(chunks, cfg)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = False
        PACK_TELEMETRY["summaryBullets"] = len([l for l in txt.splitlines() if l.strip()])
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        _log(f"summarize_chunks OUT (FAST) bullets={len([l for l in txt.splitlines() if l])} chars={len(txt)} dt={dt:.2f}s")
        return txt, False
    text = "\n".join(f'{m.get("role","")}: {m.get("content","")}' for m in chunks)
    sys_inst = cfg["summary_sys_inst"]
    user_prompt = cfg["summary_user_prefix"] + text + cfg["summary_user_suffix"]
    llm = get_llm()
    out = llm.create_chat_completion(
        messages=[
            {"role": "system", "content": sys_inst},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=int(cfg["llm_summary_max_tokens"]),
        temperature=float(cfg["llm_summary_temperature"]),
        top_p=float(cfg["llm_summary_top_p"]),
        stream=False,
        stop=list(cfg["llm_summary_stop"]),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip()
    lines = [ln.strip() for ln in raw.splitlines()]
    bullets: List[str] = []
    seen = set()
    max_words = int(cfg["heuristic_max_words"])
    max_bullets = int(cfg["heuristic_max_bullets"])
    for ln in lines:
        if not ln.startswith(cfg["bullet_prefix"]):
            continue
        norm = " ".join(ln[len(cfg["bullet_prefix"]):].lower().split())
        if not norm or norm in seen:
            continue
        seen.add(norm)
        words = ln[len(cfg["bullet_prefix"]):].split()
        if len(words) > max_words:
            ln = cfg["bullet_prefix"] + " ".join(words[:max_words])
        bullets.append(ln)
        if len(bullets) >= max_bullets:
            break
    if bullets:
        txt = "\n".join(bullets)
        dt = time.time() - t0
        PACK_TELEMETRY["summarySec"] = float(dt)
        PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(txt))
        PACK_TELEMETRY["summaryUsedLLM"] = True
        PACK_TELEMETRY["summaryBullets"] = len(bullets)
        PACK_TELEMETRY["summaryAddedChars"] = len(txt)
        PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(txt))
        _log(f"summarize_chunks OUT bullets={len(bullets)} chars={len(txt)} dt={dt:.2f}s")
        return txt, True
    s = " ".join(raw.split())[:160]
    fallback = (cfg["bullet_prefix"] + s) if s else cfg["bullet_prefix"].strip()
    dt = time.time() - t0
    PACK_TELEMETRY["summarySec"] = float(dt)
    PACK_TELEMETRY["summaryTokensApprox"] = int(approx_tokens(sys_inst) + approx_tokens(user_prompt) + approx_tokens(fallback))
    PACK_TELEMETRY["summaryUsedLLM"] = True
    PACK_TELEMETRY["summaryBullets"] = len([l for l in fallback.splitlines() if l.strip()])
    PACK_TELEMETRY["summaryAddedChars"] = len(fallback)
    PACK_TELEMETRY["summaryOutTokensApprox"] = int(approx_tokens(fallback))
    _log(f"summarize_chunks OUT bullets=0 chars={len(fallback)} dt={dt:.2f}s")
    return fallback, True

def _compress_summary_block(s: str) -> str:
    cfg = _SETTINGS.get()
    max_chars = int(cfg["summary_max_chars"])
    prefix = cfg["bullet_prefix"]
    lines = [ln.strip() for ln in (s or "").splitlines()]
    out, seen = [], set()
    for ln in lines:
        if not ln.startswith(prefix):
            continue
        norm = " ".join(ln[len(prefix):].lower().split())
        if norm in seen:
            continue
        seen.add(norm)
        out.append(ln)
    text = "\n".join(out)
    PACK_TELEMETRY["summaryCompressedFromChars"] = int(len(s or ""))
    if len(text) > max_chars:
        last, total = [], 0
        for ln in reversed(out):
            if total + len(ln) + 1 > max_chars:
                break
            last.append(ln)
            total += len(ln) + 1
        text = "\n".join(reversed(last))
    PACK_TELEMETRY["summaryCompressedToChars"] = int(len(text))
    PACK_TELEMETRY["summaryCompressedDroppedChars"] = int(max(0, int(PACK_TELEMETRY["summaryCompressedFromChars"]) - int(PACK_TELEMETRY["summaryCompressedToChars"])))
    _log(f"compress_summary IN chars={len(s)} kept_lines={len(out)}")
    _log(f"compress_summary OUT chars={len(text)} lines={len(text.splitlines())}")
    return text

# ===== aimodel/file_read/core/packing_ops.py =====

from __future__ import annotations
import time
from typing import Dict, List

from .packing_memory_core import (
    _SETTINGS,
    _log,
    count_prompt_tokens,
    approx_tokens,
    summarize_chunks,
    _compress_summary_block,
    PACK_TELEMETRY,
)
from .style import STYLE_SYS

def build_system(style: str, short: bool, bullets: bool) -> str:
    cfg = _SETTINGS.get()
    _log(f"build_system flags short={short} bullets={bullets}")
    parts = [STYLE_SYS]
    if style and style != STYLE_SYS:
        parts.append(style)
    if short:
        parts.append(cfg["system_brief_directive"])
    if bullets:
        parts.append(cfg["system_bullets_directive"])
    parts.append(cfg["system_follow_user_style_directive"])
    return " ".join(parts)

def pack_messages(style: str, short: bool, bullets: bool, summary, recent, max_ctx, out_budget):
    t0_pack = time.time()
    cfg = _SETTINGS.get()
    model_ctx = int(max_ctx or cfg["model_ctx"])
    gen_budget = int(out_budget or cfg["out_budget"])
    reserved = int(cfg["reserved_system_tokens"])
    input_budget = model_ctx - gen_budget - reserved
    if input_budget < int(cfg["min_input_budget"]):
        input_budget = int(cfg["min_input_budget"])
    sys_text = build_system(style, short, bullets)
    prologue = [{"role": "user", "content": sys_text}]
    if summary:
        prologue.append({"role": "user", "content": cfg["summary_header_prefix"] + summary})
    packed = prologue + list(recent)
    try:
        PACK_TELEMETRY["packInputTokensApprox"] = int(count_prompt_tokens(packed))
        PACK_TELEMETRY["packMsgs"] = int(len(packed))
    except Exception:
        pass
    _log(f"pack_messages SETTINGS snapshot: {model_ctx=}, {gen_budget=}, {input_budget=}")
    _log(f"pack_messages OUT msgs={len(packed)} tokens~{count_prompt_tokens(packed)} (model_ctx={model_ctx}, out_budget={gen_budget}, input_budget={input_budget})")
    PACK_TELEMETRY["packSec"] += float(time.time() - t0_pack)
    return packed, input_budget

def _final_safety_trim(packed: List[Dict[str,str]], input_budget: int) -> List[Dict[str,str]]:
    t0 = time.time()
    cfg = _SETTINGS.get()
    keep_ratio = float(cfg["final_shrink_summary_keep_ratio"])
    min_keep = int(cfg["final_shrink_summary_min_chars"])
    def toks() -> int:
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    t_before = toks()
    PACK_TELEMETRY["finalTrimTokensBefore"] = int(t_before)
    dropped_msgs = 0
    dropped_tokens = 0
    _log(f"final_trim START tokens={toks()} budget={input_budget}")
    keep_head = 2 if len(packed) >= 2 and isinstance(packed[1].get("content"), str) and packed[1]["content"].startswith(cfg["summary_header_prefix"]) else 1
    while toks() > input_budget and len(packed) > keep_head + 1:
        dropped = packed.pop(keep_head)
        try:
            dropped_tokens += int(approx_tokens(dropped["content"]))
            dropped_msgs += 1
        except Exception:
            pass
        _log(f"final_trim DROP msg role={dropped['role']} size~{approx_tokens(dropped['content'])} toks={toks()}")
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        summary_msg = packed[1]
        txt = summary_msg["content"]
        n = max(min_keep, int(len(txt) * keep_ratio))
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"] = int(len(txt))
        except Exception:
            pass
        summary_msg["content"] = txt[-n:]
        try:
            PACK_TELEMETRY["finalTrimSummaryShrunkToChars"] = int(len(summary_msg["content"]))
            PACK_TELEMETRY["finalTrimSummaryDroppedChars"] = int(max(0, int(PACK_TELEMETRY["finalTrimSummaryShrunkFromChars"]) - int(PACK_TELEMETRY["finalTrimSummaryShrunkToChars"])))
        except Exception:
            pass
        _log(f"final_trim SHRINK summary to {len(summary_msg['content'])} chars toks={toks()}")
    if toks() > input_budget and keep_head == 2 and len(packed) >= 2:
        removed = packed.pop(1)
        try:
            dropped_tokens += int(approx_tokens(removed["content"]))
            dropped_msgs += 1
        except Exception:
            pass
        _log(f"final_trim REMOVE summary len~{len(removed['content'])} toks={toks()}")
    while toks() > input_budget and len(packed) > 2:
        removed = packed.pop(2 if len(packed) > 3 else 1)
        try:
            dropped_tokens += int(approx_tokens(removed["content"]))
            dropped_msgs += 1
        except Exception:
            pass
        _log(f"final_trim LAST_RESORT drop size~{approx_tokens(removed['content'])} toks={toks()}")
    t_after = toks()
    PACK_TELEMETRY["finalTrimTokensAfter"] = int(t_after)
    PACK_TELEMETRY["finalTrimDroppedMsgs"] = int(dropped_msgs)
    PACK_TELEMETRY["finalTrimDroppedApproxTokens"] = int(max(0, dropped_tokens))
    _log(f"final_trim END tokens={toks()} msgs={len(packed)}")
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0)
    return packed

def roll_summary_if_needed(packed, recent, summary, input_budget, system_text):
    cfg = _SETTINGS.get()
    _log("=== roll_summary_if_needed DEBUG START ===")
    _log(f"skip_overage_lt={cfg['skip_overage_lt']}, max_peel_per_turn={cfg['max_peel_per_turn']}, peel_min={cfg['peel_min']}, peel_frac={cfg['peel_frac']}, peel_max={cfg['peel_max']}")
    _log(f"len(recent)={len(recent)}, current_summary_len={len(summary) if summary else 0}")
    _log(f"input_budget={input_budget}, reserved_system_tokens={cfg['reserved_system_tokens']}")
    _log(f"model_ctx={cfg['model_ctx']}, out_budget={cfg['out_budget']}")
    def _tok():
        try:
            return count_prompt_tokens(packed)
        except Exception:
            return 999999
    start_tokens = _tok()
    overage = start_tokens - input_budget
    PACK_TELEMETRY["rollStartTokens"] = int(start_tokens)
    PACK_TELEMETRY["rollOverageTokens"] = int(overage)
    _log(f"roll_summary_if_needed START tokens={start_tokens} input_budget={input_budget} overage={overage}")
    if overage <= int(cfg["skip_overage_lt"]):
        _log(f"roll_summary_if_needed SKIP (overage {overage} <= {cfg['skip_overage_lt']})")
        packed = _final_safety_trim(packed, input_budget)
        PACK_TELEMETRY["rollEndTokens"] = int(count_prompt_tokens(packed))
        _log(f"roll_summary_if_needed END tokens={count_prompt_tokens(packed)}")
        _log("=== roll_summary_if_needed DEBUG END ===")
        return packed, summary
    peels_done = 0
    peeled_n = 0
    if len(recent) > 6 and peels_done < int(cfg["max_peel_per_turn"]):
        peel_min = int(cfg["peel_min"])
        peel_frac = float(cfg["peel_frac"])
        peel_max = int(cfg["peel_max"])
        target = max(peel_min, min(peel_max, int(len(recent) * peel_frac)))
        peel = []
        for _ in range(min(target, len(recent))):
            peel.append(recent.popleft())
        peeled_n = len(peel)
        _log(f"roll_summary peeled={len(peel)}")
        t0_sum = time.time()
        new_sum, _used_llm = summarize_chunks(peel)
        PACK_TELEMETRY["summarySec"] += float(time.time() - t0_sum)
        if new_sum.startswith(cfg["bullet_prefix"]):
            summary = (summary + "\n" + new_sum).strip() if summary else new_sum
        else:
            summary = new_sum
        t0_comp = time.time()
        summary = _compress_summary_block(summary)
        PACK_TELEMETRY["compressSec"] += float(time.time() - t0_comp)
        try:
            PACK_TELEMETRY["rollPeeledMsgs"] = int(peeled_n)
            PACK_TELEMETRY["rollNewSummaryChars"] = int(len(summary))
            PACK_TELEMETRY["rollNewSummaryTokensApprox"] = int(approx_tokens(summary))
        except Exception:
            pass
        packed = [
            {"role": "user", "content": system_text},
            {"role": "user", "content": cfg["summary_header_prefix"] + summary},
            *list(recent),
        ]
        _log(f"roll_summary updated summary_len={len(summary)} tokens={_tok()}")
    t0_trim = time.time()
    packed = _final_safety_trim(packed, input_budget)
    PACK_TELEMETRY["finalTrimSec"] += float(time.time() - t0_trim)
    end_tokens = count_prompt_tokens(packed)
    PACK_TELEMETRY["rollEndTokens"] = int(end_tokens)
    _log(f"roll_summary_if_needed END tokens={end_tokens}")
    _log("=== roll_summary_if_needed DEBUG END ===")
    return packed, summary

# ===== aimodel/file_read/core/schemas.py =====

# core/schemas.py
from __future__ import annotations
from typing import Optional, List, Literal
from pydantic import BaseModel


class Attachment(BaseModel):
    name: str
    source: Optional[str] = None
    sessionId: Optional[str] = None

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str
    attachments: Optional[List[Attachment]] = None  # ✅ new

class MergeChatReq(BaseModel):
    sourceId: str
    targetId: Optional[str] = None
    newChat: bool = False

class ChatMetaModel(BaseModel):
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str] = None
    createdAt: str
    updatedAt: str

class PageResp(BaseModel):
    content: List[ChatMetaModel]
    totalElements: int
    totalPages: int
    size: int
    number: int
    first: bool
    last: bool
    empty: bool

class BatchMsgDeleteReq(BaseModel):
    messageIds: List[int]

class BatchDeleteReq(BaseModel):
    sessionIds: List[str]

class EditMessageReq(BaseModel):
    messageId: int
    content: str

class ChatBody(BaseModel):
    sessionId: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None

    # ↓ make optional; defaults come from effective settings
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None

    # ↓ also optional; defaults come from settings
    autoWeb: Optional[bool] = None
    webK: Optional[int] = None
    autoRag: Optional[bool] = None   

# ===== aimodel/file_read/core/settings.py =====

# aimodel/file_read/core/settings.py
from __future__ import annotations

import json
from threading import RLock
from typing import Any, Dict, Optional

from .files import (
    DEFAULTS_SETTINGS_FILE,
    OVERRIDES_SETTINGS_FILE,
    EFFECTIVE_SETTINGS_FILE,
    load_json_file,
    save_json_file,
)


def _deep_merge(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(dst)
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)  # type: ignore[arg-type]
        else:
            out[k] = v
    return out


class _SettingsManager:
    """
    layers: defaults → adaptive(session/_global_) → overrides
    also persists the merged *global* effective to EFFECTIVE_SETTINGS_FILE
    (that’s what memory.py watches/loads)

    Dynamic access:
      - Attribute style: SETTINGS.stream_queue_maxsize
      - Dict style:      SETTINGS["stream_queue_maxsize"]
      - Safe get:        SETTINGS.get("stream_queue_maxsize", 64)

    Only keys present in the merged effective map are exposed dynamically.
    """

    def __init__(self) -> None:
        self._lock = RLock()
        self._defaults: Dict[str, Any] = self._load_defaults()
        self._overrides: Dict[str, Any] = self._load_overrides()
        self._adaptive_by_session: Dict[str, Dict[str, Any]] = {}
        # write initial effective so memory.py has something on boot
        self._persist_effective_unlocked()

    # ---------- loading ----------
    def _load_defaults(self) -> Dict[str, Any]:
        return load_json_file(DEFAULTS_SETTINGS_FILE, default={})

    def _load_overrides(self) -> Dict[str, Any]:
        return load_json_file(OVERRIDES_SETTINGS_FILE, default={})

    # ---------- saving ----------
    def _save_overrides_unlocked(self) -> None:
        save_json_file(OVERRIDES_SETTINGS_FILE, self._overrides)

    def _persist_effective_unlocked(self) -> None:
        # Persist *global* effective (session-less) for memory.py
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get("_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        save_json_file(EFFECTIVE_SETTINGS_FILE, eff)

    # ---------- helpers ----------
    def _effective_unlocked(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        eff = _deep_merge(self._defaults, self._adaptive_by_session.get(session_id or "_global_", {}))
        eff = _deep_merge(eff, self._overrides)
        return eff

    def _get_unlocked(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        eff = self._effective_unlocked(session_id)
        if key in eff:
            return eff[key]
        if default is not None:
            return default
        raise AttributeError(f"_SettingsManager has no key '{key}'")

    # ---------- public read API ----------
    @property
    def defaults(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._defaults))

    @property
    def overrides(self) -> Dict[str, Any]:
        with self._lock:
            return json.loads(json.dumps(self._overrides))

    def adaptive(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        key = session_id or "_global_"
        with self._lock:
            return json.loads(json.dumps(self._adaptive_by_session.get(key, {})))

    def effective(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        with self._lock:
            return self._effective_unlocked(session_id)

    # Dynamic attribute access (e.g., SETTINGS.stream_queue_maxsize)
    def __getattr__(self, name: str) -> Any:
        # Only called if normal attributes/methods aren't found
        with self._lock:
            return self._get_unlocked(name)

    # Dict-style access (e.g., SETTINGS["stream_queue_maxsize"])
    def __getitem__(self, key: str) -> Any:
        with self._lock:
            return self._get_unlocked(key)

    # Safe getter (optional default)
    def get(self, key: str, default: Any = None, *, session_id: Optional[str] = None) -> Any:
        with self._lock:
            try:
                return self._get_unlocked(key, default=default, session_id=session_id)
            except AttributeError:
                return default

    # ---------- public write API ----------
    def patch_overrides(self, patch: Dict[str, Any]) -> None:
        if not isinstance(patch, dict):
            return
        with self._lock:
            self._overrides = _deep_merge(self._overrides, patch)
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def replace_overrides(self, new_overrides: Dict[str, Any]) -> None:
        if not isinstance(new_overrides, dict):
            new_overrides = {}
        with self._lock:
            self._overrides = json.loads(json.dumps(new_overrides))
            self._save_overrides_unlocked()
            self._persist_effective_unlocked()

    def reload_overrides(self) -> None:
        with self._lock:
            self._overrides = self._load_overrides()
            self._persist_effective_unlocked()

    def set_adaptive_for_session(self, session_id: Optional[str], values: Dict[str, Any]) -> None:
        key = session_id or "_global_"
        if not isinstance(values, dict):
            values = {}
        with self._lock:
            self._adaptive_by_session[key] = json.loads(json.dumps(values))
            # If updating the global adaptive layer, refresh persisted effective
            if key == "_global_":
                self._persist_effective_unlocked()

    def recompute_adaptive(self, session_id: Optional[str] = None) -> None:
        # placeholder for your controller logic later
        with self._lock:
            # after recompute, also refresh persisted effective for global
            self._persist_effective_unlocked()


SETTINGS = _SettingsManager()

# ===== aimodel/file_read/core/style.py =====

from __future__ import annotations
import re
from typing import Optional, Tuple

STYLE_SYS = (
    "You are a helpful assistant. "
    "Always follow the user's explicit instructions carefully and exactly. "
    "Do not repeat yourself. Stay coherent and complete."
)

PAT_TALK_LIKE = re.compile(r"\btalk\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_RESPOND_LIKE = re.compile(r"\brespond\s+like\s+(?P<style>[^.;\n]+)", re.I)
PAT_BE = re.compile(r"\bbe\s+(?P<style>[^.;\n]+)", re.I)
PAT_FROM_NOW = re.compile(r"\bfrom\s+now\s+on[, ]+\s*(?P<style>[^.;\n]+)", re.I)

def extract_style_and_prefs(user_text: str) -> Tuple[Optional[str], bool, bool]:
    t = user_text.strip()
    style_match = (
        PAT_TALK_LIKE.search(t)
        or PAT_RESPOND_LIKE.search(t)
        or PAT_FROM_NOW.search(t)
        or PAT_BE.search(t)
    )
    style_inst: Optional[str] = None
    if style_match:
        raw = style_match.group("style").strip().rstrip(".")
        style_inst = (
            f"You must talk like {raw}. "
            f"Stay in character but remain helpful and accurate. "
            f"Follow the user's latest style instructions."
        )
    return style_inst, False, False

# ===== aimodel/file_read/rag/ingest/__init__.py =====

# aimodel/file_read/rag/ingest/__init__.py
from __future__ import annotations
from typing import Tuple
import io, json
from .xls_ingest import extract_xls
from .excel_ingest import extract_excel
from .csv_ingest import extract_csv
from .common import _utf8, _strip_html, Chunk, chunk_text, build_metas
from .docx_ingest import extract_docx
from .doc_binary_ingest import extract_doc_binary
from .ppt_ingest import extract_pptx, extract_ppt
from .pdf_ingest import extract_pdf   # <-- new
from ...core.settings import SETTINGS

__all__ = ["sniff_and_extract", "Chunk", "chunk_text", "build_metas"]

def _ing_dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[ingest]", *args)
    except Exception:
        pass

def sniff_and_extract(filename: str, data: bytes) -> Tuple[str, str]:
    name = (filename or "").lower()
    _ing_dbg("route:", name, "bytes=", len(data))

    if name.endswith((".pptx", ".pptm")):
        _ing_dbg("-> pptx/pptm")
        return extract_pptx(data)

    if name.endswith(".ppt"):
        _ing_dbg("-> ppt (ole)")
        return extract_ppt(data)

    if name.endswith((".xlsx", ".xlsm")):
        _ing_dbg("-> excel")
        return extract_excel(data)

    if name.endswith(".xls"):
        _ing_dbg("-> excel (xls via xlrd)")
        return extract_xls(data)

    if name.endswith((".csv", ".tsv")):
        _ing_dbg("-> csv/tsv")
        return extract_csv(data)

    if name.endswith(".docx"):
        _ing_dbg("-> docx")
        try:
            return extract_docx(data)
        except Exception as e:
            _ing_dbg("docx err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".doc"):
        _ing_dbg("-> doc (binary/rtf)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("doc err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".rtf"):
        _ing_dbg("-> rtf (via doc_binary)")
        try:
            return extract_doc_binary(data)
        except Exception as e:
            _ing_dbg("rtf err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".pdf"):
        _ing_dbg("-> pdf (delegating to extract_pdf)")
        print("[ingest] call extract_pdf()", flush=True)  # unconditional marker
        from .pdf_ingest import extract_pdf
        return extract_pdf(data)

    if name.endswith(".json"):
        _ing_dbg("-> json")
        try:
            return json.dumps(json.loads(_utf8(data)), ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("json err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".jsonl", ".jsonlines")):
        _ing_dbg("-> jsonl")
        lines = _utf8(data).splitlines()
        out = []
        for ln in lines:
            ln = ln.strip()
            if not ln:
                continue
            try:
                out.append(json.dumps(json.loads(ln), ensure_ascii=False, indent=2))
            except Exception:
                out.append(ln)
        return "\n".join(out).strip(), "text/plain"

    if name.endswith((".yaml", ".yml")):
        _ing_dbg("-> yaml")
        try:
            import yaml
            obj = yaml.safe_load(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("yaml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith(".toml"):
        _ing_dbg("-> toml")
        try:
            try:
                import tomllib
                obj = tomllib.loads(_utf8(data))
            except Exception:
                import toml
                obj = toml.loads(_utf8(data))
            return json.dumps(obj, ensure_ascii=False, indent=2), "text/plain"
        except Exception as e:
            _ing_dbg("toml err:", repr(e))
            return _utf8(data), "text/plain"

    if name.endswith((".htm", ".html", ".xml")):
        _ing_dbg("-> html/xml")
        return _strip_html(_utf8(data)), "text/plain"

    if name.endswith((
        ".txt", ".log", ".md",
        ".c", ".cpp", ".h", ".hpp",
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".sh", ".ps1",
        ".rs", ".java", ".go", ".rb", ".php",
        ".swift", ".kt", ".scala", ".lua", ".perl",
    )):
        _ing_dbg("-> plaintext/code")
        return _utf8(data), "text/plain"

    _ing_dbg("-> default fallback")
    return _utf8(data), "text/plain"

# ===== aimodel/file_read/rag/ingest/common.py =====

# ===== aimodel/file_read/rag/ingest/common.py =====
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional
import re
from ...core.settings import SETTINGS

@dataclass
class Chunk:
    text: str
    meta: Dict[str, str]

# --- Small helpers kept for legacy imports (ingest.__init__ depends on these) ---
def _utf8(data: bytes) -> str:

    return (data or b"").decode("utf-8", errors="ignore")

def _strip_html(txt: str) -> str:

    if not txt:
        return ""
    # drop scripts/styles
    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", txt)
    # common block/line breaks
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p>", "\n\n", txt)
    # strip tags
    txt = re.sub(r"(?is)<.*?>", " ", txt)
    # collapse whitespace
    txt = re.sub(r"[ \t]+", " ", txt)
    return txt.strip()

# --- Section & paragraph aware chunking for table/text docs -------------------
_HDR_RE = re.compile(r"^(#{1,3})\s+.*$", flags=re.MULTILINE)
_PARA_SPLIT_RE = re.compile(r"\n\s*\n+")

def _split_sections(text: str) -> List[str]:

    text = (text or "").strip()
    if not text:
        return []
    starts = [m.start() for m in _HDR_RE.finditer(text)]
    if not starts:
        return [text]
    # ensure beginning is a split point
    if 0 not in starts:
        starts = [0] + starts
    sections: List[str] = []
    for i, s in enumerate(starts):
        e = starts[i + 1] if i + 1 < len(starts) else len(text)
        block = text[s:e].strip()
        if block:
            sections.append(block)
    return sections

def _split_paragraphs(block: str) -> List[str]:
    paras = [p.strip() for p in _PARA_SPLIT_RE.split(block or "")]
    return [p for p in paras if p]

def _hard_split(text: str, max_len: int) -> List[str]:

    approx = re.split(r"(?<=[\.\!\?\;])\s+", text or "")
    out: List[str] = []
    buf = ""
    for s in approx:
        if not s:
            continue
        if len(buf) + (1 if buf else 0) + len(s) <= max_len:
            buf = s if not buf else (buf + " " + s)
        else:
            if buf:
                out.append(buf)
            if len(s) <= max_len:
                out.append(s)
            else:
                words = re.split(r"\s+", s)
                cur = ""
                for w in words:
                    if not w:
                        continue
                    if len(cur) + (1 if cur else 0) + len(w) <= max_len:
                        cur = w if not cur else (cur + " " + w)
                    else:
                        if cur:
                            out.append(cur)
                        cur = w
                if cur:
                    out.append(cur)
            buf = ""
    if buf:
        out.append(buf)
    return out

def _pack_with_budget(pieces: List[str], *, max_chars: int) -> List[str]:
    chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for p in pieces:
        plen = len(p)
        if plen > max_chars:
            chunks.extend(_hard_split(p, max_chars))
            continue
        if cur_len == 0:
            cur, cur_len = [p], plen
            continue
        if cur_len + 2 + plen <= max_chars:  # 2 for "\n\n" when joining
            cur.append(p)
            cur_len += 2 + plen
        else:
            chunks.append("\n\n".join(cur).strip())
            cur, cur_len = [p], plen
    if cur_len:
        chunks.append("\n\n".join(cur).strip())
    return chunks

def chunk_text(
    text: str,
    meta: Optional[Dict[str, str]] = None,
    *,
    max_chars: int = int(SETTINGS.get("rag_max_chars_per_chunk", 800)),
    overlap: int = int(SETTINGS.get("rag_chunk_overlap_chars", 150)),
) -> List[Chunk]:

    base_meta = (meta or {}).copy()
    text = (text or "").strip()
    if not text:
        return []

    if len(text) <= max_chars:
        return [Chunk(text=text, meta=base_meta)]

    sections = _split_sections(text)
    if not sections:
        sections = [text]

    chunks: List[Chunk] = []
    last_tail: Optional[str] = None

    for sec in sections:
        paras = _split_paragraphs(sec)
        if not paras:
            continue
        packed = _pack_with_budget(paras, max_chars=max_chars)
        for ch in packed:
            if last_tail and overlap > 0:
                tail = last_tail[-overlap:] if len(last_tail) > overlap else last_tail
                candidate = f"{tail}\n{ch}"
                chunks.append(Chunk(text=candidate if len(candidate) <= max_chars else ch, meta=base_meta))
            else:
                chunks.append(Chunk(text=ch, meta=base_meta))
            last_tail = ch

    return chunks

def build_metas(session_id: Optional[str], filename: str, chunks: List[Chunk], *, size: int = 0) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for i, c in enumerate(chunks):
        out.append({
            "id": f"{filename}:{i}",
            "sessionId": session_id or "",
            "source": filename,
            "title": filename,
            "mime": "text/plain",
            "size": str(size),
            "chunkIndex": str(i),
            "text": c.text,  # stored for RAG snippet display
        })
    return out

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====

# ===== aimodel/file_read/rag/ingest/csv_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
import io, re, csv
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_csv(data: bytes) -> Tuple[str, str]:
    S = SETTINGS.effective
    max_chars = int(S().get("csv_value_max_chars"))
    quote_strings = bool(S().get("csv_quote_strings"))
    header_normalize = bool(S().get("csv_header_normalize"))
    max_rows = int(S().get("csv_infer_max_rows"))
    max_cols = int(S().get("csv_infer_max_cols"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_val(v) -> str:
        if v is None:
            return ""
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not header_normalize:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def rightmost_nonempty_header(headers: List[str]) -> int:
        for i in range(len(headers) - 1, -1, -1):
            h = headers[i]
            if h and not h.isspace():
                return i
        return -1

    def keep_headers(headers: List[str]) -> List[int]:
        keep = []
        for i, h in enumerate(headers):
            hn = (h or "").strip().lower()
            if not hn:
                continue
            if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
                continue
            keep.append(i)
        return keep or list(range(len(headers)))

    def _row_blank_csv(row: List[str]) -> bool:
        if row is None:
            return True
        for c in row:
            if c is None:
                continue
            if str(c).strip():
                return False
        return True

    txt = io.StringIO(data.decode("utf-8", errors="ignore"))
    sample = txt.read(2048)
    txt.seek(0)
    try:
        dialect = csv.Sniffer().sniff(sample) if sample else csv.excel
    except Exception:
        dialect = csv.excel
    reader = csv.reader(txt, dialect)
    rows = list(reader)
    if not rows:
        return "", "text/plain"

    n = len(rows)
    lines: List[str] = []
    lines.append("# Sheet: CSV")

    i = 0
    while i < n:
        # skip blank blocks
        while i < n and _row_blank_csv(rows[i]):
            i += 1
        if i >= n:
            break

        # start of a non-blank block
        start = i
        while i < n and not _row_blank_csv(rows[i]):
            i += 1
        end = i - 1
        if start > end:
            continue

        # headers
        headers_raw = (rows[start] if start < n else [])[:max_cols]
        norm_headers = [normalize_header(fmt_val(h)) for h in headers_raw]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[: rmax + 1]
        norm_headers = norm_headers[:max_cols]
        keep_idx = keep_headers(norm_headers)
        kept_headers = [norm_headers[j] for j in keep_idx]

        # size limits
        total_rows_block = (end - start + 1)
        use_rows = total_rows_block if max_rows <= 0 else min(total_rows_block, max_rows + 1)
        total_cols_block = len(kept_headers)
        if max_cols > 0:
            total_cols_block = min(total_cols_block, max_cols)

        # table header
        lines.append(f"## Table: CSV!R{start+1}-{start+use_rows},C1-{max(total_cols_block,1)}")
        if any(kept_headers):
            lines.append("headers: " + ", ".join(h for h in kept_headers if h))
        lines.append("")

        # rows
        data_start = start + 1
        data_end = min(end, start + use_rows - 1)
        usable_cols_for_slice = min(len(norm_headers), max_cols if max_cols > 0 else len(norm_headers))
        for r in range(data_start, data_end + 1):
            row_vals_raw = rows[r][:usable_cols_for_slice] if r < n else []
            vals = [fmt_val(c) for c in row_vals_raw]
            vals = [vals[j] if j < len(vals) else "" for j in keep_idx]
            while vals and (vals[-1] == "" or vals[-1] is None):
                vals.pop()
            if not any(vals):
                continue

            pairs: List[str] = []
            for h, v in zip(kept_headers, vals):
                if h and v:
                    pairs.append(f"{h}={v}")

            excel_row_num = r + 1
            lines.append(f"### Row {excel_row_num} — CSV")
            lines.append(", ".join(pairs) if pairs else ", ".join(vals))
            lines.append("")

    return "\n".join(lines).strip() + "\n", "text/plain"

# ===== aimodel/file_read/rag/ingest/doc_binary_ingest.py =====

# RTF and legacy .doc/.ole helpers and extraction (no DOCX here)
from __future__ import annotations
from typing import Tuple, List
import re
from ...core.settings import SETTINGS

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _is_ole(b: bytes) -> bool:
    return len(b) >= 8 and b[:8] == b"\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1"

def _dbg(msg: str):
    try:
        S = SETTINGS.effective
        if bool(S().get("doc_debug", False)):
            print(f"[doc_ingest] {msg}")
    except Exception:
        pass

_RTF_CTRL_RE = re.compile(r"\\[a-zA-Z]+-?\d* ?")
_RTF_GROUP_RE = re.compile(r"[{}]")
_RTF_UNICODE_RE = re.compile(r"\\u(-?\d+)\??")
_RTF_HEX_RE = re.compile(r"\\'[0-9a-fA-F]{2}")
_HEX_BLOCK_RE = re.compile(r"(?:\s*[0-9A-Fa-f]{2}){120,}")

def _rtf_to_text_simple(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    def _hex_sub(m):
        try:
            return bytes.fromhex(m.group(0)[2:]).decode("latin-1", errors="ignore")
        except Exception:
            return ""
    s = _RTF_HEX_RE.sub(_hex_sub, s)
    def _uni_sub(m):
        try:
            cp = int(m.group(1))
            if cp < 0:
                cp = 65536 + cp
            return chr(cp)
        except Exception:
            return ""
    s = _RTF_UNICODE_RE.sub(_uni_sub, s)
    s = s.replace(r"\par", "\n").replace(r"\line", "\n")
    s = _RTF_CTRL_RE.sub("", s)
    s = _RTF_GROUP_RE.sub("", s)
    s = _HEX_BLOCK_RE.sub("", s)
    s = s.replace("\r", "\n")
    s = re.sub(r"\n\s*\n\s*\n+", "\n\n", s)
    s = _squeeze_spaces(s)
    return s if keep_newlines else s.replace("\n", " ")

def _rtf_to_text_via_lib(data: bytes, *, keep_newlines: bool = True) -> str:
    try:
        from striprtf.striprtf import rtf_to_text
    except Exception:
        return _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    try:
        s = data.decode("latin-1", errors="ignore")
    except Exception:
        s = data.decode("utf-8", errors="ignore")
    try:
        txt = rtf_to_text(s)
    except Exception:
        txt = _rtf_to_text_simple(data, keep_newlines=keep_newlines)
    txt = _squeeze_spaces(txt)
    return txt if keep_newlines else txt.replace("\n", " ")

def _generic_ole_text(data: bytes) -> str:
    S = SETTINGS.effective
    MIN_RUN = int(S().get("doc_ole_min_run_chars", 8))
    MAX_LINE = int(S().get("doc_ole_max_line_chars", 600))
    MIN_ALPHA_RATIO = float(S().get("doc_ole_min_alpha_ratio", 0.25))
    DROP_XMLISH = bool(S().get("doc_ole_drop_xmlish", True))
    DROP_PATHISH = bool(S().get("doc_ole_drop_pathish", True))
    DROP_SYMBOL_LINES = bool(S().get("doc_ole_drop_symbol_lines", True))
    DEDUPE_SHORT_REPEATS = bool(S().get("doc_ole_dedupe_short_repeats", True))
    XMLISH = re.compile(r"^\s*<[^>]+>", re.I)
    PATHISH = re.compile(r"[\\/].+\.(?:xml|rels|png|jpg|jpeg|gif|bmp|bin|dat)\b", re.I)
    SYMBOLLINE = re.compile(r"^[\W_]{6,}$")
    s = data.replace(b"\x00", b"")
    runs = re.findall(rb"[\t\r\n\x20-\x7E]{%d,}" % MIN_RUN, s)
    if not runs:
        return ""
    def _dec(b: bytes) -> str:
        try:
            return b.decode("cp1252", errors="ignore")
        except Exception:
            return b.decode("latin-1", errors="ignore")
    kept: List[str] = []
    for raw in runs:
        chunk = _dec(raw).replace("\r", "\n")
        for ln in re.split(r"\n+", chunk):
            t = ln.strip()
            if not t:
                continue
            if MAX_LINE > 0 and len(t) > MAX_LINE:
                t = t[:MAX_LINE].rstrip()
            t = _squeeze_spaces(t)
            letters = sum(1 for c in t if c.isalpha())
            if letters / max(1, len(t)) < MIN_ALPHA_RATIO:
                continue
            if DROP_XMLISH and XMLISH.search(t):
                continue
            if DROP_PATHISH and PATHISH.search(t):
                continue
            if DROP_SYMBOL_LINES and SYMBOLLINE.fullmatch(t):
                continue
            if DEDUPE_SHORT_REPEATS:
                t = re.sub(r"\b(\w{2,4})\1{2,}\b", r"\1\1", t)
            kept.append(t)
    out = "\n".join(kept)
    out = re.sub(r"\n\s*\n\s*\n+", "\n\n", out).strip()
    return out

def extract_doc_binary(data: bytes) -> Tuple[str, str]:
    head = (data[:64] or b"").lstrip()
    is_rtf = head.startswith(b"{\\rtf") or head.startswith(b"{\\RTF}")
    is_ole = _is_ole(data)
    _dbg(f"extract_doc_binary: bytes={len(data)} is_rtf={is_rtf} is_ole={is_ole}")
    if is_rtf:
        txt = _rtf_to_text_via_lib(data, keep_newlines=True).strip()
        return (txt + "\n" if txt else ""), "text/plain"
    if is_ole:
        txt = _generic_ole_text(data)
        return (txt + "\n" if txt else ""), "text/plain"
    try:
        txt = data.decode("utf-8", errors="ignore").strip()
    except Exception:
        txt = data.decode("latin-1", errors="ignore").strip()
    return (txt + ("\n" if txt else "")), "text/plain"

# ===== aimodel/file_read/rag/ingest/docx_ingest.py =====

# DOCX-only extraction (no .doc/RTF here)
from __future__ import annotations
from typing import Tuple, List, Optional
import io, re
from ...core.settings import SETTINGS
from .ocr import ocr_image_bytes

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = _WS_RE.sub(" ", s)
    return s.strip()

def _is_heading(style_name: str) -> Optional[int]:
    if not style_name:
        return None
    m = re.match(r"Heading\s+(\d+)", style_name, flags=re.IGNORECASE)
    if not m:
        return None
    try:
        return max(1, min(6, int(m.group(1))))
    except Exception:
        return None

def _is_list_style(style_name: str) -> bool:
    return bool(style_name) and any(k in style_name.lower() for k in ("list", "bullet", "number"))

def _extract_paragraph_text(p) -> str:
    return _squeeze_spaces(p.text)

def _docx_image_blobs(doc) -> List[bytes]:
    blobs: List[bytes] = []
    seen_rids = set()
    try:
        part = doc.part

        # inline images
        for ish in getattr(doc, "inline_shapes", []) or []:
            try:
                rId = ish._inline.graphic.graphicData.pic.blipFill.blip.embed
                if rId and rId not in seen_rids:
                    blob = part.related_parts[rId].blob
                    if blob:
                        blobs.append(blob)
                        seen_rids.add(rId)
            except Exception:
                pass

        # run-level embedded images (floating, etc.)
        for p in doc.paragraphs:
            for r in p.runs:
                for d in getattr(r._element, "xpath", lambda *_: [])(".//a:blip"):
                    try:
                        rId = d.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed")
                        if rId and rId not in seen_rids:
                            blob = part.related_parts[rId].blob
                            if blob:
                                blobs.append(blob)
                                seen_rids.add(rId)
                    except Exception:
                        pass

    except Exception:
        pass
    return blobs

def extract_docx(data: bytes) -> Tuple[str, str]:
    from docx import Document
    S = SETTINGS.effective
    HEADING_MAX_LEVEL = int(S().get("docx_heading_max_level", 3))
    USE_MARKDOWN_HEADINGS = bool(S().get("docx_use_markdown_headings", True))
    PRESERVE_BULLETS = bool(S().get("docx_preserve_bullets", True))
    INCLUDE_TABLES = bool(S().get("docx_include_tables", True))
    INCLUDE_HEADERS_FOOTERS = bool(S().get("docx_include_headers_footers", False))
    MAX_PARA_CHARS = int(S().get("docx_para_max_chars", 0))
    DROP_EMPTY_LINES = bool(S().get("docx_drop_empty_lines", True))

    doc = Document(io.BytesIO(data))
    lines: List[str] = []

    try:
        title = (getattr(doc, "core_properties", None) or {}).title
        if title:
            lines.append(f"# {title}")
            lines.append("")
    except Exception:
        pass

    def _clip(s: str) -> str:
        if MAX_PARA_CHARS > 0 and len(s) > MAX_PARA_CHARS:
            return s[:MAX_PARA_CHARS] + "…"
        return s

    if INCLUDE_HEADERS_FOOTERS:
        try:
            for i, sec in enumerate(getattr(doc, "sections", []) or []):
                if i > 0:
                    break
                try:
                    hdr_ps = getattr(sec.header, "paragraphs", []) or []
                    hdr_text = "\n".join(_squeeze_spaces(p.text) for p in hdr_ps if _squeeze_spaces(p.text))
                    if hdr_text:
                        lines.append("## Header")
                        lines.append(_clip(hdr_text))
                        lines.append("")
                except Exception:
                    pass
                try:
                    ftr_ps = getattr(sec.footer, "paragraphs", []) or []
                    ftr_text = "\n".join(_squeeze_spaces(p.text) for p in ftr_ps if _squeeze_spaces(p.text))
                    if ftr_text:
                        lines.append("## Footer")
                        lines.append(_clip(ftr_text))
                        lines.append("")
                except Exception:
                    pass
        except Exception:
            pass

    for p in doc.paragraphs:
        txt = _extract_paragraph_text(p)
        if not txt and DROP_EMPTY_LINES:
            continue
        style_name = getattr(p.style, "name", "") or ""
        lvl = _is_heading(style_name)
        if lvl and lvl <= HEADING_MAX_LEVEL and USE_MARKDOWN_HEADINGS:
            prefix = "#" * max(1, min(6, lvl))
            lines.append(f"{prefix} {txt}".strip())
            continue
        if PRESERVE_BULLETS and _is_list_style(style_name):
            if txt:
                lines.append(f"- {_clip(txt)}")
            continue
        if txt:
            lines.append(_clip(txt))
        elif not DROP_EMPTY_LINES:
            lines.append("")

    if INCLUDE_TABLES and getattr(doc, "tables", None):
        for t_idx, tbl in enumerate(doc.tables):
            try:
                non_empty = any(_squeeze_spaces(cell.text) for row in tbl.rows for cell in row.cells)
            except Exception:
                non_empty = True
            if not non_empty:
                continue
            lines.append("")
            lines.append(f"## Table {t_idx + 1}")
            try:
                for row in tbl.rows:
                    cells = [_squeeze_spaces(c.text) for c in row.cells]
                    if any(cells):
                        lines.append(" | ".join(c for c in cells if c))
            except Exception:
                pass

    if bool(S().get("docx_ocr_images", False)):
        min_bytes = int(S().get("ocr_min_image_bytes", 16384))
        seen_ocr_text = set()
        for blob in _docx_image_blobs(doc):
            if len(blob) >= min_bytes:
                t = (ocr_image_bytes(blob) or "").strip()
                if t:
                    key = t.lower()
                    if key not in seen_ocr_text:
                        lines.append(t)
                        seen_ocr_text.add(key)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest.py =====
from __future__ import annotations
from typing import Tuple, List
import io, re, hashlib
from datetime import datetime, date, time
from ...core.settings import SETTINGS
from .excel_ingest_core import (
    scan_blocks_by_blank_rows,
    rightmost_nonempty_header,
    select_indices,
)

_WS_RE = re.compile(r"[ \t]+")
def _squeeze_spaces_inline(s: str) -> str:
    return _WS_RE.sub(" ", (s or "")).strip()

def extract_excel(data: bytes) -> Tuple[str, str]:
    from openpyxl import load_workbook
    from openpyxl.utils import range_boundaries
    from openpyxl.worksheet.worksheet import Worksheet
    from openpyxl.utils.datetime import from_excel as _from_excel

    S = SETTINGS.effective

    sig = int(S().get("excel_number_sigfigs"))
    maxp = int(S().get("excel_decimal_max_places"))
    trim = bool(S().get("excel_trim_trailing_zeros"))
    drop_midnight = bool(S().get("excel_dates_drop_time_if_midnight"))
    time_prec = str(S().get("excel_time_precision"))
    max_chars = int(S().get("excel_value_max_chars"))
    quote_strings = bool(S().get("excel_quote_strings"))

    INFER_MAX_ROWS = int(S().get("excel_infer_max_rows"))
    INFER_MAX_COLS = int(S().get("excel_infer_max_cols"))
    INFER_MIN_HEADER_FILL = float(S().get("excel_infer_min_header_fill_ratio", 0.5))
    HEADER_NORMALIZE = bool(S().get("excel_header_normalize"))

    def clip(s: str) -> str:
        if max_chars > 0 and len(s) > max_chars:
            return s[:max_chars] + "…"
        return s

    def fmt_number(v) -> str:
        try:
            s = format(float(v), f".{sig}g") if sig > 0 else f"{float(v):.{maxp}f}"
        except Exception:
            s = str(v)
        if "e" in s.lower():
            try:
                s = f"{float(v):.{maxp}f}"
            except Exception:
                pass
        if trim and "." in s:
            s = s.rstrip("0").rstrip(".")
        return s

    def fmt_date(dt: datetime) -> str:
        if drop_midnight and dt.time() == time(0, 0, 0):
            return dt.date().isoformat()
        return dt.strftime("%Y-%m-%d %H:%M" if time_prec == "minute" else "%Y-%m-%d %H:%M:%S")

    def fmt_time(t: time) -> str:
        return t.strftime("%H:%M" if time_prec == "minute" else "%H:%M:%S")

    def fmt_val(v) -> str:
        if v is None:
            return ""
        if isinstance(v, (int, float)):
            return fmt_number(v)
        if isinstance(v, datetime):
            return fmt_date(v)
        if isinstance(v, date):
            return v.isoformat()
        if isinstance(v, time):
            return fmt_time(v)
        s = str(v)
        if "\n" in s or "\r" in s:
            s = s.replace("\r\n", "\n").replace("\r", "\n").replace("\n", "\\n")
        s = clip(_squeeze_spaces_inline(s))
        if quote_strings and re.search(r"[^A-Za-z0-9_.-]", s):
            return f"\"{s}\""
        return s

    def normalize_header(h: str) -> str:
        if not HEADER_NORMALIZE:
            return h
        s = (h or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", "_", s)
        s = re.sub(r"_+", "_", s).strip("_")
        return s or h

    def _sheet_used_range(ws: Worksheet):
        from openpyxl.utils import range_boundaries
        if callable(getattr(ws, "calculate_dimension", None)):
            dim_ref = ws.calculate_dimension()
            try:
                min_c, min_r, max_c, max_r = range_boundaries(dim_ref)
                return min_c, min_r, max_c, max_r
            except Exception:
                pass
        return 1, 1, ws.max_column or 1, ws.max_row or 1

    def _cap_rows(min_r: int, max_r: int) -> int:
        return max_r if INFER_MAX_ROWS <= 0 else min(max_r, min_r + INFER_MAX_ROWS - 1)

    def _keep_and_rename_phantom(headers: List[str]) -> tuple[List[int], List[str]]:
        if len(headers) >= 2 and re.fullmatch(r"\d+_\d+$", (headers[1] or "")) and (headers[0] or ""):
            keep_idx = [0, 1]
            new_headers = headers[:]
            new_headers[1] = "value"
            return keep_idx, new_headers
        keep_idx = [i for i, h in enumerate(headers) if h and not re.fullmatch(r"\d+_\d+$", h)]
        new_headers = [headers[i] for i in keep_idx]
        return keep_idx, new_headers

    wb_vals = load_workbook(io.BytesIO(data), data_only=True, read_only=True)

    def _coerce_excel_datetime(cell, v):
        try:
            if getattr(cell, "is_date", False) and isinstance(v, (int, float)):
                return _from_excel(v, wb_vals.epoch)
        except Exception:
            pass
        return v

    lines: List[str] = []
    ingest_id = hashlib.sha1(data).hexdigest()[:16]
    lines.append(f"# Ingest-ID: {ingest_id}")

    def _emit_inferred_table(ws: Worksheet, sheet_name: str, min_c, min_r, max_c, max_r):
        lines.append(f"# Sheet: {sheet_name}")
        max_c_eff = min(max_c, min_c + INFER_MAX_COLS - 1)
        max_r_eff = _cap_rows(min_r, max_r)
        headers: List[str] = []
        header_fill = 0
        for c in range(min_c, max_c_eff + 1):
            val = ws.cell(row=min_r, column=c).value
            s = fmt_val("" if val is None else str(val).strip())
            if s:
                header_fill += 1
            headers.append(s)
        fill_ratio = header_fill / max(1, (max_c_eff - min_c + 1))
        if fill_ratio < INFER_MIN_HEADER_FILL and (min_r + 1) <= max_r_eff:
            headers = []
            hdr_r = min_r + 1
            for c in range(min_c, max_c_eff + 1):
                val = ws.cell(row=hdr_r, column=c).value
                s = fmt_val("" if val is None else str(val).strip())
                headers.append(s)
            min_r = hdr_r
        norm_headers = [normalize_header(h) for h in headers]
        rmax = rightmost_nonempty_header(norm_headers)
        if rmax >= 0:
            norm_headers = norm_headers[:rmax + 1]
            max_c_eff = min(max_c_eff, min_c + rmax)
        keep_idx, norm_headers = _keep_and_rename_phantom(norm_headers)
        lines.append("## Table: " + f"{sheet_name}!R{min_r}-{max_r_eff},C{min_c}-{max_c_eff}")
        if any(h for h in norm_headers):
            lines.append("headers: " + ", ".join(h for h in norm_headers if h))
        lines.append("")
        for r in range(min_r + 1, max_r_eff + 1):
            raw_vals: List[str] = []
            for c in range(min_c, max_c_eff + 1):
                cell = ws.cell(row=r, column=c)
                vv = _coerce_excel_datetime(cell, cell.value)
                raw_vals.append(fmt_val(vv))
            row_vals = select_indices(raw_vals, keep_idx)
            while row_vals and (row_vals[-1] == "" or row_vals[-1] is None):
                row_vals.pop()
            if not any(v for v in row_vals):
                continue
            pairs: List[str] = []
            for h, v in zip(norm_headers, row_vals):
                if h and v:
                    pairs.append(f"{h}={v}")
            lines.append(f"### Row {r} — {sheet_name}")
            lines.append(", ".join(pairs) if pairs else ", ".join(row_vals))
            lines.append("")

    for sheet_name in wb_vals.sheetnames:
        ws_v: Worksheet = wb_vals[sheet_name]
        min_c, min_r, max_c, max_r = _sheet_used_range(ws_v)
        max_c = min(max_c, min_c + INFER_MAX_COLS - 1)
        for b_min_c, b_min_r, b_max_c, b_max_r in scan_blocks_by_blank_rows(ws_v, min_c, min_r, max_c, max_r):
            if b_min_r > b_max_r:
                continue
            _emit_inferred_table(ws_v, sheet_name, b_min_c, b_min_r, b_max_c, b_max_r)

    text = "\n".join(line.rstrip() for line in lines if line is not None).strip()
    return (text + "\n" if text else ""), "text/plain"

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====

# ===== aimodel/file_read/rag/ingest/excel_ingest_core.py =====
from __future__ import annotations
from typing import List
import re

_PHANTOM_RX = re.compile(r"^\d+_\d+$")

def row_blank(ws, r: int, min_c: int, max_c: int) -> bool:
    for c in range(min_c, max_c + 1):
        v = ws.cell(row=r, column=c).value
        if v not in (None, "") and not (isinstance(v, str) and not v.strip()):
            return False
    return True

def scan_blocks_by_blank_rows(ws, min_c: int, min_r: int, max_c: int, max_r: int):
    r = min_r
    while r <= max_r:
        while r <= max_r and row_blank(ws, r, min_c, max_c):
            r += 1
        if r > max_r:
            break
        start = r
        while r <= max_r and not row_blank(ws, r, min_c, max_c):
            r += 1
        end = r - 1
        yield (min_c, start, max_c, end)

def rightmost_nonempty_header(headers: List[str]) -> int:
    for i in range(len(headers) - 1, -1, -1):
        h = headers[i]
        if h and not h.isspace():
            return i
    return -1

def drop_bad_columns(headers: List[str]) -> List[int]:
    keep = []
    for i, h in enumerate(headers):
        hn = (h or "").strip().lower()
        if not hn:
            continue
        if _PHANTOM_RX.fullmatch(hn) or hn in {"0"}:
            continue
        keep.append(i)
    return keep or list(range(len(headers)))

def select_indices(seq: List[str], idxs: List[int]) -> List[str]:
    out = []
    for i in idxs:
        out.append(seq[i] if i < len(seq) else "")
    return out

# ===== aimodel/file_read/rag/ingest/ocr.py =====

# aimodel/file_read/rag/ingest/ocr.py
from __future__ import annotations
from typing import Tuple, List
import io, re
import pytesseract
from PIL import Image
import pypdfium2 as pdfium
from ...core.settings import SETTINGS

# Allow overriding the tesseract binary path via settings
_cmd = str(SETTINGS.effective().get("tesseract_cmd", "")).strip()
if _cmd:
    pytesseract.pytesseract.tesseract_cmd = _cmd

_ALNUM = re.compile(r"[A-Za-z0-9]")

def _alnum_ratio(s: str) -> float:
    if not s:
        return 0.0
    a = len(_ALNUM.findall(s))
    return a / max(1, len(s))

def is_bad_text(s: str) -> bool:
    S = SETTINGS.effective
    min_len = int(S().get("ocr_min_chars_for_ok", 32))
    min_ratio = float(S().get("ocr_min_alnum_ratio_for_ok", 0.15))
    s = (s or "").strip()
    return (len(s) < min_len) or (_alnum_ratio(s) < min_ratio)

def ocr_image_bytes(img_bytes: bytes) -> str:
    S = SETTINGS.effective
    lang = str(S().get("ocr_lang", "eng"))
    psm = str(S().get("ocr_psm", "3"))
    oem = str(S().get("ocr_oem", "3"))
    cfg = f"--oem {oem} --psm {psm}"
    with Image.open(io.BytesIO(img_bytes)) as im:
        im = im.convert("L")
        return pytesseract.image_to_string(im, lang=lang, config=cfg) or ""

def ocr_pdf(data: bytes) -> str:
    S = SETTINGS.effective
    dpi = int(S().get("pdf_ocr_dpi", 300))
    max_pages = int(S().get("pdf_ocr_max_pages", 0))
    lang = str(S().get("ocr_lang", "eng"))
    oem = str(S().get("ocr_oem", "3"))
    # default PSM, but we’ll also try a few alternates for image-only pages
    psm_default = str(S().get("ocr_psm", "6"))
    try_psm = [psm_default, "4", "7", "3"]  # single block, column, single line, auto
    min_side = 1200  # upscale tiny rasters

    def _dbg(*args):
        try:
            if bool(S().get("ingest_debug", False)):
                print("[ocr_pdf]", *args, flush=True)
        except Exception:
            pass

    try:
        doc = pdfium.PdfDocument(io.BytesIO(data))
    except Exception as e:
        _dbg("PdfDocument ERROR:", repr(e))
        return ""

    n = len(doc)
    limit = n if max_pages <= 0 else min(n, max_pages)
    _dbg(f"pages={n}", f"limit={limit}", f"dpi={dpi}", f"lang={lang}", f"psm_default={psm_default}")

    out: List[str] = []
    for i in range(limit):
        try:
            page = doc[i]
            pil = page.render(scale=dpi/72, rotation=0).to_pil().convert("L")
            base_w, base_h = pil.width, pil.height

            # Prepare a couple of preprocess variants
            variants = []

            # 1) raw gray (possibly upscaled)
            img1 = pil
            if min(base_w, base_h) < min_side:
                f = max(1.0, min_side / float(min(base_w, base_h)))
                img1 = pil.resize((int(base_w * f), int(base_h * f)))
            variants.append(("gray", img1))

            # 2) light binarization
            img2 = img1.point(lambda x: 255 if x > 180 else 0)
            variants.append(("bin180", img2))

            # 3) inverted (helps white-on-black)
            img3 = img1.point(lambda x: 255 - x)
            variants.append(("inv", img3))

            got = ""
            for tag, imgv in variants:
                for psm in try_psm:
                    cfg = f"--oem {oem} --psm {psm}"   # <-- string (not a set!)
                    txt = pytesseract.image_to_string(imgv, lang=lang, config=cfg) or ""
                    txt = txt.strip()
                    _dbg(f"page={i+1}/{limit}", f"{tag} {imgv.width}x{imgv.height}", f"psm={psm}", f"len={len(txt)}", f"prev={repr(txt[:80])}")
                    if txt:
                        got = txt
                        break
                if got:
                    break

            if got:
                out.append(got)
        except Exception as e:
            _dbg(f"page={i+1} ERROR:", repr(e))

    final = "\n\n".join(out).strip()
    _dbg("final_len=", len(final))
    return final

# ===== aimodel/file_read/rag/ingest/pdf_ingest.py =====

# aimodel/file_read/rag/ingest/pdf_ingest.py
from __future__ import annotations
from typing import Tuple
import io
from ...core.settings import SETTINGS
from .ocr import is_bad_text, ocr_pdf
from .common import _utf8

def _dbg(*args):
    try:
        if bool(SETTINGS.effective().get("ingest_debug", False)):
            print("[pdf_ingest]", *args, flush=True)
    except Exception:
        pass

def extract_pdf(data: bytes) -> Tuple[str, str]:
    print("[pdf_ingest] ENTER extract_pdf", flush=True)

    S = SETTINGS.effective
    OCR_ENABLED = bool(S().get("pdf_ocr_enable", False))
    OCR_MODE = str(S().get("pdf_ocr_mode", "auto")).lower()   # auto | force | never
    WHEN_BAD  = bool(S().get("pdf_ocr_when_bad", True))
    DPI       = int(S().get("pdf_ocr_dpi", 300))
    MAX_PAGES = int(S().get("pdf_ocr_max_pages", 0))

    # Unconditional config echo so we KNOW what the server is using
    print(f"[pdf_ingest] cfg ocr_enabled={OCR_ENABLED} mode={OCR_MODE} when_bad={WHEN_BAD} dpi={DPI} max_pages={MAX_PAGES}", flush=True)

    def _do_ocr() -> str:
        print("[pdf_ingest] OCR_CALL begin", flush=True)  # <-- undeniable marker
        txt = (ocr_pdf(data) or "").strip()
        print(f"[pdf_ingest] OCR_CALL end text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
        return txt

    # FORCE: run OCR up front
    if OCR_ENABLED and OCR_MODE == "force":
        _dbg("mode=force -> OCR first")
        ocr_txt = _do_ocr()
        if ocr_txt:
            print("[pdf_ingest] EXIT (force OCR success)", flush=True)
            return ocr_txt, "text/plain"
        _dbg("mode=force -> OCR empty, trying text extract")

    # Try embedded text (pdfminer)
    txt = ""
    try:
        from pdfminer.high_level import extract_text
        txt = (extract_text(io.BytesIO(data)) or "").strip()
        print(f"[pdf_ingest] pdfminer text_len={len(txt)} preview={repr(txt[:120])}", flush=True)
    except Exception as e:
        print(f"[pdf_in