s") or 0)
    telemetry["pack"]["rollOverageTokens"] = int(PACK_TELEMETRY.get("rollOverageTokens") or 0)

    persist_summary(session_id, st["summary"])

    budget_view = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=out_budget_adj,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()

    wb = _web_breakdown(telemetry.get("web", {}))
    telemetry.setdefault("web", {})["breakdown"] = wb
    telemetry["web"]["breakdown"]["unattributedWebSec"] = _web_unattributed(telemetry.get("web", {}), wb)
    telemetry["web"]["breakdown"]["prepSec"] = float(telemetry.get("prepSec") or 0.0)

    budget_view.setdefault("web", {}).update(telemetry.get("web", {}))
    budget_view.setdefault("rag", {}).update(telemetry.get("rag", {}))
    budget_view.setdefault("pack", {}).update(telemetry.get("pack", {}))

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_adj, margin=int(eff["clamp_margin"])
    )
    budget_view.setdefault("request", {})
    budget_view["request"]["outBudgetRequested"] = out_budget_adj
    budget_view["request"]["temperature"] = temperature
    budget_view["request"]["top_p"] = top_p

    return Prep(
        llm=llm,
        session_id=session_id,
        packed=packed,
        st=st,
        out_budget=out_budget,
        input_tokens_est=input_tokens_est,
        budget_view=budget_view,
        temperature=temperature,
        top_p=top_p,
        t_request_start=t_request_start,
    )

# ===== aimodel/file_read/services/generate_pipeline_support.py =====

# aimodel/file_read/services/generate_pipeline_support.py
# utility helpers for generate pipeline; no inline comments per request
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from ..utils.streaming import safe_token_count_messages

@dataclass
class Prep:
    llm: Any
    session_id: str
    packed: List[Dict[str, str]]
    st: Dict[str, Any]
    out_budget: int
    input_tokens_est: Optional[int]
    budget_view: Dict[str, Any]
    temperature: float
    top_p: float
    t_request_start: float

def _bool(v, default: bool = False) -> bool:
    try:
        return bool(v)
    except Exception:
        return bool(default)

def _tok_count(llm, messages: List[Dict[str, str]]) -> Optional[int]:
    try:
        return int(safe_token_count_messages(llm, messages))
    except Exception:
        return None

def _approx_block_tokens(llm, role: str, text: str) -> Optional[int]:
    return _tok_count(llm, [{"role": role, "content": text}])

def _diff_find_inserted_block(before: List[Dict[str, str]], after: List[Dict[str, str]]) -> Optional[Dict[str, str]]:
    if len(after) - len(before) != 1:
        return None
    i = 0
    while i < len(before) and before[i] == after[i]:
        i += 1
    if i < len(after):
        return after[i]
    return None

def _dump_msgs(label: str, msgs: List[Dict[str, str]], head_chars: int = 180):
    return

def _web_breakdown(web: Dict[str, Any]) -> Dict[str, float]:
    w = web or {}
    orch = w.get("orchestrator") or {}
    router = float(w.get("elapsedSec") or 0.0)
    summarize = float((w.get("summarizer") or {}).get("elapsedSec") or 0.0)
    inject = float(w.get("injectElapsedSec") or 0.0)
    search_total = 0.0
    s1 = (orch.get("search") or {})
    for k in ("elapsedSecTotal", "elapsedSec"):
        if isinstance(s1.get(k), (int, float)):
            search_total = float(s1[k])
            break
    fetch1 = float((orch.get("fetch1") or {}).get("totalSec") or 0.0)
    fetch2 = float((orch.get("fetch2") or {}).get("totalSec") or 0.0)
    orch_elapsed = float(orch.get("elapsedSec") or w.get("fetchElapsedSec") or 0.0)
    assemble = orch_elapsed - (search_total + fetch1 + fetch2)
    if assemble < 0:
        assemble = 0.0
    total_pre_ttft = router + summarize + orch_elapsed + inject
    return {
        "routerSec": round(router, 6),
        "summarizeSec": round(summarize, 6),
        "searchSec": round(search_total, 6),
        "fetchSec": round(fetch1, 6),
        "jsFetchSec": round(fetch2, 6),
        "assembleSec": round(assemble, 6),
        "orchestratorSec": round(orch_elapsed, 6),
        "injectSec": round(inject, 6),
        "totalWebPreTtftSec": round(total_pre_ttft, 6),
    }

def _web_unattributed(web: Dict[str, Any], breakdown: Dict[str, float]) -> float:
    total = float((web or {}).get("fetchElapsedSec") or 0.0)
    explained = float(breakdown.get("searchSec", 0.0)) + float(breakdown.get("fetchSec", 0.0)) + float(breakdown.get("jsFetchSec", 0.0)) + float(breakdown.get("assembleSec", 0.0))
    ua = total - explained
    return round(ua if ua > 0 else 0.0, 6)

def _enforce_fit(llm, eff: Dict[str, Any], packed: List[Dict[str, str]], out_budget_req: int) -> tuple[list[dict], int]:
    tok = _tok_count(llm, packed) or 0
    capacity = int(eff["model_ctx"]) - int(eff["clamp_margin"])
    def drop_one(px):
        keep_head = 2 if len(px) >= 2 and isinstance(px[1].get("content"), str) and px[1]["content"].startswith(eff["summary_header_prefix"]) else 1
        if len(px) > keep_head + 1:
            px.pop(keep_head)
            return True
        return False
    def remove_ephemeral_blocks(px):
        i = 0
        removed = False
        while i < len(px):
            m = px[i]
            if m.get("_ephemeral") is True:
                px.pop(i)
                removed = True
            else:
                i += 1
        return removed
    if tok + out_budget_req > capacity:
        if remove_ephemeral_blocks(packed):
            tok = _tok_count(llm, packed) or 0
    while tok + out_budget_req > capacity and drop_one(packed):
        tok = _tok_count(llm, packed) or 0
    if tok >= capacity:
        ob2 = 0
    else:
        ob2 = min(out_budget_req, max(0, capacity - tok))
    return packed, ob2

# ===== aimodel/file_read/services/packing.py =====

# ===== aimodel/file_read/services/packing.py =====
from __future__ import annotations
from typing import Tuple, List, Dict, Optional, Any
from ..rag.retrieve_pipeline import build_rag_block_with_telemetry, build_rag_block_session_only_with_telemetry
from ..core.settings import SETTINGS
from ..core.packing_ops import build_system, pack_messages, roll_summary_if_needed

def build_system_text() -> str:
    eff = SETTINGS.effective()
    base = build_system(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
    )
    guidance = str(eff["packing_guidance"])
    return base + guidance

def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    eff = SETTINGS.effective()

    packed, input_budget = pack_messages(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
        summary=summary,
        recent=recent,
        max_ctx=max_ctx,
        out_budget=out_budget,
    )

    packed, new_summary = roll_summary_if_needed(
        packed=packed,
        recent=recent,
        summary=summary,
        input_budget=input_budget,
        system_text=system_text,
    )

    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

def maybe_inject_rag_block(
    messages: list[dict],
    *,
    session_id: str | None,
    skip_rag: bool = False,
    rag_query: str | None = None,
    force_session_only: bool = False,  # NEW
) -> tuple[list[dict], Optional[Dict[str, Any]], Optional[str]]:
    if skip_rag:
        return messages, None, None
    if not SETTINGS.get("rag_enabled", True):
        return messages, None, None
    if not messages or messages[-1].get("role") != "user":
        return messages, None, None

    user_q = rag_query or (messages[-1].get("content") or "")

    # NEW: choose session-only vs global
    use_session_only = force_session_only or (not SETTINGS.get("rag_global_enabled", True))

    if use_session_only and SETTINGS.get("rag_session_enabled", True):
        from ..rag.retrieve_pipeline import build_rag_block_session_only_with_telemetry
        block, tel = build_rag_block_session_only_with_telemetry(user_q, session_id=session_id)
        mode = "session-only"
    else:
        from ..rag.retrieve_pipeline import build_rag_block_with_telemetry
        block, tel = build_rag_block_with_telemetry(user_q, session_id=session_id)
        mode = "global"

    if not block:
        print(f"[RAG INJECT] no hits (session={session_id}) q={(user_q or '')!r}")
        return messages, None, None

    print(f"[RAG INJECT] injecting (session={session_id}) chars={len(block)} mode={mode}")
    injected = messages[:-1] + [{"role": "user", "content": block}, messages[-1]]
    tel = dict(tel or {})
    tel["injected"] = True
    tel["mode"] = mode
    return injected, tel, block

# ===== aimodel/file_read/services/prompt_utils.py =====

from __future__ import annotations
import json
from datetime import datetime
from typing import Dict, List


def now_str() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total


def dump_full_prompt(
    messages: List[Dict[str, object]],
    *,
    params: Dict[str, object],
    session_id: str,
) -> None:
    try:
        print(f"[{now_str()}] PROMPT DUMP BEGIN session={session_id} msgs={len(messages)}")
        print(json.dumps({"messages": messages, "params": params}, ensure_ascii=False, indent=2))
        print(f"[{now_str()}] PROMPT DUMP END   session={session_id}")
    except Exception as e:
        print(f"[{now_str()}] PROMPT DUMP ERROR session={session_id} err={type(e).__name__}: {e}")

# ===== aimodel/file_read/services/router_text.py =====

from __future__ import annotations
from typing import Optional, List
from ..core.settings import SETTINGS


def compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: Optional[int] = None,
    summary_chars: Optional[int] = None,
    max_chars: Optional[int] = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: List[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.packing_memory_core import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
# updated to account for retrieve timings in pre-ttft accounting (no "or 0.0" fallbacks)
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List

from ..core.settings import SETTINGS
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END,
    build_run_json, watch_disconnect,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int],  t0_request: Optional[float] = None, budget_view: Optional[dict] = None,
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=SETTINGS.stream_queue_maxsize)
    SENTINEL = object()

    def produce():
        t_start = t0_request or time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        t_call: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []
        stage: dict = {"queueWaitSec": None, "genSec": None}

        try:
            try:
                t_call = time.perf_counter()
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=SETTINGS.stream_top_k,
                    repeat_penalty=SETTINGS.stream_repeat_penalty,
                    stop=SETTINGS.stream_stop_strings,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(
                        SETTINGS.stream_retry_min_tokens,
                        int(out_budget * SETTINGS.stream_retry_fraction)
                    )
                    log.warning(
                        "generate: context overflow, retrying with max_tokens=%d",
                        retry_tokens
                    )
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=SETTINGS.stream_top_k,
                        repeat_penalty=SETTINGS.stream_repeat_penalty,
                        stop=SETTINGS.stream_stop_strings,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(SETTINGS.stream_backpressure_sleep_sec)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                out_text = "".join(out_parts)

                if t_first is not None and t_last is not None:
                    stage["genSec"] = round(t_last - t_first, 3)
                if t_start is not None and t_first is not None:
                    stage["ttftSec"] = round(t_first - t_start, 3)
                if t_start is not None and t_last is not None:
                    stage["totalSec"] = round(t_last - t_start, 3)

                if t_call is not None and t_start is not None:
                    stage["preModelSec"] = round(t_call - t_start, 6)
                if t_call is not None and t_first is not None:
                    stage["modelQueueSec"] = round(t_first - t_call, 6)

                if isinstance(budget_view, dict) and "queueWaitSec" in budget_view:
                    stage["queueWaitSec"] = budget_view.get("queueWaitSec")

                engine = None
                method_used = None
                try:
                    td = None
                    g_last = getattr(llm, "get_last_timings", None)
                    if callable(g_last):
                        method_used = "get_last_timings"
                        td = g_last()
                    if td is None:
                        g = getattr(llm, "get_timings", None)
                        if callable(g):
                            method_used = "get_timings"
                            td = g()
                    if td is None:
                        if isinstance(getattr(llm, "timings", None), dict):
                            method_used = "timings_attr"
                            td = getattr(llm, "timings")
                        elif isinstance(getattr(llm, "perf", None), dict):
                            method_used = "perf_attr"
                            td = getattr(llm, "perf")

                    if isinstance(td, dict):
                        def fms(v):
                            try:
                                return float(v) / 1000.0
                            except Exception:
                                return None
                        def to_i(v):
                            try:
                                return int(v)
                            except Exception:
                                return None
                        load_ms = td.get("load_ms") or td.get("loadMs")
                        prompt_ms = td.get("prompt_ms") or td.get("promptMs") or td.get("prefill_ms")
                        eval_ms = td.get("eval_ms") or td.get("evalMs") or td.get("decode_ms")
                        prompt_n = td.get("prompt_n") or td.get("promptN") or td.get("prompt_tokens")
                        eval_n = td.get("eval_n") or td.get("evalN") or td.get("eval_tokens")
                        engine = {}
                        x = fms(load_ms)
                        if x is not None:
                            engine["loadSec"] = round(x, 3)
                        x = fms(prompt_ms)
                        if x is not None:
                            engine["promptSec"] = round(x, 3)
                        x = fms(eval_ms)
                        if x is not None:
                            engine["evalSec"] = round(x, 3)
                        n = to_i(prompt_n)
                        if n is not None:
                            engine["promptN"] = n
                        n = to_i(eval_n)
                        if n is not None:
                            engine["evalN"] = n
                        try:
                            log.debug("llm timings method=%s keys=%s", method_used, list(td.keys()))
                        except Exception:
                            pass
                    else:
                        log.debug("llm timings unavailable")
                except Exception as e:
                    log.debug("llm timings probe error: %s", e)
                    engine = None
                if engine:
                    stage["engine"] = engine

                # ---------- Accurate pre-TTFT accounting (no "or 0.0" fallbacks) ----------
                if isinstance(budget_view, dict):
                    def _fnum(x) -> float:
                        try:
                            return float(x) if x is not None else 0.0
                        except Exception:
                            return 0.0

                    ttft_raw = stage.get("ttftSec")
                    ttft_val = _fnum(ttft_raw)

                    pack   = budget_view.get("pack") or {}
                    rag    = budget_view.get("rag") or {}
                    web_bd = ((budget_view.get("web") or {}).get("breakdown")) or {}

                    # packing / trimming
                    pack_sec = _fnum(pack.get("packSec"))
                    trim_sec = _fnum(pack.get("finalTrimSec"))
                    comp_sec = _fnum(pack.get("compressSec"))

                    # rag router decision
                    rag_router = _fnum(rag.get("routerDecideSec"))

                    # prefer aggregate build timing if present; otherwise sum component steps
                    build_candidates = (
                        rag.get("injectBuildSec"),
                        rag.get("sessionOnlyBuildSec"),
                        rag.get("blockBuildSec"),
                    )
                    first_build = next((v for v in build_candidates if v is not None), None)
                    rag_build_agg = _fnum(first_build)

                    rag_embed  = _fnum(rag.get("embedSec"))
                    rag_s_chat = _fnum(rag.get("searchChatSec"))
                    rag_s_glob = _fnum(rag.get("searchGlobalSec"))
                    rag_dedupe = _fnum(rag.get("dedupeSec"))

                    if rag_build_agg > 0.0:
                        rag_pipeline_sec = rag_build_agg
                    else:
                        rag_pipeline_sec = rag_embed + rag_s_chat + rag_s_glob + rag_dedupe

                    # web and prep
                    prep_sec    = _fnum(web_bd.get("prepSec"))
                    web_pre     = _fnum(web_bd.get("totalWebPreTtftSec"))

                    # queue time between model call and first token
                    model_queue = _fnum(stage.get("modelQueueSec"))

                    pre_accounted = (
                        pack_sec + trim_sec + comp_sec
                        + rag_router + rag_pipeline_sec
                        + web_pre + prep_sec
                        + model_queue
                    )
                    unattr_ttft = ttft_val - pre_accounted
                    if unattr_ttft < 0.0:
                        unattr_ttft = 0.0

                    budget_view.setdefault("breakdown", {})
                    budget_view["breakdown"].update({
                        "ttftSec": ttft_val,
                        "preTtftAccountedSec": round(pre_accounted, 6),
                        "unattributedTtftSec": round(unattr_ttft, 6),
                    })
                # -------------------------------------------------------------------------

                run_json = build_run_json(
                    request_cfg={"temperature": temperature, "top_p": top_p, "max_tokens": out_budget},
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                    budget_view=budget_view,
                    extra_timings=stage,
                    error_text=err_text,
                )
                if SETTINGS.runjson_emit:
                    q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass
            finally:
                try:
                    llm.reset()
                except Exception:
                    pass
                try:
                    q.put_nowait(SENTINEL)
                except Exception:
                    pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set() and SETTINGS.stream_emit_stopped_line:
            yield (f"\n{SETTINGS.stopped_line_marker}\n").encode("utf-8")
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=SETTINGS.stream_producer_join_timeout_sec)
        except Exception:
            pass

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message, update_last, append_message,
    delete_message, delete_messages_batch, list_messages,
    list_paged, delete_batch,
    merge_chat, merge_chat_new, edit_message, set_summary, get_summary,  # ← add these
)
from .index import ChatMeta

__all__ = [
    # chats
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "merge_chat", "merge_chat_new", "edit_message",  # ← add these
    # index
    "ChatMeta",
    # pending
    "set_summary", "get_summary"
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile, threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..adaptive.config.paths import app_data_dir

# -------- Directories & Paths --------
APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"
PENDING_PATH = APP_DIR / "pending.json"              # NEW
OLD_PENDING_DELETES = APP_DIR / "pending_deletes.json"  # NEW

# -------- Lock for safe writes --------
_lock = threading.RLock()

# -------- Helpers --------
def now_iso() -> str:
    """UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    """Safely write JSON to a temp file then move into place."""
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    """Ensure app/chats directories exist and index.json is initialized."""
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    """Return path to chat file for a session ID."""
    return CHATS_DIR / f"{session_id}.json"

# -------- Exports --------
__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "PENDING_PATH",          # NEW
    "OLD_PENDING_DELETES",   # NEW
    "_lock",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

# ===== aimodel/file_read/store/chats.py =====
from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any

from ..core.settings import SETTINGS
from ..utils.streaming import strip_runjson
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta
from ..rag.store import delete_namespace as rag_delete_namespace


def _load_chat(session_id: str) -> Dict[str, Any]:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""  # backfill older files
        return data


@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str
    attachments: Optional[List[Dict]] = None   # ✅ added

def _normalize_attachments(atts: Optional[list[Any]]) -> Optional[list[dict]]:
    if not atts:
        return None
    out = []
    for a in atts:
        # convert dataclass/typed object to dict
        if isinstance(a, dict):
            out.append({
                "name": a.get("name"),
                "source": a.get("source"),
                "sessionId": a.get("sessionId"),
            })
        else:
            try:
                out.append({
                    "name": getattr(a, "name", None),
                    "source": getattr(a, "source", None),
                    "sessionId": getattr(a, "sessionId", None),
                })
            except Exception:
                continue
    return out or None

def upsert_on_first_message(session_id: str, title: str) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or SETTINGS["chat_default_title"]),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row)
    save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)


def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)


def append_message(session_id: str, role: str, content: str, attachments: Optional[list[Any]] = None) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq,
        "sessionId": session_id,
        "role": role,
        "content": content,
        "createdAt": now_iso(),
    }
    norm_atts = _normalize_attachments(attachments)
    if norm_atts:
        msg["attachments"] = norm_atts   # ✅ now JSON-serializable

    data["messages"].append(msg)
    data["seq"] = seq
    _save_chat(session_id, data)

    # update index meta as before...
    ...
    return ChatMessageRow(
        id=seq,
        sessionId=session_id,
        role=role,
        content=content,
        createdAt=msg["createdAt"],
        attachments=norm_atts,
    )



def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1


def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted


def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    rows: List[ChatMessageRow] = []
    for m in data.get("messages", []):
        rows.append(ChatMessageRow(
            id=m["id"],
            sessionId=m["sessionId"],
            role=m["role"],
            content=m["content"],
            createdAt=m.get("createdAt"),
            attachments=m.get("attachments", []),  # ✅ safe default
        ))
    return rows


def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    min_size = int(SETTINGS["chat_page_min_size"])
    max_size = int(SETTINGS["chat_page_max_size"])
    size = max(min_size, min(max_size, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag


def delete_batch(session_ids: List[str]) -> List[str]:
    # delete chat jsons
    for sid in session_ids:
        try:
            chat_path(sid).unlink(missing_ok=True)
        except Exception:
            pass

    # delete RAG namespaces for those sessions
    for sid in session_ids:
        try:
            rag_delete_namespace(sid)
        except Exception:
            pass

    # prune index.json
    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids


def merge_chat(source_id: str, target_id: str):
    source_msgs = list_messages(source_id)
    target_msgs = list_messages(target_id)

    merged = []
    for m in source_msgs:
        row = append_message(target_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    for m in target_msgs:
        row = append_message(target_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    return merged


def _save_chat(session_id: str, data: Dict[str, Any]):
    atomic_write(chat_path(session_id), data)


def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)


def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")


def merge_chat_new(source_id: str, target_id: Optional[str] = None):
    from uuid import uuid4
    new_id = str(uuid4())
    upsert_on_first_message(new_id, SETTINGS["chat_merged_title"])

    merged = []
    for m in list_messages(source_id):
        row = append_message(new_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    if target_id:
        for m in list_messages(target_id):
            row = append_message(new_id, m.role, m.content, attachments=m.attachments)
            merged.append(row)

    return new_id, merged


def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            # normalize attachments
            if "attachments" in m and m["attachments"] is not None:
                norm = []
                for a in m["attachments"]:
                    if hasattr(a, "dict"):
                        norm.append(a.dict())
                    elif isinstance(a, dict):
                        norm.append(a)
                    else:
                        norm.append(dict(a))
                m["attachments"] = norm
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(
        id=updated["id"],
        sessionId=updated["sessionId"],
        role=updated["role"],
        content=updated["content"],
        createdAt=updated.get("createdAt"),
        attachments=updated.get("attachments", []),
    )



__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch", "merge_chat", "merge_chat_new",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary",
]

# ===== aimodel/file_read/store/default_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 6,
  "web_fetch_max_chars": 2000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 1200,
  "web_orch_per_doc_char_budget": 700,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 1.5,
  "web_orch_overfetch_min_extra": 1,
  "web_orch_enable_js_retry": false,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.6,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe"
}

# ===== aimodel/file_read/store/effective_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 6,
  "web_fetch_max_chars": 2000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 1200,
  "web_orch_per_doc_char_budget": 700,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 1.5,
  "web_orch_overfetch_min_extra": 1,
  "web_orch_enable_js_retry": false,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.6,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe"
}

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/store/override_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 6,
  "web_fetch_max_chars": 2000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 1200,
  "web_orch_per_doc_char_budget": 700,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 1.5,
  "web_orch_overfetch_min_extra": 1,
  "web_orch_enable_js_retry": false,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.6,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe"
}

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..runtime.model_runtime import current_model_info, get_llm

RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

STOP_STRINGS = ["</s>", "User:", "\nUser:"]

def strip_runjson(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    out, i = [], 0
    while True:
        start = s.find(RUNJSON_START, i)
        if start == -1:
            out.append(s[i:])
            break
        out.append(s[i:start])
        end = s.find(RUNJSON_END, start)
        if end == -1:
            break
        i = end + len(RUNJSON_END)
    return "".join(out).strip()

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def _first(obj: dict, keys: List[str]):
    for k in keys:
        if k in obj and obj[k] is not None:
            return obj[k]
    return None

def _sec_from_ms(v: Any) -> Optional[float]:
    try:
        x = float(v)
        return round(x / 1000.0, 6)
    except Exception:
        return None

def collect_engine_timings(llm: Any) -> Optional[Dict[str, Optional[float]]]:
    src = None
    try:
        if hasattr(llm, "get_last_timings") and callable(llm.get_last_timings):
            src = llm.get_last_timings()
        elif hasattr(llm, "get_timings") and callable(llm.get_timings):
            src = llm.get_timings()
        elif hasattr(llm, "timings"):
            t = llm.timings
            src = t() if callable(t) else t
        elif hasattr(llm, "perf"):
            p = llm.perf
            src = p() if callable(p) else p
        elif hasattr(llm, "stats"):
            s = llm.stats
            src = s() if callable(s) else s
        elif hasattr(llm, "get_stats") and callable(llm.get_stats):
            src = llm.get_stats()
    except Exception:
        src = None

    if not isinstance(src, dict):
        return None

    load_ms = _first(src, ["load_ms", "loadMs", "model_load_ms", "load_time_ms"])
    prompt_ms = _first(src, ["prompt_ms", "promptMs", "prompt_eval_ms", "prompt_time_ms", "prefill_ms"])
    eval_ms = _first(src, ["eval_ms", "evalMs", "decode_ms", "eval_time_ms"])
    prompt_n = _first(src, ["prompt_n", "promptN", "prompt_tokens", "n_prompt_tokens"])
    eval_n = _first(src, ["eval_n", "evalN", "eval_tokens", "n_eval_tokens"])

    out: Dict[str, Optional[float]] = {
        "loadSec": _sec_from_ms(load_ms),
        "promptSec": _sec_from_ms(prompt_ms),
        "evalSec": _sec_from_ms(eval_ms),
        "promptN": None,
        "evalN": None,
    }
    try:
        out["promptN"] = int(prompt_n) if prompt_n is not None else None
    except Exception:
        out["promptN"] = None
    try:
        out["evalN"] = int(eval_n) if eval_n is not None else None
    except Exception:
        out["evalN"] = None
    return out

def build_run_json(
    *,
    request_cfg: Dict[str, object],
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
    budget_view: Optional[dict] = None,
    extra_timings: Optional[dict] = None,
    error_text: Optional[str] = None,
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None
    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()
    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    timings_payload = dict(extra_timings or {})
    if "engine" not in timings_payload or timings_payload.get("engine") is None:
        timings_payload["engine"] = collect_engine_timings(llm)

    encode_t = float(input_tokens_est or 0.0)
    decode_t = float(out_tokens or 0.0)
    total_t = encode_t + decode_t
    model_queue_s = float(timings_payload.get("modelQueueSec") or 0.0)
    engine_prompt_s = float((timings_payload.get("engine") or {}).get("promptSec") or 0.0)
    encode_sec = model_queue_s or engine_prompt_s or 0.0
    decode_sec = float(gen_secs or 0.0)
    total_sec = float(t_end - t_start)
    encode_tps = (encode_t / encode_sec) if encode_sec > 0 else None
    decode_tps = (decode_t / decode_sec) if decode_sec > 0 else None
    overall_tps = (total_t / total_sec) if total_sec > 0 else None

    bv = budget_view or {}
    bv_break = (bv.get("breakdown") or {}) if isinstance(bv, dict) else {}
    web = (bv.get("web") or {}) if isinstance(bv, dict) else {}
    web_bd = (web.get("breakdown") or {}) if isinstance(web, dict) else {}
    rag = (bv.get("rag") or {}) if isinstance(bv, dict) else {}

    nctx = bv.get("modelCtx") or bv.get("n_ctx") or 0
    clamp = bv.get("clampMargin") or bv.get("clamp_margin") or 0
    inp_est = bv.get("inputTokensEst") or bv.get("input_tokens_est") or (input_tokens_est or 0)
    out_chosen = bv.get("outBudgetChosen") or bv.get("clamped_out_tokens") or 0
    out_actual = out_tokens
    out_shown = out_actual or out_chosen
    used_ctx = (inp_est or 0) + (out_shown or 0) + (clamp or 0)
    ctx_pct = (float(used_ctx) / float(nctx) * 100.0) if nctx else 0.0

    rag_delta = 0
    for k in ("ragTokensAdded", "blockTokens", "blockTokensApprox", "sessionOnlyTokensApprox"):
        v = rag.get(k)
        if isinstance(v, (int, float)) and v > 0:
            rag_delta = int(v)
            break
    rag_pct_of_input = int(round((rag_delta / inp_est) * 100)) if inp_est else 0

    web_pre = (
        web_bd.get("totalWebPreTtftSec")
        or ((web.get("elapsedSec") or 0) + (web.get("fetchElapsedSec") or 0) + (web.get("injectElapsedSec") or 0))
        or 0
    )

    pre_accounted = bv_break.get("preTtftAccountedSec")
    unattributed = (
        bv_break.get("unattributedTtftSec")
        if "unattributedTtftSec" in bv_break
        else (max(0.0, (ttft_ms / 1000.0) - float(pre_accounted))) if pre_accounted is not None else None
    )

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
            "budget": budget_view or {},
            "timings": timings_payload,
            "error": error_text or None,
        },
        "budget_view": (budget_view or {}),
        "_derived": {
            "context": {
                "modelCtx": nctx,
                "clampMargin": clamp,
                "inputTokensEst": inp_est,
                "outBudgetChosen": out_chosen,
                "outActual": out_actual,
                "outShown": out_shown,
                "usedCtx": used_ctx,
                "ctxPct": ctx_pct,
            },
            "rag": {
                "ragDelta": rag_delta,
                "ragPctOfInput": rag_pct_of_input,
            },
            "web": {
                "webPre": web_pre,
            },
            "timing": {
                "accountedPreTtftSec": pre_accounted,
                "unattributedPreTtftSec": unattributed,
                "preModelSec": timings_payload.get("preModelSec"),
                "modelQueueSec": timings_payload.get("modelQueueSec"),
                "genSec": gen_secs,
                "ttftSec": round((ttft_ms or 0) / 1000.0, 3),
            },
            "throughput": {
                "encodeTokPerSec": encode_tps,
                "decodeTokPerSec": decode_tps,
                "overallTokPerSec": overall_tps,
            },
        },
    }

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/duckduckgo.py =====

# aimodel/file_read/web/duckduckgo.py
from __future__ import annotations
from typing import List, Optional, Tuple, Dict, Any
import asyncio, time
from urllib.parse import urlparse

try:
    from ddgs import DDGS  # type: ignore
except Exception:
    try:
        from duckduckgo_search import DDGS  # type: ignore
    except Exception:
        DDGS = None  # type: ignore

from ..core.settings import SETTINGS
from .provider import SearchHit

_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}

def _cache_key(query: str) -> str:
    return (query or "").strip().lower()

def _host(u: str) -> str:
    try:
        h = (urlparse(u).hostname or "").lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return ""

def _cache_get(query: str) -> Optional[List[SearchHit]]:
    eff = SETTINGS.effective()
    ttl = int(eff["web_search_cache_ttl_sec"])
    key = _cache_key(query)
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > ttl:
        _CACHE.pop(key, None)
        return None
    return hits

def _cache_set(query: str, hits: List[SearchHit]) -> None:
    _CACHE[_cache_key(query)] = (time.time(), hits)

def _ddg_sync_search(query: str, k: int, *, region: str, safesearch: str) -> List[SearchHit]:
    results: List[SearchHit] = []
    if DDGS is None:
        return results
    with DDGS() as ddg:
        for i, r in enumerate(ddg.text(query, max_results=max(1, k), safesearch=safesearch, region=region)):
            title = (r.get("title") or "").strip()
            url = (r.get("href") or "").strip()
            snippet: Optional[str] = (r.get("body") or "").strip() or None
            if not url:
                continue
            results.append(SearchHit(title=title or url, url=url, snippet=snippet, rank=i))
            if i + 1 >= k:
                break
    return results

class DuckDuckGoProvider:
    async def search(self, query: str, k: int = 3, telemetry: Optional[Dict[str, Any]] = None) -> List[SearchHit]:
        t_start = time.perf_counter()
        eff = SETTINGS.effective()
        q_norm = (query or "").strip()
        if not q_norm:
            if telemetry is not None:
                telemetry.update({"query": q_norm, "k": int(k), "supersetK": int(k), "elapsedSec": round(time.perf_counter() - t_start, 6), "cache": {"hit": False}})
            return []
        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        region = str(eff["web_search_region"])
        safesearch = str(eff["web_search_safesearch"])

        tel: Dict[str, Any] = {"query": q_norm, "k": int(k), "supersetK": superset_k, "region": region, "safesearch": safesearch}
        t_cache = time.perf_counter()
        cached = _cache_get(q_norm)
        tel["cache"] = {"hit": cached is not None, "elapsedSec": round(time.perf_counter() - t_cache, 6)}
        if cached is not None:
            out = cached[:k]
            tel["hits"] = {
                "total": len(cached),
                "returned": len(out),
                "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
            }
            tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
            if telemetry is not None:
                telemetry.update(tel)
            return out

        hits: List[SearchHit] = []
        prov_info: Dict[str, Any] = {"available": DDGS is not None}
        t_fetch = time.perf_counter()
        if DDGS is not None:
            try:
                hits = await asyncio.to_thread(_ddg_sync_search, q_norm, superset_k, region=region, safesearch=safesearch)
                prov_info["errorType"] = None
                prov_info["errorMsg"] = None
            except Exception as e:
                prov_info["errorType"] = type(e).__name__
                prov_info["errorMsg"] = str(e)
                hits = []
        else:
            prov_info["errorType"] = "ProviderUnavailable"
            prov_info["errorMsg"] = "DDGS is not installed or failed to import."
        prov_info["elapsedSec"] = round(time.perf_counter() - t_fetch, 6)
        tel["provider"] = prov_info

        _cache_set(q_norm, hits)
        out = hits[:k]
        tel["hits"] = {
            "total": len(hits),
            "returned": len(out),
            "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
        }
        tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
        if telemetry is not None:
            telemetry.update(tel)
        return out

# ===== aimodel/file_read/web/fetch.py =====

# aimodel/file_read/web/fetch.py
from __future__ import annotations
import asyncio
from typing import Tuple, List, Optional, Dict, Any
import time
import httpx

try:
    from readability import Document
except Exception:
    Document = None  # type: ignore

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None  # type: ignore

try:
    from selectolax.parser import HTMLParser
except Exception:
    HTMLParser = None  # type: ignore

from ..core.settings import SETTINGS


def _req(key: str):
    return SETTINGS[key]

def _ua() -> str:
    return str(_req("web_fetch_user_agent"))

def _timeout() -> float:
    return float(_req("web_fetch_timeout_sec"))

def _max_chars() -> int:
    return int(_req("web_fetch_max_chars"))

def _max_bytes() -> int:
    return int(_req("web_fetch_max_bytes"))

def _max_parallel() -> int:
    return max(1, int(_req("web_fetch_max_parallel")))


async def _read_capped_bytes(resp: httpx.Response, cap_bytes: int) -> bytes:
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    html = raw_html or ""
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
        except Exception:
            pass
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = tree.body.text(separator=" ", strip=True) if tree.body else tree.text(separator=" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    return html


async def fetch_clean(
    url: str,
    timeout_s: Optional[float] = None,
    max_chars: Optional[int] = None,
    max_bytes: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
) -> Tuple[str, int, str]:
    t0 = time.perf_counter()
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout, headers=headers) as client:
        r = await client.get(url)
        r.raise_for_status()
        raw_bytes = await _read_capped_bytes(r, cap_bytes)
        enc = r.encoding or "utf-8"
        raw_text = raw_bytes.decode(enc, errors="ignore")
        txt = _extract_text_from_html(raw_text, str(r.url))
        txt = (txt or "").strip().replace("\r", "")
        if len(txt) > cap_chars:
            txt = txt[:cap_chars]
        if telemetry is not None:
            telemetry.update({
                "reqUrl": url,
                "finalUrl": str(r.url),
                "status": int(r.status_code),
                "elapsedSec": round(time.perf_counter() - t0, 6),
                "bytes": len(raw_bytes),
                "chars": len(txt),
                "timeoutSec": timeout,
                "capBytes": cap_bytes,
                "capChars": cap_chars,
            })
        return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: List[str],
    per_timeout_s: Optional[float] = None,
    cap_chars: Optional[int] = None,
    cap_bytes: Optional[int] = None,
    max_parallel: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
):
    t_total0 = time.perf_counter()
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))
    tel_items: List[Dict[str, Any]] = []

    async def _one(u: str):
        item_tel: Dict[str, Any] = {"reqUrl": u}
        t0 = time.perf_counter()
        async with sem:
            try:
                res = await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                    telemetry=item_tel,
                )
                item_tel.setdefault("elapsedSec", round(time.perf_counter() - t0, 6))
                item_tel["ok"] = True
                tel_items.append(item_tel)
                return u, res
            except Exception as e:
                item_tel.update({
                    "ok": False,
                    "errorType": type(e).__name__,
                    "errorMsg": str(e),
                    "elapsedSec": round(time.perf_counter() - t0, 6),
                    "timeoutSec": (float(per_timeout_s) if per_timeout_s is not None else _timeout()),
                    "capBytes": (int(cap_bytes) if cap_bytes is not None else _max_bytes()),
                    "capChars": (int(cap_chars) if cap_chars is not None else _max_chars()),
                })
                tel_items.append(item_tel)
                return u, None

    tasks = [_one(u) for u in urls]
    results = await asyncio.gather(*tasks)

    if telemetry is not None:
        ok_cnt = sum(1 for it