elemetry["rag"]["sessionOnly"] = True
            telemetry["rag"]["mode"] = "session-only"
            telemetry["rag"]["blockChars"] = len(rag_text)
            tok = _approx_block_tokens(llm, "assistant", rag_text)
            if tok is not None:
                telemetry["rag"]["sessionOnlyTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            ephemeral_once.append({"role": "assistant", "content": rag_text, "_ephemeral": True})
        else:
            telemetry["rag"]["sessionOnly"] = False
            telemetry["rag"].setdefault("injected", False)
        telemetry["rag"]["sessionOnlyBuildSec"] = round(time.perf_counter() - t0_att, 6)

    system_text = build_system_text()
    t_pack0 = time.perf_counter()
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )
    telemetry["pack"]["packSec"] = round(time.perf_counter() - t_pack0, 6)

    return await _finish_prepare_generation_with_telemetry(
        llm, eff, data, st, router_text, latest_user_text, base_user_text, has_atts,
        force_session_only, rag_session_enabled, rag_global_enabled, auto_rag,
        telemetry, packed, out_budget_req, temperature, top_p, t_request_start, session_id
    )

# ===== aimodel/file_read/services/generate_pipeline_part2.py =====

# aimodel/file_read/services/generate_pipeline_part2.py
from __future__ import annotations
import time
from typing import Any, Dict, Optional, List
from .prompt_utils import chars_len
from .generate_pipeline_support import (
    Prep, _tok_count, _approx_block_tokens, _diff_find_inserted_block,
    _web_breakdown, _web_unattributed, _enforce_fit
)
from ..rag.router_ai import decide_rag
from .packing import maybe_inject_rag_block
from .session_io import persist_summary
from .budget import analyze_budget
from ..core.packing_memory_core import PACK_TELEMETRY
from .context_window import clamp_out_budget

async def _finish_prepare_generation_with_telemetry(
    llm, eff, data, st, router_text, latest_user_text, base_user_text, has_atts,
    force_session_only, rag_session_enabled, rag_global_enabled, auto_rag,
    telemetry, packed, out_budget_req, temperature, top_p, t_request_start, session_id
) -> Prep:
    must_inject_session = bool(
        force_session_only and rag_session_enabled and not has_atts and not telemetry.get("web", {}).get("ephemeralBlocks")
    )

    rag_router_allowed = ((rag_session_enabled or rag_global_enabled) and not (
        has_atts and bool(eff["disable_global_rag_on_attachments"])
    )) or must_inject_session

    # NEW: if web router said it's needed or we already injected web, skip RAG router
    web_needed = bool((telemetry.get("web") or {}).get("needed"))
    web_injected = bool((telemetry.get("web") or {}).get("injected"))
    if web_needed or web_injected:
        rag_router_allowed = False
        telemetry.setdefault("rag", {})
        telemetry["rag"]["routerSkipped"] = True
        telemetry["rag"]["routerSkippedReason"] = "web_needed" if web_needed else "web_block_present"

    ephemeral_once: List[Dict[str, str]] = []
    if rag_router_allowed and bool(eff["rag_enabled"]) and not ephemeral_once:
        rag_need = False
        rag_query: Optional[str] = None

        if must_inject_session:
            rag_need = True
            rag_query = (latest_user_text or base_user_text or "").strip()
            telemetry["rag"]["routerDecideSec"] = 0.0
            telemetry["rag"]["routerNeeded"] = True
            telemetry["rag"]["routerForcedSession"] = True
            telemetry["rag"]["routerQuery"] = rag_query
        else:
            t_router0 = time.perf_counter()
            if auto_rag:
                try:
                    rag_need, rag_query = decide_rag(llm, router_text)
                except Exception:
                    rag_need, rag_query = (False, None)
            telemetry["rag"]["routerDecideSec"] = round(time.perf_counter() - t_router0, 6)
            telemetry["rag"]["routerNeeded"] = bool(rag_need)
            if rag_query is not None:
                telemetry["rag"]["routerQuery"] = rag_query

        skip_rag = bool(ephemeral_once) or (not rag_need)
        tokens_before = _tok_count(llm, packed)
        t_inject0 = time.perf_counter()

        print(f"[PIPE][RAG] router query: {rag_query!r} skip_rag={skip_rag}")
        res = maybe_inject_rag_block(
            packed,
            session_id=session_id,
            skip_rag=skip_rag,
            rag_query=rag_query,
            force_session_only=force_session_only,
        )

        telemetry["rag"]["injectBuildSec"] = round(time.perf_counter() - t_inject0, 6)

        if isinstance(res, tuple):
            packed2 = res[0]
            tel = res[1] if len(res) > 1 and isinstance(res[1], dict) else {}
            block_text = res[2] if len(res) > 2 and isinstance(res[2], str) else None
        else:
            packed2 = res
            tel = {}
            block_text = None

        if tel:
            telemetry["rag"].update(tel)
        if block_text:
            print(f"[PIPE][RAG] injected block preview: {block_text[:200]!r}")
            telemetry["rag"]["blockChars"] = len(block_text)
            tok = _approx_block_tokens(llm, "user", block_text)
            if tok is not None:
                telemetry["rag"]["blockTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or ("session-only" if force_session_only else "global")
        else:
            inserted = _diff_find_inserted_block(packed, packed2)
            if inserted and isinstance(inserted.get("content"), str):
                print(f"[PIPE][RAG] diff-inserted block preview: {inserted['content'][:200]!r}")
                text = inserted["content"]
                telemetry["rag"]["blockChars"] = len(text)
                tok = _approx_block_tokens(llm, "user", text)
                if tok is not None:
                    telemetry["rag"]["blockTokensApprox"] = tok
                telemetry["rag"]["injected"] = True
                telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or ("session-only" if force_session_only else "global")

        tokens_after = _tok_count(llm, packed2)
        if tokens_before is not None:
            telemetry["rag"]["packedTokensBefore"] = tokens_before
        if tokens_after is not None:
            telemetry["rag"]["packedTokensAfter"] = tokens_after
        if tokens_before is not None and tokens_after is not None:
            telemetry["rag"]["ragTokensAdded"] = max(0, tokens_after - tokens_before)
        packed = packed2
    else:
        telemetry["rag"]["routerSkipped"] = True
        if telemetry.get("web", {}).get("ephemeralBlocks"):
            telemetry["rag"]["routerSkippedReason"] = "ephemeral_block_present"
        elif not rag_router_allowed:
            telemetry["rag"]["routerSkippedReason"] = "attachments_disable_global_or_rag_disabled"
        elif not bool(eff["rag_enabled"]):
            telemetry["rag"]["routerSkippedReason"] = "rag_disabled"
        print(f"[PIPE] rag_router_skipped reason={telemetry['rag'].get('routerSkippedReason')}")

    packed, out_budget_adj = _enforce_fit(llm, eff, packed, out_budget_req)
    packed_chars = chars_len(packed)
    telemetry["pack"]["packedChars"] = packed_chars
    telemetry["pack"]["messages"] = len(packed)
    telemetry["pack"]["summarySec"] = float(PACK_TELEMETRY.get("summarySec") or 0.0)
    telemetry["pack"]["summaryTokensApprox"] = int(PACK_TELEMETRY.get("summaryTokensApprox") or 0)
    telemetry["pack"]["summaryUsedLLM"] = bool(PACK_TELEMETRY.get("summaryUsedLLM") or False)
    telemetry["pack"]["finalTrimSec"] = float(PACK_TELEMETRY.get("finalTrimSec") or 0.0)
    telemetry["pack"]["compressSec"] = float(PACK_TELEMETRY.get("compressSec") or 0.0)
    telemetry["pack"]["packInputTokensApprox"] = int(PACK_TELEMETRY.get("packInputTokensApprox") or 0)
    telemetry["pack"]["packMsgs"] = int(PACK_TELEMETRY.get("packMsgs") or 0)
    telemetry["pack"]["finalTrimTokensBefore"] = int(PACK_TELEMETRY.get("finalTrimTokensBefore") or 0)
    telemetry["pack"]["finalTrimTokensAfter"] = int(PACK_TELEMETRY.get("finalTrimTokensAfter") or 0)
    telemetry["pack"]["finalTrimDroppedMsgs"] = int(PACK_TELEMETRY.get("finalTrimDroppedMsgs") or 0)
    telemetry["pack"]["finalTrimDroppedApproxTokens"] = int(PACK_TELEMETRY.get("finalTrimDroppedApproxTokens") or 0)
    telemetry["pack"]["finalTrimSummaryShrunkFromChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryShrunkFromChars") or 0)
    telemetry["pack"]["finalTrimSummaryShrunkToChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryShrunkToChars") or 0)
    telemetry["pack"]["finalTrimSummaryDroppedChars"] = int(PACK_TELEMETRY.get("finalTrimSummaryDroppedChars") or 0)
    telemetry["pack"]["rollStartTokens"] = int(PACK_TELEMETRY.get("rollStartTokens") or 0)
    telemetry["pack"]["rollOverageTokens"] = int(PACK_TELEMETRY.get("rollOverageTokens") or 0)

    persist_summary(session_id, st["summary"])

    budget_view = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=out_budget_adj,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()

    wb = _web_breakdown(telemetry.get("web", {}))
    telemetry.setdefault("web", {})["breakdown"] = wb
    telemetry["web"]["breakdown"]["unattributedWebSec"] = _web_unattributed(telemetry.get("web", {}), wb)
    telemetry["web"]["breakdown"]["prepSec"] = float(telemetry.get("prepSec") or 0.0)

    budget_view.setdefault("web", {}).update(telemetry.get("web", {}))
    budget_view.setdefault("rag", {}).update(telemetry.get("rag", {}))
    budget_view.setdefault("pack", {}).update(telemetry.get("pack", {}))

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_adj, margin=int(eff["clamp_margin"])
    )
    budget_view.setdefault("request", {})
    budget_view["request"]["outBudgetRequested"] = out_budget_adj
    budget_view["request"]["temperature"] = temperature
    budget_view["request"]["top_p"] = top_p

    return Prep(
        llm=llm,
        session_id=session_id,
        packed=packed,
        st=st,
        out_budget=out_budget,
        input_tokens_est=input_tokens_est,
        budget_view=budget_view,
        temperature=temperature,
        top_p=top_p,
        t_request_start=t_request_start,
    )

# ===== aimodel/file_read/services/generate_pipeline_support.py =====

# aimodel/file_read/services/generate_pipeline_support.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from ..utils.streaming import safe_token_count_messages

@dataclass
class Prep:
    llm: Any
    session_id: str
    packed: List[Dict[str, str]]
    st: Dict[str, Any]
    out_budget: int
    input_tokens_est: Optional[int]
    budget_view: Dict[str, Any]
    temperature: float
    top_p: float
    t_request_start: float

def _bool(v, default: bool = False) -> bool:
    try:
        return bool(v)
    except Exception:
        return bool(default)

def _tok_count(llm, messages: List[Dict[str, str]]) -> Optional[int]:
    try:
        return int(safe_token_count_messages(llm, messages))
    except Exception:
        return None

def _approx_block_tokens(llm, role: str, text: str) -> Optional[int]:
    return _tok_count(llm, [{"role": role, "content": text}])

def _diff_find_inserted_block(before: List[Dict[str, str]], after: List[Dict[str, str]]) -> Optional[Dict[str, str]]:
    if len(after) - len(before) != 1:
        return None
    i = 0
    while i < len(before) and before[i] == after[i]:
        i += 1
    if i < len(after):
        return after[i]
    return None

def _dump_msgs(label: str, msgs: List[Dict[str, str]], head_chars: int = 180):
    return

def _web_breakdown(web: Dict[str, Any]) -> Dict[str, float]:
    w = web or {}
    orch = w.get("orchestrator") or {}
    router = float(w.get("elapsedSec") or 0.0)
    summarize = float((w.get("summarizer") or {}).get("elapsedSec") or 0.0)
    inject = float(w.get("injectElapsedSec") or 0.0)
    search_total = 0.0
    s1 = (orch.get("search") or {})
    for k in ("elapsedSecTotal", "elapsedSec"):
        if isinstance(s1.get(k), (int, float)):
            search_total = float(s1[k])
            break
    fetch1 = float((orch.get("fetch1") or {}).get("totalSec") or 0.0)
    fetch2 = float((orch.get("fetch2") or {}).get("totalSec") or 0.0)
    orch_elapsed = float(orch.get("elapsedSec") or w.get("fetchElapsedSec") or 0.0)
    assemble = orch_elapsed - (search_total + fetch1 + fetch2)
    if assemble < 0:
        assemble = 0.0
    total_pre_ttft = router + summarize + orch_elapsed + inject
    return {
        "routerSec": round(router, 6),
        "summarizeSec": round(summarize, 6),
        "searchSec": round(search_total, 6),
        "fetchSec": round(fetch1, 6),
        "jsFetchSec": round(fetch2, 6),
        "assembleSec": round(assemble, 6),
        "orchestratorSec": round(orch_elapsed, 6),
        "injectSec": round(inject, 6),
        "totalWebPreTtftSec": round(total_pre_ttft, 6),
    }

def _web_unattributed(web: Dict[str, Any], breakdown: Dict[str, float]) -> float:
    total = float((web or {}).get("fetchElapsedSec") or 0.0)
    explained = float(breakdown.get("searchSec", 0.0)) + float(breakdown.get("fetchSec", 0.0)) + float(breakdown.get("jsFetchSec", 0.0)) + float(breakdown.get("assembleSec", 0.0))
    ua = total - explained
    return round(ua if ua > 0 else 0.0, 6)

def _enforce_fit(llm, eff: Dict[str, Any], packed: List[Dict[str, str]], out_budget_req: int) -> tuple[list[dict], int]:
    tok = _tok_count(llm, packed) or 0
    capacity = int(eff["model_ctx"]) - int(eff["clamp_margin"])
    def drop_one(px):
        keep_head = 2 if len(px) >= 2 and isinstance(px[1].get("content"), str) and px[1]["content"].startswith(eff["summary_header_prefix"]) else 1
        if len(px) > keep_head + 1:
            px.pop(keep_head)
            return True
        return False
    def remove_ephemeral_blocks(px):
        i = 0
        removed = False
        while i < len(px):
            m = px[i]
            if m.get("_ephemeral") is True:
                px.pop(i)
                removed = True
            else:
                i += 1
        return removed
    if tok + out_budget_req > capacity:
        if remove_ephemeral_blocks(packed):
            tok = _tok_count(llm, packed) or 0
    while tok + out_budget_req > capacity and drop_one(packed):
        tok = _tok_count(llm, packed) or 0
    if tok >= capacity:
        ob2 = 0
    else:
        ob2 = min(out_budget_req, max(0, capacity - tok))
    return packed, ob2

# ===== aimodel/file_read/services/licensing_service.py =====

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any
import base64, json, os, stat, sys, time
import httpx
from fastapi import HTTPException
from nacl.signing import VerifyKey
from nacl.exceptions import BadSignatureError

# ===== App paths / config =====

def _canon_email(s: str | None) -> str:
    return (s or "").strip().lower()

def _app_data_dir() -> Path:
    override = os.getenv("LOCALMIND_DATA_DIR", "").strip()
    if override:
        return Path(override)
    if sys.platform.startswith("win"):
        base = os.getenv("APPDATA") or str(Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"
    elif sys.platform == "darwin":
        return Path.home() / "Library" / "Application Support" / "LocalAI"
    else:
        return Path.home() / ".config" / "LocalAI"

APP_DIR = _app_data_dir() / "license"
APP_DIR.mkdir(parents=True, exist_ok=True)
LIC_PATH = APP_DIR / "license.json"
THROTTLE_PATH = APP_DIR / "license.throttle.json"

# migration from older location
_old = Path(os.path.expanduser("~/.localmind/license.json"))
if _old.exists() and not LIC_PATH.exists():
    try:
        print(f"[license] migrate old -> {LIC_PATH}")
        LIC_PATH.write_text(_old.read_text(encoding="utf-8"), encoding="utf-8")
        try:
            _old.unlink(missing_ok=True)  # type: ignore[arg-type]
        except Exception as e:
            print(f"[license] migrate unlink warn {e!r}")
    except Exception as e:
        print(f"[license] migrate error {e!r}")

LIC_PUB_HEX = os.getenv("LIC_ED25519_PUB_HEX", "").strip()
print(f"[license] using file {LIC_PATH}")
print(f"[license] pubkey set={bool(LIC_PUB_HEX)}")

COOLDOWN_SEC = 0                     # change if you want throttle
EXP_SOON_SEC = 30 * 24 * 3600        # 30 days

# ===== Internals =====

def _b64u_decode(s: str) -> bytes:
    pad = "=" * (-len(s) % 4)
    return base64.urlsafe_b64decode(s + pad)

def _verify(lic: str) -> dict:
    print("[license] _verify: start")
    if not lic or not lic.startswith("LM1."):
        print("[license] _verify: bad_format")
        raise ValueError("Bad format")
    try:
        _, payload_b64, sig_b64 = lic.split(".", 2)
    except ValueError:
        print("[license] _verify: malformed_token")
        raise ValueError("Malformed")
    payload = _b64u_decode(payload_b64)
    sig = _b64u_decode(sig_b64)
    if not LIC_PUB_HEX:
        print("[license] _verify: missing_public_key")
        raise ValueError("Verifier not configured")
    vk = VerifyKey(bytes.fromhex(LIC_PUB_HEX))
    try:
        vk.verify(payload, sig)
    except BadSignatureError:
        print("[license] _verify: bad_signature")
        raise ValueError("Invalid signature")
    data = json.loads(payload.decode("utf-8"))
    if "plan" not in data:
        data["plan"] = "pro"
    now = int(time.time())
    exp = int(data.get("exp") or 0)
    if exp and now > exp:
        print(f"[license] _verify: expired exp={exp} now={now}")
        raise ValueError("Expired")
    print("[license] _verify: ok")
    return data

def _save_secure(path: Path, obj: dict):
    print(f"[license] _save_secure: path={path}")
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = str(path) + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f)
    os.replace(tmp, path)
    try:
        os.chmod(path, stat.S_IRUSR | stat.S_IWUSR)
    except Exception as e:
        print(f"[license] _save_secure: chmod_warn {e!r}")

def _load_current() -> dict | None:
    exists = LIC_PATH.exists()
    print(f"[license] _load_current: file={LIC_PATH} exists={exists}")
    if not exists:
        return None
    with open(LIC_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def _lic_base() -> str:
    base = (os.getenv("LIC_SERVER_BASE") or "").strip()
    print(f"[license] _lic_base: {base or 'MISSING'}")
    if not base:
        raise HTTPException(500, "LIC_SERVER_BASE not configured")
    return base.rstrip("/")

def _throttle_ok(kind: str) -> bool:
    now = int(time.time())
    rec: Dict[str, Any] = {}
    try:
        with open(THROTTLE_PATH, "r", encoding="utf-8") as f:
            rec = json.load(f)
    except Exception:
        rec = {}
    last = int(rec.get(kind) or 0)
    if now - last < COOLDOWN_SEC:
        print(f"[license] throttle: skip kind={kind} last={last} now={now}")
        return False
    rec[kind] = now
    THROTTLE_PATH.parent.mkdir(parents=True, exist_ok=True)
    tmp = str(THROTTLE_PATH) + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(rec, f)
    os.replace(tmp, THROTTLE_PATH)
    print(f"[license] throttle: ok kind={kind} now={now}")
    return True

def email_from_auth(auth_payload: dict | None) -> str:
    if not auth_payload:
        return ""
    email = (auth_payload.get("email") or "").strip().lower()
    return email if "@" in email else ""

def apply_license_string(license_str: str) -> dict:
    claims = _verify(license_str.strip())
    _save_secure(LIC_PATH, {"license": license_str.strip(), "claims": claims})
    return {"ok": True, "plan": claims.get("plan", "pro"), "exp": claims.get("exp")}

def license_status_local(expected_email: str | None = None) -> dict:
    rec = _load_current()
    if not rec:
        return {"plan": "free", "valid": False, "exp": None}

    try:
        claims = _verify(rec["license"])   # raises if invalid/expired
        plan = claims.get("plan", "pro")
        exp  = int(claims.get("exp") or 0) or None
        sub  = _canon_email(claims.get("sub"))

        if expected_email and sub and sub != _canon_email(expected_email):
            # License belongs to a different user → treat as free for this session
            return {"plan": "free", "valid": False, "exp": None, "mismatch": True}

        return {"plan": plan, "valid": True, "exp": exp, "sub": sub}
    except Exception:
        return {"plan": "free", "valid": False, "exp": None}

async def fetch_license_by_session(session_id: str) -> dict:
    base = _lic_base()
    url = f"{base}/api/license/by-session"
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(url, params={"session_id": session_id}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        txt = r.text[:300]
        print(f"[license] by-session: error_body={txt!r}")
        raise HTTPException(r.status_code, r.text)
    return r.json()

async def install_from_session(session_id: str) -> dict:
    data = await fetch_license_by_session(session_id)
    lic = (data or {}).get("license") or ""
    if not lic:
        raise HTTPException(404, "License not available yet")
    claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": claims})
    return {"ok": True, "plan": claims.get("plan", "pro"), "exp": claims.get("exp")}

async def recover_by_email(email: str) -> dict:
    if not email:
        return {"ok": True, "status": "not_found"}
    base = _lic_base()
    url = f"{base}/api/license/by-customer"
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(url, params={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    data = r.json() or {}
    lic = data.get("license") or ""
    if not lic:
        return {"ok": True, "status": "not_found"}
    claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": claims})
    return {"ok": True, "status": "installed", "plan": claims.get("plan", "pro"), "exp": claims.get("exp")}

def remove_license_file() -> dict:
    try:
        if LIC_PATH.exists():
            LIC_PATH.unlink()
            print("[license] delete: removed")
        else:
            print("[license] delete: not_exists")
        return {"ok": True}
    except Exception as e:
        print(f"[license] delete: error {e!r}")
        raise HTTPException(500, f"Could not remove license: {e}")

async def refresh_license(email: str, force: bool) -> dict:
    email = _canon_email(email)
    rec = _load_current()
    if not rec:
        # no local file → try recover by this email
        return await recover_by_email(email)

    try:
        claims = _verify(rec["license"])
        sub = _canon_email(claims.get("sub"))
        now = int(time.time())
        exp = int(claims.get("exp") or 0)
        plan = claims.get("plan", "pro")

        # If installed license belongs to another user, try replacing with this user's license.
        if email and sub and sub != email:
            got = await recover_by_email(email)   # may install new license or return not_found
            if (got or {}).get("status") == "installed":
                # installed a new license → report updated/new status
                st = license_status_local(expected_email=email)
                return {"ok": True, "status": "updated", **st}
            # no license for this email → report free for this user
            return {"ok": True, "status": "not_found", "plan": "free"}

        # normal freshness check
        if not force and exp and exp - now > EXP_SOON_SEC:
            return {"ok": True, "status": "fresh_enough", "plan": plan, "exp": exp}
    except Exception as e:
        # local verify failed → try recover by this email
        return await recover_by_email(email)

    if not force and not _throttle_ok("refresh"):
        return {"ok": True, "status": "skipped_cooldown"}

    # ask server for this email
    base = _lic_base()
    url = f"{base}/api/license/by-customer"
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(url, params={"email": email}, headers={"Accept": "application/json"})
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)

    data = r.json() or {}
    lic = data.get("license") or ""
    if not lic:
        return {"ok": True, "status": "not_found", "plan": "free"}

    new_claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": new_claims})
    return {"ok": True, "status": "updated", "plan": new_claims.get("plan", "pro"), "exp": new_claims.get("exp")}


def read_license_claims() -> dict:
    """Return the raw signed claims saved alongside the installed license."""
    rec = _load_current()
    if not rec:
        raise HTTPException(404, "No license installed")
    # Verify again to ensure the file hasn't gone stale/corrupted
    _ = _verify(rec["license"])
    # We store {"license": <LM1...>, "claims": {...}} when saving
    claims = rec.get("claims") or {}
    if not isinstance(claims, dict) or not claims:
        # Fallback: parse from token if needed
        try:
            _, payload_b64, _ = rec["license"].split(".", 2)
            payload = _b64u_decode(payload_b64)
            claims = json.loads(payload.decode("utf-8")) or {}
        except Exception:
            claims = {}
    return claims

# ===== aimodel/file_read/services/packing.py =====

# ===== aimodel/file_read/services/packing.py =====
from __future__ import annotations
from typing import Tuple, List, Dict, Optional, Any
from ..rag.retrieve_pipeline import build_rag_block_with_telemetry, build_rag_block_session_only_with_telemetry
from ..core.settings import SETTINGS
from ..core.packing_ops import build_system, pack_messages, roll_summary_if_needed

def build_system_text() -> str:
    eff = SETTINGS.effective()
    base = build_system(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
    )
    guidance = str(eff["packing_guidance"])
    return base + guidance

def pack_with_rollup(
    *, system_text: str, summary: str, recent, max_ctx: int, out_budget: int,
    ephemeral: Optional[List[Dict[str, str]]] = None,
) -> Tuple[List[Dict[str, str]], str, int]:
    eff = SETTINGS.effective()

    packed, input_budget = pack_messages(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
        summary=summary,
        recent=recent,
        max_ctx=max_ctx,
        out_budget=out_budget,
    )

    packed, new_summary = roll_summary_if_needed(
        packed=packed,
        recent=recent,
        summary=summary,
        input_budget=input_budget,
        system_text=system_text,
    )

    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph

    return packed, new_summary, input_budget

def maybe_inject_rag_block(
    messages: list[dict],
    *,
    session_id: str | None,
    skip_rag: bool = False,
    rag_query: str | None = None,
    force_session_only: bool = False,  # NEW
) -> tuple[list[dict], Optional[Dict[str, Any]], Optional[str]]:
    if skip_rag:
        return messages, None, None
    if not SETTINGS.get("rag_enabled", True):
        return messages, None, None
    if not messages or messages[-1].get("role") != "user":
        return messages, None, None

    user_q = rag_query or (messages[-1].get("content") or "")

    # NEW: choose session-only vs global
    use_session_only = force_session_only or (not SETTINGS.get("rag_global_enabled", True))

    if use_session_only and SETTINGS.get("rag_session_enabled", True):
        from ..rag.retrieve_pipeline import build_rag_block_session_only_with_telemetry
        block, tel = build_rag_block_session_only_with_telemetry(user_q, session_id=session_id)
        mode = "session-only"
    else:
        from ..rag.retrieve_pipeline import build_rag_block_with_telemetry
        block, tel = build_rag_block_with_telemetry(user_q, session_id=session_id)
        mode = "global"

    if not block:
        print(f"[RAG INJECT] no hits (session={session_id}) q={(user_q or '')!r}")
        return messages, None, None

    print(f"[RAG INJECT] injecting (session={session_id}) chars={len(block)} mode={mode}")
    injected = messages[:-1] + [{"role": "user", "content": block}, messages[-1]]
    tel = dict(tel or {})
    tel["injected"] = True
    tel["mode"] = mode
    return injected, tel, block

# ===== aimodel/file_read/services/prompt_utils.py =====

from __future__ import annotations
import json
from datetime import datetime
from typing import Dict, List


def now_str() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def chars_len(msgs: List[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total


# ===== aimodel/file_read/services/router_text.py =====

from __future__ import annotations
from typing import Optional, List
from ..core.settings import SETTINGS


def compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: Optional[int] = None,
    summary_chars: Optional[int] = None,
    max_chars: Optional[int] = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: List[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: List[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations
from typing import Dict, List
from ..core.packing_memory_core import get_session
from ..store import set_summary as store_set_summary

def handle_incoming(session_id: str, incoming: List[Dict[str, str]]):
    st = get_session(session_id)
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st

def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations
import asyncio, json, time, logging
from typing import AsyncGenerator, Optional, List
from ..core.settings import SETTINGS
from ..utils.streaming import (
    RUNJSON_START, RUNJSON_END,
    build_run_json, watch_disconnect, collect_engine_timings,
)

log = logging.getLogger("aimodel.api.generate")

async def run_stream(
    *, llm, messages, out_budget, stop_ev, request,
    temperature: float, top_p: float, input_tokens_est: Optional[int],  t0_request: Optional[float] = None, budget_view: Optional[dict] = None,
) -> AsyncGenerator[bytes, None]:
    q: asyncio.Queue = asyncio.Queue(maxsize=SETTINGS.stream_queue_maxsize)
    SENTINEL = object()

    def produce():
        t_start = t0_request or time.perf_counter()
        t_first: Optional[float] = None
        t_last: Optional[float] = None
        t_call: Optional[float] = None
        finish_reason: Optional[str] = None
        err_text: Optional[str] = None
        out_parts: List[str] = []
        stage: dict = {"queueWaitSec": None, "genSec": None}

        try:
            try:
                t_call = time.perf_counter()
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=SETTINGS.stream_top_k,
                    repeat_penalty=SETTINGS.stream_repeat_penalty,
                    stop=SETTINGS.stream_stop_strings,
                )
            except ValueError as ve:
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(
                        SETTINGS.stream_retry_min_tokens,
                        int(out_budget * SETTINGS.stream_retry_fraction)
                    )
                    log.warning(
                        "generate: context overflow, retrying with max_tokens=%d",
                        retry_tokens
                    )
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=SETTINGS.stream_top_k,
                        repeat_penalty=SETTINGS.stream_repeat_penalty,
                        stop=SETTINGS.stream_stop_strings,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                while not stop_ev.is_set():
                    try:
                        q.put_nowait(piece)
                        break
                    except asyncio.QueueFull:
                        time.sleep(SETTINGS.stream_backpressure_sleep_sec)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                q.put_nowait(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                out_text = "".join(out_parts)

                if t_first is not None and t_last is not None:
                    stage["genSec"] = round(t_last - t_first, 3)
                if t_start is not None and t_first is not None:
                    stage["ttftSec"] = round(t_first - t_start, 3)
                if t_start is not None and t_last is not None:
                    stage["totalSec"] = round(t_last - t_start, 3)

                if t_call is not None and t_start is not None:
                    stage["preModelSec"] = round(t_call - t_start, 6)
                if t_call is not None and t_first is not None:
                    stage["modelQueueSec"] = round(t_first - t_call, 6)

                if isinstance(budget_view, dict) and "queueWaitSec" in budget_view:
                    stage["queueWaitSec"] = budget_view.get("queueWaitSec")

                try:
                    engine = collect_engine_timings(llm)
                except Exception:
                    engine = None
                if engine:
                    stage["engine"] = engine

                if isinstance(budget_view, dict):
                    def _fnum(x) -> float:
                        try:
                            return float(x) if x is not None else 0.0
                        except Exception:
                            return 0.0

                    ttft_raw = stage.get("ttftSec")
                    ttft_val = _fnum(ttft_raw)

                    pack   = budget_view.get("pack") or {}
                    rag    = budget_view.get("rag") or {}
                    web_bd = ((budget_view.get("web") or {}).get("breakdown")) or {}

                    pack_sec = _fnum(pack.get("packSec"))
                    trim_sec = _fnum(pack.get("finalTrimSec"))
                    comp_sec = _fnum(pack.get("compressSec"))

                    rag_router = _fnum(rag.get("routerDecideSec"))

                    build_candidates = (
                        rag.get("injectBuildSec"),
                        rag.get("sessionOnlyBuildSec"),
                        rag.get("blockBuildSec"),
                    )
                    first_build = next((v for v in build_candidates if v is not None), None)
                    rag_build_agg = _fnum(first_build)

                    rag_embed  = _fnum(rag.get("embedSec"))
                    rag_s_chat = _fnum(rag.get("searchChatSec"))
                    rag_s_glob = _fnum(rag.get("searchGlobalSec"))
                    rag_dedupe = _fnum(rag.get("dedupeSec"))

                    if rag_build_agg > 0.0:
                        rag_pipeline_sec = rag_build_agg
                    else:
                        rag_pipeline_sec = rag_embed + rag_s_chat + rag_s_glob + rag_dedupe

                    prep_sec    = _fnum(web_bd.get("prepSec"))
                    web_pre     = _fnum(web_bd.get("totalWebPreTtftSec"))

                    model_queue = _fnum(stage.get("modelQueueSec"))

                    pre_accounted = (
                        pack_sec + trim_sec + comp_sec
                        + rag_router + rag_pipeline_sec
                        + web_pre + prep_sec
                        + model_queue
                    )
                    unattr_ttft = ttft_val - pre_accounted
                    if unattr_ttft < 0.0:
                        unattr_ttft = 0.0

                    budget_view.setdefault("breakdown", {})
                    budget_view["breakdown"].update({
                        "ttftSec": ttft_val,
                        "preTtftAccountedSec": round(pre_accounted, 6),
                        "unattributedTtftSec": round(unattr_ttft, 6),
                    })

                run_json = build_run_json(
                    request_cfg={"temperature": temperature, "top_p": top_p, "max_tokens": out_budget},
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                    budget_view=budget_view,
                    extra_timings=stage,
                    error_text=err_text,
                )
                if SETTINGS.runjson_emit:
                    q.put_nowait(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass
            finally:
                try:
                    llm.reset()
                except Exception:
                    pass
                try:
                    q.put_nowait(SENTINEL)
                except Exception:
                    pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set() and SETTINGS.stream_emit_stopped_line:
            yield (f"\n{SETTINGS.stopped_line_marker}\n").encode("utf-8")
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=SETTINGS.stream_producer_join_timeout_sec)
        except Exception:
            pass

# ===== aimodel/file_read/store/__init__.py =====

from .chats import (
    ChatMessageRow,
    upsert_on_first_message,
    update_last,
    append_message,
    delete_message,
    delete_messages_batch,
    list_messages,
    list_paged,
    delete_batch,
    edit_message,
    set_summary,
    get_summary,
)
from .index import ChatMeta

__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message",
    "update_last",
    "append_message",
    "delete_message",
    "delete_messages_batch",
    "list_messages",
    "list_paged",
    "delete_batch",
    "edit_message",
    "set_summary",
    "get_summary",
    "ChatMeta",
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations
import json, os, shutil, tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List
from ..adaptive.config.paths import app_data_dir

APP_DIR = app_data_dir()
CHATS_DIR = APP_DIR / "chats"
INDEX_PATH = APP_DIR / "index.json"

def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def atomic_write(path: Path, data: Dict[str, Any] | List[Any]):
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

def ensure_dirs():
    APP_DIR.mkdir(parents=True, exist_ok=True)
    CHATS_DIR.mkdir(parents=True, exist_ok=True)
    if not INDEX_PATH.exists():
        atomic_write(INDEX_PATH, [])

def chat_path(session_id: str) -> Path:
    return CHATS_DIR / f"{session_id}.json"

__all__ = [
    "APP_DIR",
    "CHATS_DIR",
    "INDEX_PATH",
    "now_iso",
    "atomic_write",
    "ensure_dirs",
    "chat_path",
]

# ===== aimodel/file_read/store/chats.py =====

# ===== aimodel/file_read/store/chats.py =====
from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
from ..core.settings import SETTINGS
from ..utils.streaming import strip_runjson
from .base import chat_path, atomic_write, now_iso
from .index import load_index, save_index, refresh_index_after_change, ChatMeta
from ..rag.store import delete_namespace as rag_delete_namespace


def _load_chat(session_id: str) -> Dict[str, Any]:
    p = chat_path(session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""}
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
        if "summary" not in data:
            data["summary"] = ""
        return data


@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str
    attachments: Optional[List[Dict]] = None

def _normalize_attachments(atts: Optional[list[Any]]) -> Optional[list[dict]]:
    if not atts:
        return None
    out = []
    for a in atts:
        if isinstance(a, dict):
            out.append({
                "name": a.get("name"),
                "source": a.get("source"),
                "sessionId": a.get("sessionId"),
            })
        else:
            try:
                out.append({
                    "name": getattr(a, "name", None),
                    "source": getattr(a, "source", None),
                    "sessionId": getattr(a, "sessionId", None),
                })
            except Exception:
                continue
    return out or None

def upsert_on_first_message(session_id: str, title: str) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or SETTINGS["chat_default_title"]),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row)
    save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)


def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)


def append_message(session_id: str, role: str, content: str, attachments: Optional[list[Any]] = None) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq,
        "sessionId": session_id,
        "role": role,
        "content": content,
        "createdAt": now_iso(),
    }
    norm_atts = _normalize_attachments(attachments)
    if norm_atts:
        msg["attachments"] = norm_atts

    data["messages"].append(msg)
    data["seq"] = seq
    _save_chat(session_id, data)
    return ChatMessageRow(
        id=seq,
        sessionId=session_id,
        role=role,
        content=content,
        createdAt=msg["createdAt"],
        attachments=norm_atts,
    )


def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1


def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted


def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    rows: List[ChatMessageRow] = []
    for m in data.get("messages", []):
        rows.append(ChatMessageRow(
            id=m["id"],
            sessionId=m["sessionId"],
            role=m["role"],
            content=m["content"],
            createdAt=m.get("createdAt"),
            attachments=m.get("attachments", []),
        ))
    return rows


def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    min_size = int(SETTINGS["chat_page_min_size"])
    max_size = int(SETTINGS["chat_page_max_size"])
    size = max(min_size, min(max_size, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag


def delete_batch(session_ids: List[str]) -> List[str]:
    for sid in session_ids:
        try:
            chat_path(sid).unlink(missing_ok=True)
        except Exception:
            pass

    for sid in session_ids:
        try:
            rag_delete_namespace(sid)
        except Exception:
            pass

    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids


def _save_chat(session_id: str, data: Dict[str, Any]):
    atomic_write(chat_path(session_id), data)


def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)


def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")


def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            if "attachments" in m and m["attachments"] is not None:
                m["attachments"] = _normalize_attachments(m["attachments"])
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(
        id=updated["id"],
        sessionId=updated["sessionId"],
        role=updated["role"],
        content=updated["content"],
        createdAt=updated.get("createdAt"),
        attachments=updated.get("attachments", []),
    )


__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary",
]

# ===== aimodel/file_read/store/default_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "brave",
  "brave_api_key": "",
  "brave_worker_url": "https://brave-proxy.localmind.workers.dev/brave",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "brave_api_base": "https://api.search.brave.com/res/v1/web/search",
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "web_fetch_timeout_sec": 4,
  "web_fetch_max_chars": 2400,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 4,
  "web_orch_total_char_budget": 1800,
  "web_orch_per_doc_char_budget": 600,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 4,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.3,
  "web_orch_js_retry_low_q": 0.18,
  "web_orch_js_retry_lowish_ratio": 0.6,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": 1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.4,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_orch_diversity": "=== Diversity & Adaptive Chunking ===",
  "web_orch_min_hosts": 3,
  "__comment_web_misc": "=== Misc flags ===",
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "retitle_llm_hard_prefix": "You generate a concise chat title.\nReturn ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). No trailing punctuation. Max 6 words.",
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe",
  "style_sys": "You are a helpful assistant. Always follow the user's explicit instructions carefully and exactly. Do not repeat yourself. Stay coherent and complete.",
  "style_patterns": {
    "talk_like": "\\btalk\\s+like\\s+(?P<style>[^.;\\n]+)",
    "respond_like": "\\brespond\\s+like\\s+(?P<style>[^.;\\n]+)",
    "be": "\\bbe\\s+(?P<style>[^.;\\n]+)",
    "from_now": "\\bfrom\\s+now\\s+on[, ]+\\s*(?P<style>[^.;\\n]+)"
  },
  "style_template": "You must talk like {style}. Stay in character but remain helpful and accurate. Follow the user's latest style instructions.",
  "router_decide_prompt": "You are a router deciding whether answering the text requires the public web.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n- When uncertain whether real-time state is required, prefer need=true.\n\nText:\n{text}\nJSON:"
}

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/store/override_settings.json =====



# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from ..runtime.model_runtime import current_model_info, get_llm

RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

STOP_STRINGS = ["</s>", "User:", "\nUser:"]

def strip_runjson(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    out, i = [], 0
    while True:
        start = s.find(RUNJSON_START, i)
        if start == -1:
            out.append(s[i:])
            break
        out.append(s[i:start])
        end = s.find(RUNJSON_END, start)
        if end == -1:
            break
        i = end + len(RUNJSON_END)
    return "".join(out).strip()

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def _first(obj: dict, keys: List[str]):
    for k in keys:
        if k in obj and obj[k] is not None:
            return obj[k]
    return None

def _sec_from_ms(v: Any) -> Optional[float]:
    try:
        x = float(v)
        return round(x / 1000.0, 6)
    except Exception:
        return None

def collect_engine_timings(llm: Any) -> Optional[Dict[str, Optional[float]]]:
    src = None
    try:
        if hasattr(llm, "get_last_timings") and callable(llm.get_last_timings):
            src = llm.get_last_timings()
        elif hasattr(llm, "get_timings") and callable(llm.get_timings):
            src = llm.get_timings()
        elif hasattr(llm, "timings"):
            t = llm.timings
            src = t() if callable(t) else t
        elif hasattr(llm, "perf"):
            p = llm.perf
            src = p() if callable(p) else p
        elif hasattr(llm, "stats"):
            s = llm.stats
            src = s() if callable(s) else s
        elif hasattr(llm, "get_stats") and callable(llm.get_stats):
            src = llm.get_stats()
    except Exception:
        src = None

    if not isinstance(src, dict):
        return None

    load_ms = _first(src, ["load_ms", "loadMs", "model_load_ms", "load_time_ms"])
    prompt_ms = _first(src, ["prompt_ms", "promptMs", "prompt_eval_ms", "prompt_time_ms", "prefill_ms"])
    eval_ms = _first(src, ["eval_ms", "evalMs", "decode_ms", "eval_time_ms"])
    prompt_n = _first(src, ["prompt_n", "promptN", "prompt_tokens", "n_prompt_tokens"])
    eval_n = _first(src, ["eval_n", "evalN", "eval_tokens", "n_eval_tokens"])

    out: Dict[str, Optional[float]] = {
        "loadSec": _sec_from_ms(load_ms),
        "promptSec": _sec_from_ms(prompt_ms),
        "evalSec": _sec_from_ms(eval_ms),
        "promptN": None,
        "evalN": None,
    }
    try:
        out["promptN"] = int(prompt_n) if prompt_n is not None else None
    except Exception:
        out["promptN"] = None
    try:
        out["evalN"] = int(eval_n) if eval_n is not None else None
    except Exception:
        out["evalN"] = None
    return out

def build_run_json(
    *,
    request_cfg: Dict[str, object],
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
    budget_view: Optional[dict] = None,
    extra_timings: Optional[dict] = None,
    error_text: Optional[str] = None,
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None
    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()
    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    timings_payload = dict(extra_timings or {})
    if "engine" not in timings_payload or timings_payload.get("engine") is None:
        timings_payload["engine"] = collect_engine_timings(llm)

    encode_t = float(input_tokens_est or 0.0)
    decode_t = float(out_tokens or 0.0)
    total_t = encode_t + decode_t
    model_queue_s = float(timings_payload.get("modelQueueSec") or 0.0)
    engine_prompt_s = float((timings_payload.get("engine") or {}).get("promptSec") or 0.0)
    encode_sec = model_queue_s or engine_prompt_s or 0.0
    decode_sec = float(gen_secs or 0.0)
    total_sec = float(t_end - t_start)
    encode_tps = (encode_t / encode_sec) if encode_sec > 0 else None
    decode_tps = (decode_t / decode_sec) if decode_sec > 0 else None
    overall_tps = (total_t / total_sec) if total_sec > 0 else None

    bv = budget_view or {}
    bv_break = (bv.get("breakdown") or {}) if isinstance(bv, dict) else {}
    web = (bv.get("web") or {}) if isinstance(bv, dict) else {}
    web_bd = (web.get("breakdown") or {}) if isinstance(web, dict) else {}
    rag = (bv.get("rag") or {}) if isinstance(bv, dict) else {}

    nctx = bv.get("modelCtx") or bv.get("n_ctx") or 0
    clamp = bv.get("clampMargin") or bv.get("clamp_margin") or 0
    inp_est = bv.get("inputTokensEst") or bv.get("input_tokens_est") or (input_tokens_est or 0)
    out_chosen = bv.get("outBudgetChosen") or bv.get("clamped_out_tokens") or 0
    out_actual = out_tokens
    out_shown = out_actual or out_chosen
    used_ctx = (inp_est or 0) + (out_shown or 0) + (clamp or 0)
    ctx_pct = (float(used_ctx) / float(nctx) * 100.0) if nctx else 0.0

    rag_delta = 0
    for k in ("ragTokensAdded", "blockTokens", "blockTokensApprox", "sessionOnlyTokensApprox"):
        v = rag.get(k)
        if isinstance(v, (int, float)) and v > 0:
            rag_delta = int(v)
            break
    rag_pct_of_input = int(round((rag_delta / inp_est) * 100)) if inp_est else 0

    web_pre = (
        web_bd.get("totalWebPreTtftSec")
        or ((web.get("elapsedSec") or 0) + (web.get("fetchElapsedSec") or 0) + (web.get("injectElapsedSec") or 0))
        or 0
    )

    pre_accounted = bv_break.get("preTtftAccountedSec")
    unattributed = (
        bv_break.get("unattributedTtftSec")
        if "unattributedTtftSec" in bv_break
        else (max(0.0, (ttft_ms / 1000.0) - float(pre_accounted))) if pre_accounted is not None else None
    )

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
            "budget": budget_view or {},
            "timings": timings_payload,
            "error": error_text or None,
        },
        "budget_view": (budget_view or {}),
        "_derived": {
            "context": {
                "modelCtx": nctx,
                "clampMargin": clamp,
                "inputTokensEst": inp_est,
                "outBudgetChosen": out_chosen,
                "outActual": out_actual,
                "outShown": out_shown,
                "usedCtx": used_ctx,
                "ctxPct": ctx_pct,
            },
            "rag": {
                "ragDelta": rag_delta,
                "ragPctOfInput": rag_pct_of_input,
            },
            "web": {
                "webPre": web_pre,
            },
            "timing": {
                "accountedPreTtftSec": pre_accounted,
                "unattributedPreTtftSec": unattributed,
                "preModelSec": timings_payload.get("preModelSec"),
                "modelQueueSec": timings_payload.get("modelQueueSec"),
                "genSec": gen_secs,
                "ttftSec": round((ttft_ms or 0) / 1000.0, 3),
            },
            "throughput": {
                "encodeTokPerSec": encode_tps,
                "decodeTokPerSec": decode_tps,
                "overallTokPerSec": overall_tps,
            },
        },
    }

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/utils/text.py =====

from __future__ import annotations
import re
from typing import Optional

def clean_ws(s: Optional[str]) -> str:
    return " ".join((s or "").split())

def strip_wrappers(text: str, *, trim_whitespace: bool, split_on_blank: bool, header_regex: Optional[str]) -> str:
    t = text or ""
    if trim_whitespace:
        t = t.strip()
    if not header_regex and not split_on_blank:
        return t
    head = t
    if split_on_blank:
        head = t.split("\n\n", 1)[0]
    if header_regex:
        try:
            rx = re.compile(header_regex)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/brave.py =====

from __future__ import annotations
from typing import List, Optional, Tuple, Dict, Any
import time, urllib.parse, hashlib

try:
    import httpx
except Exception:
    httpx = None

from ..core.settings import SETTINGS
from .provider import SearchHit
from .orchestrator_common import _host

_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}


def _cache_key(query: str, base: str, key_marker: str) -> str:
    q = (query or "").strip().lower()
    b = (base or "").strip().lower()
    m = (key_marker or "").strip().lower()
    return f"{q}||{b}||{m}"


def _cache_get(key: str) -> Optional[List[SearchHit]]:
    eff = SETTINGS.effective()
    ttl = int(eff["web_search_cache_ttl_sec"])
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > ttl:
        _CACHE.pop(key, None)
        return None
    return hits


def _cache_set(key: str, hits: List[SearchHit]) -> None:
    _CACHE[key] = (time.time(), hits)


def _set_hits_telemetry(tel: Dict[str, Any], all_hits: List[SearchHit], out: List[SearchHit]) -> None:
    tel["hits"] = {
        "total": len(all_hits),
        "returned": len(out),
        "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
    }


def _build_url(base: str, q: str, k: int) -> str:
    params = {"q": q, "count": str(max(1, k))}
    return f"{base}?{urllib.parse.urlencode(params)}"


def _num(x: Any) -> Optional[int]:
    try:
        return int(str(x))
    except Exception:
        return None


class BraveProvider:
    async def search(
        self,
        query: str,
        k: int = 3,
        telemetry: Optional[Dict[str, Any]] = None,
        xid: Optional[str] = None
    ) -> List[SearchHit]:
        t_start = time.perf_counter()
        eff = SETTINGS.effective()
        q_norm = (query or "").strip()
        if not q_norm:
            if telemetry is not None:
                telemetry.update({
                    "query": q_norm,
                    "k": int(k),
                    "supersetK": int(k),
                    "elapsedSec": round(time.perf_counter() - t_start, 6),
                    "cache": {"hit": False}
                })
            return []

        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        tel: Dict[str, Any] = {"query": q_norm, "k": int(k), "supersetK": superset_k}

        brave_base = (eff.get("brave_api_base") or "https://api.search.brave.com/res/v1/web/search").strip()
        # Global key from SETTINGS (admin-configured)
        key = (SETTINGS.get("brave_api_key", "") or "").strip()
        key_hash = hashlib.sha1(key.encode("utf-8")).hexdigest()[:8] if key else "nokey"
        ckey = _cache_key(q_norm, brave_base, key_hash)

        t_cache = time.perf_counter()
        cached = _cache_get(ckey)
        tel["cache"] = {"hit": cached is not None, "elapsedSec": round(time.perf_counter() - t_cache, 6)}
        if cached is not None:
            print("BRAVE cache hit", {"query": q_norm, "k": k, "base": brave_base, "keyHash": key_hash})
            out = cached[:k]
            _set_hits_telemetry(tel, cached, out)
            tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
            if telemetry is not None:
                telemetry.update(tel)
            return out

        hits: List[SearchHit] = []
        prov_info: Dict[str, Any] = {"available": httpx is not None}
        t_fetch = time.perf_counter()

        if httpx is None:
            prov_info["errorType"] = "ProviderUnavailable"
            prov_info["errorMsg"] = "httpx not installed"
        else:
            print("BRAVE cfg", {"base": brave_base, "byok": False, "keyHash": key_hash})
            url = _build_url(brave_base, q_norm, superset_k)
            headers: Dict[str, str] = {}
            if key:
                headers["X-Subscription-Token"] = key
            print("BRAVE headers", {"hasKey": bool(key)})

            if not key:
                prov_info["errorType"] = "Unauthorized"
                prov_info["errorMsg"] = "No Brave API key configured in settings"
            else:
                try:
                    timeout = float(eff.get("web_fetch_timeout_sec", 8))
                    print("BRAVE call", {"url": url, "timeoutSec": timeout})
                    async with httpx.AsyncClient(timeout=timeout) as client:
                        r = await client.get(url, headers=headers)
                        print("BRAVE resp", {
                            "status": r.status_code,
                            "len": len(r.text or ""),
                            "preview": (r.text[:200] if r.text else ""),
                        })
                        r.raise_for_status()
                        data = r.json()
                    rate = {
                        "minute": {
                            "limit": _num(r.headers.get("X-RateLimit-Limit-Minute")),
                            "remaining": _num(r.headers.get("X-RateLimit-Remaining-Minute")),
                            "resetMs": _num(r.headers.get("X-RateLimit-Reset-Minute")),
                        },
                        "day": {
                            "limit": _num(r.headers.get("X-RateLimit-Limit-Day")),
                            "remaining": _num(r.headers.get("X-RateLimit-Remaining-Day")),
                            "resetMs": _num(r.headers.get("X-RateLimit-Reset-Day")),
                        },
                    }
                    prov_info["rate"] = rate
                    web = (data or {}).get("web") or {}
                    results = web.get("results") or []
                    for i, item in enumerate(results[:superset_k], start=1):
                        title = (item.get("title") or "").strip()
                        url_i = (item.get("url") or "").strip()
                        snippet = (item.get("description") or "").strip() or None
                        if not url_i:
                            continue
                        hits.append(SearchHit(title=title or url_i, url=url_i, snippet=snippet, rank=i))
                    prov_info["errorType"] = None
                    prov_info["errorMsg"] = None
                except httpx.HTTPStatusError as e:
                    print("BRAVE HTTP error", {"status": r.status_code, "body": (r.text or "")[:200]})
                    prov_info["errorType"] = "HTTPStatusError"
                    prov_info["errorMsg"] = f"{e.response.status_code} {e.response.text}"
                except Exception as e:
                    print("BRAVE exception", {"type": type(e).__name__, "msg": str(e)})
                    prov_info["errorType"] = type(e).__name__
                    prov_info["errorMsg"] = str(e)

        prov_info["elapsedSec"] = round(time.perf_counter() - t_fetch, 6)
        tel["provider"] = prov_info

        _cache_set(ckey, hits)
        out = hits[:k]
        _set_hits_telemetry(tel, hits, out)
        tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
        if telemetry is not None:
            telemetry.update(tel)
        return out

# ===== aimodel/file_read/web/fetch.py =====

# aimodel/file_read/web/fetch.py
from __future__ import annotations
import asyncio
from typing import Tuple, List, Optional, Dict, Any
import time
import httpx
from urllib.parse import urlparse

try:
    from readability import Document
except Exception:
    Document = None  # optional

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None  # optional

try:
    from selectolax.parser import HTMLParser
except Exception:
    HTMLParser = None  # optional

from ..core.settings import SETTINGS


def _req(key: str):
    return SETTINGS[key]

def _ua() -> str:
    return str(_req("web_fetch_user_agent"))

def _timeout() -> float:
    return float(_req("web_fetch_timeout_sec"))

def _max_chars() -> int:
    return int(_req("web_fetch_max_chars"))

def _max_bytes() -> int:
    return int(_req("web_fetch_max_bytes"))

def _max_parallel() -> int:
    return max(1, int(_req("web_fetch_max_parallel")))


# -------------------- Adaptive cooldown (generic, no host hardcoding) --------------------
# host -> (fail_count, cooldown_until_ts)
_BAD_HOSTS: Dict[str, Tuple[int, float]] = {}

def _now() -> float:
    return time.time()

def _host_of(u: str) -> str:
    try:
        return (urlparse(u).hostname or "").lower()
    except Exception:
        return ""

def _cooldown_secs(fails: int) -> float:
    # 15m, 30m, 60m, ... capped at 24h
    base = 15 * 60.0
    cap = 24 * 60 * 60.0
    return min(cap, base * (2 ** max(0, fails - 1)))

def _mark_bad(host: str) -> None:
    if not host:
        return
    fails, until = _BAD_HOSTS.get(host, (0, 0.0))
    fails += 1
    _BAD_HOSTS[host] = (fails, _now() + _cooldown_secs(fails))

def _mark_good(host: str) -> None:
    if not host:
        return
    if host in _BAD_HOSTS:
        fails, until = _BAD_HOSTS[host]
        fails = max(0, fails - 1)
        if fails == 0:
            _BAD_HOSTS.pop(host, None)
        else:
            _BAD_HOSTS[host] = (fails, _now() + _cooldown_secs(fails))

def _is_on_cooldown(host: str) -> bool:
    ent = _BAD_HOSTS.get(host)
    return bool(ent and ent[1] > _now())
# ----------------------------------------------------------------------------------------


async def _read_capped_bytes(resp: httpx.Response, cap_bytes: int) -> bytes:
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    html = raw_html or ""
    # Try readability first (often best for article-like pages)
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
        except Exception:
            pass
    # Try selectolax (fast, robust)
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = tree.body.text(separator=" ", strip=True) if tree.body else tree.text(separator=" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    # Fallback to BeautifulSoup full parse
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    # Last resort: return raw html (will be trimmed by char cap)
    return html


async def fetch_clean(
    url: str,
    timeout_s: Optional[float] = None,
    max_chars: Optional[int] = None,
    max_bytes: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
) -> Tuple[str, int, str]:
    t0 = time.perf_counter()
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout, headers=headers) as client:
        r = await client.get(url)
        r.raise_for_status()
        ctype = (r.headers.get("content-type") or "").lower()

        raw_bytes = await _read_capped_bytes(r, cap_bytes)
        enc = r.encoding or "utf-8"
        raw_text = raw_bytes.decode(enc, errors="ignore")
        txt = _extract_text_from_html(raw_text, str(r.url))
        txt = (txt or "").strip().replace("\r", "")
        if len(txt) > cap_chars:
            txt = txt[:cap_chars]

        # Generic usefulness test: skip non-HTML or extremely short bodies
        MIN_USEFUL_CHARS = 80
        host_final = _host_of(str(r.url))
        if ("text/html" not in ctype) or (len(txt) < MIN_USEFUL_CHARS):
            _mark_bad(host_final)
        else:
            _mark_good(host_final)

        if telemetry is not None:
            telemetry.update({
                "reqUrl": url,
                "finalUrl": str(r.url),
                "status": int(r.status_code),
                "elapsedSec": round(time.perf_counter() - t0, 6),
                "bytes": len(raw_bytes),
                "chars": len(txt),
                "timeoutSec": timeout,
                "capBytes": cap_bytes,
                "capChars": cap_chars,
                "contentType": ctype,
                "cooldownFails": _BAD_HOSTS.get(host_final, (0, 0.0))[0] if host_final in _BAD_HOSTS else 0,
            })
        return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: List[str],
    per_timeout_s: Optional[float] = None,
    cap_chars: Optional[int] = None,
    cap_bytes: Optional[int] = None,
    max_parallel: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
):
    t_total0 = time.perf_counter()
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))
    tel_items: List[Dict[str, Any]] = []

    async def _one(u: str):
        item_tel: Dict[str, Any] = {"reqUrl": u}
        host = _host_of(u)

        # Skip hosts currently on adaptive cooldown (generic, no lists)
        if _is_on_cooldown(host):
            item_tel.update({
                "ok": False,
                "skipped": True,
                "skipReason": "cooldown",
                "host": host,
            })
            tel_items.append(item_tel)
            return u, None

        t0 = time.perf_counter()
        async with sem:
            try:
                res = await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                    telemetry=item_tel,
                )
                item_tel.setdefault("elapsedSec", round(time.perf_counter() - t0, 6))
                item_tel["ok"] = True
                item_tel["host"] = host
                tel_items.append(item_tel)
                return u, res
            except Exception as e:
                _mark_bad(host)  # network/HTTP error counts as a fail
                item_tel.update({
                    "ok": False,
                    "errorType": type(e).__name__,
                    "errorMsg": str(e),
                    "elapsedSec": round(time.perf_counter() - t0, 6),
                    "timeoutSec": (float(per_timeout_s) if per_timeout_s is not None else _timeout()),
                    "capBytes": (int(cap_bytes) if cap_bytes is not None else _max_bytes()),
                    "capChars": (int(cap_chars) if cap_chars is not None else _max_chars()),
                    "host": host,
                })
                tel_items.append(item_tel)
                return u, None

    tasks = [_one(u) for u in urls]
    results = await asyncio.gather(*tasks)

    if telemetry is not None:
        ok_cnt = sum(1 for it in tel_items if it.get("ok"))
        telemetry.update({
            "totalSec": round(time.perf_counter() - t_total0, 6),
            "requested": len(urls),
            "ok": ok_cnt,
            "miss": len(urls) - ok_cnt,
            "items": tel_items,
            "settings": {
                "userAgent": _ua(),
                "defaultTimeoutSec": _timeout(),
                "defaultCapChars": _max_chars(),
                "defaultCapBytes": _max_bytes(),
                "maxParallel": _max_parallel() if max_parallel is None else int(max_parallel),
            },
        })

    return results

# ===== aimodel/file_read/web/orchestrator.py =====

from __future__ import annotations
from typing import List, Tuple, Optional, Dict, Any
import time
from collections import defaultdict

from .brave import BraveProvider
from .provider import SearchHit
from .orchestrator_common import (
    _as_int, _as_float, _as_bool, _as_str,
    condense_doc, content_quality_score,
    _dedupe_by_host, score_hit, _head_tail,
    _fetch_round, _host,   # NOTE: make sure _host is imported
)
from ..core.request_ctx import get_x_id


async def build_web_block(query: str, k: Optional[int] = None, per_url_timeout_s: Optional[float] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    tel: Dict[str, Any] = {"query": (query or "").strip()}
    cfg_k               = (int(k) if k is not None else _as_int("web_orch_default_k"))
    total_char_budget   = _as_int("web_orch_total_char_budget")
    per_doc_budget      = _as_int("web_orch_per_doc_char_budget")
    max_parallel        = _as_int("web_orch_max_parallel_fetch")
    overfetch_factor    = _as_float("web_orch_overfetch_factor")
    overfetch_min_extra = _as_int("web_orch_overfetch_min_extra")
    enable_js_retry     = _as_bool("web_orch_enable_js_retry")
    js_avg_q_thresh     = _as_float("web_orch_js_retry_avg_q")
    js_low_q_thresh     = _as_float("web_orch_js_retry_low_q")
    js_lowish_ratio     = _as_float("web_orch_js_retry_lowish_ratio")
    js_timeout_add      = _as_float("web_orch_js_retry_timeout_add")
    js_timeout_cap      = _as_float("web_orch_js_retry_timeout_cap")
    js_parallel_delta   = _as_int("web_orch_js_retry_parallel_delta")
    js_min_parallel     = _as_int("web_orch_js_retry_min_parallel")
    header_tpl          = _as_str("web_block_header")
    sep_str             = _as_str("web_orch_block_separator")
    min_chunk_after     = _as_int("web_orch_min_chunk_after_shrink")
    min_block_reserve   = _as_int("web_orch_min_block_reserve")
    per_timeout = (float(per_url_timeout_s) if per_url_timeout_s is not None else _as_float("web_fetch_timeout_sec"))
    start_time = time.perf_counter()

    provider = BraveProvider()
    # widen search result pool a bit for better host variety
    overfetch = max(cfg_k + overfetch_min_extra, int(round(cfg_k * overfetch_factor)))
    tel["search"] = {"requestedK": cfg_k, "overfetch": overfetch}
    t0 = time.perf_counter()
    try:
        hits: List[SearchHit] = await provider.search(query, k=overfetch, telemetry=tel["search"], xid=get_x_id())
    except Exception as e:
        tel["error"] = {"stage": "search", "type": type(e).__name__, "msg": str(e)}
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) due to search error:", tel["error"])
        return None, tel
    tel["search"]["elapsedSecTotal"] = round(time.perf_counter() - t0, 6)

    if not hits:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) — no hits")
        return None, tel

    # score & keep uniques by URL
    seen_urls = set()
    scored: List[Tuple[int, SearchHit]] = []
    for h in hits:
        u = (h.url or "").strip()
        if not u or u in seen_urls:
            continue
        seen_urls.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
    tel["scoring"] = {"inputHits": len(hits), "scored": len(scored)}

    if not scored:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        print("[web-block] (empty) — no scored hits")
        return None, tel

    # IMPORTANT: dedupe by host to a PREFETCH set (larger than k) to increase variety
    prefetch = max(cfg_k * 2, cfg_k + 6)
    top_hits = _dedupe_by_host(scored, prefetch)
    tel["scoring"]["picked"] = len(top_hits)
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]

    # fetch round 1
    t_f = time.perf_counter()
    tel["fetch1"] = {}
    results = await _fetch_round(
        urls, meta, per_url_timeout_s=per_timeout, max_par