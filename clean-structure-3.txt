={_host(h.url)} title={(h.title or '')[:80]!r}")

    # --- FETCH ROUND 1 (static) ---
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]
    print(f"[orchestrator] FETCH[1] start urls={[ _host(u) for u in urls ]}")

    t_f = time.perf_counter()
    results = await _fetch_round(
        urls, meta, per_url_timeout_s=per_timeout, max_parallel=max_parallel, use_js=False
    )
    dt_f = time.perf_counter() - t_f
    print(f"[orchestrator] FETCH[1] done n={len(results)} dt={dt_f:.3f}s")

    texts: List[Tuple[str, str, str]] = []  # (title, final_url, text)
    quality_scores: List[float] = []

    for original_url, res in results:
        if not res:
            print(f"[orchestrator]   fetch MISS url={original_url}")
            continue
        final_url, status, text = res
        title = next((t for (t, u) in meta if u == original_url), final_url)
        tl = len(text or "")
        qscore = content_quality_score(text or "")
        quality_scores.append(qscore)
        print(f"[orchestrator]   fetch OK   status={status} host={_host(final_url)} len={tl} q={qscore:.2f} title={(title or '')[:80]!r}")
        if text:
            texts.append((title, final_url, text))

    # --- JS retry decision (configurable thresholds) ---
    try_js = False
    if enable_js_retry and quality_scores:
        avg_q = sum(quality_scores) / len(quality_scores)
        lowish = sum(1 for q in quality_scores if q < js_low_q_thresh)
        if avg_q < js_avg_q_thresh or (lowish / max(1, len(quality_scores))) >= js_lowish_ratio:
            try_js = True

    if try_js:
        print("[orchestrator] FETCH[2-JS] trying JS-rendered fetch due to low content quality")
        js_timeout   = min(per_timeout + js_timeout_add, js_timeout_cap)
        js_parallel  = max(js_min_parallel, max_parallel + js_parallel_delta)

        results_js = await _fetch_round(
            urls, meta, per_url_timeout_s=js_timeout, max_parallel=js_parallel, use_js=True
        )
        texts_js: List[Tuple[str, str, str]] = []
        for original_url, res in results_js:
            if not res:
                continue
            final_url, status, text = res
            title = next((t for (t, u) in meta if u == original_url), final_url)
            tl = len(text or "")
            qscore = content_quality_score(text or "")
            print(f"[orchestrator]   fetch JS OK status={status} host={_host(final_url)} len={tl} q={qscore:.2f} title={(title or '')[:80]!r}")
            if text:
                texts_js.append((title, final_url, text))

        if texts_js:
            texts = texts_js

    if not texts:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no chunks | elapsed={time.time()-start_time:.3f}s")
        return None

    # --- Build chunks; order by quality ---
    texts.sort(key=lambda t: content_quality_score(t[2]), reverse=True)

    chunks: List[str] = []
    for title, final_url, text in texts:
        chunk = condense_doc(title, final_url, text, max_chars=per_doc_budget)
        chunks.append(chunk)
        print(f"[orchestrator]   chunk len={len(chunk)} host={_host(final_url)}")

    # --- Enforce total budget ---
    header = header_tpl.format(query=query)
    sep = _as_str("web_orch_block_separator")
    available = max(_as_int("web_orch_min_block_reserve"),
                    total_char_budget - len(header) - len(sep))
    block_parts: List[str] = []
    used = 0
    for idx, ch in enumerate(chunks):
        cl = len(ch)
        sep_len = (len(sep) if block_parts else 0)
        if used + cl + sep_len > available:
            shrunk = _head_tail(ch, max(min_chunk_after, available - used - sep_len))
            print(f"[orchestrator]   budget hit at chunk[{idx}] orig={cl} shrunk={len(shrunk)} used_before={used} avail={available}")
            if len(shrunk) > min_chunk_after:
                block_parts.append(shrunk)
                used += len(shrunk) + sep_len
            break
        block_parts.append(ch)
        used += cl + sep_len
        print(f"[orchestrator]   take chunk[{idx}] len={cl} used_total={used}/{available}")

    body = sep.join(block_parts)
    block = f"{header}{sep}{body}" if body else header

    end_time = time.time()
    print(f"[orchestrator] OUT @ {end_time:.3f}s | elapsed={end_time-start_time:.3f}s | chunks={len(block_parts)} | chars={len(block)}")
    return block

# ===== aimodel/file_read/web/provider.py =====

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SearchHit:
    title: str
    url: str
    snippet: Optional[str] = None
    rank: int = 0

class SearchProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        raise NotImplementedError

# ===== aimodel/file_read/web/query_summarizer.py =====

# aimodel/file_read/web/query_summarizer.py
from __future__ import annotations
from typing import Any, Iterable
import re

from ..core.settings import SETTINGS

def _tokens(s: str) -> set[str]:
    return set(re.findall(r"\w+", (s or "").lower()))

def _as_list(v) -> list:
    if v is None:
        return []
    if isinstance(v, (list, tuple)):
        return list(v)
    return [v]

def summarize_query(llm: Any, user_text: str) -> str:
    """
    Settings used (all optional; no in-code fallbacks):
      - query_sum_bypass_short_enabled : bool
      - query_sum_short_max_chars      : int
      - query_sum_short_max_words      : int
      - query_sum_prompt               : str (must contain '{text}')
      - query_sum_max_tokens           : int
      - query_sum_temperature          : float
      - query_sum_top_p                : float
      - query_sum_stop                 : list[str]
      - query_sum_overlap_check_enabled: bool
      - query_sum_overlap_jaccard_min  : float (0..1)
    """
    txt = (user_text or "").strip()
    print(f"[SUMMARIZER] IN user_text={txt!r}")

    # --- Bypass for very short queries (only if fully configured) ---
    bypass_enabled = SETTINGS.get("query_sum_bypass_short_enabled")
    short_chars = SETTINGS.get("query_sum_short_max_chars")
    short_words = SETTINGS.get("query_sum_short_max_words")
    if bypass_enabled is True and isinstance(short_chars, int) and isinstance(short_words, int):
        if len(txt) <= short_chars and len(txt.split()) <= short_words:
            print(f"[SUMMARIZER] BYPASS (short) -> {txt!r}")
            return txt

    # --- LLM prompt construction (only if prompt provided) ---
    prompt = SETTINGS.get("query_sum_prompt")
    if isinstance(prompt, str) and "{text}" in prompt:
        params = {}
        # Only include configured generation params; nothing hardcoded.
        max_tokens = SETTINGS.get("query_sum_max_tokens")
        if isinstance(max_tokens, int):
            params["max_tokens"] = max_tokens

        temperature = SETTINGS.get("query_sum_temperature")
        if isinstance(temperature, (int, float)):
            params["temperature"] = float(temperature)

        top_p = SETTINGS.get("query_sum_top_p")
        if isinstance(top_p, (int, float)):
            params["top_p"] = float(top_p)

        stops = _as_list(SETTINGS.get("query_sum_stop"))
        if stops:
            params["stop"] = [str(s) for s in stops if isinstance(s, str)]

        params["stream"] = False  # summarizer uses non-streamed call

        out = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt.format(text=txt)}],
            **params,  # only what’s configured is sent
        )
        result = (out["choices"][0]["message"]["content"] or "").strip()
    else:
        # If no prompt configured, do not attempt to summarize — return original text.
        print("[SUMMARIZER] SKIP (no prompt configured) -> retain input")
        return txt

    # --- Optional overlap check (guard against paraphrase drift) ---
    overlap_enabled = SETTINGS.get("query_sum_overlap_check_enabled")
    j_min = SETTINGS.get("query_sum_overlap_jaccard_min")
    if overlap_enabled is True and isinstance(j_min, (int, float)):
        src_toks = _tokens(txt)
        out_toks = _tokens(result)
        if not result or not out_toks:
            print(f"[SUMMARIZER] RETAIN (empty/none) -> {txt!r}")
            print(f"[SUMMARIZER] OUT query={txt!r}")
            return txt
        jaccard = (len(src_toks & out_toks) / len(src_toks | out_toks)) if (src_toks or out_toks) else 1.0
        if jaccard < float(j_min):
            print(f"[SUMMARIZER] RETAIN (low overlap {jaccard:.2f} < {float(j_min):.2f}) -> {txt!r}")
            print(f"[SUMMARIZER] OUT query={txt!r}")
            return txt
        print(f"[SUMMARIZER] OUT query={result!r} (overlap {jaccard:.2f})")
        return result

    # If overlap check not enabled/configured, just return the LLM result.
    print(f"[SUMMARIZER] OUT query={result!r} (no overlap check)")
    return result

# ===== aimodel/file_read/web/router_ai.py =====

# aimodel/file_read/web/router_ai.py
from __future__ import annotations
from typing import Tuple, Optional, Any
import json, re
from ..core.settings import SETTINGS

# ---- Hardcoded, brace-safe prompt (only {text} is formatted) -----------------
_DECIDE_PROMPT = (
    "You are a router deciding whether answering the text requires the public web.\n"
    "Respond with JSON only in exactly this schema:\n"
    "{{\"need\": true|false, \"query\": \"<text or empty>\"}}\n\n"
    "Decision principle:\n"
    "- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n"
    "- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n"
    "- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n"
    "- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n"
    "- When uncertain whether real-time state is required, prefer need=true.\n\n"
    "Text:\n{text}\n"
    "JSON:"
)

# ---- JSON extraction (configurable) -----------------------------------------
def _force_json(s: str) -> dict:
    if not s:
        return {}
    # Primary: straight JSON
    try:
        v = json.loads(s)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass

    # Settings-provided regex (recommended: non-greedy first-block)
    rgx = SETTINGS.get("router_json_extract_regex")
    cand = None
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
        except Exception:
            cand = None

    # Fallback: first non-greedy {...}
    if not cand:
        m2 = re.search(r"\{.*?\}", s, re.DOTALL)
        cand = m2.group(0) if m2 else None

    if not cand:
        return {}
    try:
        v = json.loads(cand)
        return v if isinstance(v, dict) else {}
    except Exception:
        return {}

# ---- Strip wrappers (configurable) ------------------------------------------
def _strip_wrappers(text: str) -> str:
    t = (text or "")
    if SETTINGS.get("router_trim_whitespace") is True:
        t = t.strip()

    if SETTINGS.get("router_strip_wrappers_enabled") is not True:
        return t

    head = t
    if SETTINGS.get("router_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]

    pat = SETTINGS.get("router_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

# ---- Core router -------------------------------------------------------------
def decide_web(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    try:
        if not user_text or not user_text.strip():
            print("[ROUTER] SKIP (empty user_text)")
            return (False, None)

        t_raw = user_text.strip()
        core_text = _strip_wrappers(t_raw)
        print(f"[ROUTER] INPUT raw={t_raw!r} core={core_text!r}")

        # Explicit overrides (prefixes configurable)
        prefixes = SETTINGS.get("router_explicit_prefixes")
        if isinstance(prefixes, list) and prefixes:
            low = t_raw.lower()
            for p in prefixes:
                ps = str(p or "").lower()
                if ps and low.startswith(ps):
                    q = t_raw.split(":", 1)[1].strip() if ":" in t_raw else t_raw
                    q = _strip_wrappers(q)
                    print(f"[ROUTER] EXPLICIT override need_web=True query={q!r}")
                    return (True, q)

        # Hardcoded prompt; all other knobs from settings
        the_prompt = _DECIDE_PROMPT.format(text=core_text)
        print(f"[ROUTER] PROMPT >>>\n{the_prompt}\n<<< PROMPT")

        # Build generation params from settings (filter out None)
        params = {
            "max_tokens": SETTINGS.get("router_decide_max_tokens"),
            "temperature": SETTINGS.get("router_decide_temperature"),
            "top_p": SETTINGS.get("router_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}

        raw_out_obj = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw_out_obj.get("choices", [{}])[0]
                                  .get("message", {})
                                  .get("content") or "").strip()

        try:
            print(f"[ROUTER] RAW OBJ keys={list(raw_out_obj.keys())}")
        except Exception:
            pass
        print(f"[ROUTER] RAW OUT str={text_out!r}")

        data = _force_json(text_out) or {}
        print(f"[ROUTER] PARSED JSON={data}")

        # Keep it strict: bool only; otherwise fallback to default
        need_val = data.get("need", None)
        if isinstance(need_val, bool):
            need = need_val
        else:
            need_default = SETTINGS.get("router_default_need_when_invalid")
            need = bool(need_default) if isinstance(need_default, bool) else False

        query_field = data.get("query", "")
        try:
            query = _strip_wrappers(str(query_field or "").strip())
        except Exception:
            query = ""

        if not need:
            query = None

        print(f"[ROUTER] DECISION need_web={need} query={query!r}")
        return (need, query)

    except Exception as e:
        print(f"[ROUTER] FATAL in decide_web: {type(e).__name__}: {e}")
        return (False, None)

# ---- One-hop decide + fetch --------------------------------------------------
async def decide_web_and_fetch(llm: Any, user_text: str, *, k: int = 3) -> Optional[str]:
    t = (user_text or "").strip()
    prev = (t[:160] + "…") if len(t) > 160 else t
    print(f"[ROUTER:FETCH] IN text_len={len(t)} k={k} text_preview={prev!r}")

    need, proposed_q = decide_web(llm, t)
    print(f"[ROUTER:FETCH] decide_web -> need={need} proposed_q={proposed_q!r}")
    if not need:
        print("[ROUTER:FETCH] no web needed")
        return None

    # Lazy imports to avoid circulars
    from .query_summarizer import summarize_query
    from .orchestrator import build_web_block

    base_query = _strip_wrappers((proposed_q or t).strip())
    try:
        q_summary = (summarize_query(llm, base_query) or "").strip().strip('"\'' ) or base_query
        q_summary = _strip_wrappers(q_summary)
        print(f"[ROUTER:FETCH] summarize_query base={base_query!r} -> q_summary={q_summary!r}")
    except Exception as e:
        print(f"[ROUTER:FETCH] summarize_query ERROR {type(e).__name__}: {e}")
        q_summary = base_query

    try:
        block = await build_web_block(q_summary, k=k)
        print(f"[ROUTER:FETCH] build_web_block len={(len(block) if block else 0)}")
    except Exception as e:
        print(f"[ROUTER:FETCH] build_web_block ERROR {type(e).__name__}: {e}")
        block = None

    return block or None

# ===== aimodel/file_read/workers/retitle_worker.py =====

# aimodel/file_read/retitle_worker.py
from __future__ import annotations
import asyncio, logging, re
from typing import Dict, List, Optional, Tuple

from ..runtime.model_runtime import get_llm
from ..store.index import load_index, save_index
from ..store.base import now_iso
from ..services.cancel import is_active, GEN_SEMAPHORE  # serialize with main generation
from ..store.chats import _load_chat  # for current seq watermark
from ..core.settings import SETTINGS  # centralized settings loader


# -----------------------------------------------------------------------------
# Settings access (STRICT: no fallbacks)
# -----------------------------------------------------------------------------
def S(key: str):
    # Raise KeyError immediately if missing so misconfig is visible
    return SETTINGS[key]


# -----------------------------------------------------------------------------
# Coalesced per-session queue (last-write-wins)
# -----------------------------------------------------------------------------
# We store only the latest snapshot + watermark per session. The worker consumes
# session IDs; when it handles a session, it reads the latest snapshot once.
_PENDING: Dict[str, dict] = {}
_ENQUEUED: set[str] = set()
_queue: asyncio.Queue[str] = asyncio.Queue(maxsize=int(S("retitle_queue_maxsize")))
_lock = asyncio.Lock()  # guards _PENDING/_ENQUEUED


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def _preview(s: str) -> str:
    n = int(S("retitle_preview_chars"))
    ell = S("retitle_preview_ellipsis")
    s = (s or "")
    return (s[:n] + ell) if len(s) > n else s


def _is_substantial(text: str) -> bool:
    t = (text or "").strip()
    min_chars = int(S("retitle_min_substantial_chars"))
    require_alpha = bool(S("retitle_require_alpha"))
    if len(t) < min_chars:
        return False
    return (re.search(r"[A-Za-z]", t) is not None) if require_alpha else True


def _pick_source(messages: List[dict]) -> Optional[str]:
    """
    Choose best text to title based on policy toggles:
      - retitle_pick_first_substantial
      - retitle_pick_latest_substantial
      - retitle_pick_first_user_fallback
    """
    if not messages:
        return None

    if bool(S("retitle_pick_first_substantial")):
        for m in messages:
            if (m.get("role") == "user") and _is_substantial(m.get("content", "")):
                return m.get("content", "")

    if bool(S("retitle_pick_latest_substantial")):
        for m in reversed(messages):
            if (m.get("role") == "user") and _is_substantial(m.get("content", "")):
                return m.get("content", "")

    if bool(S("retitle_pick_first_user_fallback")):
        for m in messages:
            if m.get("role") == "user":
                return m.get("content", "")

    return None


def _sanitize_title(s: str) -> str:
    """
    One line, trimmed, limited words/chars, with configurable regex steps.
    All behavior is driven by retitle_* settings.
    """
    if not s:
        return ""

    s = s.strip()

    # Drop common prefixes (bullets, quotes, numbering)
    drop_prefix_re = S("retitle_sanitize_drop_prefix_regex")
    if drop_prefix_re:
        s = re.sub(drop_prefix_re, "", s)

    # Strip quotes
    if bool(S("retitle_sanitize_strip_quotes")):
        s = s.strip().strip('"').strip("'").strip()

    # Replace non-allowed chars
    replace_not_allowed_re = S("retitle_sanitize_replace_not_allowed_regex")
    replace_with = S("retitle_sanitize_replace_with")
    if replace_not_allowed_re:
        s = re.sub(replace_not_allowed_re, replace_with, s)

    # Collapse whitespace
    s = re.sub(r"\s+", " ", s).strip()

    # Word & char limits
    max_words = int(S("retitle_sanitize_max_words"))
    max_chars = int(S("retitle_sanitize_max_chars"))
    if max_words > 0:
        words = s.split()
        s = " ".join(words[:max_words])
    if max_chars > 0 and len(s) > max_chars:
        s = s[:max_chars].rstrip()

    return s


def _make_title(llm, src: str) -> str:
    # System instruction and examples from settings
    sys = S("retitle_llm_sys_inst")
    examples: List[dict] = list(S("retitle_llm_examples"))

    # Optional prefix/suffix on the user text
    user_text = f"{S('retitle_user_prefix')}{src}{S('retitle_user_suffix')}"

    messages = [{"role": "system", "content": sys}, *examples, {"role": "user", "content": user_text}]

    out = llm.create_chat_completion(
        messages=messages,
        max_tokens=int(S("retitle_llm_max_tokens")),
        temperature=float(S("retitle_llm_temperature")),
        top_p=float(S("retitle_llm_top_p")),
        stream=False,
        stop=S("retitle_llm_stop"),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip().strip('"').strip("'")
    return raw


# -----------------------------------------------------------------------------
# Worker loop
# -----------------------------------------------------------------------------
async def start_worker():
    """Background loop to process retitle jobs one by one."""
    while True:
        sid = await _queue.get()
        try:
            await _process_session(sid)
        except Exception:
            logging.exception("Retitle worker failed")
        finally:
            _queue.task_done()


def _extract_job(snapshot: dict) -> Tuple[List[dict], int]:
    msgs = snapshot.get("messages") or []
    job_seq = int(snapshot.get("job_seq") or 0)
    return msgs, job_seq


async def _process_session(session_id: str):
    # global enable flag
    if not bool(S("retitle_enable")):
        return

    # debounce after stream
    await asyncio.sleep(int(S("retitle_grace_ms")) / 1000.0)

    # lower priority than generation: wait while this session is active
    waited = 0
    backoff = int(S("retitle_active_backoff_start_ms"))
    backoff_max = int(S("retitle_active_backoff_max_ms"))
    backoff_total = int(S("retitle_active_backoff_total_ms"))
    growth = float(S("retitle_active_backoff_growth"))

    while is_active(session_id) and waited < backoff_total:
        await asyncio.sleep(backoff / 1000.0)
        waited += backoff
        # exponential backoff with cap
        backoff = min(int(backoff * growth), backoff_max)

    # fetch latest coalesced snapshot
    async with _lock:
        snapshot = _PENDING.pop(session_id, None)
        _ENQUEUED.discard(session_id)

    if not snapshot:
        return

    messages, job_seq = _extract_job(snapshot)

    # stale guard: if chat seq has advanced, skip (a newer enqueue will run)
    try:
        cur_seq = int((_load_chat(session_id) or {}).get("seq") or 0)
    except Exception:
        cur_seq = job_seq
    if cur_seq > job_seq:
        print(f"[retitle] SKIP (stale) session={session_id} job_seq={job_seq} current_seq={cur_seq}")
        return

    # pick source text
    src = _pick_source(messages) or ""
    if not src.strip():
        return

    print(f"[retitle] START session={session_id} job_seq={job_seq} src={_preview(src)!r}")

    # Serialize with main generation semaphore; run LLM call in a worker thread
    async with GEN_SEMAPHORE:
        llm = get_llm()
        try:
            title_raw = await asyncio.to_thread(_make_title, llm, src)
        except Exception as e:
            logging.exception("retitle: LLM error: %s", e)
            return
        finally:
            try:
                llm.reset()
            except Exception:
                pass

    # Optional sanitize (fully controlled by settings)
    if bool(S("retitle_enable_sanitize")):
        title = _sanitize_title(title_raw)
    else:
        title = title_raw

    print(f"[retitle] FINISH session={session_id} -> {title!r}")

    if not title:
        return

    # write only if changed
    idx = load_index()
    row = next((r for r in idx if r.get("sessionId") == session_id), None)
    if not row:
        return
    if (row.get("title") or "").strip() == title:
        return

    row["title"] = title
    row["updatedAt"] = now_iso()
    save_index(idx)


# -----------------------------------------------------------------------------
# Public API
# -----------------------------------------------------------------------------
def enqueue(session_id: str, messages: List[dict], *, job_seq: Optional[int] = None):
    """Coalesced enqueue; last-write-wins per session, with a seq watermark."""
    if not session_id:
        return
    if not isinstance(messages, list):
        messages = []
    # infer watermark if not provided (max message id)
    if job_seq is None:
        try:
            job_seq = max(int(m.get("id") or 0) for m in messages) if messages else 0
        except Exception:
            job_seq = 0

    snap = {"messages": messages, "job_seq": int(job_seq)}

    async def _put():
        async with _lock:
            _PENDING[session_id] = snap  # overwrite older snapshot
            if session_id not in _ENQUEUED:
                _ENQUEUED.add(session_id)
                try:
                    _queue.put_nowait(session_id)
                except Exception as e:
                    logging.warning(f"Failed to enqueue retitle: {e}")

    # enqueue can be called from sync contexts; schedule safely
    try:
        loop = asyncio.get_running_loop()
        loop.create_task(_put())
    except RuntimeError:
        # no running loop (unlikely here); run a temporary loop
        asyncio.run(_put())

# ===== frontend/src/file_read/App.tsx =====


import AgentRunner from "./pages/AgentRunner";

export default function App() {
  return (
    <main className="bg-gray-50 min-h-screen">
      <AgentRunner />
    </main>
  );
}

# ===== frontend/src/file_read/components/AssistantMetrics.tsx =====

// frontend/src/file_read/components/chat/AssistantMetrics.tsx
import { Info } from "lucide-react";
import MetricsHoverCard from "./MetricsHoverCard";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function AssistantMetrics({
  status,
  runJson,
  flat,
  align = "right",
}: { status: string; runJson?: RunJson | null; flat?: GenMetrics | null; align?: "left" | "right" }) {
  return (
    <div className="mt-2 flex justify-start">
      <div className="inline-flex items-center gap-1 px-2 py-1 rounded-full bg-white border shadow-sm text-[11px] text-gray-600">
        <Info className="w-3.5 h-3.5 opacity-70" />
        <span className="truncate max-w-[70vw] sm:max-w-none">{status || "Run details"}</span>
        <MetricsHoverCard
          data={
            runJson ??
            (flat
              ? {
                  stats: {
                    stopReason: flat.stop_reason ?? null,
                    tokensPerSecond: flat.tok_per_sec ?? null,
                    timeToFirstTokenSec: flat.ttft_ms != null ? Math.max(0, flat.ttft_ms) / 1000 : null,
                    totalTimeSec: null,
                    promptTokensCount: flat.input_tokens_est ?? null,
                    predictedTokensCount: flat.output_tokens ?? null,
                    totalTokensCount: flat.total_tokens_est ?? null,
                  },
                }
              : null)
          }
          title="Run JSON"
          align={align}
          compact
        />
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatBubble.tsx =====

import { useState } from "react";
import { Copy, Check, Trash2 } from "lucide-react";
import MarkdownMessage from "./Markdown/MarkdownMessage";
import { parseRunJson } from "../utils/parseRunjson";
import type { Attachment } from "../types/chat";   // ✅ import Attachment type

const STOP_SENTINEL_RE = /(?:\r?\n)?(?:\u23F9|\\u23F9)\s+stopped(?:\r?\n)?$/u;

export default function ChatBubble({
  role,
  text,
  attachments = [],   // ✅ new prop
  showActions = true,
  onDelete,
}: {
  role: "user" | "assistant";
  text: string;
  attachments?: Attachment[];   // ✅ allow attachments
  showActions?: boolean;
  onDelete?: () => void;
}) {
  const isUser = role === "user";
  const raw = text ?? "";
  const { text: stripped } = parseRunJson(raw);
  let content = stripped.trim();

  if (!isUser) content = content.replace(STOP_SENTINEL_RE, "");

  const hasOnlyAttachments =
    isUser && (!content || content.length === 0) && attachments.length > 0;

  if (role === "assistant" && !content && attachments.length === 0) return null;

  const [copiedMsg, setCopiedMsg] = useState(false);
  const copyWholeMessage = async () => {
    try {
      await navigator.clipboard.writeText(content);
      setCopiedMsg(true);
      setTimeout(() => setCopiedMsg(false), 2000);
    } catch {}
  };

  return (
    <div className="mb-2">
      <div className={`flex ${isUser ? "justify-end" : "justify-start"}`}>
        <div
          className={`max-w-[80%] w-fit break-words rounded-2xl px-4 py-2 shadow-sm
                      prose prose-base max-w-none
            ${isUser ? "bg-black text-white prose-invert" : "bg-white border text-gray-900"}`}
        >
          {/* ✅ Render attachments */}
          {attachments.length > 0 && (
            <div className="mb-2 flex flex-wrap gap-2">
              {attachments.map((att) => (
                <div
                  key={`${att.sessionId || "global"}:${att.source || att.name}`}
                  className={`border rounded px-2 py-1 text-sm flex items-center gap-2 ${
                    isUser
                      ? "bg-white/10 border-white/30"
                      : "bg-white"
                  }`}
                  title={att.name || att.source}
                >
                  📎 <span className="truncate max-w-[220px]">{att.name}</span>
                </div>
              ))}
            </div>
          )}

          {content ? (
            <div className="max-w-full">
              <MarkdownMessage text={content} />
            </div>
          ) : hasOnlyAttachments ? null : isUser ? null : (
            <span className="opacity-60">…</span>
          )}
        </div>
      </div>

      {showActions && (
        <div className={`mt-1 flex ${isUser ? "justify-end" : "justify-start"}`}>
          <div className="flex items-center gap-2">
            <button
              type="button"
              onClick={copyWholeMessage}
              title={copiedMsg ? "Copied" : "Copy"}
              aria-label={copiedMsg ? "Copied" : "Copy message"}
              className="inline-flex items-center justify-center w-7 h-7 rounded border
                         bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
            >
              {copiedMsg ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
            </button>
            {onDelete && (
              <button
                type="button"
                onClick={onDelete}
                title="Delete message"
                aria-label="Delete message"
                className="inline-flex items-center justify-center w-7 h-7 rounded border
                           bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
              >
                <Trash2 className="w-4 h-4" />
              </button>
            )}
          </div>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatComposer.tsx =====

// frontend/src/file_read/components/ChatComposer.tsx
import { useEffect, useRef, useState } from "react";
import { SendHorizonal, Square, Paperclip, X, Check } from "lucide-react";
import { uploadRagWithProgress, deleteUploadHard } from "../data/ragApi";
import { API_BASE } from "../services/http"; // ⬅️ NEW
import type { Attachment } from "../types/chat";

const FORCE_SCROLL_EVT = "chat:force-scroll-bottom";

type Props = {
  input: string;
  setInput: (v: string) => void;
  loading: boolean;
  queued?: boolean;
  onSend: (text: string, attachments?: Attachment[]) => void | Promise<void>;  // ✅ updated
  onStop: () => void | Promise<void>;
  onHeightChange?: (h: number) => void;
  onRefreshChats?: () => void;
  sessionId?: string;
};

type Att = {
  id: string;               // unique per pick
  name: string;             // file.name (used as source)
  pct: number;              // 0..100
  status: "uploading" | "ready" | "error";
  abort?: AbortController;  // to cancel in-flight
};

export default function ChatComposer({
  input,
  setInput,
  loading,
  queued = false,
  onSend,
  onStop,
  onHeightChange,
  onRefreshChats,
  sessionId,
}: Props) {
  const wrapRef = useRef<HTMLDivElement>(null);
  const taRef = useRef<HTMLTextAreaElement>(null);
  const fileRef = useRef<HTMLInputElement>(null);
  const MAX_HEIGHT_PX = 192;

  const [isClamped, setIsClamped] = useState(false);
  const [draft, setDraft] = useState(input);

  // local attachment list
  const [atts, setAtts] = useState<Att[]>([]);

  useEffect(() => setDraft(input), [input]);

  const autogrow = () => {
    const ta = taRef.current;
    if (!ta) return;
    ta.style.height = "auto";
    const next = Math.min(ta.scrollHeight, MAX_HEIGHT_PX);
    ta.style.height = `${next}px`;
    setIsClamped(ta.scrollHeight > MAX_HEIGHT_PX);
    if (wrapRef.current && onHeightChange) {
      onHeightChange(wrapRef.current.getBoundingClientRect().height);
    }
  };

  useEffect(() => {
    autogrow();
    const onResize = () => autogrow();
    window.addEventListener("resize", onResize);
    return () => window.removeEventListener("resize", onResize);
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  useEffect(() => { autogrow(); }, [draft, atts.length]); // reflow when chips change

  const hasText = draft.trim().length > 0;
  const anyUploading = atts.some(a => a.status === "uploading");
  const anyReady = atts.some(a => a.status === "ready");

  const forceScroll = (behavior: ScrollBehavior = "auto") => {
    window.dispatchEvent(new CustomEvent(FORCE_SCROLL_EVT, { detail: { behavior } }));
  };

  // ⬇️ NEW: build attachments payload for POST
  function attachmentsForPost() {
    if (!sessionId) return [];
    return atts
      .filter(a => a.status === "ready")
      .map(a => ({
        name: a.name,
        source: a.name,   // RAG uses filename as source
        sessionId,
        // optionally: size, mime, chunks if you track them
      }));
  }

const handleSendClick = async () => {
  const v = draft.trim();
  if ((loading || queued) || (!v && !anyReady) || anyUploading) return;

  forceScroll("auto");

  if (sessionId) {
    const API = (API_BASE || "").replace(/\/$/, "");
    const body = JSON.stringify({ role: "user", content: v, attachments: attachmentsForPost() });
    try {
      await fetch(`${API}/api/chats/${sessionId}/messages`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body,
      });
    } catch {}
  }

  setDraft("");
  setInput("");
  setAtts([]);

  try {
    await onSend(v, attachmentsForPost());
  } finally {
    onRefreshChats?.();
    requestAnimationFrame(() => forceScroll("smooth"));
  }
};

// ✅ moved out of handleSendClick
const handleStopClick = () => {
  if (!loading && !queued) return;
  void Promise.resolve(onStop()).finally(() => onRefreshChats?.());
};

  const pickFile = () => fileRef.current?.click();

  const onFilePicked: React.ChangeEventHandler<HTMLInputElement> = async (e) => {
    const files = e.target.files;
    if (!files || files.length === 0) return;
    if (!sessionId) { e.target.value = ""; return; }

    const picked = Array.from(files);
    // create local chip entries
    const news: Att[] = picked.map((f, i) => ({
      id: `${Date.now()}-${i}-${f.name}`,
      name: f.name,
      pct: 0,
      status: "uploading",
      abort: new AbortController(),
    }));
    setAtts(prev => [...prev, ...news]);

    // kick uploads
    news.forEach((att, idx) => {
      const f = picked[idx];
      uploadRagWithProgress(
        f,
        sessionId,
        (pct) => setAtts(prev => prev.map(a => a.id === att.id ? { ...a, pct } : a)),
        att.abort?.signal
      ).then(() => {
        setAtts(prev => prev.map(a => a.id === att.id ? { ...a, pct: 100, status: "ready", abort: undefined } : a));
        onRefreshChats?.();
      }).catch(() => {
        setAtts(prev => prev.map(a => a.id === att.id ? { ...a, status: "error", abort: undefined } : a));
      });
    });

    e.target.value = ""; // allow re-pick
  };

const removeAtt = async (att: Att) => {
  // 1) Cancel in-flight upload immediately
  if (att.status === "uploading" && att.abort) {
    att.abort.abort();
  }

  // 2) Always ask backend to purge any partial/queued ingest
  if (sessionId) {
    try { await deleteUploadHard(att.name, sessionId); } catch {}
  }

  // 3) Remove chip right away (UI feels instant)
  setAtts(prev => prev.filter(a => a.id !== att.id));

  onRefreshChats?.();
};

  function onKeyDown(e: React.KeyboardEvent<HTMLTextAreaElement>) {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      void handleSendClick();
    }
  }

  const disableActions = loading || queued || anyUploading;
  const showSend = hasText || anyReady; // allow send even with empty text if file ready

  return (
    <div ref={wrapRef} className="relative z-50 bg-white/95 backdrop-blur border-t p-3">
      {/* Attachment chips */}
      {atts.length > 0 && (
        <div className="mb-2 flex flex-wrap gap-2">
          {atts.map((a) => (
            <div key={a.id} className="min-w-[160px] max-w-[280px] border rounded-lg px-2 py-2">
              <div className="flex items-center justify-between gap-2">
                <div className="truncate text-sm" title={a.name}>{a.name}</div>
                <button
                  className="p-1 rounded hover:bg-gray-100"
                  aria-label="Remove file"
                  onClick={() => removeAtt(a)}
                >
                  <X size={14} />
                </button>
              </div>
              <div className="mt-2 h-1.5 w-full bg-gray-200 rounded">
                <div
                  className={`h-1.5 rounded ${a.status === "error" ? "bg-red-500" : "bg-black"}`}
                  style={{ width: `${a.pct}%` }}
                />
              </div>
              <div className="mt-1 text-xs text-gray-500 flex items-center gap-1">
                {a.status === "uploading" && <span>Uploading… {a.pct}%</span>}
                {a.status === "ready" && <><Check size={14} /> Ready</>}
                {a.status === "error" && <span>Error</span>}
              </div>
            </div>
          ))}
        </div>
      )}

      <div className="flex gap-2">
        <input
          ref={fileRef}
          type="file"
          multiple
          className="hidden"
          onChange={onFilePicked}
        />

        <textarea
          ref={taRef}
          value={draft}
          onChange={(e) => { setDraft(e.target.value); setInput(e.target.value); autogrow(); }}
          onInput={autogrow}
          onKeyDown={onKeyDown}
          placeholder="Ask anything…"
          className={`flex-1 border rounded-lg px-3 py-2 resize-none focus:outline-none focus:ring ${
            isClamped ? "overflow-y-auto" : "overflow-hidden"
          }`}
          rows={1}
          style={{ maxHeight: MAX_HEIGHT_PX }}
          disabled={queued} // typing allowed during upload if you prefer, keep as-is
        />

        <div className="flex items-end gap-2">
          <button
            className={`p-2 rounded-lg border hover:bg-gray-50 ${disableActions || !sessionId ? "opacity-60 cursor-not-allowed" : ""}`}
            onClick={pickFile}
            title="Upload to this chat"
            aria-label="Upload to this chat"
            disabled={disableActions || !sessionId}
          >
            <Paperclip size={18} />
          </button>

          {(loading || queued) ? (
            <button
              className="p-2 rounded-lg border hover:bg-gray-50"
              onClick={handleStopClick}
              title={queued ? "Cancel queued message" : "Stop generating"}
              aria-label={queued ? "Cancel queued message" : "Stop generating"}
            >
              <Square size={18} />
            </button>
          ) : showSend ? (
            <button
              className="p-2 rounded-lg bg-black text-white hover:bg-black/90 active:translate-y-px disabled:opacity-60"
              onClick={handleSendClick}
              title="Send"
              aria-label="Send"
              disabled={disableActions}
            >
              <SendHorizonal size={18} />
            </button>
          ) : null}
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatContainer.tsx =====

// frontend/src/file_read/components/ChatContainer.tsx
import { useState, useRef, useEffect } from "react";
import ChatView from "./ChatView/ChatView";
import ChatComposer from "./ChatComposer";
import type { ChatMsg } from "../types/chat";
import type { GenMetrics, RunJson } from "../hooks/useChatStream";
import type { Attachment } from "../types/chat";

interface Props {
  messages: ChatMsg[];
  input: string;
  setInput: (s: string) => void;
  loading: boolean;
  queued?: boolean;
  send: (text?: string, attachments?: Attachment[]) => Promise<void>;
  stop: () => Promise<void> | void;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onRefreshChats?: () => void;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
  sessionId?: string;
}

export default function ChatContainer({
  messages,
  input,
  setInput,
  loading,
  queued = false,
  send,
  stop,
  runMetrics,
  runJson,
  onRefreshChats,
  onDeleteMessages,
  autoFollow = true,
  sessionId,
}: Props) {
  const [composerH, setComposerH] = useState(0);
  const containerRef = useRef<HTMLDivElement>(null);
  const [pinned, setPinned] = useState(false);

  useEffect(() => {
    const el = containerRef.current;
    if (!el) return;
    const threshold = 120;
    const isNearBottom = () =>
      el.scrollHeight - el.scrollTop - el.clientHeight <= threshold;
    const onScroll = () => setPinned(!isNearBottom());
    el.addEventListener("scroll", onScroll, { passive: true });
    setPinned(!isNearBottom());
    return () => el.removeEventListener("scroll", onScroll);
  }, []);

  const forceScrollToBottom = (behavior: ScrollBehavior = "smooth") => {
    const el = containerRef.current;
    if (!el) return;
    el.scrollTo({ top: el.scrollHeight, behavior });
  };

  const handleSend = async (text?: string, attachments?: Attachment[]) => {
    if (!pinned) forceScrollToBottom("auto");
    await send(text, attachments);
    onRefreshChats?.();
    if (!pinned) requestAnimationFrame(() => forceScrollToBottom("smooth"));
  };

  return (
    <div className="flex flex-col h-full border rounded-lg overflow-hidden bg-white">
      <div ref={containerRef} data-chat-scroll className="flex-1 overflow-y-auto min-w-0">
        <ChatView
          messages={messages}
          loading={loading}
          queued={queued}
          bottomPad={composerH}
          runMetrics={runMetrics}
          runJson={runJson}
          onDeleteMessages={onDeleteMessages}
          autoFollow={autoFollow}
        />
      </div>

      <ChatComposer
        input={input}
        setInput={setInput}
        loading={loading}
        queued={queued}
        onSend={handleSend}
        onStop={stop}
        onHeightChange={setComposerH}
        onRefreshChats={onRefreshChats}
        sessionId={sessionId}
      />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatItem.tsx =====

// frontend/src/file_read/components/chat/ChatItem.tsx
import ChatBubble from "./ChatBubble";
import AssistantMetrics from "./AssistantMetrics";
import { buildStatus } from "./ChatView/StatusLine";
import type { ChatMsg } from "../types/chat";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function ChatItem({
  m,
  idx,
  loading,
  lastAssistantIndex,
  runJsonLive,
  runMetricsLive,
  onDelete,
}: {
  m: ChatMsg;
  idx: number;
  loading: boolean;
  lastAssistantIndex: number;
  runJsonLive?: RunJson | null;
  runMetricsLive?: GenMetrics | null;
  onDelete?: (id: string) => void;
}) {
  const isAssistant = m.role === "assistant";
  const isCurrentStreamingAssistant = isAssistant && loading && idx === lastAssistantIndex;

  let jsonForThis: RunJson | null = null;
  let flatForThis: GenMetrics | null = null;

  if (isAssistant) {
    // @ts-ignore meta bag
    const meta = m.meta as { runJson?: RunJson | null; flat?: GenMetrics | null } | undefined;
    jsonForThis = meta?.runJson ?? null;
    flatForThis = meta?.flat ?? null;

    if (isCurrentStreamingAssistant) {
      if (runJsonLive) jsonForThis = runJsonLive;
      if (runMetricsLive) flatForThis = runMetricsLive;
    }
  }

  const status = isAssistant ? buildStatus(jsonForThis, flatForThis) : "";
  const showMetrics = isAssistant && (jsonForThis || flatForThis);

  return (
    <div>
      <ChatBubble
        role={m.role}
        text={m.text}
        attachments={m.attachments} 
        showActions={m.role === "user" || (m.role === "assistant" && !isCurrentStreamingAssistant)}
        onDelete={onDelete ? () => onDelete(m.id) : undefined}
      />
      {showMetrics && <AssistantMetrics status={status} runJson={jsonForThis} flat={flatForThis} />}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/ChatSidebar.tsx =====

import { useState } from "react";
import { deleteChatsBatch } from "../../hooks/data/chatApi";
import type { ChatRow } from "../../types/chat";
import { useMultiSelect } from "../../hooks/useMultiSelect";
import { useChatsPager } from "../../hooks/useChatsPager";
import SidebarHeader from "./SidebarHeader";
import SidebarListItem from "./SidebarListItem";

const PAGE_SIZE = 10;

type Props = {
  onOpen: (id: string) => Promise<void>;
  onNew: () => Promise<void>;
  refreshKey?: number;
  activeId?: string;
  onHideSidebar?: () => void;
  onCancelSessions?: (ids: string[]) => Promise<void>;
};

export default function ChatSidebar({
  onOpen, onNew, refreshKey, activeId, onHideSidebar, onCancelSessions,
}: Props) {
  const {
    chats, page, hasMore, total, totalPages,
    initialLoading, loadingMore,
    scrollRef, sentinelRef, loadMore, setChats,
  } = useChatsPager(PAGE_SIZE, refreshKey);

  const [isEditing, setIsEditing] = useState(false);
  const [deleting, setDeleting] = useState(false);
  const [newPending, setNewPending] = useState(false);

  const allIds = chats.map(c => c.sessionId);
  const { selected, setSelected, allSelected, toggleOne, toggleAll } = useMultiSelect(allIds);

  async function handleNew() {
    if (newPending) return;
    setNewPending(true);
    try { await onNew(); } finally { setNewPending(false); }
  }

  async function onDeleteSelected(): Promise<void> {
    const count = selected.size;
    if (!count || deleting) return;

    const isAll = count === chats.length;
    const ok = window.confirm(
      isAll
        ? `Delete ALL ${count} chats? This cannot be undone.`
        : `Delete ${count} selected chat${count > 1 ? "s" : ""}?`
    );
    if (!ok) return;

    const ids = [...selected];
    try { await onCancelSessions?.(ids); await Promise.resolve(); } catch {}

    const deletingActive = activeId ? selected.has(activeId) : false;
    const fallback = chats.find(c => !selected.has(c.sessionId))?.sessionId;

    setDeleting(true);
    try {
      const deleted = await deleteChatsBatch(ids);
      if (!deleted.length) return;
      setChats(prev => prev.filter(c => !deleted.includes(c.sessionId)));
      setSelected(new Set());
      setIsEditing(false);

      if (deletingActive) {
        if (fallback) void onOpen(fallback);
        else void onOpen("");
      }
    } finally {
      setDeleting(false);
    }
  }

  return (
    <aside className="w-full md:w-72 h-full border-r bg-white p-0 flex flex-col">
      <SidebarHeader
        isEditing={isEditing}
        setIsEditing={(v) => { setIsEditing(v); setSelected(new Set()); }}
        newPending={newPending}
        onNew={handleNew}
        onHideSidebar={onHideSidebar}
        selectedCount={selected.size}
        deleting={deleting}
        onDelete={onDeleteSelected}
      />

      {/* LIST */}
      <div
        ref={scrollRef}
        className="flex-1 overflow-y-auto p-2 overscroll-contain"
        style={{ WebkitOverflowScrolling: "touch" }}
      >
        {initialLoading && (
          <div className="px-2 py-1 text-xs text-gray-500">Loading…</div>
        )}

        <ul className="space-y-1">
          {chats.map((c: ChatRow) => (
            <SidebarListItem
              key={c.sessionId}
              c={c}
              isActive={activeId === c.sessionId}
              isEditing={isEditing}
              isChecked={selected.has(c.sessionId)}
              onToggle={() => toggleOne(c.sessionId)}
              onOpen={() => void onOpen(c.sessionId)}
            />
          ))}
        </ul>

        <div className="h-6" ref={sentinelRef} />

        {hasMore && (
          <div className="px-2 pb-2">
            <button
              className={`w-full text-xs px-3 py-1 rounded border ${loadingMore ? "opacity-50 cursor-wait" : ""}`}
              onClick={() => void loadMore()}
              disabled={loadingMore}
              title="Load next page"
            >
              {loadingMore ? "Loading…" : `Load more (${chats.length}/${total || "?"})`}
            </button>
          </div>
        )}

        {!hasMore && chats.length > 0 && (
          <div className="px-2 py-2 text-[11px] text-gray-400 text-center">
            End of list • showing {chats.length} of {total || chats.length}
          </div>
        )}
      </div>

      {/* FOOTER META */}
      <div className="border-t px-3 py-2 text-[11px] text-gray-500">
        <span className="uppercase tracking-wide">Chats</span>{" "}
        <span className="text-gray-400">
          ({chats.length}{total ? `/${total}` : ""} • page {Math.max(page, 1)} of {Math.max(totalPages || 1, 1)})
        </span>
        {isEditing && (
          <label className="ml-2 text-[11px]">
            <input
              type="checkbox"
              className="mr-1 align-middle"
              checked={allSelected}
              onChange={toggleAll}
            />
            Select all
          </label>
        )}
      </div>
    </aside>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/SidebarHeader.tsx =====

import { PanelLeftClose, Plus, Pencil, Trash2 } from "lucide-react";

export default function SidebarHeader({
  isEditing, setIsEditing, newPending, onNew, onHideSidebar,
  selectedCount, deleting, onDelete,
}: {
  isEditing: boolean;
  setIsEditing: (v: boolean) => void;
  newPending: boolean;
  onNew: () => Promise<void>;
  onHideSidebar?: () => void;
  selectedCount: number;
  deleting: boolean;
  onDelete: () => void;
}) {
  return (
    <div className="sticky top-0 z-10 bg-white border-b">
      <div className="flex items-center justify-between px-3 py-2">
        <div className="text-[11px] md:text-xs uppercase text-gray-500">Chats</div>
        <div className="flex items-center gap-2">
          <button
            className={`h-9 px-3 inline-flex items-center gap-2 justify-center rounded border ${
              newPending ? "opacity-50 cursor-not-allowed" : ""
            }`}
            onClick={async () => { if (!newPending) await onNew(); }}
            title="New chat"
            disabled={newPending}
          >
            <Plus className="w-4 h-4" />
            <span className="text-xs md:text-[11px] leading-none">New</span>
          </button>

          <button
            className="h-9 px-3 inline-flex items-center gap-2 justify-center rounded border"
            onClick={() => setIsEditing(!isEditing)}
            aria-pressed={isEditing}
            title={isEditing ? "Exit edit mode" : "Edit chats"}
          >
            <Pencil className="w-4 h-4" />
            <span className="text-xs md:text-[11px] leading-none">
              {isEditing ? "Done" : "Edit"}
            </span>
          </button>

          {onHideSidebar && (
            <button
              className="hidden md:inline-flex h-9 w-9 items-center justify-center rounded border"
              onClick={onHideSidebar}
              title="Hide sidebar"
              aria-label="Hide sidebar"
            >
              <PanelLeftClose className="w-4 h-4" />
            </button>
          )}
        </div>
      </div>

      {isEditing && (
        <div className="px-3 py-2 border-t bg-white flex items-center gap-3">
          <div className="text-sm text-gray-600">{selectedCount} selected</div>
          <button
            className={`ml-auto inline-flex items-center gap-2 text-sm px-3 py-1 rounded ${
              selectedCount && !deleting
                ? "bg-red-600 text-white"
                : "bg-gray-200 text-gray-500 cursor-not-allowed"
            }`}
            disabled={!selectedCount || deleting}
            onClick={onDelete}
            title={selectedCount ? "Delete selected chats" : "Select chats to delete"}
          >
            <Trash2 className="w-4 h-4" />
            {deleting ? "Deleting…" : `Delete (${selectedCount})`}
          </button>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/SidebarListItem.tsx =====

import { firstLineSmart } from "../../shared/lib/text";
import type { ChatRow } from "../../types/chat";

export default function SidebarListItem({
  c, isActive, isEditing, isChecked, onToggle, onOpen,
}: {
  c: ChatRow;
  isActive: boolean;
  isEditing: boolean;
  isChecked: boolean;
  onToggle: () => void;
  onOpen: () => void;
}) {
  const displayTitle =
    (c.title && c.title.trim()) ||
    firstLineSmart(c.lastMessage || "", 48) ||
    "New Chat";
  const preview = c.lastMessage ? firstLineSmart(c.lastMessage, 120) : "";

  return (
    <li>
      <div
        className={`w-full flex items-start gap-2 px-2 py-2 rounded ${
          isActive ? "bg-black text-white" : "hover:bg-gray-50"
        }`}
      >
        {isEditing && (
          <input
            type="checkbox"
            checked={isChecked}
            onChange={onToggle}
            className="mt-1"
            aria-label={`Select chat ${displayTitle}`}
          />
        )}
        <button
          className="text-left flex-1"
          aria-current={isActive ? "true" : undefined}
          onClick={() => { if (!isEditing) onOpen(); }}
          title={displayTitle}
        >
          <div className="font-medium truncate">{displayTitle}</div>
          {preview && (
            <div className={`text-xs line-clamp-2 ${isActive ? "text-white/80" : "text-gray-500"}`}>
              {preview}
            </div>
          )}
          <div className="text-[10px] mt-1 opacity-60">
            {new Date(c.updatedAt).toLocaleString()}
          </div>
        </button>
      </div>
    </li>
  );
}

# ===== frontend/src/file_read/components/ChatView/ChatView.tsx =====

// frontend/src/file_read/components/ChatView/ChatView.tsx
import { useEffect, useMemo, useRef } from "react";
import type { ChatMsg } from "../../types/chat";
import type { GenMetrics, RunJson } from "../../hooks/useChatStream";
import ChatItem from "../ChatItem";
import TypingIndicator from "../../shared/ui/TypingIndicator";

// Autofollow sensitivity as a fraction of the visible height.
const SCROLL_THRESHOLD_VH = 0.75;
const MIN_THRESHOLD_PX = 24;

// Custom event name dispatched by ChatComposer on Send
const FORCE_SCROLL_EVT = "chat:force-scroll-bottom";

export default function ChatView({
  messages,
  loading,
  queued = false,
  bottomPad,
  runMetrics,
  runJson,
  onDeleteMessages,
  autoFollow = true,
}: {
  messages: ChatMsg[];
  loading: boolean;
  queued?: boolean;
  bottomPad: number;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
}) {
  // Inner wrapper (not the scroller) + bottom sentinel for scrollIntoView
  const listRef = useRef<HTMLDivElement>(null);
  const bottomRef = useRef<HTMLDivElement>(null);

  // Track previous stream state & assistant text length for streaming nudge
  const prevLoadingRef = useRef<boolean>(loading);
  const prevAsstLenRef = useRef<number>(0);
  const didInitialAutoscrollRef = useRef(false);

  // The *real* scroll container (ChatContainer sets data-chat-scroll)
  function getScrollEl(): HTMLElement | null {
    const el = listRef.current;
    if (!el) return null;
    return el.closest<HTMLElement>("[data-chat-scroll]") ?? el;
  }

  // Are we close enough to the bottom of the scroll container?
  function isNearBottom(ratio = SCROLL_THRESHOLD_VH): boolean {
    const el = getScrollEl();
    if (!el) return true; // default to follow if unsure
    const threshold = Math.max(MIN_THRESHOLD_PX, el.clientHeight * ratio);
    const dist = el.scrollHeight - el.scrollTop - el.clientHeight;
    return dist <= threshold;
  }

  // Programmatic scroll helper
  const scrollToBottom = (behavior: ScrollBehavior = "smooth") => {
    bottomRef.current?.scrollIntoView({ behavior, block: "end" });
  };

  // 🔹 NEW: On first open (first time messages appear), auto-scroll to bottom
  useEffect(() => {
    if (didInitialAutoscrollRef.current) return;
    if (messages.length === 0) return;
    didInitialAutoscrollRef.current = true;

    // Do it twice (now and next frame) to account for layout/measure changes
    scrollToBottom("auto");
    requestAnimationFrame(() => scrollToBottom("auto"));
  }, [messages.length]);

  // 🔹 Respond to a global "force scroll" event from ChatComposer (Send)
  useEffect(() => {
    const handler = (evt: Event) => {
      const behavior =
        (evt as CustomEvent<{ behavior?: ScrollBehavior }>).detail?.behavior ??
        "smooth";
      bottomRef.current?.scrollIntoView({ behavior, block: "end" });
    };
    window.addEventListener(FORCE_SCROLL_EVT, handler as EventListener);
    return () => window.removeEventListener(FORCE_SCROLL_EVT, handler as EventListener);
  }, []);

  // --- Assistant streaming deltas: gentle nudge only if already near bottom ---
  const lastAssistantIndex = useMemo(() => {
    for (let i = messages.length - 1; i >= 0; i--) {
      if (messages[i].role === "assistant") return i;
    }
    return -1;
  }, [messages]);

  const asstText =
    lastAssistantIndex >= 0 ? (messages[lastAssistantIndex]?.text ?? "") : "";

useEffect(() => {
  if (lastAssistantIndex < 0) return;

  const len = asstText.length;
  const prev = prevAsstLenRef.current || 0;
  prevAsstLenRef.current = len;

  if (!autoFollow) return;

  // Nudge whenever content grows, but only if already near bottom
  if (len > prev && isNearBottom()) {
    scrollToBottom("auto");
  }
}, [lastAssistantIndex, asstText, autoFollow]);

  // --- Streaming finished: settle to bottom only if near bottom ---
  useEffect(() => {
    const prev = prevLoadingRef.current;
    const cur = loading;
    prevLoadingRef.current = cur;

    if (prev && !cur && autoFollow && isNearBottom()) {
      scrollToBottom("smooth");
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [loading, autoFollow]);

  // --- Optional: when composer height changes, keep bottom visible only if near bottom ---
  useEffect(() => {
    if (autoFollow && isNearBottom()) {
      bottomRef.current?.scrollIntoView({ behavior: "auto", block: "end" });
    }
  }, [bottomPad, autoFollow]);

  const lastMsg = messages[messages.length - 1];

  return (
    <div
      ref={listRef}
      className="p-4 space-y-3 bg-gray-50 min-w-0"
      style={{ paddingBottom: bottomPad }}
    >
      {messages.map((m, idx) => (
        <ChatItem
          key={m.id}
          m={m}
          idx={idx}
          loading={loading}
          lastAssistantIndex={lastAssistantIndex}
          runJsonLive={runJson ?? null}
          runMetricsLive={runMetrics ?? null}
          onDelete={onDeleteMessages ? (id) => onDeleteMessages([id]) : undefined}
        />
      ))}

      {(loading || queued) &&
        !(lastMsg?.role === "assistant" && (lastMsg.text?.trim().length ?? 0) > 0) && (
          <TypingIndicator />
        )}

      {/* Sentinel used for programmatic scrolls */}
      <div ref={bottomRef} className="h-0" />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatView/StatusLine.ts =====

// frontend/src/file_read/components/chat/StatusLine.ts
import type { RunJson, GenMetrics } from "../../shared/lib/runjson";

const oneDec = (n?: number | null) =>
  typeof n === "number" && Number.isFinite(n) ? n.toFixed(1) : undefined;

export function buildStatus(json?: RunJson | null, flat?: GenMetrics | null) {
  const st = json?.stats;
  if (st) {
    const parts: string[] = [];
    if (st.predictedTokensCount != null) parts.push(`${st.predictedTokensCount} tok`);
    if (st.tokensPerSecond != null) parts.push(`${oneDec(st.tokensPerSecond) ?? st.tokensPerSecond} tok/s`);
    if (st.timeToFirstTokenSec != null) parts.push(`TTFT ${Math.round(st.timeToFirstTokenSec * 1000)} ms`);
    if (st.stopReason) parts.push(`stop: ${st.stopReason}`);
    return parts.join(" • ");
  }
  if (flat) {
    const parts: string[] = [];
    if (flat.output_tokens != null) parts.push(`${flat.output_tokens} tok`);
    if (flat.tok_per_sec != null) parts.push(`${oneDec(flat.tok_per_sec) ?? flat.tok_per_sec} tok/s`);
    if (flat.ttft_ms != null) parts.push(`TTFT ${Math.round(flat.ttft_ms)} ms`);
    if (flat.stop_reason) parts.push(`stop: ${flat.stop_reason}`);
    return parts.join(" • ");
  }
  return "";
}

# ===== frontend/src/file_read/components/DesktopHeader.tsx =====

import { PanelLeftOpen } from "lucide-react";

export default function DesktopHeader({
  sidebarOpen,
  onShowSidebar,
  title = "Local AI Model",
}: {
  sidebarOpen: boolean;
  onShowSidebar: () => void;
  title?: string;
}) {
  return (
    <div className="hidden md:flex h-14 shrink-0 items-center justify-between px-4 border-b bg-white">
      <div className="flex items-center gap-2">
        {!sidebarOpen && (
          <button
            className="h-9 w-9 inline-flex items-center justify-center rounded-lg border hover:bg-gray-50"
            onClick={onShowSidebar}
            aria-label="Show sidebar"
            title="Show sidebar"
          >
            <PanelLeftOpen className="w-4 h-4" />
          </button>
        )}
        <div className="font-semibold">{title}</div>
      </div>
      <div />
    </div>
  );
}

# ===== frontend/src/file_read/components/KnowledgePanel.tsx =====

import { useState, useEffect } from "react";
import {
  uploadRag,
  searchRag,
  listUploads,
  deleteUploadHard,
  type UploadRow,
} from "../data/ragApi";

export default function KnowledgePanel({
  sessionId,
  onClose,
  toast,
}: {
  sessionId?: string;
  onClose?: () => void;
  toast?: (msg: string) => void;
}) {
  const [files, setFiles] = useState<FileList | null>(null);
  const [busy, setBusy] = useState(false);
  const [query, setQuery] = useState("");
  const [hits, setHits] = useState<{ text: string; source?: string; score: number }[]>([]);
  const [searching, setSearching] = useState(false);

  const [scope, setScope] = useState<"all" | "session">("all");
  const [uploads, setUploads] = useState<UploadRow[]>([]);
  const [loadingUploads, setLoadingUploads] = useState(false);

  useEffect(() => {
    void refreshUploads();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [scope, sessionId]);

  async function refreshUploads() {
    setLoadingUploads(true);
    try {
      const out = await listUploads(sessionId, scope);
      setUploads(out.uploads || []);
    } catch (e: any) {
      toast?.(e?.message || "Failed to load uploads");
    } finally {
      setLoadingUploads(false);
    }
  }

  async function handleDeleteHard(source: string, ns?: string | null) {
    try {
      const res = await deleteUploadHard(source, ns ?? undefined);
      toast?.(`Removed ${res.removed} chunk${res.removed === 1 ? "" : "s"}. Remaining: ${res.remaining}`);
      await refreshUploads();
    } catch (e: any) {
      toast?.(e?.message || "Delete failed");
    }
  }

  async function doUpload() {
    if (!files || !files.length) return;
    setBusy(true);
    try {
      let total = 0;
      for (const f of Array.from(files)) {
        const out = await uploadRag(f, undefined);
        total += (out as any)?.added || 0;
      }
      toast?.(`Added ${total} chunk${total === 1 ? "" : "s"}`);
      setFiles(null);
      await refreshUploads();
    } catch (e: any) {
      toast?.(e?.message || "Upload failed");
    } finally {
      setBusy(false);
    }
  }

  async function doSearch() {
    const q = query.trim();
    if (!q) return;
    setSearching(true);
    try {
      const out = await searchRag(q, { sessionId, kChat: 6, kGlobal: 4, alpha: 0.5 });
      setHits(out.hits || []);
    } catch (e: any) {
      toast?.(e?.message || "Search failed");
    } finally {
      setSearching(false);
    }
  }

  return (
    <div className="fixed inset-0 z-50 bg-black/40 flex items-center justify-center p-3">
      <div className="w-full max-w-5xl rounded-2xl bg-white shadow-xl border overflow-hidden">
        <div className="px-4 py-3 border-b flex items-center gap-2">
          <div className="font-semibold">Knowledge</div>
          <div className="ml-auto flex items-center gap-2">
            <button className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50" onClick={onClose}>
              Close
            </button>
          </div>
        </div>

        <div className="p-4 grid gap-6 md:grid-cols-2">
          {/* Upload */}
          <div>
            <div className="font-medium mb-2">Upload documents</div>
            <input type="file" multiple className="block w-full text-sm" onChange={(e) => setFiles(e.target.files)} />
            <button
              className={`mt-2 text-sm px-3 py-1.5 rounded ${busy ? "opacity-60 cursor-not-allowed" : "bg-black text-white"}`}
              disabled={busy || !files || files.length === 0}
              onClick={doUpload}
            >
              {busy ? "Uploading…" : "Upload"}
            </button>
            <div className="text-[11px] text-gray-500 mt-2">
              Tip: CSV, TXT, MD, PDF (text extracted). Uploads can be global or per chat.
            </div>

            <div className="mt-6">
              <div className="flex items-center gap-2 mb-2">
                <div className="font-medium">Your uploads</div>
                <select
                  className="ml-auto border rounded px-2 py-1 text-xs"
                  value={scope}
                  onChange={(e) => setScope(e.target.value as "all" | "session")}
                  title="Scope"
                >
                  <option value="all">All (global + this chat)</option>
                  <option value="session">This chat only</option>
                </select>
                <button
                  className="text-xs px-2 py-1 rounded border hover:bg-gray-50"
                  onClick={refreshUploads}
                  disabled={loadingUploads}
                >
                  {loadingUploads ? "Refreshing…" : "Refresh"}
                </button>
              </div>

              <ul className="space-y-2 max-h-64 overflow-auto">
                {uploads.length === 0 && (
                  <li className="text-xs text-gray-500">No uploads yet.</li>
                )}
                {uploads.map((u, i) => (
                  <li key={`${u.source}-${u.sessionId ?? "global"}-${i}`} className="p-2 border rounded bg-gray-50">
                    <div className="flex items-center gap-2">
                      <div className="font-mono text-xs break-all">{u.source}</div>
                      <span className="text-[11px] text-gray-500">
                        {u.sessionId ? "session" : "global"} • {u.chunks} chunk{u.chunks === 1 ? "" : "s"}
                      </span>
                      <button
                        className="ml-auto text-xs px-2 py-1 rounded border hover:bg-gray-100"
                        title="Delete (hard delete by Source)"
                        onClick={() => handleDeleteHard(u.source, u.sessionId ?? undefined)}
                      >
                        Delete
                      </button>
                    </div>
                  </li>
                ))}
              </ul>
            </div>
          </div>

          {/* Search */}
          <div>
            <div className="font-medium mb-2">Quick search</div>
            <div className="flex gap-2">
              <input
                value={query}
                onChange={(e) => setQuery(e.target.value)}
                placeholder="Find in your knowledge…"
                className="flex-1 border rounded px-2 py-1.5 text-sm"
              />
              <button
                className={`text-sm px-3 py-1.5 rounded ${searching ? "opacity-60 cursor-wait" : "border hover:bg-gray-50"}`}
                onClick={doSearch}
                disabled={searching}
              >
                {searching ? "Searching…" : "Search"}
              </button>
            </div>

            <ul className="mt-3 space-y-2 max-h-64 overflow-auto">
              {hits.map((h, i) => (
                <li key={i} className="p-2 border rounded bg-gray-50">
                  <div className="text-[11px] text-gray-500 mb-1">
                    {h.source || "uploaded"} • score {Number.isFinite(h.score) ? h.score.toFixed(3) : "—"}
                  </div>
                  <div className="text-sm whitespace-pre-wrap">{h.text}</div>
                </li>
              ))}
              {!hits.length && <li className="text-xs text-gray-500">No results yet.</li>}
            </ul>
          </div>
        </div>

        <div className="px-4 py-3 border-t text-[11px] text-gray-500">
          “Delete” performs a hard delete: removes chunks for that Source and rebuilds the index.
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Markdown/MarkdownMessage.tsx =====

// frontend/src/file_read/components/MarkdownMessage.tsx
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import rehypeHighlight from "rehype-highlight";
import "highlight.js/styles/github.css";
import CodeCopyButton from "../../shared/ui/CodeCopyButton";

type Props = { text: string };

export default function MarkdownMessage({ text }: Props) {
  return (
    <>
      {/* keep pre spacing at zero; don't overwrite token colors */}
      <style>{`
        pre { margin: 0 !important; padding: 0 !important; background: transparent !important; }
        pre code { display: block; margin: 0 !important; padding: 0 !important; }
        .hljs { background: transparent !important; }
      `}</style>

      <ReactMarkdown
        remarkPlugins={[remarkGfm]}
        rehypePlugins={[[rehypeHighlight, { detect: true, ignoreMissing: true }]]}
        components={{
          code({
            inline,
            className,
            children,
            ...props
          }: {
            inline?: boolean;
            className?: string;
            children?: React.ReactNode;
          }) {
            const raw = String(children ?? "");
            const lang = (className || "").replace("language-", "");

            if (inline) {
              return (
                <code
                  className="px-1.5 py-0.5 rounded bg-gray-100 text-gray-900 font-mono text-[14px]"
                  {...props}
                >
                  {children}
                </code>
              );
            }

            return (
              <div className="relative w-full">
                <pre className="m-0 p-0 w-full overflow-x-auto rounded-md border border-gray-300">
                  {/* Let hljs theme color tokens; no text color override here */}
                  <code className={`${className ?? ""} hljs font-mono text-sm`} {...props}>
                    {children}
                  </code>
                </pre>

                <div className="absolute top-2 right-2 flex items-center gap-1">
                  {lang && (
                    <span className="text-[11px] px-1.5 py-0.5 rounded bg-gray-200 text-gray-700">
                      {lang}
                    </span>
                  )}
                  <CodeCopyButton text={raw} />
                </div>
              </div>
            );
          },
        }}
      >
        {text}
      </ReactMarkdown>
    </>
  );
}

# ===== frontend/src/file_read/components/MetricsHoverCard.tsx =====

import { useMemo, useRef, useState } from "react";
import { Info, Copy, Check, X } from "lucide-react";

type Props = {
  data: unknown;                 // anything JSON-serializable
  title?: string;                // optional header
  align?: "left" | "right";      // popover alignment
  maxWidthPx?: number;           // width of the panel
  compact?: boolean;             // shrink button & paddings
};

export default function MetricsHoverCard({
  data,
  title = "Run details",
  align = "right",
  maxWidthPx = 460,
  compact = true,
}: Props) {
  const [open, setOpen] = useState(false);
  const [copied, setCopied] = useState(false);
  const btnRef = useRef<HTMLButtonElement>(null);

  const json = useMemo(() => {
    try {
      return JSON.stringify(data, null, 2);
    } catch {
      return String(data ?? "");
    }
  }, [data]);

  async function copy() {
    try {
      await navigator.clipboard.writeText(json);
      setCopied(true);
      setTimeout(() => setCopied(false), 1500);
    } catch {}
  }

  // accessibility: close on ESC
  function onKeyDown(e: React.KeyboardEvent) {
    if (e.key === "Escape") {
      setOpen(false);
      btnRef.current?.focus();
    }
  }

  return (
    <div className="relative inline-block">
      <button
        ref={btnRef}
        type="button"
        className={`inline-flex items-center justify-center rounded border bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition ${
          compact ? "h-7 w-7" : "h-8 w-8"
        }`}
        title="Show run JSON"
        aria-haspopup="dialog"
        aria-expanded={open ? "true" : "false"}
        onClick={() => setOpen((v) => !v)}
        onMouseEnter={() => setOpen(true)}
        onMouseLeave={() => setOpen(false)}
      >
        <Info className={compact ? "w-4 h-4" : "w-5 h-5"} />
      </button>

      {/* Panel */}
      <div
        role="dialog"
        aria-label={title}
        onKeyDown={onKeyDown}
        className={`absolute z-50 mt-2 ${align === "right" ? "right-0" : "left-0"}`}
        style={{ width: Math.min(maxWidthPx, window.innerWidth - 40) }}
        onMouseEnter={() => setOpen(true)}
        onMouseLeave={() => setOpen(false)}
      >
        <div
          className={`rounded-xl border bg-white shadow-xl overflow-hidden transition
          ${open ? "opacity-100 translate-y-0" : "pointer-events-none opacity-0 -translate-y-1"}`}
        >
          <div className="px-3 py-2 border-b flex items-center justify-between bg-gray-50">
            <div className="text-xs font-semibold text-gray-700">{title}</div>
            <div className="flex items-center gap-1">
              <button
                className="inline-flex items-center justify-center h-7 w-7 rounded border bg-white text-gray-700 hover:bg-gray-50"
                onClick={copy}
                title="Copy JSON"
              >
                {copied ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
              </button>
              <button
                className="inline-flex items-center justify-center h-7 w-7 rounded border bg-white text-gray-700 hover:bg-gray-50"
                onClick={() => setOpen(false)}
                title="Close"
              >
                <X className="w-4 h-4" />
              </button>
            </div>
          </div>

          <div className="p-3">
            <pre
              className="m-0 p-0 text-xs leading-relaxed overflow-auto"
              style={{ maxHeight: 360 }}
            >
              <code>{json}</code>
            </pre>
          </div>
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/MobileDrawer.tsx =====

import { PanelLeftOpen } from "lucide-react";
import ChatSidebar from "./ChatSidebar/ChatSidebar";

export default function MobileDrawer({
  onOpenSession,
  onNewChat,
  refreshKey,
  activeId,
  openMobileDrawer,
  closeMobileDrawer,
}: {
  onOpenSession: (id: string) => Promise<void>;
  onNewChat: () => Promise<void>;
  refreshKey: number;
  activeId?: string;
  openMobileDrawer: () => void;
  closeMobileDrawer: () => void;
}) {
  return (
    <>
      {/* Mobile top bar */}
      <div className="md:hidden fixed top-0 left-0 right-0 z-40 bg-white border-b">
        <div className="h-14 flex items-center justify-between px-3">
          <button
            className="inline-flex items-center justify-center h-9 w-9 rounded-lg border hover:bg-gray-50"
            onClick={openMobileDrawer}
            aria-label="Open sidebar"
            title="Open sidebar"
          >
            <PanelLeftOpen className="w-4 h-4" />
          </button>
          <div className="font-semibold">Local AI Model</div>
          <div className="w-9" />
        </div>
      </div>

      {/* Backdrop */}
      <div
        id="mobile-backdrop"
        className="md:hidden fixed inset-0 z-40 bg-black/40 hidden"
        onClick={closeMobileDrawer}
      />

      {/* Drawer */}
      <aside
        id="mobile-drawer"
        role="dialog"
        aria-modal="true"
        className="md:hidden fixed inset-y-0 left-0 z-50 w-80 max-w-[85vw] bg-white border-r shadow-xl hidden animate-[slideIn_.2s_ease-out]"
      >
        <div className="h-14 flex items-center justify-between px-3 border-b">
          <div className="font-medium">Chats</div>
          <button
            className="h-9 w-9 inline-flex items-center justify-center rounded-lg border hover:bg-gray-50"
            onClick={closeMobileDrawer}
            aria-label="Close sidebar"
          >
            <span className="rotate-45 text-xl leading-none">+</span>
          </button>
        </div>

        <ChatSidebar
          onOpen={async (id) => { await onOpenSession(id); closeMobileDrawer(); }}
          onNew={async () => { await onNewChat(); closeMobileDrawer(); }}
          refreshKey={refreshKey}
          activeId={activeId}
        />
      </aside>

      <style>{`@keyframes slideIn{from{transform:translateX(-12px);opacity:.0}to{transform:translateX(0);opacity:1}}`}</style>
    </>
  );
}

# ===== frontend/src/file_read/components/SettingsPanel.tsx =====

import { useEffect, useMemo, useState } from "react";
import { useSettings } from "../hooks/useSettings";

export default function SettingsPanel({ sessionId, onClose }: { sessionId?: string; onClose?: () => void }) {
  const { loading, error, effective, overrides, defaults, adaptive, saveOverrides, runAdaptive, reload } =
    useSettings(sessionId);

  const [tab, setTab] = useState<"effective"|"overrides"|"adaptive"|"defaults">("effective");
  const [draft, setDraft] = useState(() => JSON.stringify(overrides ?? {}, null, 2));
  const [saveBusy, setSaveBusy] = useState(false);
  const [saveErr, setSaveErr] = useState<string | null>(null);

  useEffect(() => { setDraft(JSON.stringify(overrides ?? {}, null, 2)); }, [overrides]);

  const view = useMemo(() => {
    switch (tab) {
      case "effective": return effective;
      case "adaptive":  return adaptive;
      case "defaults":  return defaults;
      case "overrides": return null;
    }
  }, [tab, effective, adaptive, defaults]);

  async function onSave(method: "patch" | "put") {
    setSaveErr(null); setSaveBusy(true);
    try {
      const parsed = draft.trim() ? JSON.parse(draft) : {};
      await saveOverrides(parsed, method);
    } catch (e: any) {
      setSaveErr(e?.message || "Invalid JSON or save failed");
    } finally {
      setSaveBusy(false);
    }
  }

  return (
    <div className="fixed inset-0 z-50 bg-black/40 flex items-center justify-center p-3">
      <div className="w-full max-w-4xl rounded-2xl bg-white shadow-xl border">
        {/* header */}
        <div className="px-4 py-3 border-b flex items-center gap-2">
          <div className="font-semibold">Settings</div>
          <div className="ml-auto flex items-center gap-2">
            <button
              className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50"
              onClick={() => runAdaptive()}
              title="Recompute adaptive with current context"
            >
              Recompute Adaptive
            </button>
            <button
              className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50"
              onClick={() => reload()}
              title="Reload"
            >
              Reload
            </button>
            <button
              className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50"
              onClick={onClose}
              title="Close"
            >
              Close
            </button>
          </div>
        </div>

        {/* tabs */}
        <div className="px-4 py-2 border-b">
          {(["effective","overrides","adaptive","defaults"] as const).map(key => (
            <button
              key={key}
              onClick={() => setTab(key)}
              className={`text-xs mr-2 px-3 py-1.5 rounded ${tab===key ? "bg-black text-white" : "border hover:bg-gray-50"}`}
            >
              {key}
            </button>
          ))}
        </div>

        {/* body */}
        <div className="p-4">
          {loading && <div className="text-sm text-gray-500">Loading…</div>}
          {error && <div className="text-sm text-red-600">{error}</div>}

          {tab !== "overrides" && (
            <pre className="text-xs bg-gray-50 border rounded p-3 overflow-auto max-h-[60vh]">
              {JSON.stringify(view ?? {}, null, 2)}
            </pre>
          )}

          {tab === "overrides" && (
            <div className="space-y-2">
              <div className="text-xs text-gray-600">
                Edit <code>user_overrides</code> JSON. Use <b>Patch</b> to merge or <b>Replace</b> to overwrite.
              </div>
              <textarea
                value={draft}
                onChange={(e) => setDraft(e.target.value)}
                className="w-full h-[50vh] border rounded p-2 font-mono text-xs"
                spellCheck={false}
              />
              <div className="flex items-center gap-2">
                <button
                  className={`text-xs px-3 py-1.5 rounded ${saveBusy ? "opacity-60 cursor-not-allowed" : "bg-black text-white"}`}
                  disabled={saveBusy}
                  onClick={() => onSave("patch")}
                  title="Deep-merge with existing overrides"
                >
                  Save (Patch)
                </button>
                <button
                  className={`text-xs px-3 py-1.5 rounded border ${saveBusy ? "opacity-60 cursor-not-allowed" : "hover:bg-gray-50"}`}
                  disabled={saveBusy}
                  onClick={() => onSave("put")}
                  title="Replace overrides entirely"
                >
                  Save (Replace)
                </button>
                {saveErr && <div className="text-xs text-red-600 ml-2">{saveErr}</div>}
              </div>
            </div>
          )}
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/data/ragApi.ts =====

// frontend/src/file_read/data/ragApi.ts
import { request, API_BASE } from "../services/http";

export function uploadRag(file: File, sessionId?: string, forceGlobal = false) {
  const form = new FormData();
  form.append("file", file);
  if (sessionId && !forceGlobal) form.append("sessionId", sessionId);

  return request<{ ok: boolean; added: number }>(
    "/api/rag/upload",
    { method: "POST", body: form }
  );
}

export function uploadRagWithProgress(
  file: File,
  sessionId: string,
  onProgress: (pct: number) => void,
  signal?: AbortSignal
): Promise<{ ok: boolean; added: number }> {
  return new Promise((resolve, reject) => {
    const form = new FormData();
    form.append("file", file);
    form.append("sessionId", sessionId);

    const xhr = new XMLHttpRequest();
    const url = `${API_BASE}/api/rag/upload`.replace(/([^:]\/)\/+/g, "$1");
    xhr.open("POST", url);

    xhr.upload.onprogress = (e) => {
      if (e.lengthComputable) onProgress(Math.round((e.loaded / e.total) * 100));
    };

    xhr.onload = () => {
      if (xhr.status >= 200 && xhr.status < 300) {
        try { resolve(JSON.parse(xhr.responseText)); }
        catch { resolve({ ok: true, added: 0 }); }
      } else {
        reject(new Error(`Upload failed (${xhr.status})`));
      }
    };

    // treat abort as a silent resolution, not an error
    xhr.onabort = () => resolve({ ok: false, added: 0 });
    xhr.onerror = () => reject(new Error("Network error"));

    if (signal) {
      if (signal.aborted) { xhr.abort(); return resolve({ ok: false, added: 0 }); }
      signal.addEventListener("abort", () => xhr.abort(), { once: true });
    }

    xhr.send(form);
  });
}


export function searchRag(query: string, opts?: {
  sessionId?: string;
  kChat?: number;
  kGlobal?: number;
  alpha?: number; // hybrid_alpha
}) {
  const body = {
    query,
    sessionId: opts?.sessionId ?? undefined,
    kChat: opts?.kChat ?? 6,
    kGlobal: opts?.kGlobal ?? 4,
    hybrid_alpha: opts?.alpha ?? 0.5,
  };

  return request<{
    hits: Array<{
      id?: string;
      score: number;
      source?: string;
      title?: string;
      text: string;
      sessionId?: string | null;
    }>;
  }>(
    "/api/rag/search",   // ✅ relative path, request() adds API_BASE
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(body),
    }
  );
}

export type UploadRow = {
  source: string;
  sessionId?: string | null;
  chunks: number;
};

export async function listUploads(sessionId?: string, scope: "all" | "session" = "all") {
  const p = new URLSearchParams();
  if (sessionId) p.set("sessionId", sessionId);
  if (scope) p.set("scope", scope);

  return request<{ uploads: UploadRow[] }>(
    `/api/rag/uploads?${p.toString()}`,
    { method: "GET" }
  );
}

export async function deleteUploadHard(source: string, sessionId?: string) {
  return request<{ ok: boolean; removed: number; remaining: number }>(
    `/api/rag/uploads/delete-hard`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ source, sessionId }),
    }
  );
}

# ===== frontend/src/file_read/data/settingsApi.ts =====

// frontend/src/file_read/data/settingsApi.ts
import { request } from "../services/http";

export function getDefaults() {
  return request<Record<string, any>>("/api/settings/defaults");
}

export function getAdaptive(sessionId?: string) {
  const qs = sessionId ? `?sessionId=${encodeURIComponent(sessionId)}` : "";
  return request<Record<string, any>>(`/api/settings/adaptive${qs}`);
}

export function getOverrides() {
  return request<Record<string, any>>("/api/settings/overrides");
}

export function getEffective(sessionId?: string) {
  const qs = sessionId ? `?sessionId=${encodeURIComponent(sessionId)}` : "";
  return request<Record<string, any>>(`/api/settings/effective${qs}`);
}

export function putOverrides(overrides: Record<string, any>) {
  return request<{ ok: boolean; overrides: any }>("/api/settings/overrides", {
    method: "PUT",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(overrides), // ← send raw object
  });
}

export function patchOverrides(patch: Record<string, any>) {
  return request<{ ok: boolean; overrides: any }>("/api/settings/overrides", {
    method: "PATCH",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(patch), // ← send raw object
  });
}

export function recomputeAdaptive(sessionId?: string) {
  const qs = sessionId ? `?sessionId=${encodeURIComponent(sessionId)}` : "";
  return request<{ ok: boolean; adaptive: any }>(
    `/api/settings/adaptive/recompute${qs}`,
    { method: "POST" }
  );
}

# ===== frontend/src/file_read/hooks/data/aiApi.ts =====

import { API_BASE } from "../../services/http";

export function streamGenerate(
  payload: unknown,
  abortSignal: AbortSignal,
): ReadableStream<Uint8Array> | null {
  // Note: fetch streaming body works in modern browsers + Vite dev server
  let stream: ReadableStream<Uint8Array> | null = null;

  // We purposefully do NOT await; the caller will read res.body
  fetch(`${API_BASE}/api/ai/generate/stream`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(payload),
    signal: abortSignal,
  }).then(res => {
    if (res.ok) stream = res.body ?? null;
  }).catch(() => { /* surface in hook if needed */ });

  // The caller will poll for stream being set in the next tick
  // (or just call fetch directly if they prefer).
  return stream;
}

export async function cancelSession(sessionId: string): Promise<void> {
  try {
    await fetch(`${API_BASE}/api/ai/cancel/${encodeURIComponent(sessionId)}`, { method: "POST" });
  } catch { /* best-effort */ }
}

# ===== frontend/src/file_read/hooks/data/chatApi.ts =====

import type { Attachment, ChatRow, ChatMessageRow} from "../../types/chat";
import { request } from "../../services/http";

// Spring Page<T> type
export type PageResp<T> = {
  content: T[];
  totalElements: number;
  totalPages: number;
  size: number;
  number: number;      // current page index (0-based)
  first: boolean;
  last: boolean;
  empty: boolean;
};

export function createChat(sessionId: string, title: string) {
  return request<ChatRow>("/api/chats", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ sessionId, title }),
  });
}

export function updateChatLast(sessionId: string, lastMessage: string, title?: string) {
  return request<ChatRow>(`/api/chats/${encodeURIComponent(sessionId)}/last`, {
    method: "PUT",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ lastMessage, title: title || "" }),
  });
}

// Legacy (unused after pagination in sidebar)
export function listChats() {
  return request<ChatRow[]>("/api/chats");
}

// NEW: paginated list
export function listChatsPage(page = 0, size = 30, ceiling?: string) {
  const qs = new URLSearchParams({ page: String(page), size: String(size) });
  if (ceiling) qs.set("ceiling", ceiling);
  return request<PageResp<ChatRow>>(`/api/chats/paged?${qs.toString()}`);
}

export function listMessages(sessionId: string) {
  return request<ChatMessageRow[]>(`/api/chats/${encodeURIComponent(sessionId)}/messages`);
}

export async function appendMessage(
  sessionId: string,
  role: "user" | "assistant",
  content: string,
  attachments?: Attachment[]
) {
  const body: any = { role, content };
  if (attachments && attachments.length) body.attachments = attachments;

  return request<ChatMessageRow>(`/api/chats/${sessionId}/messages`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
  });
}

export async function deleteChatsBatch(sessionIds: string[]) {
  const data = await request<{ deleted: string[] }>("/api/chats/batch", {
    method: "DELETE",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ sessionIds }),
  });
  return data.deleted;
}

export function deleteMessage(sessionId: string, messageId: string | number) {
  return request<{ deleted: number }>(
    `/api/chats/${encodeURIComponent(sessionId)}/messages/${encodeURIComponent(String(messageId))}`,
    { method: "DELETE" }
  );
}

/** Delete a batch of messages. Backend returns { deleted: number[] } */
export function deleteMessagesBatch(sessionId: string, messageIds: (number | string)[]) {
  const ids = messageIds
    .map((id) => (typeof id === "string" ? Number(id) : id))
    .filter((n) => Number.isFinite(n)) as number[];

  return request<{ deleted: number[] }>(  // <-- number[]
    `/api/chats/${encodeURIComponent(sessionId)}/messages/batch`,
    {
      method: "DELETE",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ messageIds: ids }),
    }
  );
}

# ===== frontend/src/file_read/hooks/stream/core/buffer.ts =====

// frontend/src/file_read/hooks/stream/core/buffer.ts
import type { GenMetrics, RunJson } from "../../../shared/lib/runjson";
import { extractRunJsonFromBuffer } from "../../../shared/lib/runjson";
import { STOP_SENTINEL_AT_END } from "./constants";

export type BufferStep = {
  cleanText: string;
  delta: string;
  metrics?: { json?: RunJson; flat?: GenMetrics };
};

export function processChunk(prevClean: string, rawBufIn: string): BufferStep {
  let rawBuf = rawBufIn;
  if (STOP_SENTINEL_AT_END.test(rawBuf)) {
    rawBuf = rawBuf.replace(STOP_SENTINEL_AT_END, "");
  }
  const { clean, json, flat } = extractRunJsonFromBuffer(rawBuf);
  const delta = clean.slice(prevClean.length);
  return { cleanText: clean, delta, metrics: json || flat ? { json, flat } : undefined };
}

# ===== frontend/src/file_read/hooks/stream/core/cancel.ts =====

import { postCancel } from "./network";

type Deps = {
  getVisibleSid: () => string;
  setLoadingFor: (sid: string, v: boolean) => void;
  setQueuedFor: (sid: string, v: boolean) => void;
  getController: () => AbortController | null;
  getReader: () => ReadableStreamDefaultReader<Uint8Array> | null;
  setCancelForSid: (sid: string | null) => void;
  isActiveSid: (sid: string) => boolean;
  dropJobsForSid: (sid: string) => void;
};

export function createCanceller(d: Deps) {
  async function cancelBySessionId(id: string) {
    d.setCancelForSid(id);

    if (d.isActiveSid(id)) {
      try { d.getController()?.abort(); } catch {}
      try { d.getReader()?.cancel(); } catch {}
      d.setLoadingFor(id, false);
    } else {
      d.dropJobsForSid(id);
      d.setQueuedFor(id, false);
      d.setCancelForSid(null);
    }

    // fire-and-forget server stop
    postCancel(id).catch(() => {});
  }

  async function stopVisible() {
    const id = d.getVisibleSid();
    await cancelBySessionId(id);
  }

  return { cancelBySessionId, stopVisible };
}

# ===== frontend/src/file_read/hooks/stream/core/constants.ts =====

// frontend/src/file_read/hooks/stream/core/constants.ts
export const STOP_SENTINEL_AT_END = /(?:\r?\n)?\u23F9 stopped(?:\r?\n)?$/u;
export const STOP_FLUSH_TIMEOUT_MS = 3500;

# ===== frontend/src/file_read/hooks/stream/core/controller.ts =====

import { appendMessage } from "../../data/chatApi";
import type { StreamController, StreamCoreOpts } from "./types";
import { createScheduler, type QueueItem } from "./queue";
import { createCanceller } from "./cancel";
import { runStreamOnce } from "./runner";
import type { Attachment } from "../../../types/chat";

export function createStreamController(opts: StreamCoreOpts): StreamController {
  let cancelForSid: string | null = null;
  let controllerRef: AbortController | null = null;
  let readerRef: ReadableStreamDefaultReader<Uint8Array> | null = null;

  const scheduler = createScheduler(async (job: QueueItem) => {
    try { opts.setQueuedFor(job.sid, false); } catch {}
    await runStreamOnce(job, {
      opts,
      getCancelForSid: () => cancelForSid,
      clearCancelIf: (sid) => { if (cancelForSid === sid) cancelForSid = null; },
      setController: (c) => { controllerRef = c; },
      setReader: (r) => { readerRef = r; },
    });
  });

  const canceller = createCanceller({
    getVisibleSid: opts.getSessionId,
    setLoadingFor: opts.setLoadingFor,
    setQueuedFor: opts.setQueuedFor,
    getController: () => controllerRef,
    getReader: () => readerRef,
    setCancelForSid: (sid) => { cancelForSid = sid; },
    isActiveSid: scheduler.isActiveSid,
    dropJobsForSid: scheduler.dropJobsForSid,
  });

  async function send(override?: string, attachments?: Attachment[]) {
    const prompt = (override ?? "").trim();
    const atts = (attachments ?? []).filter(Boolean);
    if (!prompt && atts.length === 0) return; // allow attachments-only, but not truly empty

    await opts.ensureChatCreated();
    const sid = opts.getSessionId();

    const userCid = crypto.randomUUID();
    const asstCid = crypto.randomUUID();

    // optimistic bubbles
    opts.setMessagesFor(sid, (prev) => [
      ...prev,
      { id: userCid, serverId: null, role: "user", text: prompt, attachments: atts.length ? atts : undefined },
      { id: asstCid, serverId: null, role: "assistant", text: "" },
    ]);
    opts.setInput("");

    // persist user
    appendMessage(sid, "user", prompt, atts.length ? atts : undefined)
      .then((row) => {
        if (row?.id != null) {
          opts.setServerIdFor(sid, userCid, Number(row.id));
          try { window.dispatchEvent(new CustomEvent("chats:refresh")); } catch {}
        }
      })
      .catch(() => {});

    // enqueue generation with attachments
    try { opts.setQueuedFor(sid, true); } catch {}
    scheduler.enqueue({ sid, prompt, asstId: asstCid, attachments: atts.length ? atts : undefined });
  }

  async function stop() { await canceller.stopVisible(); }
  async function cancelBySessionId(id: string) { await canceller.cancelBySessionId(id); }
  function dispose() {
    try { controllerRef?.abort(); } catch {}
    try { readerRef?.cancel(); } catch {}
    controllerRef = null; readerRef = null;
  }

  return { send, stop, cancelBySessionId, dispose };
}

# ===== frontend/src/file_read/hooks/stream/core/network.ts =====

// frontend/src/file_read/hooks/stream/core/network.ts
import { API_BASE } from "../../../services/http";

export async function postStream(body: unknown, signal: AbortSignal) {
  const res = await fetch(`${API_BASE}/api/ai/generate/stream`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
    signal,
  });
  if (!res.ok || !res.body) throw new Error(`HTTP ${res.status}`);
  return res.body.getReader();
}

export async function postCancel(sessionId: string) {
  try {
    await fetch(`${API_BASE}/api/ai/cancel/${encodeURIComponent(sessionId)}`, {
      method: "POST",
    });
  } catch {
    /* best-effort */
  }
}

# ===== frontend/src/file_read/hooks/stream/core/queue.ts =====

// frontend/src/file_read/hooks/stream/core/queue.ts
import type { Attachment } from "../../../types/chat";

export type QueueItem = {
  sid: string;
  prompt: string;              // may be empty when attachments-only
  asstId: string;              // client id of the assistant bubble to stream into
  attachments?: Attachment[];  // optional attachments for this turn
};

export type RunJob = (job: QueueItem) => Promise<void>;

export function createScheduler(runJob: RunJob) {
  const q: QueueItem[] = [];
  let active: { sid: string } | null = null;

  