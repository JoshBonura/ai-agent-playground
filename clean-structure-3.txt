tr) -> ChatMeta:
    idx = load_index()
    existing = next((r for r in idx if r["sessionId"] == session_id), None)
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(**existing)

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": (title.strip() or SETTINGS["chat_default_title"]),
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
    }
    idx.append(row)
    save_index(idx)
    _save_chat(session_id, {"sessionId": session_id, "messages": [], "seq": 0, "summary": ""})
    return ChatMeta(**row)


def update_last(session_id: str, last_message: Optional[str], maybe_title: Optional[str]) -> ChatMeta:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(**row)


def append_message(session_id: str, role: str, content: str, attachments: Optional[list[Any]] = None) -> ChatMessageRow:
    data = _load_chat(session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq,
        "sessionId": session_id,
        "role": role,
        "content": content,
        "createdAt": now_iso(),
    }
    norm_atts = _normalize_attachments(attachments)
    if norm_atts:
        msg["attachments"] = norm_atts   # ✅ now JSON-serializable

    data["messages"].append(msg)
    data["seq"] = seq
    _save_chat(session_id, data)

    # update index meta as before...
    ...
    return ChatMessageRow(
        id=seq,
        sessionId=session_id,
        role=role,
        content=content,
        createdAt=msg["createdAt"],
        attachments=norm_atts,
    )



def delete_message(session_id: str, message_id: int) -> int:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)
    return 1


def delete_messages_batch(session_id: str, message_ids: List[int]) -> List[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(session_id, data)
    refresh_index_after_change(session_id, keep)
    return deleted


def list_messages(session_id: str) -> List[ChatMessageRow]:
    data = _load_chat(session_id)
    rows: List[ChatMessageRow] = []
    for m in data.get("messages", []):
        rows.append(ChatMessageRow(
            id=m["id"],
            sessionId=m["sessionId"],
            role=m["role"],
            content=m["content"],
            createdAt=m.get("createdAt"),
            attachments=m.get("attachments", []),  # ✅ safe default
        ))
    return rows


def list_paged(page: int, size: int, ceiling_iso: Optional[str]) -> Tuple[List[ChatMeta], int, int, bool]:
    rows = load_index()
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    min_size = int(SETTINGS["chat_page_min_size"])
    max_size = int(SETTINGS["chat_page_max_size"])
    size = max(min_size, min(max_size, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size

    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(ChatMeta(**r))
    return metas, total, total_pages, last_flag


def delete_batch(session_ids: List[str]) -> List[str]:
    # delete chat jsons
    for sid in session_ids:
        try:
            chat_path(sid).unlink(missing_ok=True)
        except Exception:
            pass

    # delete RAG namespaces for those sessions
    for sid in session_ids:
        try:
            rag_delete_namespace(sid)
        except Exception:
            pass

    # prune index.json
    idx = load_index()
    keep = [r for r in idx if r["sessionId"] not in set(session_ids)]
    save_index(keep)
    return session_ids


def merge_chat(source_id: str, target_id: str):
    source_msgs = list_messages(source_id)
    target_msgs = list_messages(target_id)

    merged = []
    for m in source_msgs:
        row = append_message(target_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    for m in target_msgs:
        row = append_message(target_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    return merged


def _save_chat(session_id: str, data: Dict[str, Any]):
    atomic_write(chat_path(session_id), data)


def set_summary(session_id: str, new_summary: str) -> None:
    data = _load_chat(session_id)
    data["summary"] = new_summary or ""
    _save_chat(session_id, data)


def get_summary(session_id: str) -> str:
    data = _load_chat(session_id)
    return str(data.get("summary") or "")


def merge_chat_new(source_id: str, target_id: Optional[str] = None):
    from uuid import uuid4
    new_id = str(uuid4())
    upsert_on_first_message(new_id, SETTINGS["chat_merged_title"])

    merged = []
    for m in list_messages(source_id):
        row = append_message(new_id, m.role, m.content, attachments=m.attachments)
        merged.append(row)

    if target_id:
        for m in list_messages(target_id):
            row = append_message(new_id, m.role, m.content, attachments=m.attachments)
            merged.append(row)

    return new_id, merged


def edit_message(session_id: str, message_id: int, new_content: str) -> Optional[ChatMessageRow]:
    data = _load_chat(session_id)
    msgs = data.get("messages", [])
    updated = None

    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            # normalize attachments
            if "attachments" in m and m["attachments"] is not None:
                norm = []
                for a in m["attachments"]:
                    if hasattr(a, "dict"):
                        norm.append(a.dict())
                    elif isinstance(a, dict):
                        norm.append(a)
                    else:
                        norm.append(dict(a))
                m["attachments"] = norm
            updated = m
            break

    if not updated:
        return None

    _save_chat(session_id, data)
    refresh_index_after_change(session_id, msgs)

    return ChatMessageRow(
        id=updated["id"],
        sessionId=updated["sessionId"],
        role=updated["role"],
        content=updated["content"],
        createdAt=updated.get("createdAt"),
        attachments=updated.get("attachments", []),
    )



__all__ = [
    "ChatMessageRow",
    "upsert_on_first_message", "update_last", "append_message",
    "delete_message", "delete_messages_batch", "list_messages",
    "list_paged", "delete_batch", "merge_chat", "merge_chat_new",
    "_load_chat", "_save_chat", "edit_message", "set_summary", "get_summary",
]

# ===== aimodel/file_read/store/default_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": false,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 6,
  "web_fetch_max_chars": 2000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 1200,
  "web_orch_per_doc_char_budget": 700,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 1.5,
  "web_orch_overfetch_min_extra": 1,
  "web_orch_enable_js_retry": false,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.6,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": false,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 120,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": true,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "warmup_enabled": true,
  "warmup_on_start": true,
  "warmup_on_interval_sec": 600,
  "warmup_jitter_sec": 8,
  "warmup_llm_enabled": true,
  "warmup_web_enabled": false,
  "web_enabled": false,
  "warmup_web_k": 2,
  "warmup_timeout_sec": 5,
  "warmup_queries": [
    "current US president",
    "S&P 500 today",
    "weather tomorrow"
  ],
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe"
}

# ===== aimodel/file_read/store/effective_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": false,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 6,
  "web_fetch_max_chars": 2000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 1200,
  "web_orch_per_doc_char_budget": 700,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 1.5,
  "web_orch_overfetch_min_extra": 1,
  "web_orch_enable_js_retry": false,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.6,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": false,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 120,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": true,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "warmup_enabled": true,
  "warmup_on_start": true,
  "warmup_on_interval_sec": 600,
  "warmup_jitter_sec": 8,
  "warmup_llm_enabled": true,
  "warmup_web_enabled": false,
  "web_enabled": false,
  "warmup_web_k": 2,
  "warmup_timeout_sec": 5,
  "warmup_queries": [
    "current US president",
    "S&P 500 today",
    "weather tomorrow"
  ],
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe"
}

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from .base import INDEX_PATH, atomic_write, ensure_dirs, now_iso

def load_index() -> List[Dict]:
    ensure_dirs()
    try:
        with INDEX_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

def save_index(rows: List[Dict]):
    atomic_write(INDEX_PATH, rows)

@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: Optional[str]
    createdAt: str
    updatedAt: str

def refresh_index_after_change(session_id: str, messages: List[Dict]) -> None:
    idx = load_index()
    row = next((r for r in idx if r["sessionId"] == session_id), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(idx)

# ===== aimodel/file_read/store/override_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": false,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "ddg",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "web_search_region": "us-en",
  "web_search_safesearch": "moderate",
  "web_search_debug_logging": true,
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_merged_title": "Merged Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "__comment_web_fetch": "=== Web Fetch (HTML fetch & clean) ===",
  "web_fetch_timeout_sec": 6,
  "web_fetch_max_chars": 2000,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 3,
  "web_orch_total_char_budget": 1200,
  "web_orch_per_doc_char_budget": 700,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 1.5,
  "web_orch_overfetch_min_extra": 1,
  "web_orch_enable_js_retry": false,
  "web_orch_js_retry_avg_q": 0.55,
  "web_orch_js_retry_low_q": 0.45,
  "web_orch_js_retry_lowish_ratio": 0.5,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": -1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.6,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": false,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 120,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": true,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "warmup_enabled": true,
  "warmup_on_start": true,
  "warmup_on_interval_sec": 600,
  "warmup_jitter_sec": 8,
  "warmup_llm_enabled": true,
  "warmup_web_enabled": false,
  "web_enabled": false,
  "warmup_web_k": 2,
  "warmup_timeout_sec": 5,
  "warmup_queries": [
    "current US president",
    "S&P 500 today",
    "weather tomorrow"
  ],
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe"
}

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..runtime.model_runtime import current_model_info, get_llm

RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

STOP_STRINGS = ["</s>", "User:", "\nUser:"]

def strip_runjson(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    out, i = [], 0
    while True:
        start = s.find(RUNJSON_START, i)
        if start == -1:
            out.append(s[i:])
            break
        out.append(s[i:start])
        end = s.find(RUNJSON_END, start)
        if end == -1:
            break
        i = end + len(RUNJSON_END)
    return "".join(out).strip()

def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))
        except Exception:
            return max(1, len(text) // 4)

def safe_token_count_messages(llm: Any, msgs: List[Dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)

def model_ident_and_cfg() -> Tuple[str, Dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg

def derive_stop_reason(stop_set: bool, finish_reason: Optional[str], err_text: Optional[str]) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"

def _first(obj: dict, keys: List[str]):
    for k in keys:
        if k in obj and obj[k] is not None:
            return obj[k]
    return None

def _sec_from_ms(v: Any) -> Optional[float]:
    try:
        x = float(v)
        return round(x / 1000.0, 6)
    except Exception:
        return None

def collect_engine_timings(llm: Any) -> Optional[Dict[str, Optional[float]]]:
    src = None
    try:
        if hasattr(llm, "get_last_timings") and callable(llm.get_last_timings):
            src = llm.get_last_timings()
        elif hasattr(llm, "get_timings") and callable(llm.get_timings):
            src = llm.get_timings()
        elif hasattr(llm, "timings"):
            t = llm.timings
            src = t() if callable(t) else t
        elif hasattr(llm, "perf"):
            p = llm.perf
            src = p() if callable(p) else p
        elif hasattr(llm, "stats"):
            s = llm.stats
            src = s() if callable(s) else s
        elif hasattr(llm, "get_stats") and callable(llm.get_stats):
            src = llm.get_stats()
    except Exception:
        src = None

    if not isinstance(src, dict):
        return None

    load_ms = _first(src, ["load_ms", "loadMs", "model_load_ms", "load_time_ms"])
    prompt_ms = _first(src, ["prompt_ms", "promptMs", "prompt_eval_ms", "prompt_time_ms", "prefill_ms"])
    eval_ms = _first(src, ["eval_ms", "evalMs", "decode_ms", "eval_time_ms"])
    prompt_n = _first(src, ["prompt_n", "promptN", "prompt_tokens", "n_prompt_tokens"])
    eval_n = _first(src, ["eval_n", "evalN", "eval_tokens", "n_eval_tokens"])

    out: Dict[str, Optional[float]] = {
        "loadSec": _sec_from_ms(load_ms),
        "promptSec": _sec_from_ms(prompt_ms),
        "evalSec": _sec_from_ms(eval_ms),
        "promptN": None,
        "evalN": None,
    }
    try:
        out["promptN"] = int(prompt_n) if prompt_n is not None else None
    except Exception:
        out["promptN"] = None
    try:
        out["evalN"] = int(eval_n) if eval_n is not None else None
    except Exception:
        out["evalN"] = None
    return out

def build_run_json(
    *,
    request_cfg: Dict[str, object],
    out_text: str,
    t_start: float,
    t_first: Optional[float],
    t_last: Optional[float],
    stop_set: bool,
    finish_reason: Optional[str],
    input_tokens_est: Optional[int],
    budget_view: Optional[dict] = None,
    extra_timings: Optional[dict] = None,
    error_text: Optional[str] = None,
) -> Dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None
    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()
    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    timings_payload = dict(extra_timings or {})
    if "engine" not in timings_payload or timings_payload.get("engine") is None:
        timings_payload["engine"] = collect_engine_timings(llm)

    encode_t = float(input_tokens_est or 0.0)
    decode_t = float(out_tokens or 0.0)
    total_t = encode_t + decode_t
    model_queue_s = float(timings_payload.get("modelQueueSec") or 0.0)
    engine_prompt_s = float((timings_payload.get("engine") or {}).get("promptSec") or 0.0)
    encode_sec = model_queue_s or engine_prompt_s or 0.0
    decode_sec = float(gen_secs or 0.0)
    total_sec = float(t_end - t_start)
    encode_tps = (encode_t / encode_sec) if encode_sec > 0 else None
    decode_tps = (decode_t / decode_sec) if decode_sec > 0 else None
    overall_tps = (total_t / total_sec) if total_sec > 0 else None

    bv = budget_view or {}
    bv_break = (bv.get("breakdown") or {}) if isinstance(bv, dict) else {}
    web = (bv.get("web") or {}) if isinstance(bv, dict) else {}
    web_bd = (web.get("breakdown") or {}) if isinstance(web, dict) else {}
    rag = (bv.get("rag") or {}) if isinstance(bv, dict) else {}

    nctx = bv.get("modelCtx") or bv.get("n_ctx") or 0
    clamp = bv.get("clampMargin") or bv.get("clamp_margin") or 0
    inp_est = bv.get("inputTokensEst") or bv.get("input_tokens_est") or (input_tokens_est or 0)
    out_chosen = bv.get("outBudgetChosen") or bv.get("clamped_out_tokens") or 0
    out_actual = out_tokens
    out_shown = out_actual or out_chosen
    used_ctx = (inp_est or 0) + (out_shown or 0) + (clamp or 0)
    ctx_pct = (float(used_ctx) / float(nctx) * 100.0) if nctx else 0.0

    rag_delta = 0
    for k in ("ragTokensAdded", "blockTokens", "blockTokensApprox", "sessionOnlyTokensApprox"):
        v = rag.get(k)
        if isinstance(v, (int, float)) and v > 0:
            rag_delta = int(v)
            break
    rag_pct_of_input = int(round((rag_delta / inp_est) * 100)) if inp_est else 0

    web_pre = (
        web_bd.get("totalWebPreTtftSec")
        or ((web.get("elapsedSec") or 0) + (web.get("fetchElapsedSec") or 0) + (web.get("injectElapsedSec") or 0))
        or 0
    )

    pre_accounted = bv_break.get("preTtftAccountedSec")
    unattributed = (
        bv_break.get("unattributedTtftSec")
        if "unattributedTtftSec" in bv_break
        else (max(0.0, (ttft_ms / 1000.0) - float(pre_accounted))) if pre_accounted is not None else None
    )

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {"key": "llm.load.llama.acceleration.offloadRatio", "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0},
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
            "budget": budget_view or {},
            "timings": timings_payload,
            "error": error_text or None,
        },
        "budget_view": (budget_view or {}),
        "_derived": {
            "context": {
                "modelCtx": nctx,
                "clampMargin": clamp,
                "inputTokensEst": inp_est,
                "outBudgetChosen": out_chosen,
                "outActual": out_actual,
                "outShown": out_shown,
                "usedCtx": used_ctx,
                "ctxPct": ctx_pct,
            },
            "rag": {
                "ragDelta": rag_delta,
                "ragPctOfInput": rag_pct_of_input,
            },
            "web": {
                "webPre": web_pre,
            },
            "timing": {
                "accountedPreTtftSec": pre_accounted,
                "unattributedPreTtftSec": unattributed,
                "preModelSec": timings_payload.get("preModelSec"),
                "modelQueueSec": timings_payload.get("modelQueueSec"),
                "genSec": gen_secs,
                "ttftSec": round((ttft_ms or 0) / 1000.0, 3),
            },
            "throughput": {
                "encodeTokPerSec": encode_tps,
                "decodeTokPerSec": decode_tps,
                "overallTokPerSec": overall_tps,
            },
        },
    }

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/duckduckgo.py =====

# aimodel/file_read/web/duckduckgo.py
from __future__ import annotations
from typing import List, Optional, Tuple, Dict, Any
import asyncio, time
from urllib.parse import urlparse

try:
    from ddgs import DDGS  # type: ignore
except Exception:
    try:
        from duckduckgo_search import DDGS  # type: ignore
    except Exception:
        DDGS = None  # type: ignore

from ..core.settings import SETTINGS
from .provider import SearchHit

_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}

def _cache_key(query: str) -> str:
    return (query or "").strip().lower()

def _host(u: str) -> str:
    try:
        h = (urlparse(u).hostname or "").lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return ""

def _cache_get(query: str) -> Optional[List[SearchHit]]:
    eff = SETTINGS.effective()
    ttl = int(eff["web_search_cache_ttl_sec"])
    key = _cache_key(query)
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > ttl:
        _CACHE.pop(key, None)
        return None
    return hits

def _cache_set(query: str, hits: List[SearchHit]) -> None:
    _CACHE[_cache_key(query)] = (time.time(), hits)

def _ddg_sync_search(query: str, k: int, *, region: str, safesearch: str) -> List[SearchHit]:
    results: List[SearchHit] = []
    if DDGS is None:
        return results
    with DDGS() as ddg:
        for i, r in enumerate(ddg.text(query, max_results=max(1, k), safesearch=safesearch, region=region)):
            title = (r.get("title") or "").strip()
            url = (r.get("href") or "").strip()
            snippet: Optional[str] = (r.get("body") or "").strip() or None
            if not url:
                continue
            results.append(SearchHit(title=title or url, url=url, snippet=snippet, rank=i))
            if i + 1 >= k:
                break
    return results

class DuckDuckGoProvider:
    async def search(self, query: str, k: int = 3, telemetry: Optional[Dict[str, Any]] = None) -> List[SearchHit]:
        t_start = time.perf_counter()
        eff = SETTINGS.effective()
        q_norm = (query or "").strip()
        if not q_norm:
            if telemetry is not None:
                telemetry.update({"query": q_norm, "k": int(k), "supersetK": int(k), "elapsedSec": round(time.perf_counter() - t_start, 6), "cache": {"hit": False}})
            return []
        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        region = str(eff["web_search_region"])
        safesearch = str(eff["web_search_safesearch"])

        tel: Dict[str, Any] = {"query": q_norm, "k": int(k), "supersetK": superset_k, "region": region, "safesearch": safesearch}
        t_cache = time.perf_counter()
        cached = _cache_get(q_norm)
        tel["cache"] = {"hit": cached is not None, "elapsedSec": round(time.perf_counter() - t_cache, 6)}
        if cached is not None:
            out = cached[:k]
            tel["hits"] = {
                "total": len(cached),
                "returned": len(out),
                "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
            }
            tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
            if telemetry is not None:
                telemetry.update(tel)
            return out

        hits: List[SearchHit] = []
        prov_info: Dict[str, Any] = {"available": DDGS is not None}
        t_fetch = time.perf_counter()
        if DDGS is not None:
            try:
                hits = await asyncio.to_thread(_ddg_sync_search, q_norm, superset_k, region=region, safesearch=safesearch)
                prov_info["errorType"] = None
                prov_info["errorMsg"] = None
            except Exception as e:
                prov_info["errorType"] = type(e).__name__
                prov_info["errorMsg"] = str(e)
                hits = []
        else:
            prov_info["errorType"] = "ProviderUnavailable"
            prov_info["errorMsg"] = "DDGS is not installed or failed to import."
        prov_info["elapsedSec"] = round(time.perf_counter() - t_fetch, 6)
        tel["provider"] = prov_info

        _cache_set(q_norm, hits)
        out = hits[:k]
        tel["hits"] = {
            "total": len(hits),
            "returned": len(out),
            "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
        }
        tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
        if telemetry is not None:
            telemetry.update(tel)
        return out

# ===== aimodel/file_read/web/fetch.py =====

# aimodel/file_read/web/fetch.py
from __future__ import annotations
import asyncio
from typing import Tuple, List, Optional, Dict, Any
import time
import httpx

try:
    from readability import Document
except Exception:
    Document = None  # type: ignore

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None  # type: ignore

try:
    from selectolax.parser import HTMLParser
except Exception:
    HTMLParser = None  # type: ignore

from ..core.settings import SETTINGS


def _req(key: str):
    return SETTINGS[key]

def _ua() -> str:
    return str(_req("web_fetch_user_agent"))

def _timeout() -> float:
    return float(_req("web_fetch_timeout_sec"))

def _max_chars() -> int:
    return int(_req("web_fetch_max_chars"))

def _max_bytes() -> int:
    return int(_req("web_fetch_max_bytes"))

def _max_parallel() -> int:
    return max(1, int(_req("web_fetch_max_parallel")))


async def _read_capped_bytes(resp: httpx.Response, cap_bytes: int) -> bytes:
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    html = raw_html or ""
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
        except Exception:
            pass
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = tree.body.text(separator=" ", strip=True) if tree.body else tree.text(separator=" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    return html


async def fetch_clean(
    url: str,
    timeout_s: Optional[float] = None,
    max_chars: Optional[int] = None,
    max_bytes: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
) -> Tuple[str, int, str]:
    t0 = time.perf_counter()
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout, headers=headers) as client:
        r = await client.get(url)
        r.raise_for_status()
        raw_bytes = await _read_capped_bytes(r, cap_bytes)
        enc = r.encoding or "utf-8"
        raw_text = raw_bytes.decode(enc, errors="ignore")
        txt = _extract_text_from_html(raw_text, str(r.url))
        txt = (txt or "").strip().replace("\r", "")
        if len(txt) > cap_chars:
            txt = txt[:cap_chars]
        if telemetry is not None:
            telemetry.update({
                "reqUrl": url,
                "finalUrl": str(r.url),
                "status": int(r.status_code),
                "elapsedSec": round(time.perf_counter() - t0, 6),
                "bytes": len(raw_bytes),
                "chars": len(txt),
                "timeoutSec": timeout,
                "capBytes": cap_bytes,
                "capChars": cap_chars,
            })
        return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: List[str],
    per_timeout_s: Optional[float] = None,
    cap_chars: Optional[int] = None,
    cap_bytes: Optional[int] = None,
    max_parallel: Optional[int] = None,
    telemetry: Optional[Dict[str, Any]] = None,
):
    t_total0 = time.perf_counter()
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))
    tel_items: List[Dict[str, Any]] = []

    async def _one(u: str):
        item_tel: Dict[str, Any] = {"reqUrl": u}
        t0 = time.perf_counter()
        async with sem:
            try:
                res = await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                    telemetry=item_tel,
                )
                item_tel.setdefault("elapsedSec", round(time.perf_counter() - t0, 6))
                item_tel["ok"] = True
                tel_items.append(item_tel)
                return u, res
            except Exception as e:
                item_tel.update({
                    "ok": False,
                    "errorType": type(e).__name__,
                    "errorMsg": str(e),
                    "elapsedSec": round(time.perf_counter() - t0, 6),
                    "timeoutSec": (float(per_timeout_s) if per_timeout_s is not None else _timeout()),
                    "capBytes": (int(cap_bytes) if cap_bytes is not None else _max_bytes()),
                    "capChars": (int(cap_chars) if cap_chars is not None else _max_chars()),
                })
                tel_items.append(item_tel)
                return u, None

    tasks = [_one(u) for u in urls]
    results = await asyncio.gather(*tasks)

    if telemetry is not None:
        ok_cnt = sum(1 for it in tel_items if it.get("ok"))
        telemetry.update({
            "totalSec": round(time.perf_counter() - t_total0, 6),
            "requested": len(urls),
            "ok": ok_cnt,
            "miss": len(urls) - ok_cnt,
            "items": tel_items,
            "settings": {
                "userAgent": _ua(),
                "defaultTimeoutSec": _timeout(),
                "defaultCapChars": _max_chars(),
                "defaultCapBytes": _max_bytes(),
                "maxParallel": _max_parallel() if max_parallel is None else int(max_parallel),
            },
        })

    return results

# ===== aimodel/file_read/web/orchestrator.py =====

# aimodel/file_read/web/orchestrator.py
from __future__ import annotations
from typing import List, Tuple, Optional, Dict, Any
import time

from ..core.settings import SETTINGS
from .duckduckgo import DuckDuckGoProvider
from .provider import SearchHit
from .orchestrator_common import (
    _as_int, _as_float, _as_bool, _as_str,
    condense_doc, content_quality_score,
    _dedupe_by_host, score_hit, _head_tail,
    _fetch_round,
)

async def build_web_block(query: str, k: Optional[int] = None, per_url_timeout_s: Optional[float] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    tel: Dict[str, Any] = {"query": (query or "").strip()}
    cfg_k               = (int(k) if k is not None else _as_int("web_orch_default_k"))
    total_char_budget   = _as_int("web_orch_total_char_budget")
    per_doc_budget      = _as_int("web_orch_per_doc_char_budget")
    max_parallel        = _as_int("web_orch_max_parallel_fetch")
    overfetch_factor    = _as_float("web_orch_overfetch_factor")
    overfetch_min_extra = _as_int("web_orch_overfetch_min_extra")
    enable_js_retry     = _as_bool("web_orch_enable_js_retry")
    js_avg_q_thresh     = _as_float("web_orch_js_retry_avg_q")
    js_low_q_thresh     = _as_float("web_orch_js_retry_low_q")
    js_lowish_ratio     = _as_float("web_orch_js_retry_lowish_ratio")
    js_timeout_add      = _as_float("web_orch_js_retry_timeout_add")
    js_timeout_cap      = _as_float("web_orch_js_retry_timeout_cap")
    js_parallel_delta   = _as_int("web_orch_js_retry_parallel_delta")
    js_min_parallel     = _as_int("web_orch_js_retry_min_parallel")
    header_tpl          = _as_str("web_block_header")
    sep_str             = _as_str("web_orch_block_separator")
    min_block_reserve   = _as_int("web_orch_min_block_reserve")
    min_chunk_after     = _as_int("web_orch_min_chunk_after_shrink")
    per_timeout = (float(per_url_timeout_s) if per_url_timeout_s is not None else _as_float("web_fetch_timeout_sec"))
    start_time = time.perf_counter()

    provider = DuckDuckGoProvider()
    overfetch = max(cfg_k + overfetch_min_extra, int(round(cfg_k * overfetch_factor)))
    tel["search"] = {"requestedK": cfg_k, "overfetch": overfetch}
    t0 = time.perf_counter()
    try:
        hits: List[SearchHit] = await provider.search(query, k=overfetch, telemetry=tel["search"])
    except Exception as e:
        tel["error"] = {"stage": "search", "type": type(e).__name__, "msg": str(e)}
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return None, tel
    tel["search"]["elapsedSecTotal"] = round(time.perf_counter() - t0, 6)
    if not hits:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return None, tel

    seen_urls = set()
    scored: List[Tuple[int, SearchHit]] = []
    for idx, h in enumerate(hits):
        u = (h.url or "").strip()
        if not u:
            continue
        if u in seen_urls:
            continue
        seen_urls.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
    tel["scoring"] = {"inputHits": len(hits), "scored": len(scored)}
    if not scored:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return None, tel

    top_hits = _dedupe_by_host(scored, cfg_k)
    tel["scoring"]["picked"] = len(top_hits)
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]

    t_f = time.perf_counter()
    tel["fetch1"] = {}
    results = await _fetch_round(
        urls, meta, per_url_timeout_s=per_timeout, max_parallel=max_parallel, use_js=False, telemetry=tel["fetch1"]
    )
    dt_f = time.perf_counter() - t_f
    tel["fetch1"]["roundSec"] = round(dt_f, 6)

    texts: List[Tuple[str, str, str]] = []
    quality_scores: List[float] = []
    for original_url, res in results:
        if not res:
            continue
        final_url, status, text = res
        title = next((t for (t, u) in meta if u == original_url), final_url)
        tl = len(text or "")
        qscore = content_quality_score(text or "")
        quality_scores.append(qscore)
        if text:
            texts.append((title, final_url, text))
    tel["fetch1"]["docs"] = {"ok": len(texts), "qAvg": (sum(quality_scores)/len(quality_scores) if quality_scores else 0.0)}

    try_js = False
    if enable_js_retry and quality_scores:
        avg_q = sum(quality_scores) / len(quality_scores)
        lowish = sum(1 for q in quality_scores if q < js_low_q_thresh)
        if avg_q < js_avg_q_thresh or (lowish / max(1, len(quality_scores))) >= js_lowish_ratio:
            try_js = True
        tel["jsRetry"] = {
            "considered": True,
            "triggered": try_js,
            "avgQ": round(avg_q, 4),
            "lowishRatio": round((lowish / max(1, len(quality_scores))), 4),
            "thresholds": {"avg": js_avg_q_thresh, "low": js_low_q_thresh, "ratio": js_lowish_ratio},
        }
    else:
        tel["jsRetry"] = {"considered": bool(enable_js_retry), "triggered": False}

    if try_js:
        js_timeout   = min(per_timeout + js_timeout_add, js_timeout_cap)
        js_parallel  = max(js_min_parallel, max_parallel + js_parallel_delta)
        tel["fetch2"] = {"timeoutSec": js_timeout, "maxParallel": js_parallel}
        results_js = await _fetch_round(
            urls, meta, per_url_timeout_s=js_timeout, max_parallel=js_parallel, use_js=True, telemetry=tel["fetch2"]
        )
        texts_js: List[Tuple[str, str, str]] = []
        for original_url, res in results_js:
            if not res:
                continue
            final_url, status, text = res
            title = next((t for (t, u) in meta if u == original_url), final_url)
            tl = len(text or "")
            qscore = content_quality_score(text or "")
            if text:
                texts_js.append((title, final_url, text))
        if texts_js:
            texts = texts_js

    if not texts:
        tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
        return None, tel

    texts.sort(key=lambda t: content_quality_score(t[2]), reverse=True)
    chunks: List[str] = []
    for title, final_url, text in texts:
        chunk = condense_doc(title, final_url, text, max_chars=per_doc_budget)
        chunks.append(chunk)

    header = header_tpl.format(query=query)
    sep = _as_str("web_orch_block_separator")
    available = max(_as_int("web_orch_min_block_reserve"), total_char_budget - len(header) - len(sep))
    block_parts: List[str] = []
    used = 0
    for idx, ch in enumerate(chunks):
        cl = len(ch)
        sep_len = (len(sep) if block_parts else 0)
        if used + cl + sep_len > available:
            shrunk = _head_tail(ch, max(min_chunk_after, available - used - sep_len))
            if len(shrunk) > min_chunk_after:
                block_parts.append(shrunk)
                used += len(shrunk) + sep_len
            break
        block_parts.append(ch)
        used += cl + sep_len
    body = sep.join(block_parts)
    block = f"{header}{sep}{body}" if body else header

    tel["assembly"] = {
        "chunksPicked": len(block_parts),
        "chars": len(block),
        "available": available,
        "headerChars": len(header),
    }
    tel["elapsedSec"] = round(time.perf_counter() - start_time, 6)
    return block, tel

# ===== aimodel/file_read/web/orchestrator_common.py =====

# aimodel/file_read/web/orchestrator_common.py
from __future__ import annotations
from typing import List, Tuple, Optional, Dict, Any
from urllib.parse import urlparse
import re

from ..core.settings import SETTINGS
from .provider import SearchHit
from .fetch import fetch_many

def _req(key: str):
    return SETTINGS[key]

def _as_int(key: str) -> int: return int(_req(key))
def _as_float(key: str) -> float: return float(_req(key))
def _as_bool(key: str) -> bool: return bool(_req(key))
def _as_str(key: str) -> str:
    v = _req(key)
    return "" if v is None else str(v)

def _clean_ws(s: str) -> str:
    return " ".join((s or "").split())

def _host(url: str) -> str:
    h = (urlparse(url).hostname or "").lower()
    pref = _as_str("web_orch_www_prefix")
    return h[len(pref):] if pref and h.startswith(pref) else h

def _tokens(s: str) -> List[str]:
    return [t for t in re.findall(r"\w+", (s or "").lower()) if t]

def _head_tail(text: str, max_chars: int) -> str:
    text = text or ""
    if max_chars <= 0 or len(text) <= max_chars:
        return _clean_ws(text)
    head_frac      = _as_float("web_orch_head_fraction")
    tail_min_chars = _as_int("web_orch_tail_min_chars")
    ellipsis       = _as_str("web_orch_ellipsis")
    head = int(max_chars * head_frac)
    tail = max_chars - head
    if tail < tail_min_chars:
        head = max(1, max_chars - tail_min_chars)
        tail = tail_min_chars
    return _clean_ws(text[:head] + ellipsis + text[-tail:])

def condense_doc(title: str, url: str, text: str, *, max_chars: int) -> str:
    body = _head_tail(text or "", max_chars)
    safe_title = _clean_ws(title or url)
    bullet = _as_str("web_orch_bullet_prefix") or "- "
    indent = _as_str("web_orch_indent_prefix") or "  "
    return f"{bullet}{safe_title}\n{indent}{url}\n{indent}{body}"

def score_hit(hit: SearchHit, query: str) -> int:
    w_exact      = _as_int("web_orch_score_w_exact")
    w_substr     = _as_int("web_orch_score_w_substr")
    w_title_full = _as_int("web_orch_score_w_title_full")
    w_title_part = _as_int("web_orch_score_w_title_part")
    w_snip_touch = _as_int("web_orch_score_w_snip_touch")
    score = 0
    q = (query or "").strip().lower()
    title = (hit.title or "").strip()
    snippet = (hit.snippet or "").strip()
    title_l = title.lower()
    snip_l  = snippet.lower()
    if q:
        if title_l == q:
            score += w_exact
        elif q in title_l:
            score += w_substr
    qtoks = _tokens(q)
    if qtoks:
        cov_title = sum(1 for t in qtoks if t in title_l)
        if cov_title == len(qtoks) and len(qtoks) > 0:
            score += w_title_full
        elif cov_title > 0:
            score += w_title_part
        cov_snip = sum(1 for t in qtoks if t in snip_l)
        if cov_snip > 0:
            score += w_snip_touch
    return score

def _type_ratio(text: str, sub: str) -> float:
    if not text:
        return 1.0
    cnt = text.lower().count(sub)
    return float(cnt) / max(1, len(text))

def content_quality_score(text: str) -> float:
    if not text:
        return 0.0
    t = text.strip()
    n = len(t)
    len_div     = _as_float("web_orch_q_len_norm_divisor")
    w_len       = _as_float("web_orch_q_len_weight")
    w_div       = _as_float("web_orch_q_diversity_weight")
    length_score = min(1.0, n / len_div) if len_div > 0 else 0.0
    toks = _tokens(t)
    if not toks:
        return 0.1 * length_score
    uniq = len(set(toks))
    diversity = uniq / max(1.0, float(len(toks)))
    pen = 0.0
    for rule in _req("web_orch_q_penalties"):
        token = str(rule.get("token") or "")
        mult  = float(rule.get("mult") or 0.0)
        cap   = float(rule.get("cap") or 1.0)
        pen += min(cap, _type_ratio(t, token) * mult)
    raw = (w_len * length_score) + (w_div * diversity) - pen
    return max(0.0, min(1.0, raw))

def _dedupe_by_host(scored_hits: List[Tuple[int, SearchHit]], k: int) -> List[SearchHit]:
    picked: List[SearchHit] = []
    seen_hosts = set()
    for s, h in sorted(scored_hits, key=lambda x: x[0], reverse=True):
        u = (h.url or "").strip()
        if not u:
            continue
        host = _host(u)
        if host in seen_hosts:
            continue
        seen_hosts.add(host)
        picked.append(h)
        if len(picked) >= k:
            break
    return picked

async def _fetch_round(
    urls: List[str],
    meta: List[Tuple[str, str]],
    per_url_timeout_s: float,
    max_parallel: int,
    use_js: bool = False,
    telemetry: Optional[Dict[str, Any]] = None,
) -> List[Tuple[str, Optional[Tuple[str, int, str]]]]:
    fetch_fn = fetch_many
    if use_js:
        try:
            from . import fetch as _fetch_mod  # type: ignore
            fetch_fn = getattr(_fetch_mod, "fetch_many_js", fetch_many)
        except Exception:
            fetch_fn = fetch_many
    cap_mult        = _as_float("web_orch_fetch_cap_multiplier")
    per_doc_budget  = _as_int("web_orch_per_doc_char_budget")
    fetch_max_chars = _as_int("web_fetch_max_chars")
    per_doc_cap     = min(int(per_doc_budget * cap_mult), fetch_max_chars)
    results = await fetch_fn(
        urls,
        per_timeout_s=per_url_timeout_s,
        cap_chars=per_doc_cap,
        max_parallel=max_parallel,
        telemetry=telemetry,
    )
    return results

# ===== aimodel/file_read/web/provider.py =====

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SearchHit:
    title: str
    url: str
    snippet: Optional[str] = None
    rank: int = 0

class SearchProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        raise NotImplementedError

# ===== aimodel/file_read/web/query_summarizer.py =====

# aimodel/file_read/web/query_summarizer.py
from __future__ import annotations
from typing import Any, Iterable, Dict, Tuple
import re, time

from ..core.settings import SETTINGS
from ..utils.streaming import safe_token_count_messages

def _tokens(s: str) -> set[str]:
    return set(re.findall(r"\w+", (s or "").lower()))

def _as_list(v) -> list:
    if v is None:
        return []
    if isinstance(v, (list, tuple)):
        return list(v)
    return [v]

def summarize_query(llm: Any, user_text: str) -> Tuple[str, Dict[str, Any]]:
    telemetry: Dict[str, Any] = {}
    txt = (user_text or "").strip()

    bypass_enabled = SETTINGS.get("query_sum_bypass_short_enabled")
    short_chars = SETTINGS.get("query_sum_short_max_chars")
    short_words = SETTINGS.get("query_sum_short_max_words")
    if bypass_enabled is True and isinstance(short_chars, int) and isinstance(short_words, int):
        if len(txt) <= short_chars and len(txt.split()) <= short_words:
            telemetry.update({"bypass": True})
            return txt, telemetry
    telemetry.update({"bypass": False})

    prompt = SETTINGS.get("query_sum_prompt")
    if isinstance(prompt, str) and "{text}" in prompt:
        params = {}
        max_tokens = SETTINGS.get("query_sum_max_tokens")
        if isinstance(max_tokens, int):
            params["max_tokens"] = max_tokens
        temperature = SETTINGS.get("query_sum_temperature")
        if isinstance(temperature, (int, float)):
            params["temperature"] = float(temperature)
        top_p = SETTINGS.get("query_sum_top_p")
        if isinstance(top_p, (int, float)):
            params["top_p"] = float(top_p)
        stops = _as_list(SETTINGS.get("query_sum_stop"))
        if stops:
            params["stop"] = [str(s) for s in stops if isinstance(s, str)]
        params["stream"] = False

        t_start = time.perf_counter()
        out = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt.format(text=txt)}],
            **params,
        )
        elapsed = time.perf_counter() - t_start
        result = (out["choices"][0]["message"]["content"] or "").strip()
        in_tokens = safe_token_count_messages(llm, [{"role": "user", "content": prompt.format(text=txt)}]) or 0
        out_tokens = safe_token_count_messages(llm, [{"role": "assistant", "content": result}]) or 0
        telemetry.update({
            "elapsedSec": round(elapsed, 4),
            "inputTokens": in_tokens,
            "outputTokens": out_tokens,
        })
    else:
        return txt, telemetry

    overlap_enabled = SETTINGS.get("query_sum_overlap_check_enabled")
    j_min = SETTINGS.get("query_sum_overlap_jaccard_min")
    if overlap_enabled is True and isinstance(j_min, (int, float)):
        src_toks = _tokens(txt)
        out_toks = _tokens(result)
        if not result or not out_toks:
            telemetry.update({"overlapRetained": True, "overlapScore": 0.0})
            return txt, telemetry
        jaccard = (len(src_toks & out_toks) / len(src_toks | out_toks)) if (src_toks or out_toks) else 1.0
        telemetry.update({"overlapScore": round(jaccard, 4)})
        if jaccard < float(j_min):
            telemetry.update({"overlapRetained": True})
            return txt, telemetry
        telemetry.update({"overlapRetained": False})
        return result, telemetry

    return result, telemetry

# ===== aimodel/file_read/web/router_ai.py =====

# aimodel/file_read/web/router_ai.py
from __future__ import annotations
from typing import Tuple, Optional, Any, Dict
import json, re, time
from ..core.settings import SETTINGS
from ..utils.streaming import safe_token_count_messages

_DECIDE_PROMPT = (
    "You are a router deciding whether answering the text requires the public web.\n"
    "Respond with JSON only in exactly this schema:\n"
    "{{\"need\": true|false, \"query\": \"<text or empty>\"}}\n\n"
    "Decision principle:\n"
    "- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n"
    "- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n"
    "- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n"
    "- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n"
    "- When uncertain whether real-time state is required, prefer need=true.\n\n"
    "Text:\n{text}\n"
    "JSON:"
)

def _force_json(s: str) -> dict:
    """
    Try very hard to extract a JSON object like:
      {"need": true|false, "query": "..."}
    …even if the model wrapped it in prose or ```json fences.
    """
    if not s:
        return {}

    raw = s.strip()

    # Strip common code-fence wrappers
    # ```json\n{...}\n```  OR  ```\n{...}\n```
    try:
        cf = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", raw, re.IGNORECASE)
        if cf:
            raw = cf.group(1).strip()
    except Exception:
        pass

    # 1) Try direct parse
    try:
        v = json.loads(raw)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass

    # 2) Try to find a minimal JSON object that *mentions* "need"
    #    This avoids the nested-brace problem while being specific enough.
    try:
        m = None
        for m in re.finditer(r"\{[^{}]*\"need\"\s*:\s*(?:true|false|\"true\"|\"false\")[^{}]*\}", raw, re.IGNORECASE):
            pass
        if m:
            frag = m.group(0)
            v = json.loads(frag)
            return v if isinstance(v, dict) else {}
    except Exception:
        pass

    # 3) Fallback: grab the last {...} blob and attempt to parse
    try:
        last = None
        for last in re.finditer(r"\{[\s\S]*\}", raw):
            pass
        if last:
            frag = last.group(0)
            v = json.loads(frag)
            return v if isinstance(v, dict) else {}
    except Exception:
        pass

    return {}


def _strip_wrappers(text: str) -> str:
    t = (text or "")
    if SETTINGS.get("router_trim_whitespace") is True:
        t = t.strip()
    if SETTINGS.get("router_strip_wrappers_enabled") is not True:
        return t
    head = t
    if SETTINGS.get("router_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]
    pat = SETTINGS.get("router_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

def decide_web(llm: Any, user_text: str) -> Tuple[bool, Optional[str], Dict[str, Any]]:
    telemetry: Dict[str, Any] = {}
    try:
        if not user_text or not user_text.strip():
            return (False, None, telemetry)

        t_start = time.perf_counter()
        t_raw = user_text.strip()
        core_text = _strip_wrappers(t_raw)

        the_prompt = _DECIDE_PROMPT.format(text=core_text)
        params = {
            "max_tokens": SETTINGS.get("router_decide_max_tokens"),
            "temperature": SETTINGS.get("router_decide_temperature"),
            "top_p": SETTINGS.get("router_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}

        raw_out_obj = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw_out_obj.get("choices", [{}])[0]
                                  .get("message", {})
                                  .get("content") or "").strip()

        # NEW: keep raw router output for debugging
        telemetry["rawRouterOut"] = text_out[:2000]  # cap for safety

        data = _force_json(text_out) or {}

        # Coerce 'need' with string tolerance
        need_val = data.get("need", None)
        if isinstance(need_val, str):
            nv = need_val.strip().lower()
            if nv in ("true", "yes", "y", "1"):
                need_val = True
            elif nv in ("false", "no", "n", "0"):
                need_val = False

        if isinstance(need_val, bool):
            need = need_val
            parsed_ok = True
        else:
            parsed_ok = False
            need_default = SETTINGS.get("router_default_need_when_invalid")
            # If you want ingestion to proceed when parsing fails, set this to true in settings.
            need = bool(need_default) if isinstance(need_default, bool) else False

        # Extract query (safe) and strip wrappers
        query_field = data.get("query", "")
        try:
            query = _strip_wrappers(str(query_field or "").strip())
        except Exception:
            query = ""

        if not need:
            query = None  # ignore query when we decided 'no web'

        t_elapsed = time.perf_counter() - t_start
        in_tokens = safe_token_count_messages(llm, [{"role": "user", "content": the_prompt}]) or 0
        out_tokens = safe_token_count_messages(llm, [{"role": "assistant", "content": text_out}]) or 0

        telemetry.update({
            "needed": bool(need),
            "routerQuery": query if need else None,
            "elapsedSec": round(t_elapsed, 4),
            "inputTokens": in_tokens,
            "outputTokens": out_tokens,
            "parsedOk": parsed_ok,
        })

        return (need, query, telemetry)

    except Exception:
        return (False, None, telemetry)


async def decide_web_and_fetch(llm: Any, user_text: str, *, k: int = 3) -> Tuple[Optional[str], Dict[str, Any]]:
    telemetry: Dict[str, Any] = {}
    need, proposed_q, tel_decide = decide_web(llm, (user_text or "").strip())
    telemetry.update(tel_decide)
    if not need:
        return None, telemetry

    from .query_summarizer import summarize_query
    from .orchestrator import build_web_block

    base_query = _strip_wrappers((proposed_q or user_text).strip())
    try:
        q_summary, tel_sum = summarize_query(llm, base_query)
        q_summary = _strip_wrappers((q_summary or "").strip()) or base_query
        telemetry["summarizer"] = tel_sum
        telemetry["summarizedQuery"] = q_summary
    except Exception:
        q_summary = base_query

    t_start = time.perf_counter()
    try:
        block_res = await build_web_block(q_summary, k=k)
        if isinstance(block_res, tuple):
            block, tel_orch = block_res
            telemetry["orchestrator"] = tel_orch or {}
        else:
            block = block_res
    except Exception:
        block = None
    t_elapsed = time.perf_counter() - t_start

    telemetry.update({
        "fetchElapsedSec": round(t_elapsed, 4),
        "blockChars": len(block) if block else 0,
    })

    return (block or None, telemetry)

# ===== aimodel/file_read/workers/retitle_worker.py =====

# aimodel/file_read/retitle_worker.py
from __future__ import annotations
import asyncio, logging, re
from typing import Dict, List, Optional, Tuple

from ..runtime.model_runtime import get_llm
from ..store.index import load_index, save_index
from ..store.base import now_iso
from ..services.cancel import is_active, GEN_SEMAPHORE
from ..store.chats import _load_chat
from ..core.settings import SETTINGS

def S(key: str):
    return SETTINGS[key]

_PENDING: Dict[str, dict] = {}
_ENQUEUED: set[str] = set()
_queue: asyncio.Queue[str] = asyncio.Queue(maxsize=int(S("retitle_queue_maxsize")))
_lock = asyncio.Lock()

def _preview(s: str) -> str:
    n = int(S("retitle_preview_chars"))
    ell = S("retitle_preview_ellipsis")
    s = (s or "")
    return (s[:n] + ell) if len(s) > n else s

def _is_substantial(text: str) -> bool:
    t = (text or "").strip()
    min_chars = int(S("retitle_min_substantial_chars"))
    require_alpha = bool(S("retitle_require_alpha"))
    if len(t) < min_chars:
        return False
    return (re.search(r"[A-Za-z]", t) is not None) if require_alpha else True

def _pick_source(messages: List[dict]) -> Optional[str]:
    if not messages:
        print("[retitle] no messages to pick from")
        return None

    min_user_len = int(S("retitle_min_user_chars"))
    print(f"[retitle] min_user_len={min_user_len}, total_msgs={len(messages)}")

    # 1. Prefer last substantial user message
    for m in reversed(messages):
        if m.get("role") == "user":
            txt = (m.get("content") or "").strip()
            print(f"[retitle] check user msg len={len(txt)} preview={txt[:40]!r}")
            if len(txt) >= min_user_len and _is_substantial(txt):
                print("[retitle] PICK user message")
                return txt

    # 2. Fallback to last substantial assistant message
    for m in reversed(messages):
        if m.get("role") == "assistant":
            txt = (m.get("content") or "").strip()
            print(f"[retitle] check assistant msg len={len(txt)} preview={txt[:40]!r}")
            if _is_substantial(txt):
                print("[retitle] PICK assistant message")
                return txt

    print("[retitle] no suitable message found")
    return None

def _sanitize_title(s: str) -> str:
    if not s:
        return ""
    s = s.strip()
    s = re.sub(r"(?i)^(?:how\s+to|steps?\s+to|guide\s+to|tutorial:\s*|to)\s+", "", s).strip()
    drop_prefix_re = S("retitle_sanitize_drop_prefix_regex")
    if drop_prefix_re:
        s = re.sub(drop_prefix_re, "", s)
    if bool(S("retitle_sanitize_strip_quotes")):
        s = s.strip().strip('"').strip("'").strip()
    replace_not_allowed_re = S("retitle_sanitize_replace_not_allowed_regex")
    replace_with = S("retitle_sanitize_replace_with")
    if replace_not_allowed_re:
        s = re.sub(replace_not_allowed_re, replace_with, s)
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r"[.:;,\-\s]+$", "", s)
    s = re.sub(r"(?i)^(?:a|an|the)\s+", "", s).strip()
    max_words = int(S("retitle_sanitize_max_words"))
    max_chars = int(S("retitle_sanitize_max_chars"))
    if max_words > 0:
        words = s.split()
        s = " ".join(words[:max_words])
    if max_chars > 0 and len(s) > max_chars:
        s = s[:max_chars].rstrip()
    return s

def _make_title(llm, src: str) -> str:
    sys = S("retitle_llm_sys_inst")
    hard = ("You generate a concise chat title.\n"
            "Return ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). "
            "No trailing punctuation. Max 6 words.")
    sys = f"{hard}\n\n{sys}" if sys else hard

    user_text = f"{S('retitle_user_prefix')}{src}{S('retitle_user_suffix')}"
    messages = [
        {"role": "system", "content": sys},
        {"role": "user", "content": user_text},
    ]

    out = llm.create_chat_completion(
        messages=messages,
        max_tokens=int(S("retitle_llm_max_tokens")),
        temperature=float(S("retitle_llm_temperature")),
        top_p=float(S("retitle_llm_top_p")),
        stream=False,
        stop=S("retitle_llm_stop"),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip().strip('"').strip("'")
    raw = re.sub(r"(?i)^(?:how\s+to|to)\s+", "", raw).strip()
    raw = re.sub(r"^`{1,3}|`{1,3}$", "", raw).strip()
    raw = re.sub(r"[.:;,\-\s]+$", "", raw)
    return raw

async def start_worker():
    while True:
        sid = await _queue.get()
        try:
            await _process_session(sid)
        except Exception:
            logging.exception("Retitle worker failed")
        finally:
            _queue.task_done()

def _extract_job(snapshot: dict) -> Tuple[List[dict], int]:
    msgs = snapshot.get("messages") or []
    job_seq = int(snapshot.get("job_seq") or 0)
    return msgs, job_seq

async def _process_session(session_id: str):
    if not bool(S("retitle_enable")):
        return
    await asyncio.sleep(int(S("retitle_grace_ms")) / 1000.0)
    waited = 0
    backoff = int(S("retitle_active_backoff_start_ms"))
    backoff_max = int(S("retitle_active_backoff_max_ms"))
    backoff_total = int(S("retitle_active_backoff_total_ms"))
    growth = float(S("retitle_active_backoff_growth"))
    while is_active(session_id) and waited < backoff_total:
        await asyncio.sleep(backoff /