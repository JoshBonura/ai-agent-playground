re", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {"key": "llm.prediction.topPSampling", "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)}},
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
        },
    }

# ---------- connection helper ----------

async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/duckduckgo.py =====

# aimodel/file_read/web/duckduckgo.py
from __future__ import annotations
from typing import List, Optional, Tuple
import asyncio, time
from urllib.parse import urlparse

# Prefer new package; fallback for compatibility
try:
    from ddgs import DDGS  # type: ignore
except Exception:
    try:
        from duckduckgo_search import DDGS  # type: ignore
    except Exception:
        DDGS = None  # no provider

from ..core.settings import SETTINGS
from .provider import SearchHit

# -------- simple in-memory cache (superset caching) --------------------------
_CACHE: dict[str, Tuple[float, List[SearchHit]]] = {}

def _cache_key(query: str) -> str:
    return (query or "").strip().lower()

def _host(u: str) -> str:
    try:
        h = (urlparse(u).hostname or "").lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return ""

def _cache_get(query: str) -> Optional[List[SearchHit]]:
    eff = SETTINGS.effective()  # no fallbacks allowed
    ttl = int(eff["web_search_cache_ttl_sec"])
    key = _cache_key(query)
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if (time.time() - ts) > ttl:
        _CACHE.pop(key, None)
        return None
    return hits

def _cache_set(query: str, hits: List[SearchHit]) -> None:
    _CACHE[_cache_key(query)] = (time.time(), hits)

# -------- DDGS (official client) --------------------------------------------
def _ddg_sync_search(query: str, k: int, *, region: str, safesearch: str) -> List[SearchHit]:
    results: List[SearchHit] = []
    if DDGS is None:
        print(f"[{time.strftime('%X')}] ddg: PROVIDER MISSING (DDGS=None)")
        return results
    with DDGS() as ddg:
        for i, r in enumerate(ddg.text(query, max_results=max(1, k),
                                       safesearch=safesearch, region=region)):
            title = (r.get("title") or "").strip()
            url = (r.get("href") or "").strip()
            snippet: Optional[str] = (r.get("body") or "").strip() or None
            if not url:
                continue
            results.append(SearchHit(title=title or url, url=url, snippet=snippet, rank=i))
            if i + 1 >= k:
                break
    return results

# -------- public provider ----------------------------------------------------
class DuckDuckGoProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        eff = SETTINGS.effective()  # strict read; raise if missing
        q_norm = (query or "").strip()
        if not q_norm:
            return []

        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        region = str(eff["web_search_region"])
        safesearch = str(eff["web_search_safesearch"])
        verbose = bool(eff["web_search_debug_logging"])

        t0 = time.time()
        if verbose:
            print(f"[{time.strftime('%X')}] ddg: START q={q_norm!r} k={k}")

        # cache
        cached = _cache_get(q_norm)
        if cached is not None:
            out = cached[:k]
            if verbose:
                top_preview = [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]]
                print(f"[{time.strftime('%X')}] ddg: CACHE HIT dt={time.time()-t0:.2f}s hits={len(out)} top={top_preview}")
            return out

        # fetch superset once
        hits: List[SearchHit] = []
        if DDGS is not None:
            try:
                step = time.time()
                hits = await asyncio.to_thread(_ddg_sync_search, q_norm, superset_k,
                                               region=region, safesearch=safesearch)
                if verbose:
                    print(f"[{time.strftime('%X')}] ddg: HITS RECEIVED dt={time.time()-step:.2f}s count={len(hits)}")
                    for h in hits[:5]:
                        print(f"[{time.strftime('%X')}] ddg:   {h.rank:>2} | host={_host(h.url)} | "
                              f"title={(h.title or '')[:80]!r}")
            except Exception as e:
                print(f"[{time.strftime('%X')}] ddg: ERROR {e}")
        else:
            print(f"[{time.strftime('%X')}] ddg: SKIP (DDGS unavailable)")

        _cache_set(q_norm, hits)
        out = hits[:k]
        if verbose:
            print(f"[{time.strftime('%X')}] ddg: RETURN dt={time.time()-t0:.2f}s hits={len(out)}")
        return out

# ===== aimodel/file_read/web/fetch.py =====

# aimodel/file_read/web/fetch.py
from __future__ import annotations
import asyncio
from typing import Tuple, List, Optional
import httpx

# Optional extractors (use whatever is installed)
try:
    from readability import Document  # readability-lxml
except Exception:
    Document = None  # type: ignore

try:
    from bs4 import BeautifulSoup  # beautifulsoup4 + lxml
except Exception:
    BeautifulSoup = None  # type: ignore

try:
    from selectolax.parser import HTMLParser  # selectolax
except Exception:
    HTMLParser = None  # type: ignore

from ..core.settings import SETTINGS


def _ua() -> str:
    return str(SETTINGS.get("web_fetch_user_agent", "LocalAI/0.1 (+clean-fetch)"))

def _timeout() -> float:
    try:
        return float(SETTINGS.get("web_fetch_timeout_sec", 8.0))
    except Exception:
        return 8.0

def _max_chars() -> int:
    try:
        return int(SETTINGS.get("web_fetch_max_chars", 3000))
    except Exception:
        return 3000

def _max_bytes() -> int:
    try:
        return int(SETTINGS.get("web_fetch_max_bytes", 1_048_576))
    except Exception:
        return 1_048_576

def _max_parallel() -> int:
    try:
        return max(1, int(SETTINGS.get("web_fetch_max_parallel", 3)))
    except Exception:
        return 3


async def _read_capped_bytes(resp: httpx.Response, cap_bytes: int) -> bytes:
    # Stream and cap to avoid huge downloads
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    """
    Best-effort HTML → text:
      1) readability-lxml (if available): main-article extraction
      2) selectolax (if available): fast DOM text
      3) BeautifulSoup (if available): generic text fallback
      4) Raw text as last resort
    """
    html = raw_html or ""

    # 1) readability main content
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
                # If bs4 missing, fall back to selectolax/plain below
        except Exception:
            pass

    # 2) selectolax full text
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            # Remove script/style/noscript quickly if present
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = tree.body.text(separator=" ", strip=True) if tree.body else tree.text(separator=" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass

    # 3) BeautifulSoup full text
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            # Strip scripts/styles
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass

    # 4) Raw (no HTML parsing available)
    return html


async def fetch_clean(
    url: str,
    timeout_s: Optional[float] = None,
    max_chars: Optional[int] = None,
    max_bytes: Optional[int] = None,
) -> Tuple[str, int, str]:
    """
    Returns: (final_url, status_code, cleaned_text)
    - cleaned_text is extracted via readability/selectolax/bs4 if available; else raw
    - caps by bytes first, then by chars
    """
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout, headers=headers) as client:
        r = await client.get(url)
        r.raise_for_status()

        # Stream with byte cap
        raw_bytes = await _read_capped_bytes(r, cap_bytes)
        # Text decode with fallback (httpx sets encoding heuristically)
        enc = r.encoding or "utf-8"
        raw_text = raw_bytes.decode(enc, errors="ignore")

        # Extract/clean HTML to text (no trafilatura)
        txt = _extract_text_from_html(raw_text, str(r.url))
        txt = (txt or "").strip().replace("\r", "")

        if len(txt) > cap_chars:
            txt = txt[:cap_chars]

        return (str(r.url), r.status_code, txt)


async def fetch_many(
    urls: List[str],
    per_timeout_s: Optional[float] = None,
    cap_chars: Optional[int] = None,
    cap_bytes: Optional[int] = None,
    max_parallel: Optional[int] = None,
):
    sem = asyncio.Semaphore(_max_parallel() if max_parallel is None else int(max_parallel))

    async def _one(u: str):
        async with sem:
            try:
                return u, await fetch_clean(
                    u,
                    timeout_s=per_timeout_s,
                    max_chars=cap_chars,
                    max_bytes=cap_bytes,
                )
            except Exception:
                return u, None

    tasks = [_one(u) for u in urls]
    return await asyncio.gather(*tasks)

# ===== aimodel/file_read/web/orchestrator.py =====

# aimodel/file_read/web/orchestrator.py
from __future__ import annotations
from typing import List, Tuple, Optional
from urllib.parse import urlparse
import time
import re

from ..core.settings import SETTINGS
from .duckduckgo import DuckDuckGoProvider
from .provider import SearchHit
from .fetch import fetch_many  # optional JS path resolved dynamically (see below)

# ===== helpers to read config (strict: no silent defaults) ====================

def _req(key: str):
    return SETTINGS[key]

def _as_int(key: str) -> int: return int(_req(key))
def _as_float(key: str) -> float: return float(_req(key))
def _as_bool(key: str) -> bool: return bool(_req(key))
def _as_str(key: str) -> str:
    v = _req(key)
    return "" if v is None else str(v)

# ===== small utils ============================================================

def _clean_ws(s: str) -> str:
    return " ".join((s or "").split())

def _host(url: str) -> str:
    h = (urlparse(url).hostname or "").lower()
    pref = _as_str("web_orch_www_prefix")
    return h[len(pref):] if pref and h.startswith(pref) else h

def _tokens(s: str) -> List[str]:
    return [t for t in re.findall(r"\w+", (s or "").lower()) if t]

def _head_tail(text: str, max_chars: int) -> str:
    """
    Trim long text to head/tail, using settings:
      - web_orch_head_fraction
      - web_orch_tail_min_chars
      - web_orch_ellipsis
    Mirrors the old behavior but configurable.
    """
    text = text or ""
    if max_chars <= 0 or len(text) <= max_chars:
        return _clean_ws(text)

    head_frac      = _as_float("web_orch_head_fraction")      # e.g., 0.67
    tail_min_chars = _as_int("web_orch_tail_min_chars")       # e.g., 200
    ellipsis       = _as_str("web_orch_ellipsis")             # e.g., " … "

    head = int(max_chars * head_frac)
    tail = max_chars - head
    if tail < tail_min_chars:
        head = max(1, max_chars - tail_min_chars)
        tail = tail_min_chars

    return _clean_ws(text[:head] + ellipsis + text[-tail:])

def condense_doc(title: str, url: str, text: str, *, max_chars: int) -> str:
    body = _head_tail(text or "", max_chars)
    safe_title = _clean_ws(title or url)
    bullet = _as_str("web_orch_bullet_prefix") or "- "
    indent = _as_str("web_orch_indent_prefix") or "  "
    return f"{bullet}{safe_title}\n{indent}{url}\n{indent}{body}"

# ===== scoring (generic; no domain/date heuristics) ===========================

def score_hit(hit: SearchHit, query: str) -> int:
    """
    Generic, content-based scoring.
      - exact phrase in title (+web_orch_score_w_exact)
      - substring in title (+web_orch_score_w_substr)
      - token coverage in title (+0..web_orch_score_w_title_full/title_part)
      - any token in snippet (+web_orch_score_w_snip_touch)
    """
    w_exact      = _as_int("web_orch_score_w_exact")
    w_substr     = _as_int("web_orch_score_w_substr")
    w_title_full = _as_int("web_orch_score_w_title_full")
    w_title_part = _as_int("web_orch_score_w_title_part")
    w_snip_touch = _as_int("web_orch_score_w_snip_touch")

    score = 0
    q = (query or "").strip().lower()
    title = (hit.title or "").strip()
    snippet = (hit.snippet or "").strip()
    title_l = title.lower()
    snip_l  = snippet.lower()

    if q:
        if title_l == q:
            score += w_exact
        elif q in title_l:
            score += w_substr

    qtoks = _tokens(q)
    if qtoks:
        cov_title = sum(1 for t in qtoks if t in title_l)
        if cov_title == len(qtoks) and len(qtoks) > 0:
            score += w_title_full
        elif cov_title > 0:
            score += w_title_part
        cov_snip = sum(1 for t in qtoks if t in snip_l)
        if cov_snip > 0:
            score += w_snip_touch

    return score

# ===== quality estimate (generic; configurable penalties) =====================

def _type_ratio(text: str, sub: str) -> float:
    if not text:
        return 1.0
    cnt = text.lower().count(sub)
    return float(cnt) / max(1, len(text))

def content_quality_score(text: str) -> float:
    if not text:
        return 0.0
    t = text.strip()
    n = len(t)

    len_div     = _as_float("web_orch_q_len_norm_divisor")
    w_len       = _as_float("web_orch_q_len_weight")
    w_div       = _as_float("web_orch_q_diversity_weight")

    length_score = min(1.0, n / len_div) if len_div > 0 else 0.0

    toks = _tokens(t)
    if not toks:
        return 0.1 * length_score  # tiny signal if no tokens

    uniq = len(set(toks))
    diversity = uniq / max(1.0, float(len(toks)))

    pen = 0.0
    # penalties: [{"token": str, "mult": float, "cap": float}, ...]
    for rule in _req("web_orch_q_penalties"):
        token = str(rule.get("token") or "")
        mult  = float(rule.get("mult") or 0.0)
        cap   = float(rule.get("cap") or 1.0)
        pen += min(cap, _type_ratio(t, token) * mult)

    raw = (w_len * length_score) + (w_div * diversity) - pen
    return max(0.0, min(1.0, raw))

def _dedupe_by_host(scored_hits: List[Tuple[int, SearchHit]], k: int) -> List[SearchHit]:
    picked: List[SearchHit] = []
    seen_hosts = set()
    for s, h in sorted(scored_hits, key=lambda x: x[0], reverse=True):
        u = (h.url or "").strip()
        if not u:
            continue
        host = _host(u)
        if host in seen_hosts:
            continue
        seen_hosts.add(host)
        picked.append(h)
        if len(picked) >= k:
            break
    return picked

# ===== fetch layer ============================================================

async def _fetch_round(
    urls: List[str],
    meta: List[Tuple[str, str]],
    per_url_timeout_s: float,
    max_parallel: int,
    use_js: bool = False,
) -> List[Tuple[str, Optional[Tuple[str, int, str]]]]:

    fetch_fn = fetch_many
    if use_js:
        try:
            from . import fetch as _fetch_mod  # type: ignore
            fetch_fn = getattr(_fetch_mod, "fetch_many_js", fetch_many)
        except Exception:
            fetch_fn = fetch_many

    # Cap per doc by multiplier, but never exceed global max bytes/chars
    cap_mult        = _as_float("web_orch_fetch_cap_multiplier")
    per_doc_budget  = _as_int("web_orch_per_doc_char_budget")
    fetch_max_chars = _as_int("web_fetch_max_chars")
    per_doc_cap     = min(int(per_doc_budget * cap_mult), fetch_max_chars)

    results = await fetch_fn(
        urls,
        per_timeout_s=per_url_timeout_s,
        cap_chars=per_doc_cap,
        max_parallel=max_parallel,
    )
    return results

# ===== main ===================================================================

async def build_web_block(query: str, k: Optional[int] = None, per_url_timeout_s: Optional[float] = None) -> str | None:
    # pull config each call (to honor hot-reloads)
    cfg_k               = (int(k) if k is not None else _as_int("web_orch_default_k"))
    total_char_budget   = _as_int("web_orch_total_char_budget")
    per_doc_budget      = _as_int("web_orch_per_doc_char_budget")
    max_parallel        = _as_int("web_orch_max_parallel_fetch")
    overfetch_factor    = _as_float("web_orch_overfetch_factor")
    overfetch_min_extra = _as_int("web_orch_overfetch_min_extra")

    enable_js_retry     = _as_bool("web_orch_enable_js_retry")
    js_avg_q_thresh     = _as_float("web_orch_js_retry_avg_q")
    js_low_q_thresh     = _as_float("web_orch_js_retry_low_q")
    js_lowish_ratio     = _as_float("web_orch_js_retry_lowish_ratio")
    js_timeout_add      = _as_float("web_orch_js_retry_timeout_add")
    js_timeout_cap      = _as_float("web_orch_js_retry_timeout_cap")
    js_parallel_delta   = _as_int("web_orch_js_retry_parallel_delta")
    js_min_parallel     = _as_int("web_orch_js_retry_min_parallel")

    header_tpl          = _as_str("web_block_header")
    sep_str             = _as_str("web_orch_block_separator")
    min_block_reserve   = _as_int("web_orch_min_block_reserve")
    min_chunk_after     = _as_int("web_orch_min_chunk_after_shrink")

    # timeouts
    per_timeout = (float(per_url_timeout_s)
                   if per_url_timeout_s is not None
                   else _as_float("web_fetch_timeout_sec"))

    start_time = time.time()
    print(f"[orchestrator] IN  @ {start_time:.3f}s | query={query!r}")

    provider = DuckDuckGoProvider()

    # --- SEARCH (configurable overfetch) ---
    overfetch = max(cfg_k + overfetch_min_extra, int(round(cfg_k * overfetch_factor)))
    print(f"[orchestrator] SEARCH start overfetch={overfetch} k={cfg_k}")

    t0 = time.perf_counter()
    try:
        hits: List[SearchHit] = await provider.search(query, k=overfetch)
    except Exception as e:
        print(f"[orchestrator] ERROR during search for {query!r}: {e}")
        return None
    print(f"[orchestrator] SEARCH done hits={len(hits)} dt={time.perf_counter() - t0:.3f}s")

    if not hits:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no hits | elapsed={time.time()-start_time:.3f}s")
        return None

    # --- SCORING / DEDUPE ---
    print(f"[orchestrator] SCORING generic (no hardcoded boosts)")
    seen_urls = set()
    scored: List[Tuple[int, SearchHit]] = []
    for idx, h in enumerate(hits):
        u = (h.url or "").strip()
        if not u:
            print(f"[orchestrator]   skip[{idx}] empty url")
            continue
        if u in seen_urls:
            print(f"[orchestrator]   dup [{idx}] host={_host(u)} title={(h.title or '')[:60]!r}")
            continue
        seen_urls.add(u)
        s = score_hit(h, query)
        scored.append((s, h))
        print(f"[orchestrator]   meta[{idx}] score={s} host={_host(u)} title={(h.title or '')[:80]!r} url={u}")

    if not scored:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no unique hits | elapsed={time.time()-start_time:.3f}s")
        return None

    top_hits = _dedupe_by_host(scored, cfg_k)
    for i, h in enumerate(top_hits, 1):
        # reuse computed scores when printing
        s = next((sc for sc, hh in scored if hh is h), 0)
        print(f"[orchestrator] PICK {i}/{cfg_k} score={s} host={_host(h.url)} title={(h.title or '')[:80]!r}")

    # --- FETCH ROUND 1 (static) ---
    urls = [h.url for h in top_hits]
    meta = [(h.title or h.url, h.url) for h in top_hits]
    print(f"[orchestrator] FETCH[1] start urls={[ _host(u) for u in urls ]}")

    t_f = time.perf_counter()
    results = await _fetch_round(
        urls, meta, per_url_timeout_s=per_timeout, max_parallel=max_parallel, use_js=False
    )
    dt_f = time.perf_counter() - t_f
    print(f"[orchestrator] FETCH[1] done n={len(results)} dt={dt_f:.3f}s")

    texts: List[Tuple[str, str, str]] = []  # (title, final_url, text)
    quality_scores: List[float] = []

    for original_url, res in results:
        if not res:
            print(f"[orchestrator]   fetch MISS url={original_url}")
            continue
        final_url, status, text = res
        title = next((t for (t, u) in meta if u == original_url), final_url)
        tl = len(text or "")
        qscore = content_quality_score(text or "")
        quality_scores.append(qscore)
        print(f"[orchestrator]   fetch OK   status={status} host={_host(final_url)} len={tl} q={qscore:.2f} title={(title or '')[:80]!r}")
        if text:
            texts.append((title, final_url, text))

    # --- JS retry decision (configurable thresholds) ---
    try_js = False
    if enable_js_retry and quality_scores:
        avg_q = sum(quality_scores) / len(quality_scores)
        lowish = sum(1 for q in quality_scores if q < js_low_q_thresh)
        if avg_q < js_avg_q_thresh or (lowish / max(1, len(quality_scores))) >= js_lowish_ratio:
            try_js = True

    if try_js:
        print("[orchestrator] FETCH[2-JS] trying JS-rendered fetch due to low content quality")
        js_timeout   = min(per_timeout + js_timeout_add, js_timeout_cap)
        js_parallel  = max(js_min_parallel, max_parallel + js_parallel_delta)

        results_js = await _fetch_round(
            urls, meta, per_url_timeout_s=js_timeout, max_parallel=js_parallel, use_js=True
        )
        texts_js: List[Tuple[str, str, str]] = []
        for original_url, res in results_js:
            if not res:
                continue
            final_url, status, text = res
            title = next((t for (t, u) in meta if u == original_url), final_url)
            tl = len(text or "")
            qscore = content_quality_score(text or "")
            print(f"[orchestrator]   fetch JS OK status={status} host={_host(final_url)} len={tl} q={qscore:.2f} title={(title or '')[:80]!r}")
            if text:
                texts_js.append((title, final_url, text))

        if texts_js:
            texts = texts_js

    if not texts:
        print(f"[orchestrator] OUT @ {time.time():.3f}s | no chunks | elapsed={time.time()-start_time:.3f}s")
        return None

    # --- Build chunks; order by quality ---
    texts.sort(key=lambda t: content_quality_score(t[2]), reverse=True)

    chunks: List[str] = []
    for title, final_url, text in texts:
        chunk = condense_doc(title, final_url, text, max_chars=per_doc_budget)
        chunks.append(chunk)
        print(f"[orchestrator]   chunk len={len(chunk)} host={_host(final_url)}")

    # --- Enforce total budget ---
    header = header_tpl.format(query=query)
    sep = _as_str("web_orch_block_separator")
    available = max(_as_int("web_orch_min_block_reserve"),
                    total_char_budget - len(header) - len(sep))
    block_parts: List[str] = []
    used = 0
    for idx, ch in enumerate(chunks):
        cl = len(ch)
        sep_len = (len(sep) if block_parts else 0)
        if used + cl + sep_len > available:
            shrunk = _head_tail(ch, max(min_chunk_after, available - used - sep_len))
            print(f"[orchestrator]   budget hit at chunk[{idx}] orig={cl} shrunk={len(shrunk)} used_before={used} avail={available}")
            if len(shrunk) > min_chunk_after:
                block_parts.append(shrunk)
                used += len(shrunk) + sep_len
            break
        block_parts.append(ch)
        used += cl + sep_len
        print(f"[orchestrator]   take chunk[{idx}] len={cl} used_total={used}/{available}")

    body = sep.join(block_parts)
    block = f"{header}{sep}{body}" if body else header

    end_time = time.time()
    print(f"[orchestrator] OUT @ {end_time:.3f}s | elapsed={end_time-start_time:.3f}s | chunks={len(block_parts)} | chars={len(block)}")
    return block

# ===== aimodel/file_read/web/provider.py =====

from __future__ import annotations
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class SearchHit:
    title: str
    url: str
    snippet: Optional[str] = None
    rank: int = 0

class SearchProvider:
    async def search(self, query: str, k: int = 3) -> List[SearchHit]:
        raise NotImplementedError

# ===== aimodel/file_read/web/query_summarizer.py =====

# aimodel/file_read/web/query_summarizer.py
from __future__ import annotations
from typing import Any, Iterable
import re

from ..core.settings import SETTINGS

def _tokens(s: str) -> set[str]:
    return set(re.findall(r"\w+", (s or "").lower()))

def _as_list(v) -> list:
    if v is None:
        return []
    if isinstance(v, (list, tuple)):
        return list(v)
    return [v]

def summarize_query(llm: Any, user_text: str) -> str:
    """
    Settings used (all optional; no in-code fallbacks):
      - query_sum_bypass_short_enabled : bool
      - query_sum_short_max_chars      : int
      - query_sum_short_max_words      : int
      - query_sum_prompt               : str (must contain '{text}')
      - query_sum_max_tokens           : int
      - query_sum_temperature          : float
      - query_sum_top_p                : float
      - query_sum_stop                 : list[str]
      - query_sum_overlap_check_enabled: bool
      - query_sum_overlap_jaccard_min  : float (0..1)
    """
    txt = (user_text or "").strip()
    print(f"[SUMMARIZER] IN user_text={txt!r}")

    # --- Bypass for very short queries (only if fully configured) ---
    bypass_enabled = SETTINGS.get("query_sum_bypass_short_enabled")
    short_chars = SETTINGS.get("query_sum_short_max_chars")
    short_words = SETTINGS.get("query_sum_short_max_words")
    if bypass_enabled is True and isinstance(short_chars, int) and isinstance(short_words, int):
        if len(txt) <= short_chars and len(txt.split()) <= short_words:
            print(f"[SUMMARIZER] BYPASS (short) -> {txt!r}")
            return txt

    # --- LLM prompt construction (only if prompt provided) ---
    prompt = SETTINGS.get("query_sum_prompt")
    if isinstance(prompt, str) and "{text}" in prompt:
        params = {}
        # Only include configured generation params; nothing hardcoded.
        max_tokens = SETTINGS.get("query_sum_max_tokens")
        if isinstance(max_tokens, int):
            params["max_tokens"] = max_tokens

        temperature = SETTINGS.get("query_sum_temperature")
        if isinstance(temperature, (int, float)):
            params["temperature"] = float(temperature)

        top_p = SETTINGS.get("query_sum_top_p")
        if isinstance(top_p, (int, float)):
            params["top_p"] = float(top_p)

        stops = _as_list(SETTINGS.get("query_sum_stop"))
        if stops:
            params["stop"] = [str(s) for s in stops if isinstance(s, str)]

        params["stream"] = False  # summarizer uses non-streamed call

        out = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt.format(text=txt)}],
            **params,  # only what’s configured is sent
        )
        result = (out["choices"][0]["message"]["content"] or "").strip()
    else:
        # If no prompt configured, do not attempt to summarize — return original text.
        print("[SUMMARIZER] SKIP (no prompt configured) -> retain input")
        return txt

    # --- Optional overlap check (guard against paraphrase drift) ---
    overlap_enabled = SETTINGS.get("query_sum_overlap_check_enabled")
    j_min = SETTINGS.get("query_sum_overlap_jaccard_min")
    if overlap_enabled is True and isinstance(j_min, (int, float)):
        src_toks = _tokens(txt)
        out_toks = _tokens(result)
        if not result or not out_toks:
            print(f"[SUMMARIZER] RETAIN (empty/none) -> {txt!r}")
            print(f"[SUMMARIZER] OUT query={txt!r}")
            return txt
        jaccard = (len(src_toks & out_toks) / len(src_toks | out_toks)) if (src_toks or out_toks) else 1.0
        if jaccard < float(j_min):
            print(f"[SUMMARIZER] RETAIN (low overlap {jaccard:.2f} < {float(j_min):.2f}) -> {txt!r}")
            print(f"[SUMMARIZER] OUT query={txt!r}")
            return txt
        print(f"[SUMMARIZER] OUT query={result!r} (overlap {jaccard:.2f})")
        return result

    # If overlap check not enabled/configured, just return the LLM result.
    print(f"[SUMMARIZER] OUT query={result!r} (no overlap check)")
    return result

# ===== aimodel/file_read/web/router_ai.py =====

# aimodel/file_read/web/router_ai.py
from __future__ import annotations
from typing import Tuple, Optional, Any
import json, re
from ..core.settings import SETTINGS

# ---- Hardcoded, brace-safe prompt (only {text} is formatted) -----------------
_DECIDE_PROMPT = (
    "You are a router deciding whether answering the text requires the public web.\n"
    "Respond with JSON only in exactly this schema:\n"
    "{{\"need\": true|false, \"query\": \"<text or empty>\"}}\n\n"
    "Decision principle:\n"
    "- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n"
    "- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n"
    "- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n"
    "- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n"
    "- When uncertain whether real-time state is required, prefer need=true.\n\n"
    "Text:\n{text}\n"
    "JSON:"
)

# ---- JSON extraction (configurable) -----------------------------------------
def _force_json(s: str) -> dict:
    if not s:
        return {}
    # Primary: straight JSON
    try:
        v = json.loads(s)
        return v if isinstance(v, dict) else {}
    except Exception:
        pass

    # Settings-provided regex (recommended: non-greedy first-block)
    rgx = SETTINGS.get("router_json_extract_regex")
    cand = None
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
        except Exception:
            cand = None

    # Fallback: first non-greedy {...}
    if not cand:
        m2 = re.search(r"\{.*?\}", s, re.DOTALL)
        cand = m2.group(0) if m2 else None

    if not cand:
        return {}
    try:
        v = json.loads(cand)
        return v if isinstance(v, dict) else {}
    except Exception:
        return {}

# ---- Strip wrappers (configurable) ------------------------------------------
def _strip_wrappers(text: str) -> str:
    t = (text or "")
    if SETTINGS.get("router_trim_whitespace") is True:
        t = t.strip()

    if SETTINGS.get("router_strip_wrappers_enabled") is not True:
        return t

    head = t
    if SETTINGS.get("router_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]

    pat = SETTINGS.get("router_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

# ---- Core router -------------------------------------------------------------
def decide_web(llm: Any, user_text: str) -> Tuple[bool, Optional[str]]:
    try:
        if not user_text or not user_text.strip():
            print("[ROUTER] SKIP (empty user_text)")
            return (False, None)

        t_raw = user_text.strip()
        core_text = _strip_wrappers(t_raw)
        print(f"[ROUTER] INPUT raw={t_raw!r} core={core_text!r}")

        # Explicit overrides (prefixes configurable)
        prefixes = SETTINGS.get("router_explicit_prefixes")
        if isinstance(prefixes, list) and prefixes:
            low = t_raw.lower()
            for p in prefixes:
                ps = str(p or "").lower()
                if ps and low.startswith(ps):
                    q = t_raw.split(":", 1)[1].strip() if ":" in t_raw else t_raw
                    q = _strip_wrappers(q)
                    print(f"[ROUTER] EXPLICIT override need_web=True query={q!r}")
                    return (True, q)

        # Hardcoded prompt; all other knobs from settings
        the_prompt = _DECIDE_PROMPT.format(text=core_text)
        print(f"[ROUTER] PROMPT >>>\n{the_prompt}\n<<< PROMPT")

        # Build generation params from settings (filter out None)
        params = {
            "max_tokens": SETTINGS.get("router_decide_max_tokens"),
            "temperature": SETTINGS.get("router_decide_temperature"),
            "top_p": SETTINGS.get("router_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}

        raw_out_obj = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}],
            **params,
        )
        text_out = (raw_out_obj.get("choices", [{}])[0]
                                  .get("message", {})
                                  .get("content") or "").strip()

        try:
            print(f"[ROUTER] RAW OBJ keys={list(raw_out_obj.keys())}")
        except Exception:
            pass
        print(f"[ROUTER] RAW OUT str={text_out!r}")

        data = _force_json(text_out) or {}
        print(f"[ROUTER] PARSED JSON={data}")

        # Keep it strict: bool only; otherwise fallback to default
        need_val = data.get("need", None)
        if isinstance(need_val, bool):
            need = need_val
        else:
            need_default = SETTINGS.get("router_default_need_when_invalid")
            need = bool(need_default) if isinstance(need_default, bool) else False

        query_field = data.get("query", "")
        try:
            query = _strip_wrappers(str(query_field or "").strip())
        except Exception:
            query = ""

        if not need:
            query = None

        print(f"[ROUTER] DECISION need_web={need} query={query!r}")
        return (need, query)

    except Exception as e:
        print(f"[ROUTER] FATAL in decide_web: {type(e).__name__}: {e}")
        return (False, None)

# ---- One-hop decide + fetch --------------------------------------------------
async def decide_web_and_fetch(llm: Any, user_text: str, *, k: int = 3) -> Optional[str]:
    t = (user_text or "").strip()
    prev = (t[:160] + "…") if len(t) > 160 else t
    print(f"[ROUTER:FETCH] IN text_len={len(t)} k={k} text_preview={prev!r}")

    need, proposed_q = decide_web(llm, t)
    print(f"[ROUTER:FETCH] decide_web -> need={need} proposed_q={proposed_q!r}")
    if not need:
        print("[ROUTER:FETCH] no web needed")
        return None

    # Lazy imports to avoid circulars
    from .query_summarizer import summarize_query
    from .orchestrator import build_web_block

    base_query = _strip_wrappers((proposed_q or t).strip())
    try:
        q_summary = (summarize_query(llm, base_query) or "").strip().strip('"\'' ) or base_query
        q_summary = _strip_wrappers(q_summary)
        print(f"[ROUTER:FETCH] summarize_query base={base_query!r} -> q_summary={q_summary!r}")
    except Exception as e:
        print(f"[ROUTER:FETCH] summarize_query ERROR {type(e).__name__}: {e}")
        q_summary = base_query

    try:
        block = await build_web_block(q_summary, k=k)
        print(f"[ROUTER:FETCH] build_web_block len={(len(block) if block else 0)}")
    except Exception as e:
        print(f"[ROUTER:FETCH] build_web_block ERROR {type(e).__name__}: {e}")
        block = None

    return block or None

# ===== aimodel/file_read/workers/retitle_worker.py =====

# aimodel/file_read/retitle_worker.py
from __future__ import annotations
import asyncio, logging, re
from typing import Dict, List, Optional, Tuple

from ..runtime.model_runtime import get_llm
from ..store.index import load_index, save_index
from ..store.base import now_iso
from ..services.cancel import is_active, GEN_SEMAPHORE
from ..store.chats import _load_chat
from ..core.settings import SETTINGS

def S(key: str):
    return SETTINGS[key]

_PENDING: Dict[str, dict] = {}
_ENQUEUED: set[str] = set()
_queue: asyncio.Queue[str] = asyncio.Queue(maxsize=int(S("retitle_queue_maxsize")))
_lock = asyncio.Lock()

def _preview(s: str) -> str:
    n = int(S("retitle_preview_chars"))
    ell = S("retitle_preview_ellipsis")
    s = (s or "")
    return (s[:n] + ell) if len(s) > n else s

def _is_substantial(text: str) -> bool:
    t = (text or "").strip()
    min_chars = int(S("retitle_min_substantial_chars"))
    require_alpha = bool(S("retitle_require_alpha"))
    if len(t) < min_chars:
        return False
    return (re.search(r"[A-Za-z]", t) is not None) if require_alpha else True

def _pick_source(messages: List[dict]) -> Optional[str]:
    if not messages:
        return None
    for m in reversed(messages):
        if m.get("role") == "user":
            txt = (m.get("content") or "").strip()
            if _is_substantial(txt):
                return txt
    for m in reversed(messages):
        if m.get("role") == "assistant":
            txt = (m.get("content") or "").strip()
            if _is_substantial(txt):
                return txt
    return None

def _sanitize_title(s: str) -> str:
    if not s:
        return ""
    s = s.strip()
    drop_prefix_re = S("retitle_sanitize_drop_prefix_regex")
    if drop_prefix_re:
        s = re.sub(drop_prefix_re, "", s)
    if bool(S("retitle_sanitize_strip_quotes")):
        s = s.strip().strip('"').strip("'").strip()
    replace_not_allowed_re = S("retitle_sanitize_replace_not_allowed_regex")
    replace_with = S("retitle_sanitize_replace_with")
    if replace_not_allowed_re:
        s = re.sub(replace_not_allowed_re, replace_with, s)
    s = re.sub(r"\s+", " ", s).strip()
    max_words = int(S("retitle_sanitize_max_words"))
    max_chars = int(S("retitle_sanitize_max_chars"))
    if max_words > 0:
        words = s.split()
        s = " ".join(words[:max_words])
    if max_chars > 0 and len(s) > max_chars:
        s = s[:max_chars].rstrip()
    return s

def _make_title(llm, src: str) -> str:
    sys = S("retitle_llm_sys_inst")
    examples: List[dict] = list(S("retitle_llm_examples"))
    user_text = f"{S('retitle_user_prefix')}{src}{S('retitle_user_suffix')}"
    messages = [{"role": "system", "content": sys}, *examples, {"role": "user", "content": user_text}]
    out = llm.create_chat_completion(
        messages=messages,
        max_tokens=int(S("retitle_llm_max_tokens")),
        temperature=float(S("retitle_llm_temperature")),
        top_p=float(S("retitle_llm_top_p")),
        stream=False,
        stop=S("retitle_llm_stop"),
    )
    raw = (out["choices"][0]["message"]["content"] or "").strip().strip('"').strip("'")
    return raw

async def start_worker():
    while True:
        sid = await _queue.get()
        try:
            await _process_session(sid)
        except Exception:
            logging.exception("Retitle worker failed")
        finally:
            _queue.task_done()

def _extract_job(snapshot: dict) -> Tuple[List[dict], int]:
    msgs = snapshot.get("messages") or []
    job_seq = int(snapshot.get("job_seq") or 0)
    return msgs, job_seq

async def _process_session(session_id: str):
    if not bool(S("retitle_enable")):
        return
    await asyncio.sleep(int(S("retitle_grace_ms")) / 1000.0)
    waited = 0
    backoff = int(S("retitle_active_backoff_start_ms"))
    backoff_max = int(S("retitle_active_backoff_max_ms"))
    backoff_total = int(S("retitle_active_backoff_total_ms"))
    growth = float(S("retitle_active_backoff_growth"))
    while is_active(session_id) and waited < backoff_total:
        await asyncio.sleep(backoff / 1000.0)
        waited += backoff
        backoff = min(int(backoff * growth), backoff_max)
    async with _lock:
        snapshot = _PENDING.pop(session_id, None)
        _ENQUEUED.discard(session_id)
    if not snapshot:
        return
    messages, job_seq = _extract_job(snapshot)
    try:
        cur_seq = int((_load_chat(session_id) or {}).get("seq") or 0)
    except Exception:
        cur_seq = job_seq
    if cur_seq > job_seq:
        print(f"[retitle] SKIP (stale) session={session_id} job_seq={job_seq} current_seq={cur_seq}")
        return
    src = _pick_source(messages) or ""
    if not src.strip():
        return
    print(f"[retitle] START session={session_id} job_seq={job_seq} src={_preview(src)!r}")
    async with GEN_SEMAPHORE:
        llm = get_llm()
        try:
            title_raw = await asyncio.to_thread(_make_title, llm, src)
        except Exception as e:
            logging.exception("retitle: LLM error: %s", e)
            return
        finally:
            try:
                llm.reset()
            except Exception:
                pass
    if bool(S("retitle_enable_sanitize")):
        title = _sanitize_title(title_raw)
    else:
        title = title_raw
    print(f"[retitle] FINISH session={session_id} -> {title!r}")
    if not title:
        return
    idx = load_index()
    row = next((r for r in idx if r.get("sessionId") == session_id), None)
    if not row:
        return
    if (row.get("title") or "").strip() == title:
        return
    row["title"] = title
    row["updatedAt"] = now_iso()
    save_index(idx)

def enqueue(session_id: str, messages: List[dict], *, job_seq: Optional[int] = None):
    if not session_id:
        return
    if not isinstance(messages, list):
        messages = []
    if job_seq is None:
        try:
            job_seq = max(int(m.get("id") or 0) for m in messages) if messages else 0
        except Exception:
            job_seq = 0
    snap = {"messages": messages, "job_seq": int(job_seq)}
    async def _put():
        async with _lock:
            _PENDING[session_id] = snap
            if session_id not in _ENQUEUED:
                _ENQUEUED.add(session_id)
                try:
                    _queue.put_nowait(session_id)
                except Exception as e:
                    logging.warning(f"Failed to enqueue retitle: {e}")
    try:
        loop = asyncio.get_running_loop()
        loop.create_task(_put())
    except RuntimeError:
        asyncio.run(_put())

# ===== frontend/src/file_read/App.tsx =====


import AgentRunner from "./pages/AgentRunner";

export default function App() {
  return (
    <main className="bg-gray-50 min-h-screen">
      <AgentRunner />
    </main>
  );
}

# ===== frontend/src/file_read/components/AssistantMetrics.tsx =====

// frontend/src/file_read/components/chat/AssistantMetrics.tsx
import { Info } from "lucide-react";
import MetricsHoverCard from "./MetricsHoverCard";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function AssistantMetrics({
  status,
  runJson,
  flat,
  align = "right",
}: { status: string; runJson?: RunJson | null; flat?: GenMetrics | null; align?: "left" | "right" }) {
  return (
    <div className="mt-2 flex justify-start">
      <div className="inline-flex items-center gap-1 px-2 py-1 rounded-full bg-white border shadow-sm text-[11px] text-gray-600">
        <Info className="w-3.5 h-3.5 opacity-70" />
        <span className="truncate max-w-[70vw] sm:max-w-none">{status || "Run details"}</span>
        <MetricsHoverCard
          data={
            runJson ??
            (flat
              ? {
                  stats: {
                    stopReason: flat.stop_reason ?? null,
                    tokensPerSecond: flat.tok_per_sec ?? null,
                    timeToFirstTokenSec: flat.ttft_ms != null ? Math.max(0, flat.ttft_ms) / 1000 : null,
                    totalTimeSec: null,
                    promptTokensCount: flat.input_tokens_est ?? null,
                    predictedTokensCount: flat.output_tokens ?? null,
                    totalTokensCount: flat.total_tokens_est ?? null,
                  },
                }
              : null)
          }
          title="Run JSON"
          align={align}
          compact
        />
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatBubble.tsx =====

import { useState } from "react";
import { Copy, Check, Trash2 } from "lucide-react";
import MarkdownMessage from "./Markdown/MarkdownMessage";
import { stripRunJson } from "../shared/lib/runjson";
import type { Attachment } from "../types/chat";   // ✅ import Attachment type

const STOP_SENTINEL_RE = /(?:\r?\n)?(?:\u23F9|\\u23F9)\s+stopped(?:\r?\n)?$/u;

export default function ChatBubble({
  role,
  text,
  attachments = [],   // ✅ new prop
  showActions = true,
  onDelete,
}: {
  role: "user" | "assistant";
  text: string;
  attachments?: Attachment[];   // ✅ allow attachments
  showActions?: boolean;
  onDelete?: () => void;
}) {
  const isUser = role === "user";
  const raw = text ?? "";
  const { text: stripped } = stripRunJson(raw);
  let content = stripped.trim();

  if (!isUser) content = content.replace(STOP_SENTINEL_RE, "");

  const hasOnlyAttachments =
    isUser && (!content || content.length === 0) && attachments.length > 0;

  if (role === "assistant" && !content && attachments.length === 0) return null;

  const [copiedMsg, setCopiedMsg] = useState(false);
  const copyWholeMessage = async () => {
    try {
      await navigator.clipboard.writeText(content);
      setCopiedMsg(true);
      setTimeout(() => setCopiedMsg(false), 2000);
    } catch {}
  };

  return (
    <div className="mb-2">
      <div className={`flex ${isUser ? "justify-end" : "justify-start"}`}>
        <div
          className={`max-w-[80%] w-fit break-words rounded-2xl px-4 py-2 shadow-sm
                      prose prose-base max-w-none
            ${isUser ? "bg-black text-white prose-invert" : "bg-white border text-gray-900"}`}
        >
          {/* ✅ Render attachments */}
          {attachments.length > 0 && (
            <div className="mb-2 flex flex-wrap gap-2">
              {attachments.map((att) => (
                <div
                  key={`${att.sessionId || "global"}:${att.source || att.name}`}
                  className={`border rounded px-2 py-1 text-sm flex items-center gap-2 ${
                    isUser
                      ? "bg-white/10 border-white/30"
                      : "bg-white"
                  }`}
                  title={att.name || att.source}
                >
                  📎 <span className="truncate max-w-[220px]">{att.name}</span>
                </div>
              ))}
            </div>
          )}

          {content ? (
            <div className="max-w-full">
              <MarkdownMessage text={content} />
            </div>
          ) : hasOnlyAttachments ? null : isUser ? null : (
            <span className="opacity-60">…</span>
          )}
        </div>
      </div>

      {showActions && (
        <div className={`mt-1 flex ${isUser ? "justify-end" : "justify-start"}`}>
          <div className="flex items-center gap-2">
            <button
              type="button"
              onClick={copyWholeMessage}
              title={copiedMsg ? "Copied" : "Copy"}
              aria-label={copiedMsg ? "Copied" : "Copy message"}
              className="inline-flex items-center justify-center w-7 h-7 rounded border
                         bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
            >
              {copiedMsg ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
            </button>
            {onDelete && (
              <button
                type="button"
                onClick={onDelete}
                title="Delete message"
                aria-label="Delete message"
                className="inline-flex items-center justify-center w-7 h-7 rounded border
                           bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition"
              >
                <Trash2 className="w-4 h-4" />
              </button>
            )}
          </div>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatComposer.tsx =====

// frontend/src/file_read/components/ChatComposer.tsx
import { useEffect, useRef, useState } from "react";
import ComposerActions from "./Composer/ComposerActions";
import AttachmentChip from "./Composer/AttachmentChip";
import { useAttachmentUploads } from "../hooks/useAttachmentUploads";
import type { Attachment } from "../types/chat";

const FORCE_SCROLL_EVT = "chat:force-scroll-bottom";

type Props = {
  input: string;
  setInput: (v: string) => void;
  loading: boolean;
  queued?: boolean;
  onSend: (text: string, attachments?: Attachment[]) => void | Promise<void>;
  onStop: () => void | Promise<void>;
  onHeightChange?: (h: number) => void;
  onRefreshChats?: () => void;
  sessionId?: string;
};

export default function ChatComposer({
  input,
  setInput,
  loading,
  queued = false,
  onSend,
  onStop,
  onHeightChange,
  onRefreshChats,
  sessionId,
}: Props) {
  const wrapRef = useRef<HTMLDivElement>(null);
  const taRef = useRef<HTMLTextAreaElement>(null);
  const fileRef = useRef<HTMLInputElement>(null);
  const MAX_HEIGHT_PX = 192;

  const [isClamped, setIsClamped] = useState(false);
  const [draft, setDraft] = useState(input);

  const { atts, addFiles, removeAtt, anyUploading, anyReady, attachmentsForPost, reset } =
    useAttachmentUploads(sessionId, onRefreshChats);

  useEffect(() => setDraft(input), [input]);

  const autogrow = () => {
    const ta = taRef.current;
    if (!ta) return;
    ta.style.height = "auto";
    const next = Math.min(ta.scrollHeight, MAX_HEIGHT_PX);
    ta.style.height = `${next}px`;
    setIsClamped(ta.scrollHeight > MAX_HEIGHT_PX);
    if (wrapRef.current && onHeightChange) {
      onHeightChange(wrapRef.current.getBoundingClientRect().height);
    }
  };

  useEffect(() => {
    autogrow();
    const onResize = () => autogrow();
    window.addEventListener("resize", onResize);
    return () => window.removeEventListener("resize", onResize);
  }, []);

  useEffect(() => { autogrow(); }, [draft, atts.length]);

  const hasText = draft.trim().length > 0;

  const forceScroll = (behavior: ScrollBehavior = "auto") => {
    window.dispatchEvent(new CustomEvent(FORCE_SCROLL_EVT, { detail: { behavior } }));
  };

  const handleSendClick = async () => {
    const v = draft.trim();
    if ((loading || queued) || (!v && !anyReady) || anyUploading) return;
    forceScroll("auto");
    setDraft("");
    setInput("");
    reset();
    try {
      await onSend(v, attachmentsForPost());
    } finally {
      onRefreshChats?.();
      requestAnimationFrame(() => forceScroll("smooth"));
    }
  };

  const handleStopClick = () => {
    if (!loading && !queued) return;
    void Promise.resolve(onStop()).finally(() => onRefreshChats?.());
  };

  const pickFile = () => fileRef.current?.click();

  const onFilePicked: React.ChangeEventHandler<HTMLInputElement> = async (e) => {
    const files = e.target.files;
    if (!files || files.length === 0) return;
    if (!sessionId) { e.target.value = ""; return; }
    await addFiles(files);
    e.target.value = "";
  };

  function onKeyDown(e: React.KeyboardEvent<HTMLTextAreaElement>) {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      void handleSendClick();
    }
  }

  const disableActions = loading || queued || anyUploading;
  const showSend = hasText || anyReady;

  return (
    <div ref={wrapRef} className="relative z-50 bg-white/95 backdrop-blur border-t p-3">
      {atts.length > 0 && (
        <div className="mb-2 flex flex-wrap gap-2">
          {atts.map((a) => (
            <AttachmentChip key={a.id} a={a} onRemove={removeAtt} />
          ))}
        </div>
      )}

      <div className="flex gap-2">
        <input ref={fileRef} type="file" multiple className="hidden" onChange={onFilePicked} />

        <textarea
          ref={taRef}
          value={draft}
          onChange={(e) => { setDraft(e.target.value); setInput(e.target.value); autogrow(); }}
          onInput={autogrow}
          onKeyDown={onKeyDown}
          placeholder="Ask anything…"
          className={`flex-1 border rounded-lg px-3 py-2 resize-none focus:outline-none focus:ring ${
            isClamped ? "overflow-y-auto" : "overflow-hidden"
          }`}
          rows={1}
          style={{ maxHeight: MAX_HEIGHT_PX }}
          disabled={queued}
        />

        <ComposerActions
          disabledUpload={disableActions || !sessionId}
          onPickFile={pickFile}
          showStop={loading || queued}
          onStop={handleStopClick}
          showSend={showSend}
          onSend={handleSendClick}
        />
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatContainer.tsx =====

// frontend/src/file_read/components/ChatContainer.tsx
import { useState, useRef, useEffect } from "react";
import ChatView from "./ChatView/ChatView";
import ChatComposer from "./ChatComposer";
import type { ChatMsg } from "../types/chat";
import type { GenMetrics, RunJson } from "../hooks/useChatStream";
import type { Attachment } from "../types/chat";

interface Props {
  messages: ChatMsg[];
  input: string;
  setInput: (s: string) => void;
  loading: boolean;
  queued?: boolean;
  send: (text?: string, attachments?: Attachment[]) => Promise<void>;
  stop: () => Promise<void> | void;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onRefreshChats?: () => void;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
  sessionId?: string;
}

export default function ChatContainer({
  messages,
  input,
  setInput,
  loading,
  queued = false,
  send,
  stop,
  runMetrics,
  runJson,
  onRefreshChats,
  onDeleteMessages,
  autoFollow = true,
  sessionId,
}: Props) {
  const [composerH, setComposerH] = useState(0);
  const containerRef = useRef<HTMLDivElement>(null);
  const [pinned, setPinned] = useState(false);

  useEffect(() => {
    const el = containerRef.current;
    if (!el) return;
    const threshold = 120;
    const isNearBottom = () =>
      el.scrollHeight - el.scrollTop - el.clientHeight <= threshold;
    const onScroll = () => setPinned(!isNearBottom());
    el.addEventListener("scroll", onScroll, { passive: true });
    setPinned(!isNearBottom());
    return () => el.removeEventListener("scroll", onScroll);
  }, []);

  const forceScrollToBottom = (behavior: ScrollBehavior = "smooth") => {
    const el = containerRef.current;
    if (!el) return;
    el.scrollTo({ top: el.scrollHeight, behavior });
  };

  const handleSend = async (text?: string, attachments?: Attachment[]) => {
    if (!pinned) forceScrollToBottom("auto");
    await send(text, attachments);
    onRefreshChats?.();
    if (!pinned) requestAnimationFrame(() => forceScrollToBottom("smooth"));
  };

  return (
    <div className="flex flex-col h-full border rounded-lg overflow-hidden bg-white">
      <div ref={containerRef} data-chat-scroll className="flex-1 overflow-y-auto min-w-0">
        <ChatView
          messages={messages}
          loading={loading}
          queued={queued}
          bottomPad={composerH}
          runMetrics={runMetrics}
          runJson={runJson}
          onDeleteMessages={onDeleteMessages}
          autoFollow={autoFollow}
        />
      </div>

      <ChatComposer
        input={input}
        setInput={setInput}
        loading={loading}
        queued={queued}
        onSend={handleSend}
        onStop={stop}
        onHeightChange={setComposerH}
        onRefreshChats={onRefreshChats}
        sessionId={sessionId}
      />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatItem.tsx =====

// frontend/src/file_read/components/chat/ChatItem.tsx
import ChatBubble from "./ChatBubble";
import AssistantMetrics from "./AssistantMetrics";
import { buildStatus } from "./ChatView/StatusLine";
import type { ChatMsg } from "../types/chat";
import type { RunJson, GenMetrics } from "../shared/lib/runjson";

export default function ChatItem({
  m,
  idx,
  loading,
  lastAssistantIndex,
  runJsonLive,
  runMetricsLive,
  onDelete,
}: {
  m: ChatMsg;
  idx: number;
  loading: boolean;
  lastAssistantIndex: number;
  runJsonLive?: RunJson | null;
  runMetricsLive?: GenMetrics | null;
  onDelete?: (id: string) => void;
}) {
  const isAssistant = m.role === "assistant";
  const isCurrentStreamingAssistant = isAssistant && loading && idx === lastAssistantIndex;

  let jsonForThis: RunJson | null = null;
  let flatForThis: GenMetrics | null = null;

  if (isAssistant) {
    // @ts-ignore meta bag
    const meta = m.meta as { runJson?: RunJson | null; flat?: GenMetrics | null } | undefined;
    jsonForThis = meta?.runJson ?? null;
    flatForThis = meta?.flat ?? null;

    if (isCurrentStreamingAssistant) {
      if (runJsonLive) jsonForThis = runJsonLive;
      if (runMetricsLive) flatForThis = runMetricsLive;
    }
  }

  const status = isAssistant ? buildStatus(jsonForThis, flatForThis) : "";
  const showMetrics = isAssistant && (jsonForThis || flatForThis);

  return (
    <div>
      <ChatBubble
        role={m.role}
        text={m.text}
        attachments={m.attachments} 
        showActions={m.role === "user" || (m.role === "assistant" && !isCurrentStreamingAssistant)}
        onDelete={onDelete ? () => onDelete(m.id) : undefined}
      />
      {showMetrics && <AssistantMetrics status={status} runJson={jsonForThis} flat={flatForThis} />}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/ChatSidebar.tsx =====

import { useState } from "react";
import { deleteChatsBatch } from "../../data/chatApi";
import type { ChatRow } from "../../types/chat";
import { useMultiSelect } from "../../hooks/useMultiSelect";
import { useChatsPager } from "../../hooks/useChatsPager";
import SidebarHeader from "./SidebarHeader";
import SidebarListItem from "./SidebarListItem";

const PAGE_SIZE = 10;

type Props = {
  onOpen: (id: string) => Promise<void>;
  onNew: () => Promise<void>;
  refreshKey?: number;
  activeId?: string;
  onHideSidebar?: () => void;
  onCancelSessions?: (ids: string[]) => Promise<void>;
};

export default function ChatSidebar({
  onOpen, onNew, refreshKey, activeId, onHideSidebar, onCancelSessions,
}: Props) {
  const {
    chats, page, hasMore, total, totalPages,
    initialLoading, loadingMore,
    scrollRef, sentinelRef, loadMore, setChats,
  } = useChatsPager(PAGE_SIZE, refreshKey);

  const [isEditing, setIsEditing] = useState(false);
  const [deleting, setDeleting] = useState(false);
  const [newPending, setNewPending] = useState(false);

  const allIds = chats.map(c => c.sessionId);
  const { selected, setSelected, allSelected, toggleOne, toggleAll } = useMultiSelect(allIds);

  async function handleNew() {
    if (newPending) return;
    setNewPending(true);
    try { await onNew(); } finally { setNewPending(false); }
  }

  async function onDeleteSelected(): Promise<void> {
    const count = selected.size;
    if (!count || deleting) return;

    const isAll = count === chats.length;
    const ok = window.confirm(
      isAll
        ? `Delete ALL ${count} chats? This cannot be undone.`
        : `Delete ${count} selected chat${count > 1 ? "s" : ""}?`
    );
    if (!ok) return;

    const ids = [...selected];
    try { await onCancelSessions?.(ids); await Promise.resolve(); } catch {}

    const deletingActive = activeId ? selected.has(activeId) : false;
    const fallback = chats.find(c => !selected.has(c.sessionId))?.sessionId;

    setDeleting(true);
    try {
      const deleted = await deleteChatsBatch(ids);
      if (!deleted.length) return;
      setChats(prev => prev.filter(c => !deleted.includes(c.sessionId)));
      setSelected(new Set());
      setIsEditing(false);

      if (deletingActive) {
        if (fallback) void onOpen(fallback);
        else void onOpen("");
      }
    } finally {
      setDeleting(false);
    }
  }

  return (
    <aside className="w-full md:w-72 h-full border-r bg-white p-0 flex flex-col">
      <SidebarHeader
        isEditing={isEditing}
        setIsEditing={(v) => { setIsEditing(v); setSelected(new Set()); }}
        newPending={newPending}
        onNew={handleNew}
        onHideSidebar={onHideSidebar}
        selectedCount={selected.size}
        deleting={deleting}
        onDelete={onDeleteSelected}
      />

      {/* LIST */}
      <div
        ref={scrollRef}
        className="flex-1 overflow-y-auto p-2 overscroll-contain"
        style={{ WebkitOverflowScrolling: "touch" }}
      >
        {initialLoading && (
          <div className="px-2 py-1 text-xs text-gray-500">Loading…</div>
        )}

        <ul className="space-y-1">
          {chats.map((c: ChatRow) => (
            <SidebarListItem
              key={c.sessionId}
              c={c}
              isActive={activeId === c.sessionId}
              isEditing={isEditing}
              isChecked={selected.has(c.sessionId)}
              onToggle={() => toggleOne(c.sessionId)}
              onOpen={() => void onOpen(c.sessionId)}
            />
          ))}
        </ul>

        <div className="h-6" ref={sentinelRef} />

        {hasMore && (
          <div className="px-2 pb-2">
            <button
              className={`w-full text-xs px-3 py-1 rounded border ${loadingMore ? "opacity-50 cursor-wait" : ""}`}
              onClick={() => void loadMore()}
              disabled={loadingMore}
              title="Load next page"
            >
              {loadingMore ? "Loading…" : `Load more (${chats.length}/${total || "?"})`}
            </button>
          </div>
        )}

        {!hasMore && chats.length > 0 && (
          <div className="px-2 py-2 text-[11px] text-gray-400 text-center">
            End of list • showing {chats.length} of {total || chats.length}
          </div>
        )}
      </div>

      {/* FOOTER META */}
      <div className="border-t px-3 py-2 text-[11px] text-gray-500">
        <span className="uppercase tracking-wide">Chats</span>{" "}
        <span className="text-gray-400">
          ({chats.length}{total ? `/${total}` : ""} • page {Math.max(page, 1)} of {Math.max(totalPages || 1, 1)})
        </span>
        {isEditing && (
          <label className="ml-2 text-[11px]">
            <input
              type="checkbox"
              className="mr-1 align-middle"
              checked={allSelected}
              onChange={toggleAll}
            />
            Select all
          </label>
        )}
      </div>
    </aside>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/SidebarHeader.tsx =====

import { PanelLeftClose, Plus, Pencil, Trash2 } from "lucide-react";

export default function SidebarHeader({
  isEditing, setIsEditing, newPending, onNew, onHideSidebar,
  selectedCount, deleting, onDelete,
}: {
  isEditing: boolean;
  setIsEditing: (v: boolean) => void;
  newPending: boolean;
  onNew: () => Promise<void>;
  onHideSidebar?: () => void;
  selectedCount: number;
  deleting: boolean;
  onDelete: () => void;
}) {
  return (
    <div className="sticky top-0 z-10 bg-white border-b">
      <div className="flex items-center justify-between px-3 py-2">
        <div className="text-[11px] md:text-xs uppercase text-gray-500">Chats</div>
        <div className="flex items-center gap-2">
          <button
            className={`h-9 px-3 inline-flex items-center gap-2 justify-center rounded border ${
              newPending ? "opacity-50 cursor-not-allowed" : ""
            }`}
            onClick={async () => { if (!newPending) await onNew(); }}
            title="New chat"
            disabled={newPending}
          >
            <Plus className="w-4 h-4" />
            <span className="text-xs md:text-[11px] leading-none">New</span>
          </button>

          <button
            className="h-9 px-3 inline-flex items-center gap-2 justify-center rounded border"
            onClick={() => setIsEditing(!isEditing)}
            aria-pressed={isEditing}
            title={isEditing ? "Exit edit mode" : "Edit chats"}
          >
            <Pencil className="w-4 h-4" />
            <span className="text-xs md:text-[11px] leading-none">
              {isEditing ? "Done" : "Edit"}
            </span>
          </button>

          {onHideSidebar && (
            <button
              className="hidden md:inline-flex h-9 w-9 items-center justify-center rounded border"
              onClick={onHideSidebar}
              title="Hide sidebar"
              aria-label="Hide sidebar"
            >
              <PanelLeftClose className="w-4 h-4" />
            </button>
          )}
        </div>
      </div>

      {isEditing && (
        <div className="px-3 py-2 border-t bg-white flex items-center gap-3">
          <div className="text-sm text-gray-600">{selectedCount} selected</div>
          <button
            className={`ml-auto inline-flex items-center gap-2 text-sm px-3 py-1 rounded ${
              selectedCount && !deleting
                ? "bg-red-600 text-white"
                : "bg-gray-200 text-gray-500 cursor-not-allowed"
            }`}
            disabled={!selectedCount || deleting}
            onClick={onDelete}
            title={selectedCount ? "Delete selected chats" : "Select chats to delete"}
          >
            <Trash2 className="w-4 h-4" />
            {deleting ? "Deleting…" : `Delete (${selectedCount})`}
          </button>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatSidebar/SidebarListItem.tsx =====

import { firstLineSmart } from "../../shared/lib/text";
import type { ChatRow } from "../../types/chat";

export default function SidebarListItem({
  c, isActive, isEditing, isChecked, onToggle, onOpen,
}: {
  c: ChatRow;
  isActive: boolean;
  isEditing: boolean;
  isChecked: boolean;
  onToggle: () => void;
  onOpen: () => void;
}) {
  const displayTitle =
    (c.title && c.title.trim()) ||
    firstLineSmart(c.lastMessage || "", 48) ||
    "New Chat";
  const preview = c.lastMessage ? firstLineSmart(c.lastMessage, 120) : "";

  return (
    <li>
      <div
        className={`w-full flex items-start gap-2 px-2 py-2 rounded ${
          isActive ? "bg-black text-white" : "hover:bg-gray-50"
        }`}
      >
        {isEditing && (
          <input
            type="checkbox"
            checked={isChecked}
            onChange={onToggle}
            className="mt-1"
            aria-label={`Select chat ${displayTitle}`}
          />
        )}
        <button
          className="text-left flex-1"
          aria-current={isActive ? "true" : undefined}
          onClick={() => { if (!isEditing) onOpen(); }}
          title={displayTitle}
        >
          <div className="font-medium truncate">{displayTitle}</div>
          {preview && (
            <div className={`text-xs line-clamp-2 ${isActive ? "text-white/80" : "text-gray-500"}`}>
              {preview}
            </div>
          )}
          <div className="text-[10px] mt-1 opacity-60">
            {new Date(c.updatedAt).toLocaleString()}
          </div>
        </button>
      </div>
    </li>
  );
}

# ===== frontend/src/file_read/components/ChatView/ChatView.tsx =====

// frontend/src/file_read/components/ChatView/ChatView.tsx
import type { ChatMsg } from "../../types/chat";
import type { GenMetrics, RunJson } from "../../hooks/useChatStream";
import ChatItem from "../ChatItem";
import TypingIndicator from "../../shared/ui/TypingIndicator";
import { useChatAutofollow } from "../../hooks/useChatAutoFollow";

export default function ChatView({
  messages,
  loading,
  queued = false,
  bottomPad,
  runMetrics,
  runJson,
  onDeleteMessages,
  autoFollow = true,
}: {
  messages: ChatMsg[];
  loading: boolean;
  queued?: boolean;
  bottomPad: number;
  runMetrics?: GenMetrics | null;
  runJson?: RunJson | null;
  onDeleteMessages?: (ids: string[]) => void;
  autoFollow?: boolean;
}) {
  const { listRef, bottomRef, lastAssistantIndex } = useChatAutofollow({
    messages,
    loading,
    autoFollow,
    bottomPad,
  });

  const lastMsg = messages[messages.length - 1];

  return (
    <div
      ref={listRef}
      className="p-4 space-y-3 bg-gray-50 min-w-0"
      style={{ paddingBottom: bottomPad }}
    >
      {messages.map((m, idx) => (
        <ChatItem
          key={m.id}
          m={m}
          idx={idx}
          loading={loading}
          lastAssistantIndex={lastAssistantIndex}
          runJsonLive={runJson ?? null}
          runMetricsLive={runMetrics ?? null}
          onDelete={onDeleteMessages ? (id) => onDeleteMessages([id]) : undefined}
        />
      ))}

      {(loading || queued) &&
        !(lastMsg?.role === "assistant" && (lastMsg.text?.trim().length ?? 0) > 0) && (
          <TypingIndicator />
        )}

      <div ref={bottomRef} className="h-0" />
    </div>
  );
}

# ===== frontend/src/file_read/components/ChatView/StatusLine.ts =====

// frontend/src/file_read/components/chat/StatusLine.ts
import type { RunJson, GenMetrics } from "../../shared/lib/runjson";

const oneDec = (n?: number | null) =>
  typeof n === "number" && Number.isFinite(n) ? n.toFixed(1) : undefined;

export function buildStatus(json?: RunJson | null, flat?: GenMetrics | null) {
  const st = json?.stats;
  if (st) {
    const parts: string[] = [];
    if (st.predictedTokensCount != null) parts.push(`${st.predictedTokensCount} tok`);
    if (st.tokensPerSecond != null) parts.push(`${oneDec(st.tokensPerSecond) ?? st.tokensPerSecond} tok/s`);
    if (st.timeToFirstTokenSec != null) parts.push(`TTFT ${Math.round(st.timeToFirstTokenSec * 1000)} ms`);
    if (st.stopReason) parts.push(`stop: ${st.stopReason}`);
    return parts.join(" • ");
  }
  if (flat) {
    const parts: string[] = [];
    if (flat.output_tokens != null) parts.push(`${flat.output_tokens} tok`);
    if (flat.tok_per_sec != null) parts.push(`${oneDec(flat.tok_per_sec) ?? flat.tok_per_sec} tok/s`);
    if (flat.ttft_ms != null) parts.push(`TTFT ${Math.round(flat.ttft_ms)} ms`);
    if (flat.stop_reason) parts.push(`stop: ${flat.stop_reason}`);
    return parts.join(" • ");
  }
  return "";
}

# ===== frontend/src/file_read/components/Composer/AttachmentChip.tsx =====

import { X, Check } from "lucide-react";
import ProgressBar from "./ProgressBar";
import type { Att } from "../../hooks/useAttachmentUploads";

export default function AttachmentChip({ a, onRemove }: { a: Att; onRemove: (a: Att) => void }) {
  return (
    <div className="min-w-[160px] max-w-[280px] border rounded-lg px-2 py-2">
      <div className="flex items-center justify-between gap-2">
        <div className="truncate text-sm" title={a.name}>{a.name}</div>
        <button className="p-1 rounded hover:bg-gray-100" aria-label="Remove file" onClick={() => onRemove(a)}>
          <X size={14} />
        </button>
      </div>
      <ProgressBar pct={a.pct} error={a.status === "error"} />
      <div className="mt-1 text-xs text-gray-500 flex items-center gap-1">
        {a.status === "uploading" && <span>Uploading… {a.pct}%</span>}
        {a.status === "ready" && <><Check size={14} /> Ready</>}
        {a.status === "error" && <span>Error</span>}
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Composer/ComposerActions.tsx =====

import { Paperclip, Square, SendHorizonal } from "lucide-react";

type Props = {
  disabledUpload: boolean;
  onPickFile: () => void;
  showStop: boolean;
  onStop: () => void;
  showSend: boolean;
  onSend: () => void;
};

export default function ComposerActions({
  disabledUpload,
  onPickFile,
  showStop,
  onStop,
  showSend,
  onSend,
}: Props) {
  return (
    <div className="flex items-end gap-2">
      <button
        className={`p-2 rounded-lg border hover:bg-gray-50 ${disabledUpload ? "opacity-60 cursor-not-allowed" : ""}`}
        onClick={onPickFile}
        title="Upload to this chat"
        aria-label="Upload to this chat"
        disabled={disabledUpload}
      >
        <Paperclip size={18} />
      </button>

      {showStop ? (
        <button
          className="p-2 rounded-lg border hover:bg-gray-50"
          onClick={onStop}
          title="Stop generating"
          aria-label="Stop generating"
        >
          <Square size={18} />
        </button>
      ) : showSend ? (
        <button
          className="p-2 rounded-lg bg-black text-white hover:bg-black/90 active:translate-y-px"
          onClick={onSend}
          title="Send"
          aria-label="Send"
        >
          <SendHorizonal size={18} />
        </button>
      ) : null}
    </div>
  );
}

# ===== frontend/src/file_read/components/Composer/ProgressBar.tsx =====

export default function ProgressBar({ pct, error }: { pct: number; error?: boolean }) {
  return (
    <div className="mt-2 h-1.5 w-full bg-gray-200 rounded">
      <div
        className={`h-1.5 rounded ${error ? "bg-red-500" : "bg-black"}`}
        style={{ width: `${pct}%` }}
      />
    </div>
  );
}

# ===== frontend/src/file_read/components/DesktopHeader.tsx =====

import { PanelLeftOpen } from "lucide-react";

export default function DesktopHeader({
  sidebarOpen,
  onShowSidebar,
  title = "Local AI Model",
}: {
  sidebarOpen: boolean;
  onShowSidebar: () => void;
  title?: string;
}) {
  return (
    <div className="hidden md:flex h-14 shrink-0 items-center justify-between px-4 border-b bg-white">
      <div className="flex items-center gap-2">
        {!sidebarOpen && (
          <button
            className="h-9 w-9 inline-flex items-center justify-center rounded-lg border hover:bg-gray-50"
            onClick={onShowSidebar}
            aria-label="Show sidebar"
            title="Show sidebar"
          >
            <PanelLeftOpen className="w-4 h-4" />
          </button>
        )}
        <div className="font-semibold">{title}</div>
      </div>
      <div />
    </div>
  );
}

# ===== frontend/src/file_read/components/KnowledgePanel.tsx =====

import { useState, useEffect } from "react";
import {
  uploadRag,
  searchRag,
  listUploads,
  deleteUploadHard,
  type UploadRow,
} from "../data/ragApi";

export default function KnowledgePanel({
  sessionId,
  onClose,
  toast,
}: {
  sessionId?: string;
  onClose?: () => void;
  toast?: (msg: string) => void;
}) {
  const [files, setFiles] = useState<FileList | null>(null);
  const [busy, setBusy] = useState(false);
  const [query, setQuery] = useState("");
  const [hits, setHits] = useState<{ text: string; source?: string; score: number }[]>([]);
  const [searching, setSearching] = useState(false);

  const [scope, setScope] = useState<"all" | "session">("all");
  const [uploads, setUploads] = useState<UploadRow[]>([]);
  const [loadingUploads, setLoadingUploads] = useState(false);

  useEffect(() => {
    void refreshUploads();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [scope, sessionId]);

  async function refreshUploads() {
    setLoadingUploads(true);
    try {
      const out = await listUploads(sessionId, scope);
      setUploads(out.uploads || []);
    } catch (e: any) {
      toast?.(e?.message || "Failed to load uploads");
    } finally {
      setLoadingUploads(false);
    }
  }

  async function handleDeleteHard(source: string, ns?: string | null) {
    try {
      const res = await deleteUploadHard(source, ns ?? undefined);
      toast?.(`Removed ${res.removed} chunk${res.removed === 1 ? "" : "s"}. Remaining: ${res.remaining}`);
      await refreshUploads();
    } catch (e: any) {
      toast?.(e?.message || "Delete failed");
    }
  }

  async function doUpload() {
    if (!files || !files.length) return;
    setBusy(true);
    try {
      let total = 0;
      for (const f of Array.from(files)) {
        const out = await uploadRag(f, undefined);
        total += (out as any)?.added || 0;
      }
      toast?.(`Added ${total} chunk${total === 1 ? "" : "s"}`);
      setFiles(null);
      await refreshUploads();
    } catch (e: any) {
      toast?.(e?.message || "Upload failed");
    } finally {
      setBusy(false);
    }
  }

  async function doSearch() {
    const q = query.trim();
    if (!q) return;
    setSearching(true);
    try {
      const out = await searchRag(q, { sessionId, kChat: 6, kGlobal: 4, alpha: 0.5 });
      setHits(out.hits || []);
    } catch (e: any) {
      toast?.(e?.message || "Search failed");
    } finally {
      setSearching(false);
    }
  }

  return (
    <div className="fixed inset-0 z-50 bg-black/40 flex items-center justify-center p-3">
      <div className="w-full max-w-5xl rounded-2xl bg-white shadow-xl border overflow-hidden">
        <div className="px-4 py-3 border-b flex items-center gap-2">
          <div className="font-semibold">Knowledge</div>
          <div className="ml-auto flex items-center gap-2">
            <button className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50" onClick={onClose}>
              Close
            </button>
          </div>
        </div>

        <div className="p-4 grid gap-6 md:grid-cols-2">
          {/* Upload */}
          <div>
            <div className="font-medium mb-2">Upload documents</div>
            <input type="file" multiple className="block w-full text-sm" onChange={(e) => setFiles(e.target.files)} />
            <button
              className={`mt-2 text-sm px-3 py-1.5 rounded ${busy ? "opacity-60 cursor-not-allowed" : "bg-black text-white"}`}
              disabled={busy || !files || files.length === 0}
              onClick={doUpload}
            >
              {busy ? "Uploading…" : "Upload"}
            </button>
            <div className="text-[11px] text-gray-500 mt-2">
              Tip: CSV, TXT, MD, PDF (text extracted). Uploads can be global or per chat.
            </div>

            <div className="mt-6">
              <div className="flex items-center gap-2 mb-2">
                <div className="font-medium">Your uploads</div>
                <select
                  className="ml-auto border rounded px-2 py-1 text-xs"
                  value={scope}
                  onChange={(e) => setScope(e.target.value as "all" | "session")}
                  title="Scope"
                >
                  <option value="all">All (global + this chat)</option>
                  <option value="session">This chat only</option>
                </select>
                <button
                  className="text-xs px-2 py-1 rounded border hover:bg-gray-50"
                  onClick={refreshUploads}
                  disabled={loadingUploads}
                >
                  {loadingUploads ? "Refreshing…" : "Refresh"}
                </button>
              </div>

              <ul className="space-y-2 max-h-64 overflow-auto">
                {uploads.length === 0 && (
                  <li className="text-xs text-gray-500">No uploads yet.</li>
                )}
                {uploads.map((u, i) => (
                  <li key={`${u.source}-${u.sessionId ?? "global"}-${i}`} className="p-2 border rounded bg-gray-50">
                    <div className="flex items-center gap-2">
                      <div className="font-mono text-xs break-all">{u.source}</div>
                      <span className="text-[11px] text-gray-500">
                        {u.sessionId ? "session" : "global"} • {u.chunks} chunk{u.chunks === 1 ? "" : "s"}
                      </span>
                      <button
                        className="ml-auto text-xs px-2 py-1 rounded border hover:bg-gray-100"
                        title="Delete (hard delete by Source)"
                        onClick={() => handleDeleteHard(u.source, u.sessionId ?? undefined)}
                      >
                        Delete
                      </button>
                    </div>
                  </li>
                ))}
              </ul>
            </div>
          </div>

          {/* Search */}
          <div>
            <div className="font-medium mb-2">Quick search</div>
            <div className="flex gap-2">
              <input
                value={query}
                onChange={(e) => setQuery(e.target.value)}
                placeholder="Find in your knowledge…"
                className="flex-1 border rounded px-2 py-1.5 text-sm"
              />
              <button
                className={`text-sm px-3 py-1.5 rounded ${searching ? "opacity-60 cursor-wait" : "border hover:bg-gray-50"}`}
                onClick={doSearch}
                disabled={searching}
              >
                {searching ? "Searching…" : "Search"}
              </button>
            </div>

            <ul className="mt-3 space-y-2 max-h-64 overflow-auto">
              {hits.map((h, i) => (
                <li key={i} className="p-2 border rounded bg-gray-50">
                  <div className="text-[11px] text-gray-500 mb-1">
                    {h.source || "uploaded"} • score {Number.isFinite(h.score) ? h.score.toFixed(3) : "—"}
                  </div>
                  <div className="text-sm whitespace-pre-wrap">{h.text}</div>
                </li>
              ))}
              {!hits.length && <li className="text-xs text-gray-500">No results yet.</li>}
            </ul>
          </div>
        </div>

        <div className="px-4 py-3 border-t text-[11px] text-gray-500">
          “Delete” performs a hard delete: removes chunks for that Source and rebuilds the index.
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/components/Markdown/MarkdownMessage.tsx =====

// frontend/src/file_read/components/MarkdownMessage.tsx
import ReactMarkdown from "react-markdown";
import remarkGfm from "remark-gfm";
import rehypeHighlight from "rehype-highlight";
import "highlight.js/styles/github.css";
import CodeCopyButton from "../../shared/ui/CodeCopyButton";

type Props = { text: string };

export default function MarkdownMessage({ text }: Props) {
  return (
    <>
      {/* keep pre spacing at zero; don't overwrite token colors */}
      <style>{`
        pre { margin: 0 !important; padding: 0 !important; background: transparent !important; }
        pre code { display: block; margin: 0 !important; padding: 0 !important; }
        .hljs { background: transparent !important; }
      `}</style>

      <ReactMarkdown
        remarkPlugins={[remarkGfm]}
        rehypePlugins={[[rehypeHighlight, { detect: true, ignoreMissing: true }]]}
        components={{
          code({
            inline,
            className,
            children,
            ...props
          }: {
            inline?: boolean;
            className?: string;
            children?: React.ReactNode;
          }) {
            const raw = String(children ?? "");
            const lang = (className || "").replace("language-", "");

            if (inline) {
              return (
                <code
                  className="px-1.5 py-0.5 rounded bg-gray-100 text-gray-900 font-mono text-[14px]"
                  {...props}
                >
                  {children}
                </code>
              );
            }

            return (
              <div className="relative w-full">
                <pre className="m-0 p-0 w-full overflow-x-auto rounded-md border border-gray-300">
                  {/* Let hljs theme color tokens; no text color override here */}
                  <code className={`${className ?? ""} hljs font-mono text-sm`} {...props}>
                    {children}
                  </code>
                </pre>

                <div className="absolute top-2 right-2 flex items-center gap-1">
                  {lang && (
                    <span className="text-[11px] px-1.5 py-0.5 rounded bg-gray-200 text-gray-700">
                      {lang}
                    </span>
                  )}
                  <CodeCopyButton text={raw} />
                </div>
              </div>
            );
          },
        }}
      >
        {text}
      </ReactMarkdown>
    </>
  );
}

# ===== frontend/src/file_read/components/MetricsHoverCard.tsx =====

// frontend/src/file_read/components/MetricsHoverCard.tsx
import { useEffect, useLayoutEffect, useMemo, useRef, useState } from "react";
import { Info, Copy, Check, X } from "lucide-react";

type Props = {
  data: unknown;
  title?: string;
  align?: "left" | "right";
  maxWidthPx?: number;
  compact?: boolean;
};

export default function MetricsHoverCard({
  data,
  title = "Run details",
  align = "right",
  maxWidthPx = 460,
  compact = true,
}: Props) {
  const [open, setOpen] = useState(false);
  const [copied, setCopied] = useState(false);

  const btnRef = useRef<HTMLButtonElement>(null);
  const panelRef = useRef<HTMLDivElement>(null);

  const [panelStyle, setPanelStyle] = useState<{
    top: number;
    left: number;
    width: number;
  } | null>(null);

  const json = useMemo(() => {
    try {
      return JSON.stringify(data, null, 2);
    } catch {
      return String(data ?? "");
    }
  }, [data]);

  async function copy() {
    try {
      await navigator.clipboard.writeText(json);
      setCopied(true);
      window.setTimeout(() => setCopied(false), 1500);
    } catch {}
  }

  function close() {
    setOpen(false);
  }

  // Close on ESC
  useEffect(() => {
    if (!open) return;
    const onKey = (e: KeyboardEvent) => {
      if (e.key === "Escape") {
        e.preventDefault();
        close();
        btnRef.current?.focus();
      }
    };
    window.addEventListener("keydown", onKey);
    return () => window.removeEventListener("keydown", onKey);
  }, [open]);

  // Close on outside click
  useEffect(() => {
    if (!open) return;
    const onDown = (e: MouseEvent) => {
      const t = e.target as Node;
      if (panelRef.current?.contains(t)) return;
      if (btnRef.current?.contains(t)) return;
      close();
    };
    document.addEventListener("mousedown", onDown);
    return () => document.removeEventListener("mousedown", onDown);
  }, [open]);

  // Compute clamped viewport position when opening, and on resize/scroll
  useLayoutEffect(() => {
    function place() {
      if (!open || !btnRef.current) return;

      const margin = 8;
      const vw = window.innerWidth;
      const vh = window.innerHeight;
      const width = Math.min(maxWidthPx, vw - margin * 2);

      const btnBox = btnRef.current.getBoundingClientRect();
      let left =
        align === "right" ? btnBox.right - width : btnBox.left;
      left = Math.max(margin, Math.min(left, vw - margin - width));

      // Provisional top below the button; flip above if it would overflow
      let top = btnBox.bottom + margin;

      // We may not know the panel height yet; try to use current, else a max
      let panelH = panelRef.current?.offsetHeight || 0;
      if (!panelH) {
        panelH = 360 + 44; // body max (360) + header (~44) best-effort
      }

      if (top + panelH > vh - margin) {
        top = Math.max(margin, btnBox.top - margin - panelH);
      }

      setPanelStyle({ top, left, width });
    }

    place();
    if (!open) return;

    const onReflow = () => place();
    window.addEventListener("resize", onReflow);
    window.addEventListener("scroll", onReflow, true);
    return () => {
      window.removeEventListener("resize", onReflow);
      window.removeEventListener("scroll", onReflow, true);
    };
  }, [open, align, maxWidthPx]);

  return (
    <div className="relative inline-block">
      <button
        ref={btnRef}
        type="button"
        className={`inline-flex items-center justify-center rounded border bg-white text-gray-700 shadow-sm hover:bg-gray-50 transition ${
          compact ? "h-7 w-7" : "h-8 w-8"
        }`}
        title="Show run JSON"
        aria-haspopup="dialog"
        aria-expanded={open ? "true" : "false"}
        onClick={() => setOpen((v) => !v)}
        onMouseEnter={() => setOpen(true)}
      >
        <Info className={compact ? "w-4 h-4" : "w-5 h-5"} />
      </button>

      {open && panelStyle && (
        <div
          ref={panelRef}
          role="dialog"
          aria-label={title}
          className="fixed z-50"
          style={{
            top: panelStyle.top,
            left: panelStyle.left,
            width: panelStyle.width,
          }}
          onMouseLeave={close}
        >
          <div className="rounded-xl border bg-white shadow-xl overflow-hidden">
            <div className="px-3 py-2 border-b flex items-center justify-between bg-gray-50">
              <div className="text-xs font-semibold text-gray-700">{title}</div>
              <div className="flex items-center gap-1">
                <button
                  className="inline-flex items-center justify-center h-7 w-7 rounded border bg-white text-gray-700 hover:bg-gray-50"
                  onClick={copy}
                  title="Copy JSON"
                >
                  {copied ? <Check className="w-4 h-4" /> : <Copy className="w-4 h-4" />}
                </button>
                <button
                  className="inline-flex items-center justify-center h-7 w-7 rounded border bg-white text-gray-700 hover:bg-gray-50"
                  onClick={close}
                  title="Close"
                >
                  <X className="w-4 h-4" />
                </button>
              </div>
            </div>

            <div className="p-3">
              <pre className="m-0 p-0 text-xs leading-relaxed overflow-auto" style={{ maxHeight: 360 }}>
                <code>{json}</code>
              </pre>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}

# ===== frontend/src/file_read/components/MobileDrawer.tsx =====

import { useState } from "react";
import { PanelLeftOpen } from "lucide-react";
import ChatSidebar from "./ChatSidebar/ChatSidebar";

export default function MobileDrawer({ ...props }) {
  const {
    onOpenSession, onNewChat, refreshKey, activeId,
    openMobileDrawer, closeMobileDrawer,
  } = props;

  const [mounted, setMounted] = useState(false);

  return (
    <>
      {/* Mobile top bar */}
      <div className="md:hidden fixed top-0 left-0 right-0 z-40 bg-white border-b">
        <div className="h-14 flex items-center justify-between px-3">
          <button
            className="inline-flex items-center justify-center h-9 w-9 rounded-lg border hover:bg-gray-50"
            onClick={() => { setMounted(true); openMobileDrawer(); }}  // ⬅️ mount on open
            aria-label="Open sidebar"
            title="Open sidebar"
          >
            <PanelLeftOpen className="w-4 h-4" />
          </button>
          <div className="font-semibold">Local AI Model</div>
          <div className="w-9" />
        </div>
      </div>

      {/* Backdrop */}
      <div
        id="mobile-backdrop"
        className="md:hidden fixed inset-0 z-40 bg-black/40 hidden"
        onClick={() => { setMounted(false); closeMobileDrawer(); }}  // ⬅️ unmount on close
      />

      {/* Drawer */}
      <aside
        id="mobile-drawer"
        role="dialog"
        aria-modal="true"
        className="md:hidden fixed inset-y-0 left-0 z-50 w-80 max-w-[85vw] bg-white border-r shadow-xl hidden animate-[slideIn_.2s_ease-out]"
      >
        <div className="h-14 flex items-center justify-between px-3 border-b">
          <div className="font-medium">Chats</div>
          <button
            className="h-9 w-9 inline-flex items-center justify-center rounded-lg border hover:bg-gray-50"
            onClick={() => { setMounted(false); closeMobileDrawer(); }}  // ⬅️ unmount on close
            aria-label="Close sidebar"
          >
            <span className="rotate-45 text-xl leading-none">+</span>
          </button>
        </div>

        {mounted && (                                             // ⬅️ only mount when open
          <ChatSidebar
            onOpen={async (id) => { await onOpenSession(id); setMounted(false); closeMobileDrawer(); }}
            onNew={async () => { await onNewChat(); setMounted(false); closeMobileDrawer(); }}
            refreshKey={refreshKey}
            activeId={activeId}
          />
        )}
      </aside>

      <style>{`@keyframes slideIn{from{transform:translateX(-12px);opacity:.0}to{transform:translateX(0);opacity:1}}`}</style>
    </>
  );
}

# ===== frontend/src/file_read/components/SettingsPanel.tsx =====

import { useEffect, useMemo, useState } from "react";
import { useSettings } from "../hooks/useSettings";

export default function SettingsPanel({ sessionId, onClose }: { sessionId?: string; onClose?: () => void }) {
  const { loading, error, effective, overrides, defaults, adaptive, saveOverrides, runAdaptive, reload } =
    useSettings(sessionId);

  const [tab, setTab] = useState<"effective"|"overrides"|"adaptive"|"defaults">("effective");
  const [draft, setDraft] = useState(() => JSON.stringify(overrides ?? {}, null, 2));
  const [saveBusy, setSaveBusy] = useState(false);
  const [saveErr, setSaveErr] = useState<string | null>(null);

  useEffect(() => { setDraft(JSON.stringify(overrides ?? {}, null, 2)); }, [overrides]);

  const view = useMemo(() => {
    switch (tab) {
      case "effective": return effective;
      case "adaptive":  return adaptive;
      case "defaults":  return defaults;
      case "overrides": return null;
    }
  }, [tab, effective, adaptive, defaults]);

  async function onSave(method: "patch" | "put") {
    setSaveErr(null); setSaveBusy(true);
    try {
      const parsed = draft.trim() ? JSON.parse(draft) : {};
      await saveOverrides(parsed, method);
    } catch (e: any) {
      setSaveErr(e?.message || "Invalid JSON or save failed");
    } finally {
      setSaveBusy(false);
    }
  }

  return (
    <div className="fixed inset-0 z-50 bg-black/40 flex items-center justify-center p-3">
      <div className="w-full max-w-4xl rounded-2xl bg-white shadow-xl border">
        {/* header */}
        <div className="px-4 py-3 border-b flex items-center gap-2">
          <div className="font-semibold">Settings</div>
          <div className="ml-auto flex items-center gap-2">
            <button
              className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50"
              onClick={() => runAdaptive()}
              title="Recompute adaptive with current context"
            >
              Recompute Adaptive
            </button>
            <button
              className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50"
              onClick={() => reload()}
              title="Reload"
            >
              Reload
            </button>
            <button
              className="text-xs px-3 py-1.5 rounded border hover:bg-gray-50"
              onClick={onClose}
              title="Close"
            >
              Close
            </button>
          </div>
        </div>

        {/* tabs */}
        <div className="px-4 py-2 border-b">
          {(["effective","overrides","adaptive","defaults"] as const).map(key => (
            <button
              key={key}
              onClick={() => setTab(key)}
              className={`text-xs mr-2 px-3 py-1.5 rounded ${tab===key ? "bg-black text-white" : "border hover:bg-gray-50"}`}
            >
              {key}
            </button>
          ))}
        </div>

        {/* body */}
        <div className="p-4">
          {loading && <div className="text-sm text-gray-500">Loading…</div>}
          {error && <div className="text-sm text-red-600">{error}</div>}

          {tab !== "overrides" && (
            <pre className="text-xs bg-gray-50 border rounded p-3 overflow-auto max-h-[60vh]">
              {JSON.stringify(view ?? {}, null, 2)}
            </pre>
          )}

          {tab === "overrides" && (
            <div className="space-y-2">
              <div className="text-xs text-gray-600">
                Edit <code>user_overrides</code> JSON. Use <b>Patch</b> to merge or <b>Replace</b> to overwrite.
              </div>
              <textarea
                value={draft}
                onChange={(e) => setDraft(e.target.value)}
                className="w-full h-[50vh] border rounded p-2 font-mono text-xs"
                spellCheck={false}
              />
              <div className="flex items-center gap-2">
                <button
                  className={`text-xs px-3 py-1.5 rounded ${saveBusy ? "opacity-60 cursor-not-allowed" : "bg-black text-white"}`}
                  disabled={saveBusy}
                  onClick={() => onSave("patch")}
                  title="Deep-merge with existing overrides"
                >
                  Save (Patch)
                </button>
                <button
                  className={`text-xs px-3 py-1.5 rounded border ${saveBusy ? "opacity-60 cursor-not-allowed" : "hover:bg-gray-50"}`}
                  disabled={saveBusy}
                  onClick={() => onSave("put")}
                  title="Replace overrides entirely"
                >
                  Save (Replace)
                </button>
                {saveErr && <div className="text-xs text-red-600 ml-2">{saveErr}</div>}
              </div>
            </div>
          )}
        </div>
      </div>
    </div>
  );
}

# ===== frontend/src/file_read/data/chatApi.ts =====

import type { Atta