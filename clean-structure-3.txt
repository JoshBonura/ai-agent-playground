'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
            log.debug(
                f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}"
            )
            return (block, tel)
    t3 = time.perf_counter()
    hits_top = _dedupe_and_sort(all_hits, k=k)
    tel.dedupeSec = round(time.perf_counter() - t3, 6)
    _print_hits(f"Final top-k (k={k})", hits_top)
    t4 = time.perf_counter()
    block = build_block_for_hits(hits_top, preferred_sources=preferred_sources)
    tel.blockBuildSec = round(time.perf_counter() - t4, 6)
    tel.blockChars = len(block or "")
    preview = _mk_preview(block)
    log.debug(f'[RAG BLOCK] chars={tel.blockChars} preview="{preview}"')
    log.debug(
        f"[RAG BLOCK] kept: rerank={tel.keptAfterRerank} cap={tel.keptAfterCap} minFrac={tel.keptAfterMinFrac} fallback={tel.fallbackUsed}"
    )
    return (block, tel)


def build_rag_block(
    query: str, session_id: str | None = None, *, preferred_sources: list[str] | None = None
) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    k = int(SETTINGS.get("rag_top_k"))
    block, _ = _build_rag_block_core(
        query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources
    )
    return block


def build_rag_block_with_telemetry(
    query: str, session_id: str | None = None, *, preferred_sources: list[str] | None = None
) -> tuple[str | None, dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return (None, {})
    k = int(SETTINGS.get("rag_top_k"))
    block, tel = _build_rag_block_core(
        query, session_id=session_id, k=k, session_only=False, preferred_sources=preferred_sources
    )
    return (block, asdict(tel))


def build_rag_block_session_only(
    query: str,
    session_id: str | None,
    *,
    k: int | None = None,
    preferred_sources: list[str] | None = None,
) -> str | None:
    if not bool(SETTINGS.get("rag_enabled")):
        return None
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, _ = _build_rag_block_core(
        query,
        session_id=session_id,
        k=int(k),
        session_only=True,
        preferred_sources=preferred_sources,
    )
    return block


def build_rag_block_session_only_with_telemetry(
    query: str,
    session_id: str | None,
    *,
    k: int | None = None,
    preferred_sources: list[str] | None = None,
) -> tuple[str | None, dict[str, Any]]:
    if not bool(SETTINGS.get("rag_enabled")):
        return (None, {})
    if k is None:
        k = int(SETTINGS.get("attachments_retrieve_top_k"))
    block, tel = _build_rag_block_core(
        query,
        session_id=session_id,
        k=int(k),
        session_only=True,
        preferred_sources=preferred_sources,
    )
    return (block, asdict(tel))

# ===== aimodel/file_read/rag/retrieve_tabular.py =====

from __future__ import annotations

import os
import re
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)

_TABLE_RE = re.compile(
    r"^##\s*Table:\s*(?P<sheet>[^!]+)!\s*R(?P<r1>\d+)-(?P<r2>\d+),C(?P<c1>\d+)-(?P<c2>\d+)",
    re.MULTILINE,
)
_ROW_RE = re.compile(r"^#{0,3}\s*Row\s+(?P<row>\d+)\s+—\s+(?P<sheet>[^\r\n]+)", re.MULTILINE)


def is_csv_source(src: str) -> bool:
    try:
        _, ext = os.path.splitext(src.lower())
        return ext in {".csv", ".tsv"}
    except Exception:
        return False


def is_xlsx_source(src: str) -> bool:
    try:
        _, ext = os.path.splitext(src.lower())
        return ext in {".xlsx", ".xlsm", ".xls"}
    except Exception:
        return False


def _capture_table_block(text: str) -> str | None:
    m = _TABLE_RE.search(text or "")
    if not m:
        return None
    start = m.start()
    end = len(text)
    nxt = re.search(r"^\s*$", text[m.end() :], re.MULTILINE)
    if nxt:
        end = m.end() + nxt.start()
    return text[start:end].strip()


def _capture_row_block(text: str, row_num: int, sheet: str) -> str | None:
    if not text:
        return None
    pat = re.compile(
        rf"^#{0, 3}\s*Row\s+{row_num}\s+—\s+{re.escape(sheet)}[^\n]*\n(?P<body>.*?)(?:\n\s*\n|$)",
        re.MULTILINE | re.DOTALL,
    )
    m = pat.search(text)
    if not m:
        pat2 = re.compile(
            rf"^\s*{row_num}\s+—\s+{re.escape(sheet)}[^\n]*\n(?P<body>.*?)(?:\n\s*\n|$)",
            re.MULTILINE | re.DOTALL,
        )
        m = pat2.search(text)
        if not m:
            return None
    head = f"### Row {row_num} — {sheet}"
    body = (m.group("body") or "").strip()
    if not body:
        return head
    return f"{head}\n{body}"


def _collect_tabular_hits(hits: list[dict]) -> dict[str, Any]:
    headers: dict[tuple[str, str, int, int, int, int], dict[str, Any]] = {}
    rows: list[dict[str, Any]] = []
    for h in hits:
        src = str(h.get("source") or "")
        body = (h.get("text") or "").strip()

        for mt in _TABLE_RE.finditer(body):
            sheet = mt.group("sheet").strip()
            r1 = int(mt.group("r1"))
            r2 = int(mt.group("r2"))
            c1 = int(mt.group("c1"))
            c2 = int(mt.group("c2"))
            key = (src, sheet, r1, r2, c1, c2)
            if key not in headers:
                tb = _capture_table_block(body)
                headers[key] = {
                    "source": src,
                    "sheet": sheet,
                    "r1": r1,
                    "r2": r2,
                    "c1": c1,
                    "c2": c2,
                    "text": tb or "",
                    "score": float(h.get("score") or 0.0),
                }
            else:
                headers[key]["score"] = max(headers[key]["score"], float(h.get("score") or 0.0))

        for mr in _ROW_RE.finditer(body):
            rn = int(mr.group("row"))
            sheet = mr.group("sheet").strip()
            rows.append(
                {
                    "source": src,
                    "sheet": sheet,
                    "row": rn,
                    "hit": h,
                    "score": float(h.get("score") or 0.0),
                }
            )
    return {"headers": headers, "rows": rows}


def _pair_rows_with_headers(
    collected: dict[str, Any],
) -> dict[tuple[str, str, int, int, int, int], dict[str, Any]]:
    headers = collected["headers"]
    rows = collected["rows"]
    groups: dict[tuple[str, str, int, int, int, int], dict[str, Any]] = {}
    for r in rows:
        src = r["source"]
        sheet = r["sheet"]
        rown = r["row"]
        match_key = None
        for key in headers.keys():
            s, sh, r1, r2, c1, c2 = key
            if s == src and sh == sheet and r1 <= rown <= r2:
                match_key = key
                break
        if not match_key:
            continue
        g = groups.setdefault(match_key, {"header": headers[match_key], "rows": []})
        g["rows"].append(r)
    return groups


def _render_tabular_groups(
    groups: dict[tuple[str, str, int, int, int, int], dict[str, Any]],
    preferred_sources: list[str] | None = None,
) -> list[str]:
    total_budget = int(SETTINGS.get("rag_total_char_budget"))
    max_row_snippets = int(SETTINGS.get("rag_tabular_rows_per_table"))
    per_row_max = int(SETTINGS.get("rag_max_chars_per_chunk"))
    preamble = str(SETTINGS.get("rag_block_preamble") or "")
    preamble = preamble if not preamble or preamble.endswith(":") else preamble + ":"

    pref = set(s.strip().lower() for s in (preferred_sources or []) if s)
    boost = float(SETTINGS.get("rag_new_upload_score_boost"))

    lines: list[str] = [preamble]
    used = len(lines[0]) + 1

    def _group_score(key):
        base = groups[key]["header"]["score"]
        src = str(groups[key]["header"]["source"] or "").strip().lower()
        return base * (1.0 + boost) if src in pref else base

    keys_sorted = sorted(groups.keys(), key=_group_score, reverse=True)

    for key in keys_sorted:
        hdr = groups[key]["header"]
        hdr_text = (hdr.get("text") or "").strip()
        if not hdr_text:
            hdr_text = f"## Table: {hdr['sheet']}!R{hdr['r1']}-{hdr['r2']},C{hdr['c1']}-{hdr['c2']}"
        hdr_cost = len(hdr_text) + 1
        if used + hdr_cost > total_budget:
            break
        lines.append(hdr_text)
        used += hdr_cost

        row_list = sorted(groups[key]["rows"], key=lambda r: r["score"], reverse=True)[
            :max_row_snippets
        ]
        for r in row_list:
            body = r["hit"].get("text") or ""
            row_block = _capture_row_block(body, r["row"], r["sheet"]) or ""
            if not row_block:
                continue
            row_snip = row_block[:per_row_max].strip()
            row_cost = len(row_snip) + 1
            if used + row_cost > total_budget:
                break
            lines.append(row_snip)
            used += row_cost

        if used >= total_budget:
            break

    return lines


def make_rag_block_tabular(
    hits: list[dict], preferred_sources: list[str] | None = None
) -> str | None:
    if not hits:
        return None
    # Only keep sources that look like CSV/Excel to avoid mixing generic text
    tabular_hits = [
        h
        for h in hits
        if is_xlsx_source(str(h.get("source") or "")) or is_csv_source(str(h.get("source") or ""))
    ]
    if not tabular_hits:
        return None
    collected = _collect_tabular_hits(tabular_hits)
    groups = _pair_rows_with_headers(collected)
    if not groups:
        return None
    lines = _render_tabular_groups(groups, preferred_sources=preferred_sources)
    return "\n".join(lines)

# ===== aimodel/file_read/rag/router_ai.py =====

from __future__ import annotations

import logging

log = logging.getLogger(__name__)
import json
import re
import traceback
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)


def _dbg(msg: str):
    log.info(f"[RAG ROUTER] {msg}")
    print(f"[RAG ROUTER] {msg}")


def _force_json_strict(s: str) -> dict:
    if not s:
        return {}
    try:
        v = json.loads(s)
        print("[_force_json_strict] parsed raw JSON")
        return v if isinstance(v, dict) else {}
    except Exception:
        pass
    rgx = SETTINGS.get("router_rag_json_extract_regex")
    if isinstance(rgx, str) and rgx:
        try:
            m = re.search(rgx, s, re.DOTALL)
            if m:
                cand = m.group(0)
                v = json.loads(cand)
                print("[_force_json_strict] parsed with regex extract")
                return v if isinstance(v, dict) else {}
        except Exception:
            pass
    print("[_force_json_strict] failed to parse JSON")
    return {}


def _strip_wrappers(text: str) -> str:
    t = text or ""
    if SETTINGS.get("router_rag_trim_whitespace") is True:
        t = t.strip()
    if SETTINGS.get("router_rag_strip_wrappers_enabled") is not True:
        return t
    head = t
    if SETTINGS.get("router_rag_strip_split_on_blank") is True:
        head = t.split("\n\n", 1)[0]
    pat = SETTINGS.get("router_rag_strip_header_regex")
    if isinstance(pat, str) and pat:
        try:
            rx = re.compile(pat)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            print(f"[_strip_wrappers] stripped text={core[:100]!r}")
            return core if core else t
        except Exception:
            return head
    print(f"[_strip_wrappers] head={head[:100]!r}")
    return head


def _normalize_keys(d: dict) -> dict:
    nd = {str(k).strip().strip('"').strip("'").strip().lower(): v for k, v in d.items()}
    print(f"[_normalize_keys] keys={list(nd.keys())}")
    return nd


def _as_bool(v) -> bool | None:
    if isinstance(v, bool):
        return v
    if isinstance(v, str):
        s = v.strip().strip('"').strip("'").lower()
        if s in ("true", "yes", "y", "1"):
            return True
        if s in ("false", "no", "n", "0"):
            return False
    return None


def decide_rag(llm: Any, user_text: str) -> tuple[bool, str | None]:
    try:
        if not user_text or not user_text.strip():
            print("[decide_rag] empty user_text")
            return (False, None)
        core_text = _strip_wrappers(user_text.strip())
        prompt_tpl = SETTINGS.get("router_rag_decide_prompt")
        if not isinstance(prompt_tpl, str) or (
            "$text" not in prompt_tpl and "{text}" not in prompt_tpl
        ):
            _dbg("router_rag_decide_prompt missing/invalid")
            return (False, None)
        from string import Template

        if "$text" in prompt_tpl:
            the_prompt = Template(prompt_tpl).safe_substitute(text=core_text)
        else:
            the_prompt = prompt_tpl.format(text=core_text)
        print(f"[decide_rag] the_prompt={the_prompt[:120]!r}")
        params = {
            "max_tokens": SETTINGS.get("router_rag_decide_max_tokens"),
            "temperature": SETTINGS.get("router_rag_decide_temperature"),
            "top_p": SETTINGS.get("router_rag_decide_top_p"),
            "stream": False,
        }
        stop_list = SETTINGS.get("router_rag_decide_stop")
        if isinstance(stop_list, list) and stop_list:
            params["stop"] = stop_list
        params = {k: v for k, v in params.items() if v is not None}
        print(f"[decide_rag] params={params}")
        raw = llm.create_chat_completion(
            messages=[{"role": "user", "content": the_prompt}], **params
        )
        text_out = (raw.get("choices", [{}])[0].get("message", {}).get("content") or "").strip()
        print(f"[decide_rag] raw llm output={text_out[:200]!r}")
        data = _force_json_strict(text_out)
        print(f"[decide_rag] parsed data={data}")
        if not isinstance(data, dict):
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)
        data = _normalize_keys(data)
        need_raw = data.get("need")
        need_bool = _as_bool(need_raw) if not isinstance(need_raw, bool) else need_raw
        if need_bool is None:
            need_default = SETTINGS.get("router_rag_default_need_when_invalid")
            print("[decide_rag] invalid need, using default")
            return (bool(need_default) if isinstance(need_default, bool) else False, None)
        need = bool(need_bool)
        if not need:
            print("[decide_rag] need=False, returning early")
            return (False, None)
        query_field = data.get("query", "")
        query_clean = _strip_wrappers(str(query_field or "").strip())
        if not query_clean:
            query_clean = core_text[:512]
        print(f"[decide_rag] final query={query_clean[:120]!r}")
        return (True, query_clean)
    except Exception as e:
        _dbg(f"FATAL {type(e).__name__}: {e}")
        traceback.print_exc()
        need_default = SETTINGS.get("router_rag_default_need_when_invalid")
        return (bool(need_default) if isinstance(need_default, bool) else False, None)

# ===== aimodel/file_read/rag/schemas.py =====

from pydantic import BaseModel, Field

from ..core.logging import get_logger

log = get_logger(__name__)


class SearchReq(BaseModel):
    query: str
    sessionId: str | None = None
    kChat: int = 6
    kGlobal: int = 4
    hybrid_alpha: float = 0.5


class ItemRow(BaseModel):
    id: str
    sessionId: str | None
    source: str
    title: str | None
    mime: str | None
    size: int | None
    createdAt: str
    meta: dict[str, str] = Field(default_factory=dict)


class SearchHit(BaseModel):
    id: str
    text: str
    score: float
    source: str | None = None
    title: str | None = None
    sessionId: str | None = None
    url: str | None = None

# ===== aimodel/file_read/rag/search.py =====

from __future__ import annotations

from ..core.logging import get_logger

log = get_logger(__name__)


def reciprocal_rank_fusion(results: list[list[dict]], k: int = 60) -> list[dict]:
    scores: dict[str, float] = {}
    lookup: dict[str, dict] = {}
    for lst in results:
        for rank, r in enumerate(lst, start=1):
            rid = r["id"]
            scores[rid] = scores.get(rid, 0.0) + 1.0 / (k + rank)
            lookup[rid] = r
    fused = [{"score": s, **lookup[rid]} for rid, s in scores.items()]
    fused.sort(key=lambda x: x["score"], reverse=True)
    return fused

# ===== aimodel/file_read/rag/store.py =====

from __future__ import annotations

import logging

log = logging.getLogger(__name__)
import json
import shutil
from pathlib import Path

import faiss
import numpy as np

from ..core.logging import get_logger
from ..store.base import APP_DIR

log = get_logger(__name__)

BASE = APP_DIR / "rag"


def _ns_dir(session_id: str | None) -> Path:
    if session_id:
        return BASE / "by_session" / session_id
    return BASE / "global"


def _paths(session_id: str | None) -> tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return (d / "index.faiss", d / "meta.jsonl")


def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x


def _load_index(dim: int, p: Path) -> faiss.Index:
    if p.exists():
        return faiss.read_index(str(p))
    return faiss.IndexFlatIP(dim)


def _save_index(idx: faiss.Index, p: Path) -> None:
    faiss.write_index(idx, str(p))


def add_vectors(session_id: str | None, embeds: np.ndarray, metas: list[dict], dim: int):
    idx_path, meta_path = _paths(session_id)
    idx = _load_index(dim, idx_path)
    if not isinstance(idx, faiss.IndexFlatIP):
        idx = faiss.IndexFlatIP(dim) if idx.ntotal == 0 else idx
    embeds = _norm(embeds)
    existing_ids = set()
    if meta_path.exists():
        with meta_path.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    existing_ids.add(j["id"])
                except:
                    pass
    start = idx.ntotal
    new_embeds = []
    new_metas = []
    for i, m in enumerate(metas):
        if m["id"] in existing_ids:
            continue
        m["row"] = start + len(new_embeds)
        new_embeds.append(embeds[i])
        new_metas.append(m)
    if new_embeds:
        idx.add(np.vstack(new_embeds))
        _save_index(idx, idx_path)
        with meta_path.open("a", encoding="utf-8") as f:
            for m in new_metas:
                f.write(json.dumps(m, ensure_ascii=False) + "\n")


def search_vectors(
    session_id: str | None, query_vec: np.ndarray, topk: int, dim: int
) -> list[dict]:
    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return []
    idx = _load_index(dim, idx_path)
    query_vec = np.asarray(query_vec, dtype="float32")
    q = _norm(query_vec.reshape(1, -1))
    D, I = idx.search(q, topk)
    out: list[dict] = []
    rows: dict[int, dict] = {}
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                j = json.loads(line)
                rows[int(j["row"])] = j
            except:
                pass
    for score, row in zip(D[0].tolist(), I[0].tolist(), strict=False):
        if row < 0:
            continue
        m = rows.get(row)
        if not m:
            continue
        m = dict(m)
        m["score"] = float(score)
        out.append(m)
    return out


def search_similar(
    qvec: list[float] | np.ndarray, *, k: int = 5, session_id: str | None = None
) -> list[dict]:
    """
    Compatibility wrapper used by retrieve.py.
    qvec: a single embedding vector (list or np.ndarray)
    """
    arr = np.asarray(qvec, dtype="float32")
    dim = int(arr.shape[-1])
    return search_vectors(session_id, arr, k, dim)


def add_texts(texts: list[str], metas: list[dict], *, session_id: str | None, embed_fn) -> int:
    if not texts:
        return 0
    vecs = embed_fn(texts)
    if not isinstance(vecs, np.ndarray):
        vecs = np.asarray(vecs, dtype="float32")
    dim = int(vecs.shape[-1])
    add_vectors(session_id, vecs, metas, dim)
    return len(texts)


def delete_namespace(session_id: str) -> bool:
    d = _ns_dir(session_id)
    try:
        if d.exists():
            shutil.rmtree(d, ignore_errors=True)
            return True
        return False
    except Exception:
        return False


def session_has_any_vectors(session_id: str | None) -> bool:
    if not session_id:
        return False
    idx_path, meta_path = _paths(session_id)
    if not idx_path.exists() or not meta_path.exists():
        return False
    try:
        idx = _load_index(dim=1, p=idx_path)
        if getattr(idx, "ntotal", 0) <= 0:
            return False
    except Exception as e:
        log.error(f"[RAG STORE] failed to read index for {session_id}: {e}")
        return False
    try:
        with meta_path.open("r", encoding="utf-8") as f:
            for _ in f:
                return True
        return False
    except Exception as e:
        log.error(f"[RAG STORE] failed to read meta for {session_id}: {e}")
        return False

# ===== aimodel/file_read/rag/uploads.py =====

from __future__ import annotations

import json
from pathlib import Path

import faiss
import numpy as np

from ..core.logging import get_logger

log = get_logger(__name__)
from .store import _ns_dir

_META_FN = "meta.jsonl"
_INDEX_FN = "index.faiss"


def _meta_path_ro(session_id: str | None) -> Path:
    return _ns_dir(session_id) / _META_FN


def _index_path_ro(session_id: str | None) -> Path:
    return _ns_dir(session_id) / _INDEX_FN


def _paths_mut(session_id: str | None) -> tuple[Path, Path]:
    d = _ns_dir(session_id)
    d.mkdir(parents=True, exist_ok=True)
    return d / _INDEX_FN, d / _META_FN


def _read_meta(meta_path: Path) -> list[dict]:
    if not meta_path.exists():
        return []
    out: list[dict] = []
    with meta_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                j = json.loads(line)
                if isinstance(j, dict):
                    out.append(j)
            except:
                pass
    return out


def _write_meta(meta_path: Path, rows: list[dict]) -> None:
    tmp = meta_path.with_suffix(".jsonl.tmp")
    with tmp.open("w", encoding="utf-8") as f:
        for j in rows:
            f.write(json.dumps(j, ensure_ascii=False) + "\n")
    tmp.replace(meta_path)


def _norm(x: np.ndarray) -> np.ndarray:
    x = x.astype("float32")
    faiss.normalize_L2(x)
    return x


def list_sources(session_id: str | None, include_global: bool = True) -> list[dict]:
    def _agg(ns: str | None) -> dict[str, int]:
        mp = _meta_path_ro(ns)
        agg: dict[str, int] = {}
        for j in _read_meta(mp):
            src = (j.get("source") or "").strip()
            if not src:
                continue
            agg[src] = agg.get(src, 0) + 1
        return agg

    rows: list[dict] = []
    # session first
    if session_id is not None:
        for src, n in _agg(session_id).items():
            rows.append({"source": src, "sessionId": session_id, "chunks": n})
    if include_global:
        for src, n in _agg(None).items():
            rows.append({"source": src, "sessionId": None, "chunks": n})
    return rows


def hard_delete_source(source: str, *, session_id: str | None, embedder) -> dict:
    idx_path, meta_path = _paths_mut(session_id)
    rows = _read_meta(meta_path)
    if not rows:
        return {"ok": True, "removed": 0, "remaining": 0}

    keep: list[dict] = []
    removed = 0
    for j in rows:
        if str(j.get("source") or "").strip() == source:
            removed += 1
        else:
            keep.append(j)

    if removed == 0:
        return {"ok": True, "removed": 0, "remaining": len(keep)}

    for i, j in enumerate(keep):
        j["row"] = i

    if len(keep) == 0:
        if idx_path.exists():
            try:
                idx_path.unlink()
            except:
                pass
        _write_meta(meta_path, [])
        return {"ok": True, "removed": removed, "remaining": 0}

    texts = [str(j.get("text") or "") for j in keep]
    B = 128  # batch size
    parts: list[np.ndarray] = []
    for i in range(0, len(texts), B):
        vec = embedder(texts[i : i + B])
        if not isinstance(vec, np.ndarray):
            vec = np.asarray(vec, dtype="float32")
        parts.append(vec.astype("float32"))
    embeds = np.vstack(parts)
    embeds = _norm(embeds)

    dim = int(embeds.shape[-1])
    new_index = faiss.IndexFlatIP(dim)
    new_index.add(embeds)

    faiss.write_index(new_index, str(idx_path))
    _write_meta(meta_path, keep)

    return {"ok": True, "removed": removed, "remaining": len(keep)}

# ===== aimodel/file_read/requirements.txt =====

fastapi==0.116.1
uvicorn==0.30.6
pydantic==2.11.7
starlette==0.47.2
llama-cpp-python==0.3.4
ddgs==9.5.4
httpx==0.27.0
# HTML parsing / readability (permissive)
lxml==6.0.1
readability-lxml==0.8.1
selectolax==0.3.21
beautifulsoup4==4.12.3

# RAG
faiss-cpu==1.8.0.post1
numpy==1.26.4
sentence-transformers==3.0.1
tzlocal==5.2
openpyxl==3.1.5 
python-multipart==0.0.20

python-docx==1.1.2
PyYAML==6.0.2
toml==0.10.2
pdfminer.six==20240706

PyPDF2==3.0.1
pandas==2.2.2

striprtf==0.0.26

python-pptx==0.6.23

pytesseract==0.3.13
pypdfium2==4.30.0
Pillow==10.4.

pydantic[email]==2.11.7
python-jose==3.3.0
passlib[bcrypt]==1.7.4


pynacl==1.5.0

PyJWT>=2.8.0
cryptography>=41.0.0
python-jose[cryptography]
keyring

psutil
nvidia-ml-py3

# ===== aimodel/file_read/runtime/__init__.py =====



# ===== aimodel/file_read/runtime/model_runtime.py =====

# aimodel/file_read/runtime/model_runtime.py
from __future__ import annotations
import os, gc
from dataclasses import asdict, dataclass
from pathlib import Path
from threading import RLock, Event
from typing import Any
from ..adaptive.config.paths import read_settings, write_settings
from ..core.logging import get_logger

log = get_logger(__name__)
_progress_hits = 0

try:
    from llama_cpp import Llama
except Exception as e:
    raise RuntimeError("llama-cpp-python not installed or GPU libs missing") from e

AUTO_ON_DEMAND = os.getenv("AUTO_LOAD_ON_FIRST_USE", "").lower() in ("1", "true", "yes")

@dataclass
class ModelConfig:
    modelPath: str
    nCtx: int = 4096
    nThreads: int = 8
    nGpuLayers: int = 40
    nBatch: int = 256
    ropeFreqBase: float | None = None
    ropeFreqScale: float | None = None
    @staticmethod
    def from_dict(d: dict[str, Any]) -> "ModelConfig":
        return ModelConfig(
            modelPath=str(d.get("modelPath", "")).strip(),
            nCtx=int(d.get("nCtx", 4096)),
            nThreads=int(d.get("nThreads", 8)),
            nGpuLayers=int(d.get("nGpuLayers", 40)),
            nBatch=int(d.get("nBatch", 256)),
            ropeFreqBase=(float(d["ropeFreqBase"]) if d.get("ropeFreqBase") not in (None, "") else None),
            ropeFreqScale=(float(d["ropeFreqScale"]) if d.get("ropeFreqScale") not in (None, "") else None),
        )

_runtime_lock = RLock()
_llm: Llama | None = None
_cfg: ModelConfig | None = None
_loading: bool = False
_loading_model_path: str | None = None
_cancel_ev: Event = Event()

def _progress_cb(progress: float, *_args, **_kwargs) -> bool:
    global _progress_hits
    if _progress_hits < 5:
        _progress_hits += 1
        log.info(f"[model] progress_cb hit #{_progress_hits}: {progress:.2%}")
    return not _cancel_ev.is_set()

def _build_kwargs(cfg: ModelConfig) -> dict[str, Any]:
    kw = dict(
        model_path=cfg.modelPath,
        n_ctx=cfg.nCtx,
        n_threads=cfg.nThreads,
        n_gpu_layers=cfg.nGpuLayers,
        n_batch=cfg.nBatch,
    )
    if cfg.ropeFreqBase is not None:
        kw["rope_freq_base"] = cfg.ropeFreqBase
    if cfg.ropeFreqScale is not None:
        kw["rope_freq_scale"] = cfg.ropeFreqScale
    return kw

def _attach_introspection(llm: Llama) -> None:
    def get_last_timings():
        for attr in ("get_timings", "timings", "perf"):
            try:
                obj = getattr(llm, attr, None)
                v = obj() if callable(obj) else obj
                if isinstance(v, dict):
                    return v
            except Exception:
                pass
        return None
    try:
        llm.get_last_timings = get_last_timings  # type: ignore[attr-defined]
    except Exception:
        pass

def _close_llm():
    global _llm
    try:
        if _llm is not None:
            try:
                del _llm
            finally:
                _llm = None
                gc.collect()
    except Exception:
        _llm = None
        gc.collect()

def is_loaded() -> bool:
    with _runtime_lock:
        return _llm is not None

def current_model_info() -> dict[str, Any]:
    with _runtime_lock:
        return {
            "loaded": _llm is not None,
            "config": asdict(_cfg) if _cfg else None,
            "loading": _loading,
            "loadingPath": _loading_model_path,
        }

def request_cancel_load() -> bool:
    with _runtime_lock:
        if not _loading:
            return False
        _cancel_ev.set()
        log.info("[model] Cancellation requested for in-progress load")
        return True

def ensure_ready() -> None:
    global _llm, _cfg, _loading, _loading_model_path
    with _runtime_lock:
        if _llm is not None:
            return
        if not AUTO_ON_DEMAND:
            raise RuntimeError("No model is loaded. Load one via POST /api/models/load.")
        if _loading:
            raise RuntimeError("A model load is already in progress. Try again shortly.")

        s = read_settings()
        cfg = ModelConfig.from_dict(s)
        if not cfg.modelPath:
            raise RuntimeError("No model selected in settings; POST /api/models/load first.")
        p = Path(cfg.modelPath)
        if not p.exists():
            raise FileNotFoundError(f"Model path not found: {p}")

        _loading = True
        _loading_model_path = cfg.modelPath
        _cancel_ev.clear()
        kw = _build_kwargs(cfg)
        kw["progress_callback"] = _progress_cb

    llm = None
    try:
        if _cancel_ev.is_set():
            raise RuntimeError("CANCELLED")

        llm = Llama(**kw)
        _attach_introspection(llm)

        if _cancel_ev.is_set():
            try:
                del llm
            finally:
                gc.collect()
            with _runtime_lock:
                _loading = False
                _loading_model_path = None
            log.info("[model] Load cancelled (after init)")
            raise RuntimeError("CANCELLED")

        with _runtime_lock:
            _llm = llm
            _cfg = cfg
            log.info(f"[model] On-demand loaded: {cfg.modelPath}")
    except Exception:
        if llm is not None:
            try:
                del llm
            finally:
                gc.collect()
        raise
    finally:
        with _runtime_lock:
            _loading = False
            _loading_model_path = None
            # do NOT clear _cancel_ev here; leave it set until next load

def get_llm() -> Llama:
    ensure_ready()
    assert _llm is not None
    return _llm

def load_model(config_patch: dict[str, Any] | None = None, **kwargs) -> dict[str, Any]:
    global _llm, _cfg, _loading, _loading_model_path
    with _runtime_lock:
        if _loading:
            raise RuntimeError("A model load is already in progress.")

        patch: dict[str, Any] = dict(config_patch or {})
        if kwargs:
            patch.update(kwargs)

        reset = bool(patch.pop("resetDefaults", False) or patch.pop("reset_defaults", False))
        base = {} if reset else read_settings()
        for k, v in list(patch.items()):
            if v is None:
                patch.pop(k, None)
        base.update(patch)

        cfg = ModelConfig.from_dict(base)
        if not cfg.modelPath:
            raise ValueError("modelPath is required")
        if not Path(cfg.modelPath).exists():
            raise FileNotFoundError(f"Model not found: {cfg.modelPath}")

        _close_llm()

        _loading = True
        _loading_model_path = cfg.modelPath
        _cancel_ev.clear()
        kw = _build_kwargs(cfg)
        kw["progress_callback"] = _progress_cb

    if _cancel_ev.is_set():
        raise RuntimeError("CANCELLED")

    llm = Llama(**kw)
    _attach_introspection(llm)

    if _cancel_ev.is_set():
        try:
            del llm
        finally:
            gc.collect()
        with _runtime_lock:
            _loading = False
            _loading_model_path = None
        log.info("[model] Load cancelled (after init)")
        raise RuntimeError("CANCELLED")

    with _runtime_lock:
        try:
            _llm = llm
            _cfg = cfg
            write_settings(asdict(cfg))
            log.info(f"[model] Loaded: {cfg.modelPath} (resetDefaults={reset})")
            return current_model_info()
        finally:
            _loading = False
            _loading_model_path = None
            # leave _cancel_ev state alone here

def unload_model() -> None:
    with _runtime_lock:
        _close_llm()
        log.info("[model] Unloaded")

def list_local_models() -> list[dict[str, Any]]:
    import re
    s = read_settings()
    root = Path(s.get("modelsDir") or (Path.home() / ".localmind" / "models"))
    try:
        root.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        log.warning(f"[model] Could not create models dir {root}: {e}")

    def guess_arch(name: str) -> str | None:
        n = name.lower()
        for c in ["qwen2.5", "qwen2", "qwen3", "qwen", "mixtral", "mistral", "llama", "gemma", "phi", "yi", "orca", "vicuna"]:
            if c in n:
                return c
        return None

    def guess_params_b(name: str) -> int | None:
        m = re.search(r'(^|[^a-z0-9])(\d{1,3})\s*[bB]([^a-z0-9]|$)', name)
        if m:
            try:
                return int(m.group(2))
            except Exception:
                return None
        return None

    def guess_quant(name: str) -> str | None:
        m = re.search(r'Q\d[A-Z0-9_]*', name)
        return m.group(0) if m else None

    out: list[dict[str, Any]] = []
    try:
        for p in root.rglob("*.gguf"):
            try:
                st = p.stat()
                name = p.name
                out.append(
                    {
                        "path": str(p.resolve()),
                        "sizeBytes": st.st_size,
                        "name": name,
                        "rel": str(p.relative_to(root)),
                        "mtime": st.st_mtime,
                        "arch": guess_arch(name),
                        "paramsB": guess_params_b(name),
                        "quant": guess_quant(name),
                        "format": "GGUF",
                    }
                )
            except Exception:
                pass
    except Exception:
        pass

    out.sort(key=lambda x: x["sizeBytes"], reverse=True)
    return out

# ===== aimodel/file_read/services/__init__.py =====



# ===== aimodel/file_read/services/attachments.py =====

# aimodel/file_read/services/attachments.py
from __future__ import annotations

from collections.abc import Iterable
from typing import Any

from ..core.logging import get_logger

log = get_logger(__name__)


def att_get(att: Any, key: str, default=None):
    try:
        return att.get(key, default)
    except AttributeError:
        return getattr(att, key, default)


def join_attachment_names(attachments: Iterable[Any] | None) -> str:
    if not attachments:
        return ""
    names: list[str] = [att_get(a, "name") for a in attachments]
    names = [n for n in names if n]
    return ", ".join(names)

# ===== aimodel/file_read/services/auth_service.py =====

# ===== aimodel/file_read/services/auth_service.py =====
from __future__ import annotations

from ..core.logging import get_logger

log = get_logger(__name__)

import json
import os
import time
from typing import Any

from fastapi import HTTPException
from jose import jwt as jose_jwt
from jose.exceptions import ExpiredSignatureError, JWTError

from ..core.http import ExternalServiceError, arequest_json

_CERTS_URL = (
    "https://www.googleapis.com/robot/v1/metadata/x509/securetoken@system.gserviceaccount.com"
)
_CERTS_TTL = 60 * 60  # 1 hour
_CERTS_CACHE: dict[str, Any] = {"k2pem": {}, "fetched_at": 0}


def _get_api_key() -> str:
    v = (os.getenv("FIREBASE_WEB_API_KEY") or "").strip()
    if not v:
        # Keep this an error so it’s obvious in logs
        log.error("[auth] ERROR: FIREBASE_WEB_API_KEY not set")
    return v


def _get_project_id() -> str:
    v = (os.getenv("FIREBASE_PROJECT_ID") or "").strip()
    if not v:
        log.error("[auth] ERROR: FIREBASE_PROJECT_ID is not set")
    return v


def need_project_check() -> bool:
    return _get_project_id() == ""


async def fetch_google_certs_async() -> dict[str, str]:
    """Fetch and cache Google public certs used to verify Firebase ID tokens."""
    now = int(time.time())
    if now - int(_CERTS_CACHE.get("fetched_at") or 0) < _CERTS_TTL:
        return _CERTS_CACHE["k2pem"]

    try:
        data = await arequest_json(
            method="GET",
            url=_CERTS_URL,
            service="google_certs",
            headers={"Accept": "application/json"},
        )
        if isinstance(data, dict):
            _CERTS_CACHE["k2pem"] = data
            _CERTS_CACHE["fetched_at"] = now
            log.info(f"[auth] fetched Google certs; kids={list(data.keys())[:3]}...")
            return data
    except ExternalServiceError as e:
        log.error(f"[auth] ERROR fetching Google certs: status={e.status} detail={e.detail!r}")
    except Exception as e:
        log.error(f"[auth] ERROR fetching Google certs: {e!r}")

    return _CERTS_CACHE.get("k2pem", {})


async def verify_jwt_with_google(token: str) -> dict[str, Any]:
    """Verify a Firebase ID token using Google x509 certs (async)."""
    proj = _get_project_id()
    if not proj:
        raise HTTPException(500, "Auth not configured")

    try:
        header = jose_jwt.get_unverified_header(token)
        kid = header.get("kid")
        if not kid:
            log.info("[auth] token header missing 'kid'")
            raise HTTPException(401, "Invalid token header")

        # Lightweight preview for logs / sanity
        payload_preview = jose_jwt.get_unverified_claims(token)
        log.debug(
            "[auth] id_token preview: %s",
            {
                "header": {"alg": header.get("alg"), "kid": kid, "typ": header.get("typ")},
                "payload": {
                    "iss": payload_preview.get("iss"),
                    "aud": payload_preview.get("aud"),
                    "email": payload_preview.get("email"),
                    "email_verified": payload_preview.get("email_verified"),
                    "uid": payload_preview.get("uid"),
                    "iat": payload_preview.get("iat"),
                    "exp": payload_preview.get("exp"),
                },
            },
        )

        certs = await fetch_google_certs_async()
        pem = certs.get(kid)
        if not pem:
            log.info(f"[auth] kid {kid} not found in certs; refreshing once")
            _CERTS_CACHE["fetched_at"] = 0
            certs = await fetch_google_certs_async()
            pem = certs.get(kid)
            if not pem:
                raise HTTPException(401, "Key not found for token")

        issuer = f"https://securetoken.google.com/{proj}"
        claims = jose_jwt.decode(
            token,
            pem,
            algorithms=["RS256"],
            audience=proj,
            issuer=issuer,
            options={
                "verify_signature": True,
                "verify_aud": True,
                "verify_iat": True,
                "verify_exp": True,
            },
        )
        return claims

    except ExpiredSignatureError:
        log.warning("[auth] token expired")
        raise HTTPException(401, "Token expired")
    except JWTError as e:
        log.info(f"[auth] JWTError while verifying: {e}")
        raise HTTPException(401, "Invalid token")
    except HTTPException:
        raise
    except Exception as e:
        log.error(f"[auth] unexpected verify error: {e!r}")
        raise HTTPException(401, "Invalid token")


async def firebase_sign_in_with_password(email: str, password: str) -> dict[str, Any]:
    api_key = _get_api_key()
    if not api_key:
        raise HTTPException(500, "Server auth not configured")

    url = f"https://identitytoolkit.googleapis.com/v1/accounts:signInWithPassword?key={api_key}"
    try:
        data = await arequest_json(
            method="POST",
            url=url,
            service="firebase_auth",
            headers={"Accept": "application/json"},
            json_body={"email": email, "password": password, "returnSecureToken": True},
        )
        log.info(f"[auth] REST login ok email={email}")
        return data
    except ExternalServiceError as e:
        msg = e.body_preview or e.detail or "Invalid credentials"
        log.error(f"[auth] REST login failed status={e.status} body={str(msg)[:200]!r}")
        raise HTTPException(401, "Invalid credentials") from e


async def firebase_sign_up_with_password(email: str, password: str) -> dict[str, Any]:
    api_key = _get_api_key()
    if not api_key:
        raise HTTPException(500, "Server auth not configured")

    url = f"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key={api_key}"
    try:
        data = await arequest_json(
            method="POST",
            url=url,
            service="firebase_auth",
            headers={"Accept": "application/json"},
            json_body={"email": email, "password": password, "returnSecureToken": True},
        )
        log.info(f"[auth] REST register ok email={email}")
        return data
    except ExternalServiceError as e:
        raw = (e.body_preview or "")[:200]
        msg = "Could Not Register"
        try:
            j = json.loads(e.body_preview or "{}")
            msg = (j.get("error", {}).get("message") or "").replace("_", " ").title() or msg
        except Exception:
            pass
        raise HTTPException(400, msg) from e

# ===== aimodel/file_read/services/budget.py =====

# aimodel/file_read/services/budget.py
from __future__ import annotations

from dataclasses import asdict, dataclass
from typing import Any

from ..core.logging import get_logger
from .context_window import current_n_ctx, estimate_tokens

log = get_logger(__name__)


@dataclass
class TurnBudget:
    n_ctx: int
    input_tokens_est: int | None
    requested_out_tokens: int
    clamped_out_tokens: int
    clamp_margin: int
    reserved_system_tokens: int | None = None
    available_for_out_tokens: int | None = None
    headroom_tokens: int | None = None
    overage_tokens: int | None = None
    reason: str = "ok"

    def to_dict(self) -> dict[str, Any]:
        return asdict(self)


def analyze_budget(
    llm: Any,
    messages: list[dict[str, str]],
    *,
    requested_out_tokens: int,
    clamp_margin: int,
    reserved_system_tokens: int | None = None,
) -> TurnBudget:
    n_ctx = current_n_ctx()
    try:
        inp = estimate_tokens(llm, messages)
    except Exception:
        inp = None

    rst = int(reserved_system_tokens or 0)
    min_out = 16

    if inp is None:
        available = None
        clamped = requested_out_tokens
        headroom = None
        overage = None
        reason = "input_tokens_unknown"
    else:
        available_raw = n_ctx - inp - clamp_margin - rst
        available = max(min_out, available_raw)
        clamped = max(min_out, min(requested_out_tokens, available))
        headroom = max(0, available - clamped)
        overage = max(0, requested_out_tokens - available)
        reason = "ok" if overage == 0 else "requested_exceeds_available"

    return TurnBudget(
        n_ctx=n_ctx,
        input_tokens_est=inp,
        requested_out_tokens=requested_out_tokens,
        clamped_out_tokens=clamped,
        clamp_margin=clamp_margin,
        reserved_system_tokens=reserved_system_tokens,
        available_for_out_tokens=available,
        headroom_tokens=headroom,
        overage_tokens=overage,
        reason=reason,
    )

# ===== aimodel/file_read/services/cancel.py =====

from __future__ import annotations
import asyncio
from collections import defaultdict
from threading import Event

from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)

eff = SETTINGS.effective()
GEN_SEMAPHORE = asyncio.Semaphore(int(eff["gen_semaphore_permits"]))
_ACTIVE: dict[str, int] = {}
_CANCELS: dict[str, Event] = {}

# NEW: track the server-side tasks currently streaming per session
_TASKS: dict[str, set[asyncio.Task]] = defaultdict(set)

def is_active(session_id: str) -> bool:
    return bool(_ACTIVE.get(session_id))

def mark_active(session_id: str, delta: int):
    _ACTIVE[session_id] = max(0, int(_ACTIVE.get(session_id, 0)) + delta)
    if _ACTIVE[session_id] == 0:
        _ACTIVE.pop(session_id, None)

def cancel_event(session_id: str) -> Event:
    ev = _CANCELS.get(session_id)
    if ev is None:
        ev = Event()
        _CANCELS[session_id] = ev
    return ev

# ---------- NEW helpers for hard-kill ----------
def register_task(session_id: str, task: asyncio.Task) -> None:
    _TASKS[session_id].add(task)

def unregister_task(session_id: str, task: asyncio.Task) -> None:
    s = _TASKS.get(session_id)
    if not s:
        return
    s.discard(task)
    if not s:
        _TASKS.pop(session_id, None)

async def cancel_tasks_for(session_id: str) -> int:
    """Cancel all running tasks for a session. Returns #tasks cancelled."""
    tasks = list(_TASKS.get(session_id, ()))
    for t in tasks:
        t.cancel()
    # yield to the loop so cancellations propagate
    if tasks:
        await asyncio.sleep(0)
    return len(tasks)

def cancel_all_sessions() -> int:
    """(Optional) global kill switch."""
    for ev in _CANCELS.values():
        ev.set()
    n = 0
    for sid, tasks in list(_TASKS.items()):
        for t in list(tasks):
            t.cancel()
            n += 1
        _TASKS.pop(sid, None)
    return n

# ===== aimodel/file_read/services/context_window.py =====

# aimodel/file_read/services/context_window.py
from __future__ import annotations

from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..runtime.model_runtime import current_model_info
from ..utils.streaming import safe_token_count_messages

log = get_logger(__name__)


def estimate_tokens(llm, messages: list[dict[str, str]]) -> int | None:
    try:
        return safe_token_count_messages(llm, messages)
    except Exception:
        return None


def current_n_ctx() -> int:
    eff = SETTINGS.effective()
    try:
        info = current_model_info() or {}
        cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
        return int(cfg.get("nCtx") or eff["nctx_fallback"])
    except Exception:
        return int(eff["nctx_fallback"])


def clamp_out_budget(
    *,
    llm,
    messages: list[dict[str, str]],
    requested_out: int,
    margin: int = 32,
    reserved_system_tokens: int | None = None,
) -> tuple[int, int | None]:
    eff = SETTINGS.effective()
    inp_est = estimate_tokens(llm, messages)
    try:
        prompt_est = inp_est if inp_est is not None else safe_token_count_messages(llm, messages)
    except Exception:
        prompt_est = int(eff["token_estimate_fallback"])
    n_ctx = current_n_ctx()
    rst = int(reserved_system_tokens or 0)
    min_out = int(eff.get("min_out_tokens", 16))
    available = max(min_out, n_ctx - prompt_est - margin - rst)
    safe_out = max(min_out, min(requested_out, available))
    return safe_out, (inp_est if inp_est is not None else None)


def compute_budget_view(
    llm,
    messages: list[dict[str, str]],
    requested_out: int | None = None,
    clamp_margin: int | None = None,
    reserved_system_tokens: int | None = None,
) -> dict[str, Any]:
    eff = SETTINGS.effective()
    n_ctx = current_n_ctx()
    margin = int(clamp_margin if clamp_margin is not None else eff.get("clamp_margin", 32))
    rst = int(
        reserved_system_tokens
        if reserved_system_tokens is not None
        else eff.get("reserved_system_tokens", 0)
    )
    min_out = int(eff.get("min_out_tokens", 16))
    default_out = int(eff.get("out_budget", 512))
    req_out = int(requested_out if requested_out is not None else default_out)

    inp_opt = estimate_tokens(llm, messages)
    if inp_opt is None:
        try:
            prompt_est = safe_token_count_messages(llm, messages)
        except Exception:
            prompt_est = int(eff["token_estimate_fallback"])
    else:
        prompt_est = int(inp_opt)

    available = max(min_out, n_ctx - prompt_est - margin - rst)
    out_budget_chosen = max(min_out, min(req_out, available))
    over_by_tokens = max(0, (prompt_est + req_out + margin + rst) - n_ctx)
    usable_ctx = max(0, n_ctx - margin - rst)

    return {
        "modelCtx": n_ctx,
        "clampMargin": margin,
        "usableCtx": usable_ctx,
        "reservedSystemTokens": rst,
        "inputTokensEst": prompt_est,
        "outBudgetChosen": out_budget_chosen,
        "outBudgetDefault": default_out,
        "outBudgetRequested": req_out,
        "outBudgetMaxAllowed": available,
        "overByTokens": over_by_tokens,
        "minOutTokens": min_out,
        "queueWaitSec": None,
    }

# ===== aimodel/file_read/services/generate_flow.py =====

# aimodel/file_read/services/generate_flow.py
from __future__ import annotations

import asyncio
import time
from collections.abc import AsyncGenerator, AsyncIterator, Callable
from dataclasses import asdict

from fastapi.responses import StreamingResponse

from ..deps.license_deps import is_request_pro_activated
from ..core.settings import SETTINGS
from ..utils.streaming import RUNJSON_END, RUNJSON_START
from ..core.logging import get_logger

from .cancel import GEN_SEMAPHORE, cancel_event, mark_active
from .streaming_worker import run_stream as _run_stream
from .generate_pipeline import prepare_generation_with_telemetry

log = get_logger(__name__)
run_stream: Callable[..., AsyncIterator[bytes]] = _run_stream


# ---------- SSE helpers ----------

def _sse(event: str | None = None, data: str | dict | None = None, comment: str | None = None) -> bytes:
    """Build an SSE frame: comment (ignored by clients) or event+data."""
    if comment is not None:
        return f": {comment}\n\n".encode("utf-8")

    lines: list[str] = []
    if event:
        lines.append(f"event: {event}")

    if data is None:
        lines.append("data:")
    else:
        if isinstance(data, (dict, list)):
            from json import dumps
            payload = dumps(data, ensure_ascii=False)
        else:
            payload = str(data)
        for line in payload.splitlines() or [""]:
            lines.append(f"data: {line}")

    return ("\n".join(lines) + "\n\n").encode("utf-8")


async def _wait_for_stop(ev: asyncio.Event) -> bool:
    while not ev.is_set():
        await asyncio.sleep(0.05)
    return True


# ---------- main ----------

async def generate_stream_flow(data, request) -> StreamingResponse:
    """
    - Open SSE immediately; send comment heartbeats during PREP.
    - Race STOP vs PREP so cancel works before first token.
    - Catch PREP errors so the stream doesn't close silently.
    """
    eff0 = SETTINGS.effective()
    stopped_marker = eff0.get("stopped_line_marker") or ""
    early_sid = getattr(data, "sessionId", None) or eff0["default_session_id"]

    stop_ev = cancel_event(early_sid)
    if stop_ev.is_set():
        log.info("[gen] clearing stale stop at start sid=%s ev_id=%s", early_sid, id(stop_ev))
        stop_ev.clear()

    async def streamer() -> AsyncGenerator[bytes, None]:
        nonlocal stop_ev

        # 1) Open/flush and send invisible comment
        yield _sse(comment="open")
        await asyncio.sleep(0)
        log.info("[gen] stream opened sid=%s ev_id=%s", early_sid, id(stop_ev))

        # 2) PREP vs STOP race with heartbeats
        log.info("[gen] PREP start sid=%s", early_sid)
        prep_task = asyncio.create_task(prepare_generation_with_telemetry(data, stop_ev=stop_ev))
        stop_task = asyncio.create_task(_wait_for_stop(stop_ev))

        prep: object | None = None
        try:
            while True:
                done, _ = await asyncio.wait(
                    {prep_task, stop_task},
                    timeout=0.5,
                    return_when=asyncio.FIRST_COMPLETED,
                )
                # keep pipe warm; ignored by proper SSE clients
                yield _sse(comment="hb")

                if stop_task in done:
                    log.info("[gen] STOP during PREP sid=%s ev_id=%s", early_sid, id(stop_ev))
                    try:
                        if not prep_task.done():
                            prep_task.cancel()
                            await asyncio.gather(prep_task, return_exceptions=True)
                        else:
                            _ = prep_task.exception()
                    except Exception:
                        pass
                    if SETTINGS.stream_emit_stopped_line:
                        yield _sse(data=stopped_marker)
                    return


                if prep_task in done:
                    try:
                        prep = prep_task.result()
                    except Exception as e:
                        log.exception("[gen] PREP failed sid=%s err=%s", early_sid, e)
                        # Optional: emit a small error event that the FE can log (but ignore in chat text)
                        yield _sse(event="phase", data={"state": "prep_error"})
                        # Show stopped marker if configured; otherwise just end
                        if SETTINGS.stream_emit_stopped_line:
                            yield _sse(data=stopped_marker)
                        return
                    break
        finally:
            try:
                if prep is None:
                    if not prep_task.done():
                        prep_task.cancel()
                        await asyncio.gather(prep_task, return_exceptions=True)
                    else:
                        _ = prep_task.exception()
            except Exception:
                pass

        # 3) If PREP changed session id, swap/correct stop_ev
        if getattr(prep, "session_id", early_sid) != early_sid:
            new_sid = prep.session_id  # type: ignore[attr-defined]
            stop_ev = cancel_event(new_sid)
            log.info("[gen] sid switch %s -> %s ev_id=%s set=%s",
                     early_sid, new_sid, id(stop_ev), stop_ev.is_set())
            if stop_ev.is_set():
                log.info("[gen] clearing stale stop after sid switch sid=%s", new_sid)
                stop_ev.clear()

        # paranoid early stop
        if stop_ev.is_set():
            log.info("[gen] STOP after PREP sid=%s", getattr(prep, "session_id", early_sid))
            if SETTINGS.stream_emit_stopped_line:
                yield _sse(data=stopped_marker)
            return

        # 4) Stream output under semaphore
        q_start = time.perf_counter()
        async with GEN_SEMAPHORE:
            try:
                q_wait = time.perf_counter() - q_start
                if isinstance(prep.budget_view, dict):   # type: ignore[attr-defined]
                    prep.budget_view["queueWaitSec"] = round(q_wait, 3)  # type: ignore[attr-defined]
            except Exception:
                pass

            sid = getattr(prep, "session_id", early_sid)
            mark_active(sid, +1)
            log.info("[gen] streaming start sid=%s", sid)
            out_buf = bytearray()

            def _accum_visible(chunk_bytes: bytes):
                if not chunk_bytes:
                    return
                s = chunk_bytes.decode("utf-8", errors="ignore")
                if RUNJSON_START in s and RUNJSON_END in s:
                    return
                if s.strip() == stopped_marker:
                    return
                out_buf.extend(chunk_bytes)

            try:
                async for chunk in run_stream(
                    llm=prep.llm,                       # type: ignore[attr-defined]
                    messages=prep.packed,               # type: ignore[attr-defined]
                    out_budget=prep.out_budget,         # type: ignore[attr-defined]
                    stop_ev=stop_ev,
                    request=request,
                    temperature=prep.temperature,       # type: ignore[attr-defined]
                    top_p=prep.top_p,                   # type: ignore[attr-defined]
                    input_tokens_est=prep.input_tokens_est,   # type: ignore[attr-defined]
                    t0_request=prep.t_request_start,    # type: ignore[attr-defined]
                    budget_view=prep.budget_view,       # type: ignore[attr-defined]
                    emit_stats=is_request_pro_activated(),
                ):
                    _accum_visible(chunk if isinstance(chunk, (bytes, bytearray)) else chunk.encode("utf-8"))
                    yield chunk
            finally:
                # Persist clean assistant text (strip RUNJSON)
                try:
                    full_text = out_buf.decode("utf-8", errors="ignore").strip()
                    start = full_text.find(RUNJSON_START)
                    if start != -1:
                        end = full_text.find(RUNJSON_END, start)
                        if end != -1:
                            full_text = (full_text[:start] + full_text[end + len(RUNJSON_END):]).strip()
                    if full_text:
                        prep.st["recent"].append({"role": "assistant", "content": full_text})  # type: ignore[attr-defined]
                except Exception:
                    pass

                try:
                    from ..store import apply_pending_for
                    apply_pending_for(sid)
                except Exception:
                    pass

                try:
                    from ..store import list_messages as store_list_messages
                    from ..workers.retitle_worker import enqueue as enqueue_retitle
                    msgs = store_list_messages(sid)
                    last_seq = max((int(m.id) for m in msgs), default=0)
                    enqueue_retitle(sid, [asdict(m) for m in msgs], job_seq=last_seq)
                except Exception:
                    pass

                mark_active(sid, -1)
                log.info("[gen] streaming end sid=%s", sid)

    return StreamingResponse(
        streamer(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
        },
    )


# ---- cancel endpoints ----

async def cancel_session(session_id: str) -> dict[str, bool]:
    ev = cancel_event(session_id)
    before = ev.is_set()
    ev.set()
    after = ev.is_set()
    log.info("[cancel] set sid=%s ev_id=%s before=%s after=%s", session_id, id(ev), before, after)
    return {"ok": True}


async def cancel_session_alias(session_id: str) -> dict[str, bool]:
    return await cancel_session(session_id)

# ===== aimodel/file_read/services/generate_pipeline.py =====

# aimodel/file_read/services/generate_pipeline.py
from __future__ import annotations

import asyncio
import time
from typing import Any

from ..core.logging import get_logger

log = get_logger(__name__)

from ..core.packing_memory_core import PACK_TELEMETRY
from ..core.schemas import ChatBody
from ..core.settings import SETTINGS
from ..rag.retrieve_pipeline import build_rag_block_session_only_with_telemetry
from ..runtime import model_runtime as MR
from ..web.router_ai import decide_web_and_fetch
from .attachments import att_get
from .generate_pipeline_part2 import _finish_prepare_generation_with_telemetry
from .generate_pipeline_support import Prep, _approx_block_tokens, _bool, PrepCancelled
from .packing import build_system_text, pack_with_rollup
from .router_text import compose_router_text
from .session_io import handle_incoming
from ..deps.license_deps import is_request_pro_activated


async def _yield_if_stopping(stop_ev: asyncio.Event | None, where: str, *, hard: bool = False) -> None:
    """Cooperative checkpoint: optionally raise to hard-cancel PREP."""
    if stop_ev and stop_ev.is_set():
        log.info("[PIPE] stop observed at %s", where)
        if hard:
            raise PrepCancelled(where)
    # Let the event loop breathe so other tasks (like cancel handler) can run:
    await asyncio.sleep(0)


async def prepare_generation_with_telemetry(
    data: ChatBody,
    stop_ev: asyncio.Event | None = None,  # injected from generate_flow
) -> Prep:
    # Model readiness & handle early cancel
    MR.ensure_ready()
    await _yield_if_stopping(stop_ev, "ensure_ready.done", hard=True)

    llm = MR.get_llm()
    await _yield_if_stopping(stop_ev, "get_llm.done", hard=True)

    t_request_start = time.perf_counter()
    eff0 = SETTINGS.effective()
    session_id = data.sessionId or eff0["default_session_id"]
    eff = SETTINGS.effective(session_id=session_id)

    rag_global_enabled = bool(eff.get("rag_global_enabled", True))
    rag_session_enabled = bool(eff.get("rag_session_enabled", True))
    force_session_only = bool(eff.get("rag_force_session_only")) or not rag_global_enabled

    temperature = float(eff["default_temperature"] if data.temperature is None else data.temperature)
    top_p = float(eff["default_top_p"] if data.top_p is None else data.top_p)
    out_budget_req = int(eff["default_max_tokens"] if data.max_tokens is None else data.max_tokens)

    auto_web = _bool(eff["default_auto_web"]) if data.autoWeb is None else _bool(data.autoWeb)
    web_k = int(eff["default_web_k"] if data.webK is None else data.webK)
    web_k = max(int(eff["web_k_min"]), min(web_k, int(eff["web_k_max"])))

    auto_rag = _bool(eff["default_auto_rag"]) if data.autoRag is None else _bool(data.autoRag)
    model_ctx = int(eff["model_ctx"])

    # ---- Pro + Activation gate for both Web & RAG (no admin required) ----
    entitled = bool(is_request_pro_activated())
    allow_web = entitled
    allow_rag = entitled

    rag_global_enabled = bool(rag_global_enabled and entitled)
    rag_session_enabled = bool(rag_session_enabled and entitled)
    auto_rag = bool(auto_rag and entitled)
    auto_web = bool(auto_web and entitled)
    # ----------------------------------------------------------------------

    incoming = [
        {"role": m.role, "content": m.content, "attachments": getattr(m, "attachments", None)}
        for m in data.messages or []
    ]
    log.info(f"[PIPE] incoming_msgs={len(incoming)}")

    latest_user = next((m for m in reversed(incoming) if m["role"] == "user"), {})
    latest_user_text = (latest_user.get("content") or "").strip()
    atts = latest_user.get("attachments") or []
    has_atts = bool(atts)
    log.info(
        f"[PIPE] latest_user_text_len={len(latest_user_text)} has_atts={has_atts} att_count={len(atts)}"
    )
    if not latest_user_text and has_atts:
        names = [att_get(a, "name") for a in atts]
        names = [n for n in names if n]
        latest_user_text = "User uploaded: " + (", ".join(names) if names else "files")

    st = handle_incoming(session_id, incoming)
    base_user_text = next((m["content"] for m in reversed(incoming) if m["role"] == "user"), "")

    router_text = compose_router_text(
        st.get("recent", []),
        str(base_user_text or ""),
        st.get("summary", "") or "",
        tail_turns=int(eff["router_tail_turns"]),
        summary_chars=int(eff["router_summary_chars"]),
        max_chars=int(eff["router_max_chars"]),
    )
    await _yield_if_stopping(stop_ev, "router_text.ready", hard=True)

    telemetry: dict[str, Any] = {
        "web": {},
        "rag": {},
        "pack": {},
        "prepSec": round(time.perf_counter() - t_request_start, 6),
    }

    ephemeral_once: list[dict[str, str]] = []
    telemetry["web"]["injectElapsedSec"] = 0.0
    telemetry["web"]["ephemeralBlocks"] = 0

    # ---------------------
    # WEB: decide + fallback
    # ---------------------
    try:
        web_block: str | None = None
        web_tel: dict[str, Any] = {}

        if (
            auto_web
            and allow_web
            and not (has_atts and bool(eff.get("disable_global_rag_on_attachments")))
        ):
            # Can be slow — allow immediate cancel around it
            await _yield_if_stopping(stop_ev, "web.decide_fetch.start", hard=True)
            res = await decide_web_and_fetch(llm, router_text, k=web_k)
            await _yield_if_stopping(stop_ev, "web.decide_fetch.done", hard=True)

            if isinstance(res, tuple):
                web_block = res[0] if len(res) > 0 else None
                tel_candidate = res[1] if len(res) > 1 else None
                if isinstance(tel_candidate, dict):
                    web_tel = tel_candidate
            elif isinstance(res, str):
                web_block = res
            else:
                web_block = None

        telemetry["web"].update(web_tel or {})
        need_flag = (web_tel or {}).get("needed")
        injected_candidate = isinstance(web_block, str) and bool(web_block.strip())

        if need_flag is True and (not injected_candidate) and allow_web:
            try:
                from ..web.orchestrator import build_web_block

                await _yield_if_stopping(stop_ev, "web.orchestrator.start", hard=True)
                fb, fb_tel = await build_web_block(router_text, k=web_k)
                await _yield_if_stopping(stop_ev, "web.orchestrator.done", hard=True)

                log.info(
                    f"[PIPE][WEB] orchestrator block preview: {fb[:200]!r}"
                    if fb
                    else "[PIPE][WEB] orchestrator returned no block"
                )
                log.info(f"[PIPE][WEB] orchestrator telemetry: {fb_tel}")
                if fb and fb.strip():
                    web_block = fb
                    injected_candidate = True
            except Exception as e:
                log.error(f"[PIPE][WEB] orchestrator fallback error: {e}")

        if injected_candidate:
            t0_inject = time.perf_counter()
            web_text = str(eff["web_block_preamble"]) + "\n\n" + web_block.strip()
            max_chars = int(eff.get("web_inject_max_chars") or 0)
            if max_chars > 0 and len(web_text) > max_chars:
                web_text = web_text[:max_chars]
            telemetry["web"]["blockChars"] = len(web_text)
            tok = _approx_block_tokens(llm, "assistant", web_text)
            if tok is not None:
                telemetry["web"]["blockTokensApprox"] = tok
            telemetry["web"]["injected"] = True
            ephemeral_only = bool(eff.get("web_ephemeral_only", True))
            telemetry["web"]["ephemeral"] = ephemeral_only
            telemetry["web"]["droppedFromSummary"] = ephemeral_only
            PACK_TELEMETRY["ignore_ephemeral_in_summary"] = ephemeral_only

            # checkpoint before mutating packed state
            await _yield_if_stopping(stop_ev, "web.inject.prepend", hard=True)
            ephemeral_once.append(
                {
                    "role": "assistant",
                    "content": web_text,
                    "_ephemeral": True if ephemeral_only else False,
                    "_source": "web",
                }
            )
            telemetry["web"]["injectElapsedSec"] = round(time.perf_counter() - t0_inject, 6)
        else:
            telemetry["web"]["injected"] = False
            telemetry["web"]["injectElapsedSec"] = 0.0
    except PrepCancelled:
        # Bubble up unchanged
        raise
    except Exception as e:
        log.error(f"[PIPE][WEB] error: {e}")
        telemetry["web"].setdefault("injected", False)
        telemetry["web"].setdefault("injectElapsedSec", 0.0)

    telemetry["web"]["ephemeralBlocks"] = len(ephemeral_once)
    await _yield_if_stopping(stop_ev, "web.phase.done", hard=True)

    log.info(
        f"[PIPE] has_atts={has_atts} disable_global_rag_on_attachments={bool(eff.get('disable_global_rag_on_attachments'))}"
    )

    # ---------------------
    # RAG: session-only path on attachments
    # ---------------------
    if allow_rag and has_atts and bool(eff.get("disable_global_rag_on_attachments")):
        att_names = [att_get(a, "name") for a in atts if att_get(a, "name")]
        query_for_atts = (base_user_text or "").strip() or " ".join(att_names) or "document"
        log.info(
            f"[PIPE] session-only RAG path query_for_atts={query_for_atts!r} att_names={att_names}"
        )
        t0_att = time.perf_counter()
        try:
            await _yield_if_stopping(stop_ev, "rag.session_only.start", hard=True)
            att_block, att_tel = build_rag_block_session_only_with_telemetry(
                query_for_atts, session_id
            )
            await _yield_if_stopping(stop_ev, "rag.session_only.done", hard=True)

            log.info(f"[PIPE][RAG] session-only query: {query_for_atts!r}")
            log.info(
                f"[PIPE][RAG] session-only block preview: {att_block[:200]!r}"
                if att_block
                else "[PIPE][RAG] no session-only block"
            )
        except PrepCancelled:
            raise
        except Exception:
            att_block, att_tel = (None, {})
        if att_tel:
            telemetry["rag"].update(att_tel)
        log.info(
            f"[PIPE] session-only RAG built={bool(att_block)} block_chars={len(att_block or '')}"
        )
        if att_block:
            rag_text = str(eff["rag_block_preamble"]) + "\n\n" + att_block
            telemetry["rag"]["sessionOnly"] = True
            telemetry["rag"]["mode"] = "session-only"
            telemetry["rag"]["blockChars"] = len(rag_text)
            tok = _approx_block_tokens(llm, "assistant", rag_text)
            if tok is not None:
                telemetry["rag"]["sessionOnlyTokensApprox"] = tok
            telemetry["rag"]["injected"] = True

            await _yield_if_stopping(stop_ev, "rag.session_only.inject", hard=True)
            ephemeral_once.append({"role": "assistant", "content": rag_text, "_ephemeral": True})
        else:
            telemetry["rag"]["sessionOnly"] = False
            telemetry["rag"].setdefault("injected", False)
        telemetry["rag"]["sessionOnlyBuildSec"] = round(time.perf_counter() - t0_att, 6)

    # ---------------------
    # PACK messages (can be heavy)
    # ---------------------
    system_text = build_system_text()
    await _yield_if_stopping(stop_ev, "pack.prep", hard=True)

    t_pack0 = time.perf_counter()
    packed, st["summary"], _ = pack_with_rollup(
        system_text=system_text,
        summary=st["summary"],
        recent=st["recent"],
        max_ctx=model_ctx,
        out_budget=out_budget_req,
        ephemeral=ephemeral_once,
    )
    telemetry["packSec"] = round(time.perf_counter() - t_pack0, 6)
    await _yield_if_stopping(stop_ev, "pack.done", hard=True)

    # Hand off to part2; propagate stop_ev for more checkpoints there
    return await _finish_prepare_generation_with_telemetry(
        llm,
        eff,
        data,
        st,
        router_text,
        latest_user_text,
        base_user_text,
        has_atts,
        force_session_only,
        rag_session_enabled,
        rag_global_enabled,
        auto_rag,
        telemetry,
        packed,
        out_budget_req,
        temperature,
        top_p,
        t_request_start,
        session_id,
        stop_ev=stop_ev,  # pass through
    )

# ===== aimodel/file_read/services/generate_pipeline_part2.py =====

# aimodel/file_read/services/generate_pipeline_part2.py
from __future__ import annotations

import asyncio
import time

from ..core.logging import get_logger

log = get_logger(__name__)

from ..core.packing_memory_core import PACK_TELEMETRY
from ..rag.router_ai import decide_rag
from .budget import analyze_budget
from .context_window import clamp_out_budget
from .generate_pipeline_support import (
    Prep,
    _approx_block_tokens,
    _diff_find_inserted_block,
    _enforce_fit,
    _tok_count,
    _web_breakdown,
    _web_unattributed,
)
from .packing import maybe_inject_rag_block
from .prompt_utils import chars_len
from .session_io import persist_summary
from .generate_pipeline_support import PrepCancelled # ← hard-cancel signal from part1


async def _yield_if_stopping(
    stop_ev: asyncio.Event | None,
    where: str,
    *,
    hard: bool = False,
) -> None:
    """Cooperative checkpoint; optionally raise to abort PREP immediately."""
    if stop_ev and stop_ev.is_set():
        log.info("[PIPE] stop observed at %s", where)
        if hard:
            raise PrepCancelled(where)
    await asyncio.sleep(0)


async def _finish_prepare_generation_with_telemetry(
    llm,
    eff,
    data,
    st,
    router_text,
    latest_user_text,
    base_user_text,
    has_atts,
    force_session_only,
    rag_session_enabled,
    rag_global_enabled,
    auto_rag,
    telemetry,
    packed,
    out_budget_req,
    temperature,
    top_p,
    t_request_start,
    session_id,
    *,
    stop_ev: asyncio.Event | None = None,  # ← propagated from PREP
) -> Prep:
    must_inject_session = bool(
        force_session_only
        and rag_session_enabled
        and (not has_atts)
        and (not telemetry.get("web", {}).get("ephemeralBlocks"))
    )
    rag_router_allowed = (
        (rag_session_enabled or rag_global_enabled)
        and (not (has_atts and bool(eff["disable_global_rag_on_attachments"])))
    ) or must_inject_session

    web_needed = bool((telemetry.get("web") or {}).get("needed"))
    web_injected = bool((telemetry.get("web") or {}).get("injected"))
    if web_needed or web_injected:
        rag_router_allowed = False
        telemetry.setdefault("rag", {})
        telemetry["rag"]["routerSkipped"] = True
        telemetry["rag"]["routerSkippedReason"] = (
            "web_needed" if web_needed else "web_block_present"
        )

    ephemeral_once: list[dict[str, str]] = []

    # ---------------------
    # RAG Router + Inject
    # ---------------------
    if rag_router_allowed and bool(eff["rag_enabled"]) and (not ephemeral_once):
        rag_need = False
        rag_query: str | None = None
        if must_inject_session:
            rag_need = True
            rag_query = (latest_user_text or base_user_text or "").strip()
            telemetry["rag"]["routerDecideSec"] = 0.0
            telemetry["rag"]["routerNeeded"] = True
            telemetry["rag"]["routerForcedSession"] = True
            telemetry["rag"]["routerQuery"] = rag_query
        else:
            t_router0 = time.perf_counter()
            if auto_rag:
                await _yield_if_stopping(stop_ev, "rag.router.start", hard=True)
                try:
                    rag_need, rag_query = decide_rag(llm, router_text)
                except Exception:
                    rag_need, rag_query = (False, None)
                await _yield_if_stopping(stop_ev, "rag.router.done", hard=True)
            telemetry["rag"]["routerDecideSec"] = round(time.perf_counter() - t_router0, 6)
            telemetry["rag"]["routerNeeded"] = bool(rag_need)
            if rag_query is not None:
                telemetry["rag"]["routerQuery"] = rag_query

        skip_rag = bool(ephemeral_once) or not rag_need
        tokens_before = _tok_count(llm, packed)

        t_inject0 = time.perf_counter()
        log.info(f"[PIPE][RAG] router query: {rag_query!r} skip_rag={skip_rag}")

        await _yield_if_stopping(stop_ev, "rag.inject.start", hard=True)
        res = maybe_inject_rag_block(
            packed,
            session_id=session_id,
            skip_rag=skip_rag,
            rag_query=rag_query,
            force_session_only=force_session_only,
        )
        await _yield_if_stopping(stop_ev, "rag.inject.done", hard=True)

        telemetry["rag"]["injectBuildSec"] = round(time.perf_counter() - t_inject0, 6)

        if isinstance(res, tuple):
            packed2 = res[0]
            tel = res[1] if len(res) > 1 and isinstance(res[1], dict) else {}
            block_text = res[2] if len(res) > 2 and isinstance(res[2], str) else None
        else:
            packed2 = res
            tel = {}
            block_text = None

        if tel:
            telemetry["rag"].update(tel)

        if block_text:
            log.debug(f"[PIPE][RAG] injected block preview: {block_text[:200]!r}")
            telemetry["rag"]["blockChars"] = len(block_text)
            tok = _approx_block_tokens(llm, "user", block_text)
            if tok is not None:
                telemetry["rag"]["blockTokensApprox"] = tok
            telemetry["rag"]["injected"] = True
            telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or (
                "session-only" if force_session_only else "global"
            )
        else:
            inserted = _diff_find_inserted_block(packed, packed2)
            if inserted and isinstance(inserted.get("content"), str):
                log.debug(f"[PIPE][RAG] diff-inserted block preview: {inserted['content'][:200]!r}")
                text = inserted["content"]
                telemetry["rag"]["blockChars"] = len(text)
                tok = _approx_block_tokens(llm, "user", text)
                if tok is not None:
                    telemetry["rag"]["blockTokensApprox"] = tok
                telemetry["rag"]["injected"] = True
                telemetry["rag"]["mode"] = telemetry["rag"].get("mode") or (
                    "session-only" if force_session_only else "global"
                )

        tokens_after = _tok_count(llm, packed2)
        if tokens_before is not None:
            telemetry["rag"]["packedTokensBefore"] = tokens_before
        if tokens_after is not None:
            telemetry["rag"]["packedTokensAfter"] = tokens_after
        if tokens_before is not None and tokens_after is not None:
            telemetry["rag"]["ragTokensAdded"] = max(0, tokens_after - tokens_before)

        packed = packed2
    else:
        telemetry.setdefault("rag", {})
        telemetry["rag"]["routerSkipped"] = True
        if telemetry.get("web", {}).get("ephemeralBlocks"):
            telemetry["rag"]["routerSkippedReason"] = "ephemeral_block_present"
        elif not rag_router_allowed:
            telemetry["rag"]["routerSkippedReason"] = "attachments_disable_global_or_rag_disabled"
        elif not bool(eff["rag_enabled"]):
            telemetry["rag"]["routerSkippedReason"] = "rag_disabled"
        log.info(f"[PIPE] rag_router_skipped reason={telemetry['rag'].get('routerSkippedReason')}")

    await _yield_if_stopping(stop_ev, "rag.phase.done", hard=True)

    # ---------------------
    # Fit / Budget / Summaries
    # ---------------------
    packed, out_budget_adj = _enforce_fit(llm, eff, packed, out_budget_req)
    await _yield_if_stopping(stop_ev, "fit.done", hard=True)

    packed_chars = chars_len(packed)
    telemetry["packedChars"] = packed_chars
    telemetry["messages"] = len(packed)

    # ---- pull PackTel -> telemetry['pack'] (+ optional legacy mirrors) ----
    try:
        pack_tel = PACK_TELEMETRY.model_dump()  # Pydantic v2
    except AttributeError:
        pack_tel = PACK_TELEMETRY.dict()  # Pydantic v1

    telemetry.setdefault("pack", {}).update(pack_tel)

    for k in (
        "summarySec",
        "summaryTokensApprox",
        "summaryUsedLLM",
        "finalTrimSec",
        "compressSec",
        "packInputTokensApprox",
        "packMsgs",
        "finalTrimTokensBefore",
        "finalTrimTokensAfter",
        "finalTrimDroppedMsgs",
        "finalTrimDroppedApproxTokens",
        "finalTrimSummaryShrunkFromChars",
        "finalTrimSummaryShrunkToChars",
        "finalTrimSummaryDroppedChars",
        "rollStartTokens",
        "rollOverageTokens",
    ):
        telemetry[k] = pack_tel.get(k, telemetry.get(k))

    persist_summary(session_id, st["summary"])
    await _yield_if_stopping(stop_ev, "summary.persisted", hard=True)

    budget_view = analyze_budget(
        llm=llm,
        messages=packed,
        requested_out_tokens=out_budget_adj,
        clamp_margin=int(eff["clamp_margin"]),
        reserved_system_tokens=int(eff.get("reserved_system_tokens") or 0),
    ).to_dict()
    await _yield_if_stopping(stop_ev, "budget.analyzed", hard=True)

    wb = _web_breakdown(telemetry.get("web", {}))
    telemetry.setdefault("web", {})["breakdown"] = wb
    telemetry["web"]["breakdown"]["unattributedWebSec"] = _web_unattributed(
        telemetry.get("web", {}), wb
    )
    telemetry["web"]["breakdown"]["prepSec"] = float(telemetry.get("prepSec") or 0.0)

    budget_view.setdefault("web", {}).update(telemetry.get("web", {}))
    budget_view.setdefault("rag", {}).update(telemetry.get("rag", {}))
    budget_view.setdefault("pack", {}).update(telemetry.get("pack", {}))

    out_budget, input_tokens_est = clamp_out_budget(
        llm=llm, messages=packed, requested_out=out_budget_adj, margin=int(eff["clamp_margin"])
    )
    await _yield_if_stopping(stop_ev, "budget.clamped", hard=True)

    budget_view.setdefault("request", {})
    budget_view["request"]["outBudgetRequested"] = out_budget_adj
    budget_view["request"]["temperature"] = temperature
    budget_view["request"]["top_p"] = top_p

    return Prep(
        llm=llm,
        session_id=session_id,
        packed=packed,
        st=st,
        out_budget=out_budget,
        input_tokens_est=input_tokens_est,
        budget_view=budget_view,
        temperature=temperature,
        top_p=top_p,
        t_request_start=t_request_start,
    )

# ===== aimodel/file_read/services/generate_pipeline_support.py =====

# aimodel/file_read/services/generate_pipeline_support.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any

from ..core.logging import get_logger
from ..utils.streaming import safe_token_count_messages

log = get_logger(__name__)

class PrepCancelled(Exception):
    """Raised when stop_ev is set and we want to abort PREP immediately."""
    pass

@dataclass
class Prep:
    llm: Any
    session_id: str
    packed: list[dict[str, str]]
    st: dict[str, Any]
    out_budget: int
    input_tokens_est: int | None
    budget_view: dict[str, Any]
    temperature: float
    top_p: float
    t_request_start: float


def _bool(v, default: bool = False) -> bool:
    try:
        return bool(v)
    except Exception:
        return bool(default)


def _tok_count(llm, messages: list[dict[str, str]]) -> int | None:
    try:
        return int(safe_token_count_messages(llm, messages))
    except Exception:
        return None


def _approx_block_tokens(llm, role: str, text: str) -> int | None:
    return _tok_count(llm, [{"role": role, "content": text}])


def _diff_find_inserted_block(
    before: list[dict[str, str]], after: list[dict[str, str]]
) -> dict[str, str] | None:
    if len(after) - len(before) != 1:
        return None
    i = 0
    while i < len(before) and before[i] == after[i]:
        i += 1
    if i < len(after):
        return after[i]
    return None


def _web_breakdown(web: dict[str, Any]) -> dict[str, float]:
    w = web or {}
    orch = w.get("orchestrator") or {}
    router = float(w.get("elapsedSec") or 0.0)
    summarize = float((w.get("summarizer") or {}).get("elapsedSec") or 0.0)
    inject = float(w.get("injectElapsedSec") or 0.0)
    search_total = 0.0
    s1 = orch.get("search") or {}
    for k in ("elapsedSecTotal", "elapsedSec"):
        if isinstance(s1.get(k), (int, float)):
            search_total = float(s1[k])
            break
    fetch1 = float((orch.get("fetch1") or {}).get("totalSec") or 0.0)
    fetch2 = float((orch.get("fetch2") or {}).get("totalSec") or 0.0)
    orch_elapsed = float(orch.get("elapsedSec") or w.get("fetchElapsedSec") or 0.0)
    assemble = orch_elapsed - (search_total + fetch1 + fetch2)
    if assemble < 0:
        assemble = 0.0
    total_pre_ttft = router + summarize + orch_elapsed + inject
    return {
        "routerSec": round(router, 6),
        "summarizeSec": round(summarize, 6),
        "searchSec": round(search_total, 6),
        "fetchSec": round(fetch1, 6),
        "jsFetchSec": round(fetch2, 6),
        "assembleSec": round(assemble, 6),
        "orchestratorSec": round(orch_elapsed, 6),
        "injectSec": round(inject, 6),
        "totalWebPreTtftSec": round(total_pre_ttft, 6),
    }


def _web_unattributed(web: dict[str, Any], breakdown: dict[str, float]) -> float:
    total = float((web or {}).get("fetchElapsedSec") or 0.0)
    explained = (
        float(breakdown.get("searchSec", 0.0))
        + float(breakdown.get("fetchSec", 0.0))
        + float(breakdown.get("jsFetchSec", 0.0))
        + float(breakdown.get("assembleSec", 0.0))
    )
    ua = total - explained
    return round(ua if ua > 0 else 0.0, 6)


def _enforce_fit(
    llm, eff: dict[str, Any], packed: list[dict[str, str]], out_budget_req: int
) -> tuple[list[dict], int]:
    tok = _tok_count(llm, packed) or 0
    capacity = int(eff["model_ctx"]) - int(eff["clamp_margin"])

    def drop_one(px):
        keep_head = (
            2
            if len(px) >= 2
            and isinstance(px[1].get("content"), str)
            and px[1]["content"].startswith(eff["summary_header_prefix"])
            else 1
        )
        if len(px) > keep_head + 1:
            px.pop(keep_head)
            return True
        return False

    def remove_ephemeral_blocks(px):
        i = 0
        removed = False
        while i < len(px):
            m = px[i]
            if m.get("_ephemeral") is True:
                px.pop(i)
                removed = True
            else:
                i += 1
        return removed

    if tok + out_budget_req > capacity:
        if remove_ephemeral_blocks(packed):
            tok = _tok_count(llm, packed) or 0
    while tok + out_budget_req > capacity and drop_one(packed):
        tok = _tok_count(llm, packed) or 0
    if tok >= capacity:
        ob2 = 0
    else:
        ob2 = min(out_budget_req, max(0, capacity - tok))
    return packed, ob2

# ===== aimodel/file_read/services/licensing_core.py =====

from __future__ import annotations

import base64
import json
import os
import stat
import sys
import time
from pathlib import Path
from typing import Any

import httpx
from fastapi import HTTPException

from ..core.http import ExternalServiceError, arequest_json
from ..core.logging import get_logger

log = get_logger(__name__)

# ------------------------------------------------------------------------------
# Paths / constants
# ------------------------------------------------------------------------------

def _canon_email(s: str | None) -> str:
    return (s or "").strip().lower()


def _app_data_dir() -> Path:
    """
    Cross-platform app data root. Can be overridden with LOCALMIND_DATA_DIR.
    """
    override = os.getenv("LOCALMIND_DATA_DIR", "").strip()
    if override:
        return Path(override)

    if sys.platform.startswith("win"):
        base = os.getenv("APPDATA") or str(Path.home() / "AppData" / "Roaming")
        return Path(base) / "LocalAI"
    elif sys.platform == "darwin":
        return Path.home() / "Library" / "Application Support" / "LocalAI"
    else:
        return Path.home() / ".config" / "LocalAI"


APP_DIR = _app_data_dir() / "license"
APP_DIR.mkdir(parents=True, exist_ok=True)

LIC_PATH = APP_DIR / "license.json"
THROTTLE_PATH = APP_DIR / "license.throttle.json"

# Migrate legacy path (best-effort, ignore errors)
_old = Path(os.path.expanduser("~/.localmind/license.json"))
if _old.exists() and (not LIC_PATH.exists()):
    try:
        log.info(f"[license] migrate old -> {LIC_PATH}")
        LIC_PATH.write_text(_old.read_text(encoding="utf-8"), encoding="utf-8")
        try:
            _old.unlink(missing_ok=True)
        except Exception as e:
            log.warning(f"[license] migrate unlink warn {e!r}")
    except Exception as e:
        log.error(f"[license] migrate error {e!r}")

log.info(f"[license] using file {LIC_PATH}")

COOLDOWN_SEC = 0                    # throttle window for refresh calls
EXP_SOON_SEC = 30 * 24 * 3600       # consider license “fresh enough” if >30d left


def _pubkey_hex() -> str:
    """
    Public key (ed25519) used to verify LM1 tokens. Provided via env.
    """
    return (os.getenv("LIC_ED25519_PUB_HEX") or "").strip()


# ------------------------------------------------------------------------------
# Helpers: file IO, encoding, verification
# ------------------------------------------------------------------------------

def current_license_string() -> str:
    rec = _load_current() or {}
    s = (rec.get("license") or "").strip()
    return s


def _b64u_decode(s: str) -> bytes:
    pad = "=" * (-len(s) % 4)
    return base64.urlsafe_b64decode(s + pad)


def _verify(lic: str) -> dict:
    """
    Verify an LM1.<payload>.<sig> token:
      - correct shape
      - signature valid (ed25519)
      - not expired
    Returns parsed payload dict on success; raises ValueError on failure.
    """
    log.info("[license] _verify: start")

    if not lic or not lic.startswith("LM1."):
        log.info("[license] _verify: bad_format")
        raise ValueError("Bad format")

    try:
        _, payload_b64, sig_b64 = lic.split(".", 2)
    except ValueError:
        log.info("[license] _verify: malformed_token")
        raise ValueError("Malformed")

    payload = _b64u_decode(payload_b64)
    sig = _b64u_decode(sig_b64)

    pub = _pubkey_hex()
    if not pub:
        log.info("[license] _verify: missing_public_key")
        raise ValueError("Verifier not configured")

    from nacl.signing import VerifyKey
    from nacl.exceptions import BadSignatureError

    vk = VerifyKey(bytes.fromhex(pub))
    try:
        vk.verify(payload, sig)
    except BadSignatureError:
        log.info("[license] _verify: bad_signature")
        raise ValueError("Invalid signature")

    data = json.loads(payload.decode("utf-8"))
    if "plan" not in data:
        data["plan"] = "pro"

    now = int(time.time())
    exp = int(data.get("exp") or 0)
    if exp and now > exp:
        log.warning(f"[license] _verify: expired exp={exp} now={now}")
        raise ValueError("Expired")

    log.info("[license] _verify: ok")
    return data


def _save_secure(path: Path, obj: dict):
    """
    Atomic write + best-effort 0600 perms.
    """
    log.info(f"[license] _save_secure: path={path}")
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = str(path) + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f)
    os.replace(tmp, path)
    try:
        os.chmod(path, stat.S_IRUSR | stat.S_IWUSR)
    except Exception as e:
        log.info(f"[license] _save_secure: chmod_warn {e!r}")


def _load_current() -> dict | None:
    exists = LIC_PATH.exists()
    log.info(f"[license] _load_current: file={LIC_PATH} exists={exists}")
    if not exists:
        return None
    with open(LIC_PATH, encoding="utf-8") as f:
        return json.load(f)


def _lic_base() -> str:
    """
    Licensing API base. Must be set in env LIC_SERVER_BASE.
    """
    base = (os.getenv("LIC_SERVER_BASE") or "").strip()
    log.info(f"[license] _lic_base: {base or 'MISSING'}")
    if not base:
        raise HTTPException(500, "LIC_SERVER_BASE not configured")
    return base.rstrip("/")


def _throttle_ok(kind: str) -> bool:
    """
    Simple per-kind cooldown to avoid hammering the licensing server.
    """
    now = int(time.time())
    rec: dict[str, Any] = {}
    try:
        with open(THROTTLE_PATH, encoding="utf-8") as f:
            rec = json.load(f)
    except Exception:
        rec = {}

    last = int(rec.get(kind) or 0)
    if now - last < COOLDOWN_SEC:
        log.info(f"[license] throttle: skip kind={kind} last={last} now={now}")
        return False

    rec[kind] = now
    THROTTLE_PATH.parent.mkdir(parents=True, exist_ok=True)
    tmp = str(THROTTLE_PATH) + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(rec, f)
    os.replace(tmp, THROTTLE_PATH)
    log.info(f"[license] throttle: ok kind={kind} now={now}")
    return True


def email_from_auth(auth_payload: dict | None) -> str:
    if not auth_payload:
        return ""
    email = (auth_payload.get("email") or "").strip().lower()
    return email if "@" in email else ""


# ------------------------------------------------------------------------------
# Public license ops (local)
# ------------------------------------------------------------------------------

def apply_license_string(license_str: str) -> dict:
    """
    Verify and persist locally.
    """
    claims = _verify(license_str.strip())
    _save_secure(LIC_PATH, {"license": license_str.strip(), "claims": claims})
    return {"ok": True, "plan": claims.get("plan", "pro"), "exp": claims.get("exp")}


def license_status_local(expected_email: str | None = None) -> dict:
    """
    Return local license status:
      { plan, valid, exp, sub?, mismatch? }
    """
    rec = _load_current()
    if not rec:
        return {"plan": "free", "valid": False, "exp": None}
    try:
        claims = _verify(rec["license"])
        plan = claims.get("plan", "pro")
        exp = int(claims.get("exp") or 0) or None
        sub = _canon_email(claims.get("sub"))
        if expected_email and sub and (sub != _canon_email(expected_email)):
            return {"plan": "free", "valid": False, "exp": None, "mismatch": True}
        return {"plan": plan, "valid": True, "exp": exp, "sub": sub}
    except Exception:
        return {"plan": "free", "valid": False, "exp": None}


def remove_license_file() -> dict:
    """
    Delete local license.json and (best effort) activation.json too.
    """
    try:
        if LIC_PATH.exists():
            LIC_PATH.unlink()
            log.info("[license] delete: removed")
        else:
            log.info("[license] delete: not_exists")

        # also try deleting activation file, if the module exists
        try:
            from .licensing_service import ACT_PATH as _ACT_PATH  # lazy import
            if _ACT_PATH.exists():
                _ACT_PATH.unlink()
        except Exception:
            pass

        return {"ok": True}
    except Exception as e:
        log.error(f"[license] delete: error {e!r}")
        raise HTTPException(500, f"Could not remove license: {e}")


# ------------------------------------------------------------------------------
# Licensing server calls
# ------------------------------------------------------------------------------

async def _lic_get_json(url: str, *, params: dict[str, Any] | None = None) -> dict[str, Any]:
    """
    GET via shared arequest_json helper; maps transport errors into HTTPException.
    """
    try:
        return await arequest_json(
            method="GET",
            url=url,
            service="licensing",
            headers={"Accept": "application/json"},
            params=params or {},
        )
    except ExternalServiceError as e:
        raise HTTPException(e.status or 502, e.detail or "Licensing service error") from e


async def _lic_post_json(url: str, *, body: dict) -> dict[str, Any]:
    """
    POST helper. Our arequest_json signature in this app doesn't accept a JSON kw,
    so we attempt several names; if none work, fall back to raw httpx.
    """
    common_kwargs = ("json", "data", "payload", "body")
    last_err: Exception | None = None

    for kw in common_kwargs:
        try:
            log.info(f"[_lic_post_json] trying kw={kw} body={body}")
            return await arequest_json(
                method="POST",
                url=url,
                service="licensing",
                headers={"Accept": "application/json", "Content-Type": "application/json"},
                **{kw: body},
            )
        except TypeError as e:
            log.warning(f"[_lic_post_json] kw={kw} failed with {e!r}")
            last_err = e
            continue

    # FINAL FALLBACK: plain httpx
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            resp = await client.post(
                url,
                json=body,
                headers={"Accept": "application/json", "Content-Type": "application/json"},
            )
        resp.raise_for_status()
        return resp.json()
    except httpx.HTTPStatusError as e:
        # Bubble up status codes so callers can react (e.g., 403 device_limit_reached)
        raise HTTPException(e.response.status_code, e.response.text) from e
    except Exception as e:
        raise HTTPException(502, f"licensing POST failed: {e}") from e


async def fetch_license_by_session(session_id: str) -> dict:
    base = _lic_base()
    url = f"{base}/api/license/by-session"
    return await _lic_get_json(url, params={"session_id": session_id})


async def install_from_session(session_id: str) -> dict:
    data = await fetch_license_by_session(session_id)
    lic = (data or {}).get("license") or ""
    if not lic:
        raise HTTPException(404, "License not available yet")
    claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": claims})
    return {"ok": True, "plan": claims.get("plan", "pro"), "exp": claims.get("exp")}


async def recover_by_email(email: str) -> dict:
    """
    Try to pull license associated with this email from the licensing server and install it.
    """
    if not email:
        return {"ok": True, "status": "not_found"}

    base = _lic_base()
    url = f"{base}/api/license/by-customer"
    data = await _lic_get_json(url, params={"email": email})
    lic = (data or {}).get("license") or ""
    if not lic:
        return {"ok": True, "status": "not_found"}

    claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": claims})
    return {"ok": True, "status": "installed", "plan": claims.get("plan", "pro"), "exp": claims.get("exp")}


async def refresh_license(email: str, force: bool) -> dict:
    """
    Keep local license fresh:
      - If none installed: attempt recover_by_email(email)
      - If installed and not close to exp (unless force): short-circuit
      - If email mismatch: try to recover for that email
      - Otherwise fetch latest license for the customer and replace
    """
    email = _canon_email(email)
    rec = _load_current()

    if not rec:
        return await recover_by_email(email)

    try:
        claims = _verify(rec["license"])
        sub = _canon_email(claims.get("sub"))
        now = int(time.time())
        exp = int(claims.get("exp") or 0)
        plan = claims.get("plan", "pro")

        # License on disk belongs to a different user than the logged-in one
        if email and sub and (sub != email):
            got = await recover_by_email(email)
            if (got or {}).get("status") == "installed":
                st = license_status_local(expected_email=email)
                return {"ok": True, "status": "updated", **st}
            return {"ok": True, "status": "not_found", "plan": "free"}

        #