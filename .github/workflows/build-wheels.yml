name: build-wheels

on:
  workflow_dispatch:
    inputs:
      llama_tag:
        description: "llama-cpp-python tag (e.g., v0.3.16)"
        required: true
      build_win_cuda:
        description: "Build Windows CUDA (installs CUDA via choco; slower)"
        required: false
        default: "false"

permissions:
  contents: write

concurrency:
  group: build-wheels-${{ github.ref || github.run_id }}
  cancel-in-progress: true

env:
  PY_VER: "3.11"
  WF_VERSION: "build-wheels.yml v2025-09-27.1"
  VULKAN_VER: "1.3.296.0"
  CUDA_VER: "12.4.1" 

jobs:
  ############################
  # WINDOWS (cpu, vulkan, cuda?)
  ############################
  windows:
    runs-on: windows-latest
    strategy:
      fail-fast: false
      matrix:
        backend: [cpu, vulkan, cuda]
    steps:
      - uses: actions/checkout@v4

      - name: Build env (Windows)
        shell: pwsh
        run: |
          echo "FORCE_CMAKE=1" >> $env:GITHUB_ENV
          echo "CMAKE_BUILD_PARALLEL_LEVEL=$env:NUMBER_OF_PROCESSORS" >> $env:GITHUB_ENV

      - name: Clone llama-cpp-python @ tag (with submodules)
        run: >
          git clone --branch ${{ inputs.llama_tag }} --recurse-submodules
          https://github.com/abetlen/llama-cpp-python ext/_llama

      - name: Init submodules (llama.cpp)
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          git submodule sync --recursive
          git submodule update --init --recursive --jobs 8 --depth 1
          git submodule status
          test -f vendor/llama.cpp/CMakeLists.txt || { echo "FATAL: Missing vendor/llama.cpp/CMakeLists.txt"; ls -la vendor/llama.cpp || true; exit 1; }

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VER }}

      - name: Build tools
        run: python -m pip install --upgrade pip build wheel cmake ninja

      - name: Vulkan SDK (Windows) — winget or zip fallback
        if: matrix.backend == 'vulkan'
        shell: pwsh
        run: |
          $ErrorActionPreference = 'Stop'
          $installed = $false

          # Try winget first (present on Windows Server 2022 runners)
          try {
            winget --version
            winget install --id LunarG.VulkanSDK --exact `
              --accept-package-agreements --accept-source-agreements -s winget -e
            $installed = $true
          } catch {
            Write-Host "winget install failed or winget missing: $($_.Exception.Message)"
          }

          if (-not $installed) {
            $ver = "${{ env.VULKAN_VER }}"
            $zipUrl = "https://sdk.lunarg.com/sdk/download/$ver/windows/VulkanSDK-$ver.zip?Human=true"
            $zip    = "$env:TEMP\VulkanSDK-$ver.zip"
            $dest   = "C:\VulkanSDK\$ver"

            Write-Host "Falling back to portable ZIP: $zipUrl"
            Invoke-WebRequest -UseBasicParsing -Uri $zipUrl -OutFile $zip
            New-Item -ItemType Directory -Force -Path $dest | Out-Null
            tar -xf $zip -C $dest
            $installed = Test-Path $dest
          }

          # Discover the newest installed SDK and export env
          $vks = (Get-ChildItem 'C:\VulkanSDK' -Directory `
                  | Sort-Object Name -Descending | Select-Object -First 1).FullName
          if (-not $vks) { throw "Vulkan SDK not found under C:\VulkanSDK" }

          echo "VULKAN_SDK=$vks" >> $env:GITHUB_ENV
          echo "$vks\Bin"        >> $env:GITHUB_PATH

          Write-Host "Using Vulkan SDK at: $vks"
          & "$vks\Bin\glslc.exe" --version


      - name: Install CUDA Toolkit (Windows) — winget with fallback
        if: matrix.backend == 'cuda' && inputs.build_win_cuda == 'true'
        shell: pwsh
        run: |
          $ErrorActionPreference = 'Stop'
          $installed = $false
          try {
            winget --version
            # Try exact version first; if it fails, try latest
            try {
              winget install --id NVIDIA.CUDA --exact `
                --version "${{ env.CUDA_VER }}" `
                --accept-package-agreements --accept-source-agreements -e
              $installed = $true
            } catch {
              Write-Host "winget exact version failed; trying latest NVIDIA.CUDA"
              winget install --id NVIDIA.CUDA --accept-package-agreements --accept-source-agreements -e
              $installed = $true
            }
          } catch {
            Write-Host "winget not available; falling back to NVIDIA web installer"
            # Fallback: download a pinned network installer (update if needed)
            $url = "https://developer.download.nvidia.com/compute/cuda/${{ env.CUDA_VER }}/network_installers/cuda_${{ env.CUDA_VER }}_windows_network.exe"
            $exe = "$env:TEMP\cuda_${{ env.CUDA_VER }}_windows_network.exe"
            Invoke-WebRequest -UseBasicParsing -Uri $url -OutFile $exe
            Start-Process -FilePath $exe -ArgumentList "-s" -Wait
            $installed = $true
          }

          if (-not $installed) { throw "CUDA install failed" }

          # Locate the newest CUDA folder and export env vars
          $root = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA"
          if (-not (Test-Path $root)) { throw "CUDA root not found at $root" }
          $verDir = (Get-ChildItem $root -Directory | Sort-Object Name -Descending | Select-Object -First 1)
          if (-not $verDir) { throw "No CUDA version directories under $root" }

          $cudaPath = $verDir.FullName
          echo "CUDAToolkit_ROOT=$cudaPath" >> $env:GITHUB_ENV
          echo "$cudaPath\bin"              >> $env:GITHUB_PATH
          echo "$cudaPath\libnvvp"          >> $env:GITHUB_PATH

          & "$cudaPath\bin\nvcc.exe" --version
          Write-Host "CUDA installed at: $cudaPath"

      - name: Set CMAKE_ARGS (Windows)
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        shell: bash
        run: |
          if [[ "${{ matrix.backend }}" == "cuda" ]]; then
            echo 'CMAKE_ARGS=-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=75\;80\;86\;89\;90' >> $GITHUB_ENV
          elif [[ "${{ matrix.backend }}" == "vulkan" ]]; then
            echo "CMAKE_ARGS=-DGGML_VULKAN=ON" >> $GITHUB_ENV
          else
            echo "CMAKE_ARGS=" >> $GITHUB_ENV
          fi


      - name: Skip Windows CUDA (not requested)
        if: matrix.backend == 'cuda' && inputs.build_win_cuda != 'true'
        run: echo "Skipping Windows CUDA build because build_win_cuda=false"

      - name: "Sanity: vendored llama.cpp present (Windows)"
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          if [ ! -f vendor/llama.cpp/CMakeLists.txt ]; then
            echo "Re-initting submodules..."
            git submodule sync --recursive
            git submodule update --init --recursive --jobs 8 --depth 1
          fi
          test -f vendor/llama.cpp/CMakeLists.txt
          git -C vendor/llama.cpp log -1 --oneline || true


      - name: Build llama wheel (Windows)
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        run: |
          cd ext/_llama
          python -m pip wheel . -w dist

      - name: Write base reqs (Windows)
        shell: pwsh
        run: |
          @"
          fastapi==0.116.1
          uvicorn[standard]==0.30.6
          numpy==1.26.4
          jinja2==3.1.6
          MarkupSafe==3.0.2
          typing_extensions==4.15.0
          diskcache==5.6.3
          "@ | Out-File -FilePath ext/req_base.txt -Encoding ASCII

      - name: Collect wheels + base deps (Windows)
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        shell: pwsh
        run: |
          $out = "ext/wheels_out/win/${{ matrix.backend }}"
          New-Item -ItemType Directory -Force -Path $out | Out-Null
          Copy-Item ext/_llama/dist/*.whl $out/
          python -m pip download --only-binary=:all: --dest $out -r ext/req_base.txt

      - uses: actions/upload-artifact@v4
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        with:
          name: wheels-win-${{ matrix.backend }}
          path: ext/wheels_out/win/${{ matrix.backend }}/*.whl
          if-no-files-found: error

  ############################
  # LINUX (cpu, cuda, rocm, vulkan)
  ############################
  linux:
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        backend: [cpu, cuda, rocm, vulkan]
    steps:
      - uses: actions/checkout@v4

      - name: Build env (Linux)
        run: |
          echo "FORCE_CMAKE=1" >> $GITHUB_ENV
          echo "CMAKE_BUILD_PARALLEL_LEVEL=$(nproc)" >> $GITHUB_ENV

      - name: Clone llama-cpp-python @ tag (with submodules)
        run: >
          git clone --branch ${{ inputs.llama_tag }} --recurse-submodules
          https://github.com/abetlen/llama-cpp-python ext/_llama

      - name: Init submodules (llama.cpp)
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          git submodule sync --recursive
          git submodule update --init --recursive --jobs 8 --depth 1
          git submodule status
          test -f vendor/llama.cpp/CMakeLists.txt || { echo "FATAL: Missing vendor/llama.cpp/CMakeLists.txt"; ls -la vendor/llama.cpp || true; exit 1; }

      - name: Write base reqs (Linux)
        run: |
          cat > ext/req_base.txt <<'REQ'
          fastapi==0.116.1
          uvicorn[standard]==0.30.6
          numpy==1.26.4
          jinja2==3.1.6
          MarkupSafe==3.0.2
          typing_extensions==4.15.0
          diskcache==5.6.3
          REQ

      # CPU only
      - name: Build CPU natively
        if: matrix.backend == 'cpu'
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y python3-venv cmake ninja-build
          python3 -m venv .venv && . .venv/bin/activate
          python -m pip install --upgrade pip build wheel
          cd ext/_llama
          python -m pip wheel . -w dist
          mkdir -p ../../wheels_out/linux/cpu
          cp dist/*.whl ../../wheels_out/linux/cpu/
          cd ../../
          python -m pip download --only-binary=:all: --dest ext/wheels_out/linux/cpu -r ext/req_base.txt


      # Vulkan only
      - name: Build Vulkan natively
        if: matrix.backend == 'vulkan'
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y \
            python3-venv cmake ninja-build \
            libvulkan-dev glslang-dev spirv-tools
          python3 -m venv .venv && . .venv/bin/activate
          python -m pip install --upgrade pip build wheel
          export CMAKE_ARGS="-DGGML_VULKAN=ON"
          cd ext/_llama
          python -m pip wheel . -w dist
          mkdir -p ../../wheels_out/linux/vulkan
          cp dist/*.whl ../../wheels_out/linux/vulkan/
          cd ../../
          python -m pip download --only-binary=:all: --dest ext/wheels_out/linux/vulkan -r ext/req_base.txt




      # CUDA in NVIDIA Docker
      - name: Build CUDA in CUDA container
        if: matrix.backend == 'cuda'
        uses: addnab/docker-run-action@v3
        with:
          image: nvidia/cuda:12.4.1-devel-ubuntu22.04
          options: -v ${{ github.workspace }}:/ws -w /ws/ext/_llama
          run: |
            set -e
            apt-get update && apt-get install -y python3-pip python3-venv cmake ninja-build git
            # Allow git to operate on the bind mount paths
            git config --global --add safe.directory /ws/ext/_llama
            git config --global --add safe.directory /ws/ext/_llama/vendor/llama.cpp

            python3 -m venv /ws/.venv && . /ws/.venv/bin/activate
            python -m pip install --upgrade pip build wheel

            # Ensure submodules are present (needed for ggml backends)
            git submodule sync --recursive
            git submodule update --init --recursive --jobs 8 --depth 1
            test -f vendor/llama.cpp/CMakeLists.txt

            export CMAKE_ARGS="-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=75\;80\;86\;89\;90"
            python -m pip wheel . -w dist

            mkdir -p /ws/ext/wheels_out/linux/cuda
            cp dist/*.whl /ws/ext/wheels_out/linux/cuda/
            python -m pip download --only-binary=:all: --dest /ws/ext/wheels_out/linux/cuda -r /ws/ext/req_base.txt


      # ROCm in ROCm Docker
      - name: Build ROCm in ROCm container
        if: matrix.backend == 'rocm'
        uses: addnab/docker-run-action@v3
        with:
          image: rocm/dev-ubuntu-22.04:5.7.1
          options: -v ${{ github.workspace }}:/ws -w /ws/ext/_llama
          run: |
            set -e
            apt-get update
            apt-get install -y python3-pip python3-venv cmake ninja-build git \
                              hipblas hipblas-dev rocblas-dev   # <— add these

            # allow git to operate on the bind mount (avoids “dubious ownership”)
            git config --global --add safe.directory /ws/ext/_llama
            git config --global --add safe.directory /ws/ext/_llama/vendor/llama.cpp

            python3 -m venv /ws/.venv && . /ws/.venv/bin/activate
            python -m pip install --upgrade pip build wheel

            # make sure submodules are present
            git submodule sync --recursive
            git submodule update --init --recursive --jobs 8 --depth 1
            test -f vendor/llama.cpp/CMakeLists.txt

            export CMAKE_ARGS="-DGGML_HIP=ON -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1030\;gfx1100"
            # (optional) help CMake find ROCm packages explicitly:
            export CMAKE_PREFIX_PATH="/opt/rocm:${CMAKE_PREFIX_PATH:-}"

            python -m pip wheel . -w dist
            mkdir -p /ws/ext/wheels_out/linux/rocm
            cp dist/*.whl /ws/ext/wheels_out/linux/rocm/
            python -m pip download --only-binary=:all: --dest /ws/ext/wheels_out/linux/rocm -r /ws/ext/req_base.txt



      - uses: actions/upload-artifact@v4
        with:
          name: wheels-linux-${{ matrix.backend }}
          path: ext/wheels_out/linux/${{ matrix.backend }}/*.whl
          if-no-files-found: error

  ############################
  # macOS (cpu, metal / arm64)
  ############################
  mac:
    runs-on: macos-14
    strategy:
      fail-fast: false
      matrix:
        backend: [cpu, metal]
    steps:
      - uses: actions/checkout@v4

      - name: Build env (macOS)
        run: |
          echo "FORCE_CMAKE=1" >> $GITHUB_ENV
          echo "CMAKE_BUILD_PARALLEL_LEVEL=$(sysctl -n hw.ncpu)" >> $GITHUB_ENV

      - name: Clone llama-cpp-python @ tag (with submodules)
        run: >
          git clone --branch ${{ inputs.llama_tag }} --recurse-submodules
          https://github.com/abetlen/llama-cpp-python ext/_llama

      - name: Init submodules (llama.cpp)
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          git submodule sync --recursive
          git submodule update --init --recursive --jobs 8 --depth 1
          git submodule status
          test -f vendor/llama.cpp/CMakeLists.txt || { echo "FATAL: Missing vendor/llama.cpp/CMakeLists.txt"; ls -la vendor/llama.cpp || true; exit 1; }

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VER }}

      - name: Build tools
        run: python -m pip install --upgrade pip build wheel cmake ninja

      - name: Set mac flags (arch/deployment)
        shell: bash
        run: |
          echo "CMAKE_OSX_ARCHITECTURES=arm64" >> $GITHUB_ENV
          echo "MACOSX_DEPLOYMENT_TARGET=13.0" >> $GITHUB_ENV

      - name: Enable Metal
        if: matrix.backend == 'metal'
        shell: bash
        run: |
          echo "CMAKE_ARGS=-DGGML_METAL=ON -DLLAMA_METAL=ON" >> $GITHUB_ENV

      - name: Write base reqs (macOS)
        run: |
          cat > ext/req_base.txt <<'REQ'
          fastapi==0.116.1
          uvicorn[standard]==0.30.6
          numpy==1.26.4
          jinja2==3.1.6
          MarkupSafe==3.0.2
          typing_extensions==4.15.0
          diskcache==5.6.3
          REQ

      - name: "Sanity: vendored llama.cpp present (mac)"
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          if [ ! -f vendor/llama.cpp/CMakeLists.txt ]; then
            echo "Re-initting submodules..."
            git submodule sync --recursive
            git submodule update --init --recursive --jobs 8 --depth 1
          fi
          test -f vendor/llama.cpp/CMakeLists.txt
          git -C vendor/llama.cpp log -1 --oneline || true

          
      - name: Build llama wheel (mac)
        run: |
          cd ext/_llama
          python -m pip wheel . -w dist
          mkdir -p ../../wheels_out/mac/${{ matrix.backend }}
          cp dist/*.whl ../../wheels_out/mac/${{ matrix.backend }}/
          cd ../../
          python -m pip download --only-binary=:all: --dest ext/wheels_out/mac/${{ matrix.backend }} -r ext/req_base.txt

      - uses: actions/upload-artifact@v4
        with:
          name: wheels-mac-${{ matrix.backend }}
          path: ext/wheels_out/mac/${{ matrix.backend }}/*.whl
          if-no-files-found: error

  ############################
  # PACKAGE & RELEASE
  ############################
  release:
    needs: [windows, linux, mac]
    runs-on: ubuntu-22.04
    steps:
      - name: Ensure zip
        run: sudo apt-get update && sudo apt-get install -y zip

      - uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: Assemble per-OS zips + manifest
        run: |
          set -e
          mkdir -p out/win out/linux out/mac

          find artifacts -type f -name "*.whl" -path "*/wheels-win-*"   -exec cp {} out/win/ \;
          find artifacts -type f -name "*.whl" -path "*/wheels-linux-*" -exec cp {} out/linux/ \;
          find artifacts -type f -name "*.whl" -path "*/wheels-mac-*"   -exec cp {} out/mac/ \;

          (cd out/win   && zip -r ../wheels-win-${{ inputs.llama_tag }}.zip .)
          (cd out/linux && zip -r ../wheels-linux-${{ inputs.llama_tag }}.zip .)
          (cd out/mac   && zip -r ../wheels-mac-${{ inputs.llama_tag }}.zip .)

          sha256sum out/*.zip > out/sha256s.txt

          cat > out/manifest.json <<EOF
          {
            "bundle": "rb-${{ inputs.llama_tag }}",
            "created": "$(date -u +%FT%TZ)",
            "assets": {
              "win":   { "file": "wheels-win-${{ inputs.llama_tag }}.zip" },
              "linux": { "file": "wheels-linux-${{ inputs.llama_tag }}.zip" },
              "mac":   { "file": "wheels-mac-${{ inputs.llama_tag }}.zip" }
            }
          }
          EOF

      - name: Publish GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: wheels-${{ inputs.llama_tag }}
          name: Wheels ${{ inputs.llama_tag }}
          files: |
            out/wheels-*.zip
            out/manifest.json
            out/sha256s.txt
