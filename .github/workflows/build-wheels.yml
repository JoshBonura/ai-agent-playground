name: build-wheels
on:
  workflow_dispatch:
    inputs:
      llama_tag:
        description: "llama-cpp-python tag (e.g., v0.3.16)"
        required: true
      build_win_cuda:
        description: "Build Windows CUDA (installs CUDA via choco; slower)"
        required: false
        default: "false"

permissions:
  contents: write

env:
  PY_VER: "3.11"

jobs:
  ############################
  # WINDOWS (cpu, vulkan, cuda?)
  ############################
  windows:
    runs-on: windows-latest
    strategy:
      fail-fast: false
      matrix:
        backend: [cpu, vulkan, cuda]
    steps:
      - uses: actions/checkout@v4

      - name: Clone llama-cpp-python @ tag (with submodules)
        run: >
          git clone --branch ${{ inputs.llama_tag }} --recurse-submodules
          https://github.com/abetlen/llama-cpp-python ext/_llama

      - name: Init submodules (llama.cpp)
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          git submodule deinit -f .
          git submodule update --init --recursive
          git submodule status
          test -f vendor/llama.cpp/CMakeLists.txt || { echo "FATAL: Missing vendor/llama.cpp/CMakeLists.txt"; exit 1; }

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VER }}

      - name: Build tools
        run: python -m pip install --upgrade pip build wheel cmake ninja

      - name: Vulkan SDK (Windows)
        if: matrix.backend == 'vulkan'
        run: |
          choco install vulkansdk --no-progress -y
          refreshenv

      - name: CUDA Toolkit (Windows)
        if: matrix.backend == 'cuda' && inputs.build_win_cuda == 'true'
        run: |
          choco install cuda --version=12.6 -y --no-progress
          refreshenv

      - name: Set CMAKE_ARGS
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        shell: bash
        run: |
          if [[ "${{ matrix.backend }}" == "cuda" ]]; then
            echo "CMAKE_ARGS=-DGGML_CUDA=ON" >> $GITHUB_ENV
          elif [[ "${{ matrix.backend }}" == "vulkan" ]]; then
            echo "CMAKE_ARGS=-DGGML_VULKAN=ON" >> $GITHUB_ENV
          else
            echo "CMAKE_ARGS=" >> $GITHUB_ENV
          fi

      - name: Skip Windows CUDA (not requested)
        if: matrix.backend == 'cuda' && inputs.build_win_cuda != 'true'
        run: echo "Skipping Windows CUDA build because build_win_cuda=false"

      - name: Build llama wheel (Windows)
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        run: |
          cd ext/_llama
          python -m pip wheel . -w dist

      - name: Write base reqs
        shell: pwsh
        run: |
          @"
          fastapi==0.116.1
          uvicorn[standard]==0.30.6
          numpy==1.26.4
          jinja2==3.1.6
          MarkupSafe==3.0.2
          typing_extensions==4.15.0
          diskcache==5.6.3
          "@ | Out-File -FilePath ext/req_base.txt -Encoding ASCII

      - name: Collect wheels + base deps (Windows)
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        shell: pwsh
        run: |
          $out = "ext/wheels_out/win/${{ matrix.backend }}"
          New-Item -ItemType Directory -Force -Path $out | Out-Null
          Copy-Item ext/_llama/dist/*.whl $out/
          python -m pip download --only-binary=:all: --dest $out -r ext/req_base.txt

      - uses: actions/upload-artifact@v4
        if: matrix.backend != 'cuda' || inputs.build_win_cuda == 'true'
        with:
          name: wheels-win-${{ matrix.backend }}
          path: ext/wheels_out/win/${{ matrix.backend }}/*.whl

  ############################
  # LINUX (cpu, cuda, rocm, vulkan)
  ############################
  linux:
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        backend: [cpu, cuda, rocm, vulkan]
    steps:
      - uses: actions/checkout@v4

      - name: Clone llama-cpp-python @ tag (with submodules)
        run: >
          git clone --branch ${{ inputs.llama_tag }} --recurse-submodules
          https://github.com/abetlen/llama-cpp-python ext/_llama

      - name: Init submodules (llama.cpp)
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          git submodule deinit -f .
          git submodule update --init --recursive
          git submodule status
          test -f vendor/llama.cpp/CMakeLists.txt || { echo "FATAL: Missing vendor/llama.cpp/CMakeLists.txt"; exit 1; }

      - name: Write base reqs
        run: |
          cat > ext/req_base.txt <<'REQ'
          fastapi==0.116.1
          uvicorn[standard]==0.30.6
          numpy==1.26.4
          jinja2==3.1.6
          MarkupSafe==3.0.2
          typing_extensions==4.15.0
          diskcache==5.6.3
          REQ

      # CPU & Vulkan native
      - name: Build CPU/Vulkan natively
        if: matrix.backend == 'cpu' || matrix.backend == 'vulkan'
        env:
          DEBIAN_FRONTEND: noninteractive
        run: |
          sudo apt-get update
          sudo apt-get install -y python3-pip python3-venv cmake ninja-build libvulkan-dev glslang-dev
          python3 -m venv .venv && . .venv/bin/activate
          python -m pip install --upgrade pip build wheel
          if [ "${{ matrix.backend }}" = "vulkan" ]; then export CMAKE_ARGS="-DGGML_VULKAN=ON"; fi
          cd ext/_llama
          python -m pip wheel . -w dist
          mkdir -p ../../wheels_out/linux/${{ matrix.backend }}
          cp dist/*.whl ../../wheels_out/linux/${{ matrix.backend }}/
          cd ../../
          python -m pip download --only-binary=:all: --dest ext/wheels_out/linux/${{ matrix.backend }} -r ext/req_base.txt

      # CUDA in NVIDIA Docker
      - name: Build CUDA in CUDA container
        if: matrix.backend == 'cuda'
        uses: addnab/docker-run-action@v3
        with:
          image: nvidia/cuda:12.4.1-devel-ubuntu22.04
          options: -v ${{ github.workspace }}:/ws -w /ws/ext/_llama
          run: |
            apt-get update && apt-get install -y python3-pip python3-venv cmake ninja-build
            python3 -m venv /ws/.venv && . /ws/.venv/bin/activate
            python -m pip install --upgrade pip build wheel
            export CMAKE_ARGS="-DGGML_CUDA=ON"
            python -m pip wheel . -w dist
            mkdir -p /ws/ext/wheels_out/linux/cuda
            cp dist/*.whl /ws/ext/wheels_out/linux/cuda/
            python -m pip download --only-binary=:all: --dest /ws/ext/wheels_out/linux/cuda -r /ws/ext/req_base.txt

      # ROCm in ROCm Docker
      - name: Build ROCm in ROCm container
        if: matrix.backend == 'rocm'
        uses: addnab/docker-run-action@v3
        with:
          image: rocm/dev-ubuntu-22.04:5.7.1
          options: -v ${{ github.workspace }}:/ws -w /ws/ext/_llama
          run: |
            apt-get update && apt-get install -y python3-pip python3-venv cmake ninja-build git
            python3 -m venv /ws/.venv && . /ws/.venv/bin/activate
            python -m pip install --upgrade pip build wheel
            export CMAKE_ARGS="-DGGML_HIP=ON -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1030;gfx1100"
            python -m pip wheel . -w dist
            mkdir -p /ws/ext/wheels_out/linux/rocm
            cp dist/*.whl /ws/ext/wheels_out/linux/rocm/
            python -m pip download --only-binary=:all: --dest /ws/ext/wheels_out/linux/rocm -r /ws/ext/req_base.txt

      - uses: actions/upload-artifact@v4
        with:
          name: wheels-linux-${{ matrix.backend }}
          path: ext/wheels_out/linux/${{ matrix.backend }}/*.whl

  ############################
  # macOS (cpu, metal / arm64)
  ############################
  mac:
    runs-on: macos-14
    strategy:
      fail-fast: false
      matrix:
        backend: [cpu, metal]
    steps:
      - uses: actions/checkout@v4

      - name: Clone llama-cpp-python @ tag (with submodules)
        run: >
          git clone --branch ${{ inputs.llama_tag }} --recurse-submodules
          https://github.com/abetlen/llama-cpp-python ext/_llama

      - name: Init submodules (llama.cpp)
        working-directory: ext/_llama
        shell: bash
        run: |
          set -e
          git submodule deinit -f .
          git submodule update --init --recursive
          git submodule status
          test -f vendor/llama.cpp/CMakeLists.txt || { echo "FATAL: Missing vendor/llama.cpp/CMakeLists.txt"; exit 1; }

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VER }}

      - name: Build tools
        run: python -m pip install --upgrade pip build wheel cmake ninja

      - name: Set mac flags (arch/deployment)
        shell: bash
        run: |
          echo "CMAKE_OSX_ARCHITECTURES=arm64" >> $GITHUB_ENV
          echo "MACOSX_DEPLOYMENT_TARGET=13.0" >> $GITHUB_ENV

      - name: Enable Metal
        if: matrix.backend == 'metal'
        shell: bash
        run: |
          echo "CMAKE_ARGS=-DGGML_METAL=ON -DLLAMA_METAL=ON" >> $GITHUB_ENV

      - name: Build llama wheel (mac)
        run: |
          cd ext/_llama
          python -m pip wheel . -w dist
          mkdir -p ../../wheels_out/mac/${{ matrix.backend }}
          cp dist/*.whl ../../wheels_out/mac/${{ matrix.backend }}/
          cd ../../
          cat > ext/req_base.txt <<'REQ'
          fastapi==0.116.1
          uvicorn[standard]==0.30.6
          numpy==1.26.4
          jinja2==3.1.6
          MarkupSafe==3.0.2
          typing_extensions==4.15.0
          diskcache==5.6.3
          REQ
          python -m pip download --only-binary=:all: --dest ext/wheels_out/mac/${{ matrix.backend }} -r ext/req_base.txt

      - uses: actions/upload-artifact@v4
        with:
          name: wheels-mac-${{ matrix.backend }}
          path: ext/wheels_out/mac/${{ matrix.backend }}/*.whl

  ############################
  # PACKAGE & RELEASE
  ############################
  release:
    needs: [windows, linux, mac]
    runs-on: ubuntu-22.04
    steps:
      - name: Ensure zip
        run: sudo apt-get update && sudo apt-get install -y zip

      - uses: actions/download-artifact@v4
        with: { path: ./artifacts }

      - name: Assemble per-OS zips + manifest
        run: |
          set -e
          mkdir -p out/win out/linux out/mac

          find artifacts -type f -name "*.whl" -path "*/wheels-win-*"   -exec cp {} out/win/ \;
          find artifacts -type f -name "*.whl" -path "*/wheels-linux-*" -exec cp {} out/linux/ \;
          find artifacts -type f -name "*.whl" -path "*/wheels-mac-*"   -exec cp {} out/mac/ \;

          (cd out/win   && zip -r ../wheels-win-${{ inputs.llama_tag }}.zip .)
          (cd out/linux && zip -r ../wheels-linux-${{ inputs.llama_tag }}.zip .)
          (cd out/mac   && zip -r ../wheels-mac-${{ inputs.llama_tag }}.zip .)

          sha256sum out/*.zip > out/sha256s.txt

          cat > out/manifest.json <<EOF
          {
            "bundle": "rb-${{ inputs.llama_tag }}",
            "created": "$(date -u +%FT%TZ)",
            "assets": {
              "win":   { "file": "wheels-win-${{ inputs.llama_tag }}.zip" },
              "linux": { "file": "wheels-linux-${{ inputs.llama_tag }}.zip" },
              "mac":   { "file": "wheels-mac-${{ inputs.llama_tag }}.zip" }
            }
          }
          EOF

      - name: Publish GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: wheels-${{ inputs.llama_tag }}
          name: Wheels ${{ inputs.llama_tag }}
          files: |
            out/wheels-*.zip
            out/manifest.json
            out/sha256s.txt
