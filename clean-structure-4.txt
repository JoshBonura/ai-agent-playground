 If we still have plenty of time and not forcing, skip network call
        if not force and exp and (exp - now > EXP_SOON_SEC):
            return {"ok": True, "status": "fresh_enough", "plan": plan, "exp": exp}

    except Exception:
        # Local token corrupt/expired → try recover
        return await recover_by_email(email)

    if not force and (not _throttle_ok("refresh")):
        return {"ok": True, "status": "skipped_cooldown"}

    # Pull latest license for this customer
    base = _lic_base()
    url = f"{base}/api/license/by-customer"
    data = await _lic_get_json(url, params={"email": email})

    lic = (data or {}).get("license") or ""
    if not lic:
        return {"ok": True, "status": "not_found", "plan": "free"}

    new_claims = _verify(lic)
    _save_secure(LIC_PATH, {"license": lic, "claims": new_claims})
    return {"ok": True, "status": "updated", "plan": new_claims.get("plan", "pro"), "exp": new_claims.get("exp")}

# ===== aimodel/file_read/services/licensing_service.py =====

from __future__ import annotations

import hashlib
import json
import os
import platform
import subprocess
import sys
import time
from pathlib import Path

from fastapi import HTTPException

from ..core.logging import get_logger
from .licensing_core import APP_DIR, _lic_base, _lic_post_json, _save_secure, current_license_string

log = get_logger(__name__)

# -----------------------------
# Activation paths / constants
# -----------------------------

ACT_PATH = APP_DIR / "activation.json"
APP_SALT = "lm-local-salt-v1"  # rotate to force re-activation for all devices

# -----------------------------
# Device id & local storage
# -----------------------------

def _machine_id() -> str:
    """Platform-specific, privacy-safe ID. Falls back to persistent GUID in APP_DIR."""
    try:
        if sys.platform.startswith("win"):
            out = subprocess.check_output(["wmic", "csproduct", "get", "uuid"], text=True)
            vals = [ln.strip() for ln in out.splitlines() if ln.strip() and "UUID" not in ln.upper()]
            if vals:
                return vals[0]
        elif sys.platform == "darwin":
            out = subprocess.check_output(["ioreg", "-rd1", "-c", "IOPlatformExpertDevice"], text=True)
            for line in out.splitlines():
                if "IOPlatformUUID" in line:
                    return line.split('"')[-2]
        else:
            p = Path("/etc/machine-id")
            if p.exists():
                return p.read_text().strip()
    except Exception:
        pass

    p = APP_DIR / "device.guid"
    if not p.exists():
        p.write_text(os.urandom(16).hex(), encoding="utf-8")
    return p.read_text(encoding="utf-8").strip()


def remove_activation_file() -> None:
    try:
        if ACT_PATH.exists():
            ACT_PATH.unlink()
    except Exception:
        pass


def device_id() -> str:
    mid = _machine_id()
    return hashlib.sha256((mid + APP_SALT).encode("utf-8")).hexdigest()


def _load_activation() -> dict | None:
    if not ACT_PATH.exists():
        return None
    try:
        return json.loads(ACT_PATH.read_text(encoding="utf-8"))
    except Exception:
        return None


def current_device_info() -> dict:
    """
    Handy helper for logging/UX. No network.
    """
    did = device_id()
    rec = _load_activation() or {}
    exp = int(rec.get("exp") or 0) or None
    host = platform.node() or ""
    return {
        "id": did,
        "platform": sys.platform,
        "hostname": host,
        "appVersion": os.getenv("APP_VERSION", ""),
        "activation_present": bool(rec.get("activation_token")),
        "activation_exp": exp,
    }


def get_activation_status() -> dict:
    rec = _load_activation() or {}
    exp = int(rec.get("exp") or 0) or None
    now = int(time.time())
    return {
        "present": bool(rec.get("activation_token")),
        "exp": exp,
        "stale": bool(exp and exp < now),
        "deviceId": device_id(),  # convenient for logs/UI
    }

# -----------------------------
# Network calls
# -----------------------------

async def redeem_activation(license_str: str, device_name: str = "") -> dict:
    """
    One-time per device activation with the licensing server.
    """
    base = _lic_base()
    url = f"{base}/api/activate"
    body = {
        "license": license_str.strip(),
        "device": {
            "id": device_id(),
            "platform": sys.platform,
            "appVersion": os.getenv("APP_VERSION", ""),
            "name": device_name or platform.node() or "",
        },
    }
    data = await _lic_post_json(url, body=body)
    token = (data or {}).get("activation") or ""
    if token:
        _save_secure(ACT_PATH, {"activation_token": token, "exp": data.get("exp")})
    return data


async def refresh_activation() -> dict:
    """
    Rolling refresh: re-issue activation using the stored license.
    If no local license, 404 so caller can no-op.
    """
    lic = (current_license_string() or "").strip()
    if not lic:
        raise HTTPException(404, "license_not_present")

    base = _lic_base()
    url = f"{base}/api/activate"
    body = {
        "license": lic,
        "device": {
            "id": device_id(),
            "platform": sys.platform,
            "appVersion": os.getenv("APP_VERSION", ""),
            "name": platform.node() or "",
        },
    }
    data = await _lic_post_json(url, body=body)
    token = (data or {}).get("activation") or ""
    if token:
        _save_secure(ACT_PATH, {"activation_token": token, "exp": data.get("exp")})
    return data

# ===== aimodel/file_read/services/packing.py =====

from __future__ import annotations

from ..core.logging import get_logger

log = get_logger(__name__)
from typing import Any

from ..core.packing_ops import (build_system, pack_messages,
                                roll_summary_if_needed)
from ..core.settings import SETTINGS
from ..rag.retrieve_pipeline import (
    build_rag_block_session_only_with_telemetry,
    build_rag_block_with_telemetry)


def build_system_text() -> str:
    eff = SETTINGS.effective()
    base = build_system(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
    )
    guidance = str(eff["packing_guidance"])
    return base + guidance


def pack_with_rollup(
    *,
    system_text: str,
    summary: str,
    recent,
    max_ctx: int,
    out_budget: int,
    ephemeral: list[dict[str, str]] | None = None,
) -> tuple[list[dict[str, str]], str, int]:
    eff = SETTINGS.effective()
    packed, input_budget = pack_messages(
        style=str(eff["pack_style"]),
        short=bool(eff["pack_short"]),
        bullets=bool(eff["pack_bullets"]),
        summary=summary,
        recent=recent,
        max_ctx=max_ctx,
        out_budget=out_budget,
    )
    packed, new_summary = roll_summary_if_needed(
        packed=packed,
        recent=recent,
        summary=summary,
        input_budget=input_budget,
        system_text=system_text,
    )
    if ephemeral:
        last_user_idx = None
        for i in range(len(packed) - 1, -1, -1):
            m = packed[i]
            if isinstance(m, dict) and m.get("role") == "user":
                last_user_idx = i
                break
        eph = list(ephemeral)
        if last_user_idx is not None:
            packed = packed[:last_user_idx] + eph + packed[last_user_idx:]
        else:
            packed = packed + eph
    return (packed, new_summary, input_budget)


def maybe_inject_rag_block(
    messages: list[dict],
    *,
    session_id: str | None,
    skip_rag: bool = False,
    rag_query: str | None = None,
    force_session_only: bool = False,
) -> tuple[list[dict], dict[str, Any] | None, str | None]:
    if skip_rag:
        return (messages, None, None)
    if not SETTINGS.get("rag_enabled", True):
        return (messages, None, None)
    if not messages or messages[-1].get("role") != "user":
        return (messages, None, None)

    user_q = rag_query or (messages[-1].get("content") or "")
    use_session_only = force_session_only or not SETTINGS.get("rag_global_enabled", True)

    if use_session_only and SETTINGS.get("rag_session_enabled", True):
        block, tel = build_rag_block_session_only_with_telemetry(user_q, session_id=session_id)
        mode = "session-only"
    else:
        block, tel = build_rag_block_with_telemetry(user_q, session_id=session_id)
        mode = "global"

    if not block:
        log.debug(f"[RAG INJECT] no hits (session={session_id}) q={user_q!r}")
        return (messages, None, None)

    log.debug(f"[RAG INJECT] injecting (session={session_id}) chars={len(block)} mode={mode}")
    injected = messages[:-1] + [{"role": "user", "content": block}, messages[-1]]

    tel = dict(tel or {})
    tel["injected"] = True
    tel["mode"] = mode
    return (injected, tel, block)

# ===== aimodel/file_read/services/prompt_utils.py =====

from __future__ import annotations

import json
from datetime import datetime

from ..core.logging import get_logger

log = get_logger(__name__)


def now_str() -> str:
    return datetime.now().isoformat(timespec="milliseconds")


def chars_len(msgs: list[object]) -> int:
    total = 0
    for m in msgs:
        if isinstance(m, dict):
            c = m.get("content")
        else:
            c = m
        if isinstance(c, str):
            total += len(c)
        elif c is None:
            continue
        else:
            try:
                total += len(json.dumps(c, ensure_ascii=False))
            except Exception:
                pass
    return total

# ===== aimodel/file_read/services/router_text.py =====

from __future__ import annotations

from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)


def compose_router_text(
    recent,
    latest_user_text: str,
    summary: str,
    *,
    tail_turns: int | None = None,
    summary_chars: int | None = None,
    max_chars: int | None = None,
) -> str:
    eff = SETTINGS.effective()
    tt = int(eff["router_tail_turns"]) if tail_turns is None else int(tail_turns)
    sc = int(eff["router_summary_chars"]) if summary_chars is None else int(summary_chars)
    mc = int(eff["router_max_chars"]) if max_chars is None else int(max_chars)
    context_label = eff["router_context_label"]
    summary_label = eff["router_summary_label"]

    parts: list[str] = []
    if latest_user_text:
        parts.append((latest_user_text or "").strip())

    try:
        recent_list = list(recent)
    except Exception:
        recent_list = []

    tail_src = recent_list[-tt:] if tt > 0 else []
    tail_lines: list[str] = []
    for m in reversed(tail_src):
        if not isinstance(m, dict):
            continue
        c = (m.get("content") or "").strip()
        if not c:
            continue
        role = (m.get("role") or "user").strip()
        tail_lines.append(f"{role}: {c}")

    if tail_lines:
        parts.append(context_label + "\n" + "\n".join(tail_lines))

    if summary:
        s = summary.strip()
        if sc > 0 and len(s) > sc:
            s = s[-sc:]
        parts.append(summary_label + "\n" + s)

    out = "\n\n".join(parts).strip()
    if len(out) > mc:
        out = out[:mc].rstrip()
    return out

# ===== aimodel/file_read/services/session_io.py =====

from __future__ import annotations

from ..core.logging import get_logger
from ..core.packing_memory_core import get_session
from ..store import set_summary as store_set_summary

log = get_logger(__name__)


def handle_incoming(session_id: str, incoming: list[dict[str, str]]):
    st = get_session(session_id)
    st.setdefault("_ephemeral_web", [])
    for m in incoming:
        st["recent"].append(m)
    return st


def persist_summary(session_id: str, summary: str):
    try:
        store_set_summary(session_id, summary)
    except Exception:
        pass

# ===== aimodel/file_read/services/streaming_worker.py =====

# aimodel/file_read/services/streaming_worker.py
from __future__ import annotations

import asyncio
import json
import time
from collections.abc import AsyncGenerator

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..utils.streaming import (
    RUNJSON_END,
    RUNJSON_START,
    build_run_json,
    collect_engine_timings,
    watch_disconnect,
)

log = get_logger(__name__)


async def run_stream(
    *,
    llm,
    messages,
    out_budget,
    stop_ev,
    request,
    temperature: float,
    top_p: float,
    input_tokens_est: int | None,
    t0_request: float | None = None,
    budget_view: dict | None = None,
    emit_stats: bool = True,
) -> AsyncGenerator[bytes, None]:
    """
    Thread-safe async streamer:
      - Runs the model's streaming call in a background thread
      - Bridges data into an asyncio.Queue using run_coroutine_threadsafe
      - Yields bytes to the client
    """
    loop = asyncio.get_running_loop()
    q: asyncio.Queue = asyncio.Queue(maxsize=SETTINGS.stream_queue_maxsize)
    SENTINEL = object()

    # Bridge puts from the producer thread into the event loop queue safely.
    def put_sync(item) -> None:
        timeout = getattr(SETTINGS, "stream_queue_thread_put_timeout_sec", 30)
        fut = asyncio.run_coroutine_threadsafe(q.put(item), loop)
        # Block the producer thread until the item is enqueued (bounded by timeout)
        fut.result(timeout=timeout)

    def produce():
        t_start = t0_request or time.perf_counter()
        t_first: float | None = None
        t_last: float | None = None
        t_call: float | None = None
        finish_reason: str | None = None
        err_text: str | None = None
        out_parts: list[str] = []
        stage: dict = {"queueWaitSec": None, "genSec": None}

        try:
            # Create the model stream (may block; done in this worker thread)
            try:
                t_call = time.perf_counter()
                stream = llm.create_chat_completion(
                    messages=messages,
                    stream=True,
                    max_tokens=out_budget,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=SETTINGS.stream_top_k,
                    repeat_penalty=SETTINGS.stream_repeat_penalty,
                    stop=SETTINGS.stream_stop_strings,
                )
            except ValueError as ve:
                # Retry with fewer tokens on context overflow
                if "exceed context window" in str(ve).lower():
                    retry_tokens = max(
                        SETTINGS.stream_retry_min_tokens,
                        int(out_budget * SETTINGS.stream_retry_fraction),
                    )
                    log.warning(
                        "generate: context overflow, retrying with max_tokens=%d",
                        retry_tokens,
                    )
                    stream = llm.create_chat_completion(
                        messages=messages,
                        stream=True,
                        max_tokens=retry_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=SETTINGS.stream_top_k,
                        repeat_penalty=SETTINGS.stream_repeat_penalty,
                        stop=SETTINGS.stream_stop_strings,
                    )
                else:
                    raise

            for chunk in stream:
                if stop_ev.is_set():
                    break

                try:
                    fr = chunk["choices"][0].get("finish_reason")
                    if fr:
                        finish_reason = fr
                except Exception:
                    pass

                piece = chunk["choices"][0]["delta"].get("content", "")
                if not piece:
                    continue

                now = time.perf_counter()
                if t_first is None:
                    t_first = now
                t_last = now
                out_parts.append(piece)

                # Backpressure: block briefly if queue is full, until we can put
                while not stop_ev.is_set():
                    try:
                        put_sync(piece)
                        break
                    except Exception:
                        time.sleep(SETTINGS.stream_backpressure_sleep_sec)

        except Exception as e:
            err_text = str(e)
            log.exception("generate: llm stream error: %s", e)
            try:
                put_sync(f"[aimodel] error: {e}")
            except Exception:
                pass
        finally:
            try:
                out_text = "".join(out_parts)

                if t_first is not None and t_last is not None:
                    stage["genSec"] = round(t_last - t_first, 3)
                if t_start is not None and t_first is not None:
                    stage["ttftSec"] = round(t_first - t_start, 3)
                if t_start is not None and t_last is not None:
                    stage["totalSec"] = round(t_last - t_start, 3)

                if t_call is not None and t_start is not None:
                    stage["preModelSec"] = round(t_call - t_start, 6)
                if t_call is not None and t_first is not None:
                    stage["modelQueueSec"] = round(t_first - t_call, 6)

                if isinstance(budget_view, dict) and "queueWaitSec" in budget_view:
                    stage["queueWaitSec"] = budget_view.get("queueWaitSec")

                try:
                    engine = collect_engine_timings(llm)
                except Exception:
                    engine = None
                if engine:
                    stage["engine"] = engine

                # Populate budget_view breakdown (TTFT attribution)
                if isinstance(budget_view, dict):

                    def _fnum(x) -> float:
                        try:
                            return float(x) if x is not None else 0.0
                        except Exception:
                            return 0.0

                    ttft_val = _fnum(stage.get("ttftSec"))

                    pack = budget_view.get("pack") or {}
                    rag = budget_view.get("rag") or {}
                    web_bd = ((budget_view.get("web") or {}).get("breakdown")) or {}

                    pack_sec = _fnum(pack.get("packSec"))
                    trim_sec = _fnum(pack.get("finalTrimSec"))
                    comp_sec = _fnum(pack.get("compressSec"))

                    rag_router = _fnum(rag.get("routerDecideSec"))

                    build_candidates = (
                        rag.get("injectBuildSec"),
                        rag.get("sessionOnlyBuildSec"),
                        rag.get("blockBuildSec"),
                    )
                    first_build = next((v for v in build_candidates if v is not None), None)
                    rag_build_agg = _fnum(first_build)

                    rag_embed = _fnum(rag.get("embedSec"))
                    rag_s_chat = _fnum(rag.get("searchChatSec"))
                    rag_s_glob = _fnum(rag.get("searchGlobalSec"))
                    rag_dedupe = _fnum(rag.get("dedupeSec"))

                    if rag_build_agg > 0.0:
                        rag_pipeline_sec = rag_build_agg
                    else:
                        rag_pipeline_sec = rag_embed + rag_s_chat + rag_s_glob + rag_dedupe

                    prep_sec = _fnum(web_bd.get("prepSec"))
                    web_pre = _fnum(web_bd.get("totalWebPreTtftSec"))

                    model_queue = _fnum(stage.get("modelQueueSec"))

                    pre_accounted = (
                        pack_sec
                        + trim_sec
                        + comp_sec
                        + rag_router
                        + rag_pipeline_sec
                        + web_pre
                        + prep_sec
                        + model_queue
                    )
                    unattributed = ttft_val - pre_accounted
                    if unattributed < 0.0:
                        unattributed = 0.0

                    budget_view.setdefault("breakdown", {})
                    budget_view["breakdown"].update(
                        {
                            "ttftSec": ttft_val,
                            "preTtftAccountedSec": round(pre_accounted, 6),
                            "unattributedTtftSec": round(unattributed, 6),
                        }
                    )

                run_json = build_run_json(
                    request_cfg={
                        "temperature": temperature,
                        "top_p": top_p,
                        "max_tokens": out_budget,
                    },
                    out_text=out_text,
                    t_start=t_start,
                    t_first=t_first,
                    t_last=t_last,
                    stop_set=stop_ev.is_set(),
                    finish_reason=finish_reason,
                    input_tokens_est=input_tokens_est,
                    budget_view=budget_view,
                    extra_timings=stage,
                    error_text=err_text,
                )
                if SETTINGS.runjson_emit and emit_stats:
                    put_sync(RUNJSON_START + json.dumps(run_json) + RUNJSON_END)
            except Exception:
                pass
            finally:
                try:
                    llm.reset()
                except Exception:
                    pass
                try:
                    put_sync(SENTINEL)
                except Exception:
                    pass

    disconnect_task = asyncio.create_task(watch_disconnect(request, stop_ev))
    producer = asyncio.create_task(asyncio.to_thread(produce))

    try:
        while True:
            item = await q.get()
            if item is SENTINEL:
                break
            yield (item if isinstance(item, bytes) else item.encode("utf-8"))
        if stop_ev.is_set() and SETTINGS.stream_emit_stopped_line:
            yield (f"\n{SETTINGS.stopped_line_marker}\n").encode()
    finally:
        stop_ev.set()
        disconnect_task.cancel()
        try:
            await asyncio.wait_for(producer, timeout=SETTINGS.stream_producer_join_timeout_sec)
        except Exception:
            pass

# ===== aimodel/file_read/services/system_collectors.py =====

from __future__ import annotations
import subprocess, csv, io, platform
from typing import Any, Dict

def _read_gpu_via_nvidia_smi(log) -> list[dict[str, Any]]:
    """
    Ask nvidia-smi for GPU telemetry. Returns [] on error.
    Called by a background poller thread, not in a request path.
    """
    try:
        proc = subprocess.run(
            [
                "nvidia-smi",
                "--query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu",
                "--format=csv,noheader,nounits",
            ],
            capture_output=True, text=True, check=True
        )
        out = proc.stdout.strip()
        if not out:
            return []
        reader = csv.reader(io.StringIO(out))
        gpus = []
        for row in reader:
            if not row or len(row) < 6:
                continue
            idx, name, mem_total, mem_used, mem_free, util = row[:6]
            try:
                gpus.append({
                    "index": int(idx),
                    "name": name.strip(),
                    "memoryTotalBytes": int(mem_total) * 1024 * 1024,
                    "memoryUsedBytes": int(mem_used) * 1024 * 1024,
                    "memoryFreeBytes": int(mem_free) * 1024 * 1024,
                    "utilPercent": float(util),
                })
            except Exception:
                continue
        return gpus
    except Exception as e:
        try:
            log.info("[system] nvidia-smi failed: %r", e)
        except Exception:
            pass
        return []

def read_system_resources_sync(log) -> Dict[str, Any]:
    """
    Synchronous collectors bundled together.
    Runs in a background thread (never in the request path).
    """
    data: Dict[str, Any] = {"cpu": {}, "ram": {}, "gpus": []}

    # CPU/RAM (psutil)
    try:
        import psutil  # type: ignore

        # RAM snapshot (instantaneous)
        vm = psutil.virtual_memory()
        data["ram"] = {
            "totalBytes": int(vm.total),
            "availableBytes": int(vm.available),
            "usedBytes": int(vm.used),
            "percent": float(vm.percent),
        }

        # CPU: delta since last call (non-blocking). Do NOT re-prime here.
        cpu_pct = float(psutil.cpu_percent(interval=None))

        data["cpu"] = {
            "countPhysical": psutil.cpu_count(logical=False) or 0,
            "countLogical": psutil.cpu_count(logical=True) or 0,
            "percent": cpu_pct,
        }
    except Exception as e:
        try:
            log.info("[system] psutil not available: %r", e)
        except Exception:
            pass

    # GPU via nvidia-smi
    gpus = _read_gpu_via_nvidia_smi(log)
    if gpus:
        data["gpus"] = gpus
        data["gpuSource"] = "nvidia-smi"
    else:
        data["gpuSource"] = "none"

    # Helpful debug info
    data["platform"] = platform.platform()
    return data

# ===== aimodel/file_read/services/system_snapshot.py =====

from __future__ import annotations
import asyncio, time
from typing import Any, Dict, Optional
from ..core.logging import get_logger
from .system_collectors import read_system_resources_sync

log = get_logger(__name__)

_SNAPSHOT: Optional[Dict[str, Any]] = None
_LAST_TS: float = 0.0
_LOCK = asyncio.Lock()

async def _collect_once() -> Dict[str, Any]:
    return await asyncio.to_thread(read_system_resources_sync, log)

async def poll_system_snapshot(period_sec: float = 1.0) -> None:
    global _SNAPSHOT, _LAST_TS
    try:
        # ---- One-time warmup so cpu_percent(interval=None) has a baseline ----
        try:
            import psutil  # type: ignore
            # prime baseline (returns immediately)
            await asyncio.to_thread(psutil.cpu_percent, None)
            # small delay so the first real sample isn't 0.0
            await asyncio.sleep(0.25)
        except Exception:
            pass
        # ---------------------------------------------------------------------

        while True:
            snap = await _collect_once()
            snap["ts"] = time.time()
            async with _LOCK:
                _SNAPSHOT = snap
                _LAST_TS = snap["ts"]
            await asyncio.sleep(period_sec)
    except asyncio.CancelledError:
        # clean exit on shutdown
        raise
    except Exception as e:
        log.warning("[system] poll error: %s", e)
        # brief backoff to avoid tight error loop
        try:
            await asyncio.sleep(0.5)
        except asyncio.CancelledError:
            raise

async def get_system_snapshot() -> Dict[str, Any]:
    async with _LOCK:
        return dict(_SNAPSHOT or {
            "cpu": {}, "ram": {}, "gpus": [], "gpuSource": "none", "platform": "", "ts": 0.0
        })

# ===== aimodel/file_read/store/__init__.py =====

from ..core.logging import get_logger
from .chats import (ChatMessageRow, append_message, delete_batch,
                    delete_message, delete_messages_batch, edit_message,
                    get_summary, list_messages, list_paged, set_summary,
                    update_last, upsert_on_first_message)
from .index import ChatMeta

log = get_logger(__name__)

__all__ = [
    "ChatMessageRow",
    "ChatMeta",
    "append_message",
    "delete_batch",
    "delete_message",
    "delete_messages_batch",
    "edit_message",
    "get_summary",
    "list_messages",
    "list_paged",
    "set_summary",
    "update_last",
    "upsert_on_first_message",
]

# ===== aimodel/file_read/store/base.py =====

from __future__ import annotations

import base64
import json
import os
import secrets
import shutil
import tempfile
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from cryptography.hazmat.primitives.ciphers.aead import AESGCM

from ..adaptive.config.paths import app_data_dir
from ..core.crypto_keys import get_user_dek
from ..core.logging import get_logger

log = get_logger(__name__)

APP_DIR = app_data_dir()
USERS_DIR = APP_DIR / "users"


def user_root(uid: str) -> Path:
    r = USERS_DIR / uid
    (r / "chats").mkdir(parents=True, exist_ok=True)
    return r


def index_path(root: Path) -> Path:
    return root / "index.json"


def chats_dir(root: Path) -> Path:
    d = root / "chats"
    d.mkdir(parents=True, exist_ok=True)
    return d


def chat_path(root: Path, session_id: str) -> Path:
    return chats_dir(root) / f"{session_id}.json"


def now_iso() -> str:
    return datetime.now(UTC).isoformat()


def _encrypt_bytes(uid: str, relpath: str, plaintext: bytes) -> bytes:
    key = get_user_dek(uid)  # 32 bytes
    aes = AESGCM(key)
    nonce = secrets.token_bytes(12)
    aad_obj = {"uid": uid, "path": relpath}
    aad = json.dumps(aad_obj, ensure_ascii=False).encode("utf-8")
    ct = aes.encrypt(nonce, plaintext, aad)
    wrapper = {
        "v": 1,
        "alg": "aes-256-gcm",
        "nonce": base64.b64encode(nonce).decode("utf-8"),
        "aad": base64.b64encode(aad).decode("utf-8"),
        "ct": base64.b64encode(ct).decode("utf-8"),
    }
    return json.dumps(wrapper, ensure_ascii=False).encode("utf-8")


def _decrypt_bytes(uid: str, relpath: str, blob: bytes) -> bytes:
    obj = json.loads(blob.decode("utf-8"))
    aes = AESGCM(get_user_dek(uid))
    nonce = base64.b64decode(obj["nonce"])
    aad = base64.b64decode(obj["aad"])
    expected = json.dumps({"uid": uid, "path": relpath}, ensure_ascii=False).encode("utf-8")
    if aad != expected:
        raise ValueError("AAD mismatch")
    ct = base64.b64decode(obj["ct"])
    return aes.decrypt(nonce, ct, aad)


def write_json_encrypted_org(root: Path, path: Path, data):
    # ensure folder exists
    path.parent.mkdir(parents=True, exist_ok=True)
    # use a stable org DEK (e.g., keyring entry for uid="org")
    atomic_write_encrypted("org", root, path, data)


def read_json_encrypted_org(root: Path, path: Path):
    return read_json_encrypted("org", root, path)


def atomic_write_encrypted(uid: str, root: Path, path: Path, data: dict[str, Any] | list[Any]):
    path.parent.mkdir(parents=True, exist_ok=True)
    rel = str(path.relative_to(root))
    plaintext = json.dumps(data, ensure_ascii=False).encode("utf-8")
    blob = _encrypt_bytes(uid, rel, plaintext)

    fd, tmp_path = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    try:
        with os.fdopen(fd, "wb") as f:
            f.write(blob)
            f.flush()
            os.fsync(f.fileno())
        shutil.move(tmp_path, path)
    finally:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def read_json_encrypted(uid: str, root: Path, path: Path) -> Any:
    with path.open("rb") as f:
        blob = f.read()
    rel = str(path.relative_to(root))
    plain = _decrypt_bytes(uid, rel, blob)
    return json.loads(plain.decode("utf-8"))


__all__ = [
    "APP_DIR",
    "USERS_DIR",
    "atomic_write_encrypted",
    "chat_path",
    "chats_dir",
    "index_path",
    "now_iso",
    "read_json_encrypted",
    "user_root",
]

# ===== aimodel/file_read/store/chats.py =====

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any

from ..core.logging import get_logger
from ..core.settings import SETTINGS
from ..rag.store import delete_namespace as rag_delete_namespace
from .base import (atomic_write_encrypted, chat_path, now_iso,
                   read_json_encrypted)
from .index import ChatMeta, load_index, refresh_index_after_change, save_index

log = get_logger(__name__)


def _load_chat(root: Path, uid: str, session_id: str) -> dict[str, Any]:
    p = chat_path(root, session_id)
    if not p.exists():
        return {"sessionId": session_id, "messages": [], "seq": 0, "summary": "", "ownerUid": uid}
    return read_json_encrypted(uid, root, p)


@dataclass
class ChatMessageRow:
    id: int
    sessionId: str
    role: str
    content: str
    createdAt: str
    attachments: list[dict] | None = None


def _normalize_attachments(atts: list[Any] | None) -> list[dict] | None:
    if not atts:
        return None
    out: list[dict] = []
    for a in atts:
        if isinstance(a, dict):
            out.append(
                {"name": a.get("name"), "source": a.get("source"), "sessionId": a.get("sessionId")}
            )
        else:
            try:
                out.append(
                    {
                        "name": getattr(a, "name", None),
                        "source": getattr(a, "source", None),
                        "sessionId": getattr(a, "sessionId", None),
                    }
                )
            except Exception:
                continue
    return out or None


def upsert_on_first_message(
    root: Path, uid: str, email: str, session_id: str, title: str
) -> ChatMeta:
    idx = load_index(root, uid)
    existing = next(
        (r for r in idx if r["sessionId"] == session_id and r.get("ownerUid") == uid), None
    )
    now = now_iso()
    if existing:
        if title and title.strip():
            existing["title"] = title.strip()
        existing["updatedAt"] = now
        save_index(root, uid, idx)
        existing.setdefault("lastMessage", None)
        return ChatMeta(
            id=existing["id"],
            sessionId=existing["sessionId"],
            title=existing["title"],
            lastMessage=existing.get("lastMessage"),
            createdAt=existing["createdAt"],
            updatedAt=existing["updatedAt"],
        )

    next_id = (max((r["id"] for r in idx), default=0) + 1) if idx else 1
    row = {
        "id": next_id,
        "sessionId": session_id,
        "title": title.strip() or "New Chat",
        "lastMessage": None,
        "createdAt": now,
        "updatedAt": now,
        "ownerUid": uid,
        "ownerEmail": email,
    }
    idx.append(row)
    save_index(root, uid, idx)
    _save_chat(
        root,
        uid,
        session_id,
        {"sessionId": session_id, "messages": [], "seq": 0, "summary": "", "ownerUid": uid},
    )
    return ChatMeta(
        id=row["id"],
        sessionId=row["sessionId"],
        title=row["title"],
        lastMessage=row.get("lastMessage"),
        createdAt=row["createdAt"],
        updatedAt=row["updatedAt"],
    )


def update_last(
    root: Path, uid: str, session_id: str, last_message: str | None, maybe_title: str | None
) -> ChatMeta:
    idx = load_index(root, uid)
    row = next((r for r in idx if r["sessionId"] == session_id and r.get("ownerUid") == uid), None)
    if not row:
        raise ValueError(f"Unknown sessionId: {session_id}")
    if last_message is not None:
        row["lastMessage"] = last_message
    if maybe_title and maybe_title.strip():
        row["title"] = maybe_title.strip()
    row["updatedAt"] = now_iso()
    save_index(root, uid, idx)
    row.setdefault("lastMessage", None)
    return ChatMeta(
        id=row["id"],
        sessionId=row["sessionId"],
        title=row["title"],
        lastMessage=row.get("lastMessage"),
        createdAt=row["createdAt"],
        updatedAt=row["updatedAt"],
    )


def append_message(
    root: Path,
    uid: str,
    session_id: str,
    role: str,
    content: str,
    attachments: list[Any] | None = None,
) -> ChatMessageRow:
    data = _load_chat(root, uid, session_id)
    seq = int(data.get("seq", 0)) + 1
    msg = {
        "id": seq,
        "sessionId": session_id,
        "role": role,
        "content": content,
        "createdAt": now_iso(),
    }
    norm_atts = _normalize_attachments(attachments)
    if norm_atts:
        msg["attachments"] = norm_atts
    data["messages"].append(msg)
    data["seq"] = seq
    _save_chat(root, uid, session_id, data)
    refresh_index_after_change(root, uid, session_id, data["messages"])
    return ChatMessageRow(
        id=seq,
        sessionId=session_id,
        role=role,
        content=content,
        createdAt=msg["createdAt"],
        attachments=norm_atts,
    )


def delete_message(root: Path, uid: str, session_id: str, message_id: int) -> int:
    data = _load_chat(root, uid, session_id)
    msgs = data.get("messages", [])
    before = len(msgs)
    msgs = [m for m in msgs if int(m.get("id", -1)) != int(message_id)]
    if len(msgs) == before:
        return 0
    data["messages"] = msgs
    _save_chat(root, uid, session_id, data)
    refresh_index_after_change(root, uid, session_id, msgs)
    return 1


def delete_messages_batch(
    root: Path, uid: str, session_id: str, message_ids: list[int]
) -> list[int]:
    wanted = {int(i) for i in (message_ids or [])}
    if not wanted:
        return []
    data = _load_chat(root, uid, session_id)
    msgs = data.get("messages", [])
    keep, deleted = [], []
    for m in msgs:
        mid = int(m.get("id", -1))
        if mid in wanted:
            deleted.append(mid)
        else:
            keep.append(m)
    if not deleted:
        return []
    data["messages"] = keep
    _save_chat(root, uid, session_id, data)
    refresh_index_after_change(root, uid, session_id, keep)
    return deleted


def list_messages(root: Path, uid: str, session_id: str) -> list[ChatMessageRow]:
    data = _load_chat(root, uid, session_id)
    rows: list[ChatMessageRow] = []
    for m in data.get("messages", []):
        rows.append(
            ChatMessageRow(
                id=m["id"],
                sessionId=m["sessionId"],
                role=m["role"],
                content=m["content"],
                createdAt=m.get("createdAt"),
                attachments=m.get("attachments", []),
            )
        )
    return rows


def list_paged(
    root: Path, uid: str, page: int, size: int, ceiling_iso: str | None
) -> tuple[list[ChatMeta], int, int, bool]:
    rows = load_index(root, uid)
    rows = [r for r in rows if r.get("ownerUid") == uid]
    rows.sort(key=lambda r: r["updatedAt"], reverse=True)
    if ceiling_iso:
        rows = [r for r in rows if r["updatedAt"] <= ceiling_iso]

    total = len(rows)
    min_size = int(SETTINGS["chat_page_min_size"])
    max_size = int(SETTINGS["chat_page_max_size"])
    size = max(min_size, min(max_size, int(size)))
    page = max(0, int(page))

    start = page * size
    end = start + size
    page_rows = rows[start:end]
    total_pages = (total + size - 1) // size if total else 1
    last_flag = end >= total

    metas: list[ChatMeta] = []
    for r in page_rows:
        r.setdefault("lastMessage", None)
        metas.append(
            ChatMeta(
                id=r["id"],
                sessionId=r["sessionId"],
                title=r["title"],
                lastMessage=r.get("lastMessage"),
                createdAt=r["createdAt"],
                updatedAt=r["updatedAt"],
            )
        )
    return metas, total, total_pages, last_flag


def delete_batch(root: Path, uid: str, session_ids: list[str]) -> list[str]:
    for sid in session_ids:
        try:
            chat_path(root, sid).unlink(missing_ok=True)
        except Exception:
            pass
    for sid in session_ids:
        try:
            rag_delete_namespace(sid)
        except Exception:
            pass
    idx = load_index(root, uid)
    keep = [r for r in idx if not (r["sessionId"] in set(session_ids) and r.get("ownerUid") == uid)]
    save_index(root, uid, keep)
    return session_ids


def _save_chat(root: Path, uid: str, session_id: str, data: dict[str, Any]):
    atomic_write_encrypted(uid, root, chat_path(root, session_id), data)


def set_summary(root: Path, uid: str, session_id: str, new_summary: str) -> None:
    data = _load_chat(root, uid, session_id)
    data["summary"] = new_summary or ""
    _save_chat(root, uid, session_id, data)


def get_summary(root: Path, uid: str, session_id: str) -> str:
    data = _load_chat(root, uid, session_id)
    return str(data.get("summary") or "")


def edit_message(
    root: Path, uid: str, session_id: str, message_id: int, new_content: str
) -> ChatMessageRow | None:
    data = _load_chat(root, uid, session_id)
    msgs = data.get("messages", [])
    updated = None
    for m in msgs:
        if int(m.get("id", -1)) == int(message_id):
            m["content"] = new_content
            m["updatedAt"] = now_iso()
            if "attachments" in m and m["attachments"] is not None:
                m["attachments"] = _normalize_attachments(m["attachments"])
            updated = m
            break
    if not updated:
        return None
    _save_chat(root, uid, session_id, data)
    refresh_index_after_change(root, uid, session_id, msgs)
    return ChatMessageRow(
        id=updated["id"],
        sessionId=updated["sessionId"],
        role=updated["role"],
        content=updated["content"],
        createdAt=updated.get("createdAt"),
        attachments=updated.get("attachments", []),
    )


__all__ = [
    "ChatMessageRow",
    "_load_chat",
    "_save_chat",
    "append_message",
    "delete_batch",
    "delete_message",
    "delete_messages_batch",
    "edit_message",
    "get_summary",
    "list_messages",
    "list_paged",
    "set_summary",
    "update_last",
    "upsert_on_first_message",
]

# ===== aimodel/file_read/store/default_settings.json =====

{
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": ["\n\n", "\n\n- ", "\n\n\n"],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_provider": "brave",
  "brave_api_key": "",
  "brave_worker_url": "https://brave-proxy.localmind.workers.dev/brave",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "brave_api_base": "https://api.search.brave.com/res/v1/web/search",
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "web_fetch_timeout_sec": 4,
  "web_fetch_max_chars": 2400,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 4,
  "web_orch_total_char_budget": 1800,
  "web_orch_per_doc_char_budget": 600,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 4,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.3,
  "web_orch_js_retry_low_q": 0.18,
  "web_orch_js_retry_lowish_ratio": 0.6,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": 1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.4,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_orch_diversity": "=== Diversity & Adaptive Chunking ===",
  "web_orch_min_hosts": 3,
  "__comment_web_misc": "=== Misc flags ===",
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": ["\n", "</s>"],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": ["</s>"],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": ["web:", "search:"],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": ["\n", "."],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "retitle_llm_hard_prefix": "You generate a concise chat title.\nReturn ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). No trailing punctuation. Max 6 words.",
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": ["\n⏹ stopped\n"],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": ["\n\n"],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe",
  "style_sys": "You are a helpful assistant. Always follow the user's explicit instructions carefully and exactly. Do not repeat yourself. Stay coherent and complete.",
  "style_patterns": {
    "talk_like": "\\btalk\\s+like\\s+(?P<style>[^.;\\n]+)",
    "respond_like": "\\brespond\\s+like\\s+(?P<style>[^.;\\n]+)",
    "be": "\\bbe\\s+(?P<style>[^.;\\n]+)",
    "from_now": "\\bfrom\\s+now\\s+on[, ]+\\s*(?P<style>[^.;\\n]+)"
  },
  "style_template": "You must talk like {style}. Stay in character but remain helpful and accurate. Follow the user's latest style instructions.",
  "router_decide_prompt": "You are a router deciding whether answering the text requires the public web.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n- When uncertain whether real-time state is required, prefer need=true.\n\nText:\n{text}\nJSON:"
}

# ===== aimodel/file_read/store/index.py =====

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path

from ..core.logging import get_logger
from .base import (atomic_write_encrypted, index_path, now_iso,
                   read_json_encrypted)

log = get_logger(__name__)


def load_index(root: Path, uid: str) -> list[dict]:
    p = index_path(root)
    if not p.exists():
        return []
    try:
        return read_json_encrypted(uid, root, p)
    except Exception as e:
        log.warning(f"[index] failed to read index at {p}: {e!r}")
        return []


def save_index(root: Path, uid: str, rows: list[dict]):
    atomic_write_encrypted(uid, root, index_path(root), rows)


@dataclass
class ChatMeta:
    id: int
    sessionId: str
    title: str
    lastMessage: str | None
    createdAt: str
    updatedAt: str


def refresh_index_after_change(root: Path, uid: str, session_id: str, messages: list[dict]) -> None:
    idx = load_index(root, uid)
    row = next((r for r in idx if r["sessionId"] == session_id and r.get("ownerUid") == uid), None)
    if not row:
        return
    row["updatedAt"] = now_iso()
    last_asst = None
    for m in reversed(messages or []):
        if m.get("role") == "assistant":
            last_asst = m.get("content") or None
            break
    row["lastMessage"] = last_asst
    save_index(root, uid, idx)

# ===== aimodel/file_read/store/migrate.py =====

# store/migrate.py
import json

from .base import APP_DIR, atomic_write_encrypted, index_path, user_root


def migrate_legacy_to_user(uid: str, email: str):
    legacy_idx = APP_DIR / "index.json"
    legacy_chats = APP_DIR / "chats"
    if not legacy_idx.exists() and not legacy_chats.exists():
        return

    root = user_root(uid)
    # migrate index
    if legacy_idx.exists() and not index_path(root).exists():
        try:
            rows = json.loads(legacy_idx.read_text("utf-8"))
            for r in rows:
                r["ownerUid"] = uid
                r["ownerEmail"] = email
            atomic_write_encrypted(uid, root, index_path(root), rows)
            legacy_idx.unlink(missing_ok=True)
        except Exception:
            pass

    # migrate chats
    if legacy_chats.exists():
        for p in legacy_chats.glob("*.json"):
            try:
                data = json.loads(p.read_text("utf-8"))
                data["ownerUid"] = uid
                atomic_write_encrypted(uid, root, (root / "chats" / p.name), data)
                p.unlink(missing_ok=True)
            except Exception:
                pass
        try:
            legacy_chats.rmdir()
        except Exception:
            pass

# ===== aimodel/file_read/store/override_settings.json =====

{
  "brave_api_key": "BSAJsnvyOOOULDffIM8myC1IUk34u-d",
  "web_search_provider": "brave",
  "brave_worker_url": "",
  "brave_api_key_present": true,
  "__comment_general": "=== Tokenization & Prompt Overhead ===",
  "chars_per_token": 4,
  "prompt_per_message_overhead": 4,
  "__comment_memory": "=== Session / Memory Settings ===",
  "recent_maxlen": 50,
  "__comment_summary": "=== Summarization & Compression ===",
  "heuristic_max_bullets": 5,
  "heuristic_max_words": 12,
  "bullet_prefix": "- ",
  "use_fast_summary": true,
  "llm_summary_max_tokens": 60,
  "llm_summary_temperature": 0,
  "llm_summary_top_p": 1,
  "llm_summary_stop": [
    "\n\n",
    "\n\n- ",
    "\n\n\n"
  ],
  "summary_sys_inst": "Summarize ONLY as 3–5 ultra-terse bullets. No greetings, no chit-chat, no meta, no apologies. Keep facts, questions, decisions, actions. Max 12 words per bullet. Output bullets starting with '- '.",
  "summary_user_prefix": "Summarize the following conversation slice.\n\n",
  "summary_user_suffix": "\n\nOutput:\n- ",
  "summary_max_chars": 1200,
  "summary_header_prefix": "Conversation summary so far:\n",
  "final_shrink_summary_keep_ratio": 0.5,
  "final_shrink_summary_min_chars": 200,
  "__comment_system_directives": "=== System Directives for Model Output ===",
  "system_brief_directive": "Keep answers extremely brief: max 2 sentences OR 5 short bullets.",
  "system_bullets_directive": "Use bullet points when possible; each bullet under 15 words.",
  "system_follow_user_style_directive": "Always follow the user's most recent style instructions.",
  "__comment_context": "=== Context Window & Budgets ===",
  "model_ctx": 2048,
  "out_budget": 512,
  "reserved_system_tokens": 256,
  "min_input_budget": 512,
  "skip_overage_lt": 128,
  "max_peel_per_turn": 1,
  "peel_min": 4,
  "peel_frac": 0.33,
  "peel_max": 12,
  "__comment_generation_defaults": "=== Generation Defaults ===",
  "default_temperature": 0.7,
  "default_top_p": 0.95,
  "default_max_tokens": 512,
  "__comment_web_search": "=== Web Search & Router ===",
  "default_auto_web": true,
  "default_web_k": 3,
  "web_k_min": 1,
  "web_k_max": 8,
  "router_tail_turns": 0,
  "router_summary_chars": 0,
  "router_max_chars": 900,
  "router_context_label": "Context:",
  "router_summary_label": "Summary:",
  "web_block_preamble": "Web findings (authoritative — use these to answer accurately; override older knowledge):",
  "__comment_web_provider": "=== Web Search Provider Selection & Config ===",
  "web_search_cache_ttl_sec": 900,
  "web_search_cache_superset_k": 12,
  "brave_api_base": "https://api.search.brave.com/res/v1/web/search",
  "__comment_packing": "=== Packing Settings (System Prompt Assembly) ===",
  "pack_style": "",
  "pack_short": false,
  "pack_bullets": false,
  "packing_guidance": "\nYou may consult the prior messages to answer questions about the conversation itself (e.g., “what did I say first?”). When web context is present, consider it as evidence, prefer newer info if it conflicts with older memory, and respond in your own words.",
  "__comment_misc": "=== Miscellaneous ===",
  "empty_messages_response": "No messages provided.",
  "clamp_margin": 32,
  "default_session_id": "default",
  "stopped_line_marker": "⏹ stopped",
  "__comment_runtime": "=== Runtime / Safety Fallbacks ===",
  "gen_semaphore_permits": 1,
  "nctx_fallback": 4096,
  "token_estimate_fallback": 1024,
  "min_out_tokens": 16,
  "__comment_store": "=== Chat storage ===",
  "chat_default_title": "New Chat",
  "chat_page_min_size": 1,
  "chat_page_max_size": 100,
  "web_fetch_timeout_sec": 4,
  "web_fetch_max_chars": 2400,
  "web_fetch_max_bytes": 1048576,
  "web_fetch_max_parallel": 4,
  "web_fetch_user_agent": "LocalAI/0.1 (+clean-fetch)",
  "__comment_web_orchestrator": "=== Web Orchestrator & Block Assembly ===",
  "web_orch_default_k": 4,
  "web_orch_total_char_budget": 1800,
  "web_orch_per_doc_char_budget": 600,
  "web_orch_max_parallel_fetch": 4,
  "web_orch_overfetch_factor": 2,
  "web_orch_overfetch_min_extra": 4,
  "web_orch_enable_js_retry": true,
  "web_orch_js_retry_avg_q": 0.3,
  "web_orch_js_retry_low_q": 0.18,
  "web_orch_js_retry_lowish_ratio": 0.6,
  "web_orch_js_retry_timeout_add": 4,
  "web_orch_js_retry_timeout_cap": 12,
  "web_orch_js_retry_parallel_delta": 1,
  "web_orch_js_retry_min_parallel": 2,
  "web_block_header": "Web findings for: {query}",
  "web_orch_block_separator": "\n\n",
  "web_orch_min_block_reserve": 200,
  "web_orch_min_chunk_after_shrink": 160,
  "web_orch_head_fraction": 0.4,
  "web_orch_tail_min_chars": 200,
  "web_orch_ellipsis": " … ",
  "web_orch_bullet_prefix": "- ",
  "web_orch_indent_prefix": "  ",
  "web_orch_www_prefix": "www.",
  "web_orch_fetch_cap_multiplier": 2,
  "__comment_web_orch_scoring": "=== Hit scoring weights ===",
  "web_orch_score_w_exact": 3,
  "web_orch_score_w_substr": 2,
  "web_orch_score_w_title_full": 2,
  "web_orch_score_w_title_part": 1,
  "web_orch_score_w_snip_touch": 1,
  "__comment_web_orch_quality": "=== Content quality params ===",
  "web_orch_q_len_norm_divisor": 1600,
  "web_orch_q_len_weight": 0.55,
  "web_orch_q_diversity_weight": 0.55,
  "web_orch_q_penalties": [
    {
      "token": "<script>",
      "mult": 50,
      "cap": 0.3
    },
    {
      "token": "function(",
      "mult": 20,
      "cap": 0.3
    },
    {
      "token": "{",
      "mult": 5,
      "cap": 0.2
    },
    {
      "token": "}",
      "mult": 5,
      "cap": 0.2
    }
  ],
  "__comment_web_orch_diversity": "=== Diversity & Adaptive Chunking ===",
  "web_orch_min_hosts": 3,
  "__comment_web_misc": "=== Misc flags ===",
  "web_ephemeral_only": true,
  "web_inject_max_chars": 0,
  "__comment_web_query_summarizer": "=== Web Query Summarizer ===",
  "query_sum_bypass_short_enabled": true,
  "query_sum_short_max_chars": 32,
  "query_sum_short_max_words": 3,
  "query_sum_prompt": "Summarize the user's request into a concise web search query.\nKeep only the key entities and terms.\nDo not explain, and do not surround the result in quotation marks or other punctuation.\nYou may only delete non-essential words. Do not add, replace, reorder, or paraphrase any words.\nKeep the original word order. Output only the query text.\n\nUser: {text}\nQuery:",
  "query_sum_max_tokens": 24,
  "query_sum_temperature": 0,
  "query_sum_top_p": 1,
  "query_sum_stop": [
    "\n",
    "</s>"
  ],
  "query_sum_overlap_check_enabled": true,
  "query_sum_overlap_jaccard_min": 0.6,
  "__comment_router_decider": "=== Web Router (LLM-based) ===",
  "router_decide_max_tokens": 64,
  "router_decide_temperature": 0,
  "router_decide_top_p": 1,
  "router_decide_stop": [
    "</s>"
  ],
  "__comment_router_control": "=== Router parsing & overrides ===",
  "router_explicit_prefixes": [
    "web:",
    "search:"
  ],
  "router_default_need_when_invalid": true,
  "router_json_extract_regex": "\\{.*?\\}",
  "__comment_router_wrappers": "=== Router wrapper stripping ===",
  "router_strip_wrappers_enabled": true,
  "router_strip_header_regex": "^\\s*\\w[^:\\n]{0,40}:\\s*$",
  "router_strip_split_on_blank": true,
  "router_trim_whitespace": true,
  "__comment_retitle": "=== Retitle (Chat title generation) ===",
  "retitle_enable": true,
  "retitle_queue_maxsize": 64,
  "retitle_preview_chars": 60,
  "retitle_preview_ellipsis": "…",
  "retitle_min_substantial_chars": 12,
  "retitle_require_alpha": true,
  "retitle_pick_first_substantial": true,
  "retitle_pick_latest_substantial": true,
  "retitle_pick_first_user_fallback": true,
  "retitle_grace_ms": 800,
  "retitle_active_backoff_start_ms": 75,
  "retitle_active_backoff_max_ms": 600,
  "retitle_active_backoff_total_ms": 20000,
  "retitle_active_backoff_growth": 1.5,
  "retitle_llm_sys_inst": "You generate ultra-concise chat titles.\nRules: 2–5 words, Title Case, nouns/adjectives only.\nNo articles (a, an, the). No verbs. No punctuation. One line.\nOutput only the title.",
  "retitle_user_prefix": "",
  "retitle_user_suffix": "",
  "retitle_llm_max_tokens": 12,
  "retitle_llm_temperature": 0.1,
  "retitle_llm_top_p": 1,
  "retitle_llm_stop": [
    "\n",
    "."
  ],
  "retitle_enable_sanitize": true,
  "retitle_sanitize_drop_prefix_regex": "^\\s*(\\\"[^\\\"]*\\\"|'[^']*'|[-*•]+|\\d+\\.)\\s*",
  "retitle_sanitize_strip_quotes": true,
  "retitle_sanitize_replace_not_allowed_regex": "[^\\w\\s’']",
  "retitle_sanitize_replace_with": "",
  "retitle_sanitize_max_words": 5,
  "retitle_sanitize_max_chars": 40,
  "retitle_min_user_chars": 15,
  "retitle_llm_hard_prefix": "You generate a concise chat title.\nReturn ONLY a short noun phrase (no verbs, no 'how to', no 'to ...'). No trailing punctuation. Max 6 words.",
  "__comment_rag": "=== Local RAG ===",
  "default_auto_rag": true,
  "disable_web_on_attachments": true,
  "disable_global_rag_on_attachments": true,
  "attachments_retrieve_top_k": 6,
  "rag_enabled": true,
  "rag_top_k": 3,
  "rag_max_chars_per_chunk": 900,
  "rag_chunk_overlap_chars": 0,
  "rag_min_chars": 300,
  "rag_total_char_budget": 1500,
  "rag_db_path": "corpus.sqlite",
  "rag_block_header": "",
  "rag_block_bullet_prefix": "- ",
  "rag_block_indent_prefix": "  ",
  "rag_embedding_model": "intfloat/e5-small-v2",
  "rag_default_need_when_invalid": false,
  "rag_block_preamble": "Local knowledge",
  "rag_tabular_rows_per_table": 8,
  "rag_prioritize_current_attachment": true,
  "rag_new_upload_score_boost": 0.35,
  "rag_rerank_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
  "rag_rerank_top_m": 8,
  "rag_min_score_frac": 0.6,
  "rag_per_source_cap": 2,
  "rag_nohit_message": "⛔ No relevant local entries found for this query. Do not guess.",
  "rag_min_abs_rerank": -5,
  "__comment_streaming": "=== Streaming worker tuning ===",
  "stream_queue_maxsize": 64,
  "stream_backpressure_sleep_sec": 0.005,
  "stream_top_k": 40,
  "stream_repeat_penalty": 1.25,
  "stream_retry_min_tokens": 48,
  "stream_retry_fraction": 0.4,
  "stream_stop_strings": [
    "\n⏹ stopped\n"
  ],
  "stream_emit_stopped_line": true,
  "stream_producer_join_timeout_sec": 2,
  "runjson_emit": true,
  "excel_emit_cells": false,
  "excel_max_cells_per_sheet": 250,
  "excel_named_range_preview": 6,
  "excel_emit_merged": true,
  "excel_number_sigfigs": 6,
  "excel_decimal_max_places": 4,
  "excel_trim_trailing_zeros": true,
  "excel_dates_drop_time_if_midnight": true,
  "excel_time_precision": "minute",
  "excel_value_max_chars": 160,
  "excel_quote_strings": true,
  "excel_infer_max_rows": 100,
  "excel_infer_max_cols": 26,
  "excel_infer_min_header_fill_ratio": 0.5,
  "excel_emit_key_values": false,
  "excel_emit_cell_addresses": false,
  "excel_header_normalize": true,
  "router_rag_decide_prompt": "You are a router deciding whether the user message should query the app's LOCAL knowledge (uploaded files, chat/session documents) via RAG.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- Set need=true if answering would materially benefit from the user's LOCAL knowledge base (e.g., their files, prior session uploads, or internal notes).\n- Set need=false if the answer is general knowledge or can be answered without consulting local files.\n- Do NOT consider the public web here.\n- If you set need=true and you can succinctly restate the search intent for the local KB, put that in \"query\". Otherwise leave \"query\" empty.\n\nText:\n$text\nJSON:",
  "router_rag_json_extract_regex": "\\{[\\s\\S]*?\\}",
  "router_rag_trim_whitespace": true,
  "router_rag_strip_wrappers_enabled": true,
  "router_rag_strip_split_on_blank": true,
  "router_rag_strip_header_regex": "^(Local knowledge:|Sources:)",
  "router_rag_decide_max_tokens": 300,
  "router_rag_decide_temperature": 0,
  "router_rag_decide_top_p": 1,
  "router_rag_decide_stop": [
    "\n\n"
  ],
  "router_rag_default_need_when_invalid": false,
  "csv_value_max_chars": 160,
  "csv_quote_strings": true,
  "csv_header_normalize": true,
  "csv_infer_max_rows": 50,
  "csv_infer_max_cols": 26,
  "doc_debug": false,
  "docx_heading_max_level": 3,
  "docx_use_markdown_headings": true,
  "docx_preserve_bullets": true,
  "docx_include_tables": true,
  "docx_include_headers_footers": false,
  "docx_para_max_chars": 0,
  "docx_drop_empty_lines": true,
  "doc_ole_min_run_chars": 8,
  "doc_ole_max_line_chars": 600,
  "doc_ole_min_alpha_ratio": 0.25,
  "doc_ole_drop_xmlish": true,
  "doc_ole_drop_pathish": true,
  "doc_ole_drop_symbol_lines": true,
  "doc_ole_dedupe_short_repeats": true,
  "pptx_use_markdown_headings": true,
  "pptx_include_notes": true,
  "pptx_include_tables": true,
  "pptx_drop_empty_lines": true,
  "pptx_para_max_chars": 0,
  "pptx_number_slides": true,
  "ppt_drop_empty_lines": true,
  "ppt_dedupe_lines": true,
  "ppt_min_alpha_ratio": 0.4,
  "ppt_max_punct_ratio": 0.5,
  "ppt_max_line_chars": 600,
  "ppt_token_max_chars": 40,
  "pdf_ocr_enable": true,
  "pdf_ocr_when_bad": true,
  "pdf_ocr_dpi": 300,
  "pdf_ocr_max_pages": 0,
  "pdf_ocr_mode": "auto",
  "ocr_lang": "eng",
  "ocr_psm": "3",
  "ocr_oem": "3",
  "ocr_min_chars_for_ok": 32,
  "ocr_min_alnum_ratio_for_ok": 0.15,
  "ocr_min_image_bytes": 16384,
  "pptx_ocr_images": true,
  "docx_ocr_images": true,
  "tesseract_cmd": "E:\\Programs\\Tesseract-OCR\\tesseract.exe",
  "style_sys": "You are a helpful assistant. Always follow the user's explicit instructions carefully and exactly. Do not repeat yourself. Stay coherent and complete.",
  "style_patterns": {
    "talk_like": "\\btalk\\s+like\\s+(?P<style>[^.;\\n]+)",
    "respond_like": "\\brespond\\s+like\\s+(?P<style>[^.;\\n]+)",
    "be": "\\bbe\\s+(?P<style>[^.;\\n]+)",
    "from_now": "\\bfrom\\s+now\\s+on[, ]+\\s*(?P<style>[^.;\\n]+)"
  },
  "style_template": "You must talk like {style}. Stay in character but remain helpful and accurate. Follow the user's latest style instructions.",
  "router_decide_prompt": "You are a router deciding whether answering the text requires the public web.\nRespond with JSON only in exactly this schema:\n{\"need\": true|false, \"query\": \"<text or empty>\"}\n\nDecision principle:\n- The answer requires the web if any part of it depends on information that is not contained in the user text and is not static/stable over time.\n- Capability boundary: Assume you have no access to real-time state (including the current system date/time, clocks, live data feeds) or hidden tools beyond this routing step.\n- If the correct answer depends on real-time state (e.g., ‘current’ values, now/today/tomorrow semantics, live figures, roles that may change, schedules, prices, weather, scores, news), set need=true.\n- If the answer can be derived entirely from the user text plus stable knowledge, set need=false.\n- When uncertain whether real-time state is required, prefer need=true.\n\nText:\n{text}\nJSON:"
}

# ===== aimodel/file_read/telemetry/models.py =====

# aimodel/file_read/telemetry/models.py
from __future__ import annotations

from typing import Any

from pydantic import BaseModel


class PackTel(BaseModel):
    # Timings
    packSec: float = 0.0
    summarySec: float = 0.0
    finalTrimSec: float = 0.0
    compressSec: float = 0.0

    # Summary metrics
    summaryTokensApprox: int = 0
    summaryUsedLLM: bool = False
    summaryBullets: int = 0
    summaryAddedChars: int = 0
    summaryOutTokensApprox: int = 0
    summaryCompressedFromChars: int = 0
    summaryCompressedToChars: int = 0
    summaryCompressedDroppedChars: int = 0

    # Packing metrics
    packInputTokensApprox: int = 0
    packMsgs: int = 0

    # Final trim metrics
    finalTrimTokensBefore: int = 0
    finalTrimTokensAfter: int = 0
    finalTrimDroppedMsgs: int = 0
    finalTrimDroppedApproxTokens: int = 0
    finalTrimSummaryShrunkFromChars: int = 0
    finalTrimSummaryShrunkToChars: int = 0
    finalTrimSummaryDroppedChars: int = 0

    # Rolling summary metrics
    rollStartTokens: int = 0
    rollOverageTokens: int = 0
    rollPeeledMsgs: int = 0
    rollNewSummaryChars: int = 0
    rollNewSummaryTokensApprox: int = 0
    rollEndTokens: int = 0

    # Compatibility flag (used by pipeline)
    ignore_ephemeral_in_summary: bool = False

    # ---- Compatibility helpers ----
    def __getitem__(self, key: str) -> Any:
        return getattr(self, key)

    def __setitem__(self, key: str, value: Any) -> None:
        setattr(self, key, value)

    def update_from(self, **kwargs: Any) -> None:
        for k, v in kwargs.items():
            if hasattr(self, k):
                setattr(self, k, v)

    def reset(self) -> None:
        for name, field in self.model_fields.items():
            setattr(self, name, field.default)

# ===== aimodel/file_read/utils/streaming.py =====

from __future__ import annotations

import asyncio
import time
from pathlib import Path
from typing import Any

from ..core.logging import get_logger
from ..runtime.model_runtime import current_model_info, get_llm

log = get_logger(__name__)

RUNJSON_START = "\n[[RUNJSON]]\n"
RUNJSON_END = "\n[[/RUNJSON]]\n"

STOP_STRINGS = ["</s>", "User:", "\nUser:"]


def strip_runjson(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s
    out, i = [], 0
    while True:
        start = s.find(RUNJSON_START, i)
        if start == -1:
            out.append(s[i:])
            break
        out.append(s[i:start])
        end = s.find(RUNJSON_END, start)
        if end == -1:
            break
        i = end + len(RUNJSON_END)
    return "".join(out).strip()


def safe_token_count_text(llm: Any, text: str) -> int:
    try:
        return len(llm.tokenize(text.encode("utf-8")))
    except Exception:
        try:
            return len(llm.tokenize(text, special=True))
        except Exception:
            return max(1, len(text) // 4)


def safe_token_count_messages(llm: Any, msgs: list[dict[str, str]]) -> int:
    return sum(safe_token_count_text(llm, (m.get("content") or "")) for m in msgs)


def model_ident_and_cfg() -> tuple[str, dict[str, object]]:
    info = current_model_info() or {}
    cfg = (info.get("config") or {}) if isinstance(info, dict) else {}
    model_path = cfg.get("modelPath") or ""
    ident = Path(model_path).name or "local-gguf"
    return ident, cfg


def derive_stop_reason(stop_set: bool, finish_reason: str | None, err_text: str | None) -> str:
    if stop_set:
        return "user_cancel"
    if finish_reason:
        return "eosFound" if finish_reason == "stop" else f"finish:{finish_reason}"
    if err_text:
        return "error"
    return "end_of_stream"


def _first(obj: dict, keys: list[str]):
    for k in keys:
        if k in obj and obj[k] is not None:
            return obj[k]
    return None


def _sec_from_ms(v: Any) -> float | None:
    try:
        x = float(v)
        return round(x / 1000.0, 6)
    except Exception:
        return None


def collect_engine_timings(llm: Any) -> dict[str, float | None] | None:
    src = None
    try:
        if hasattr(llm, "get_last_timings") and callable(llm.get_last_timings):
            src = llm.get_last_timings()
        elif hasattr(llm, "get_timings") and callable(llm.get_timings):
            src = llm.get_timings()
        elif hasattr(llm, "timings"):
            t = llm.timings
            src = t() if callable(t) else t
        elif hasattr(llm, "perf"):
            p = llm.perf
            src = p() if callable(p) else p
        elif hasattr(llm, "stats"):
            s = llm.stats
            src = s() if callable(s) else s
        elif hasattr(llm, "get_stats") and callable(llm.get_stats):
            src = llm.get_stats()
    except Exception:
        src = None

    if not isinstance(src, dict):
        return None

    load_ms = _first(src, ["load_ms", "loadMs", "model_load_ms", "load_time_ms"])
    prompt_ms = _first(
        src, ["prompt_ms", "promptMs", "prompt_eval_ms", "prompt_time_ms", "prefill_ms"]
    )
    eval_ms = _first(src, ["eval_ms", "evalMs", "decode_ms", "eval_time_ms"])
    prompt_n = _first(src, ["prompt_n", "promptN", "prompt_tokens", "n_prompt_tokens"])
    eval_n = _first(src, ["eval_n", "evalN", "eval_tokens", "n_eval_tokens"])

    out: dict[str, float | None] = {
        "loadSec": _sec_from_ms(load_ms),
        "promptSec": _sec_from_ms(prompt_ms),
        "evalSec": _sec_from_ms(eval_ms),
        "promptN": None,
        "evalN": None,
    }
    try:
        out["promptN"] = int(prompt_n) if prompt_n is not None else None
    except Exception:
        out["promptN"] = None
    try:
        out["evalN"] = int(eval_n) if eval_n is not None else None
    except Exception:
        out["evalN"] = None
    return out


def build_run_json(
    *,
    request_cfg: dict[str, object],
    out_text: str,
    t_start: float,
    t_first: float | None,
    t_last: float | None,
    stop_set: bool,
    finish_reason: str | None,
    input_tokens_est: int | None,
    budget_view: dict | None = None,
    extra_timings: dict | None = None,
    error_text: str | None = None,
) -> dict[str, object]:
    llm = get_llm()
    out_tokens = safe_token_count_text(llm, out_text)
    t_end = time.perf_counter()
    ttft_ms = ((t_first or t_end) - t_start) * 1000.0
    gen_secs = (t_last - t_first) if (t_first is not None and t_last is not None) else 0.0
    tok_per_sec = (out_tokens / gen_secs) if gen_secs > 0 else None
    stop_reason_final = derive_stop_reason(stop_set, finish_reason, None)
    ident, cfg = model_ident_and_cfg()
    total_tokens = (input_tokens_est or 0) + out_tokens if input_tokens_est is not None else None

    timings_payload = dict(extra_timings or {})
    if "engine" not in timings_payload or timings_payload.get("engine") is None:
        timings_payload["engine"] = collect_engine_timings(llm)

    encode_t = float(input_tokens_est or 0.0)
    decode_t = float(out_tokens or 0.0)
    total_t = encode_t + decode_t
    model_queue_s = float(timings_payload.get("modelQueueSec") or 0.0)
    engine_prompt_s = float((timings_payload.get("engine") or {}).get("promptSec") or 0.0)
    encode_sec = model_queue_s or engine_prompt_s or 0.0
    decode_sec = float(gen_secs or 0.0)
    total_sec = float(t_end - t_start)
    encode_tps = (encode_t / encode_sec) if encode_sec > 0 else None
    decode_tps = (decode_t / decode_sec) if decode_sec > 0 else None
    overall_tps = (total_t / total_sec) if total_sec > 0 else None

    bv = budget_view or {}
    bv_break = (bv.get("breakdown") or {}) if isinstance(bv, dict) else {}
    web = (bv.get("web") or {}) if isinstance(bv, dict) else {}
    web_bd = (web.get("breakdown") or {}) if isinstance(web, dict) else {}
    rag = (bv.get("rag") or {}) if isinstance(bv, dict) else {}

    nctx = bv.get("modelCtx") or bv.get("n_ctx") or 0
    clamp = bv.get("clampMargin") or bv.get("clamp_margin") or 0
    inp_est = bv.get("inputTokensEst") or bv.get("input_tokens_est") or (input_tokens_est or 0)
    out_chosen = bv.get("outBudgetChosen") or bv.get("clamped_out_tokens") or 0
    out_actual = out_tokens
    out_shown = out_actual or out_chosen
    used_ctx = (inp_est or 0) + (out_shown or 0) + (clamp or 0)
    ctx_pct = (float(used_ctx) / float(nctx) * 100.0) if nctx else 0.0

    rag_delta = 0
    for k in ("ragTokensAdded", "blockTokens", "blockTokensApprox", "sessionOnlyTokensApprox"):
        v = rag.get(k)
        if isinstance(v, (int, float)) and v > 0:
            rag_delta = int(v)
            break
    rag_pct_of_input = int(round((rag_delta / inp_est) * 100)) if inp_est else 0

    web_pre = (
        web_bd.get("totalWebPreTtftSec")
        or (
            (web.get("elapsedSec") or 0)
            + (web.get("fetchElapsedSec") or 0)
            + (web.get("injectElapsedSec") or 0)
        )
        or 0
    )

    pre_accounted = bv_break.get("preTtftAccountedSec")
    unattributed = (
        bv_break.get("unattributedTtftSec")
        if "unattributedTtftSec" in bv_break
        else (max(0.0, (ttft_ms / 1000.0) - float(pre_accounted)))
        if pre_accounted is not None
        else None
    )

    return {
        "indexedModelIdentifier": ident,
        "identifier": ident,
        "loadModelConfig": {
            "fields": [
                {"key": "llm.load.llama.cpuThreadPoolSize", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.load.contextLength", "value": int(cfg.get("nCtx") or 4096)},
                {
                    "key": "llm.load.llama.acceleration.offloadRatio",
                    "value": 1 if int(cfg.get("nGpuLayers") or 0) > 0 else 0,
                },
                {"key": "llm.load.llama.nBatch", "value": int(cfg.get("nBatch") or 0)},
                {"key": "llm.load.ropeFreqBase", "value": cfg.get("ropeFreqBase")},
                {"key": "llm.load.ropeFreqScale", "value": cfg.get("ropeFreqScale")},
            ]
        },
        "predictionConfig": {
            "fields": [
                {"key": "llm.prediction.temperature", "value": request_cfg.get("temperature", 0.6)},
                {"key": "llm.prediction.topKSampling", "value": 40},
                {
                    "key": "llm.prediction.topPSampling",
                    "value": {"checked": True, "value": request_cfg.get("top_p", 0.9)},
                },
                {"key": "llm.prediction.repeatPenalty", "value": {"checked": True, "value": 1.25}},
                {"key": "llm.prediction.maxTokens", "value": request_cfg.get("max_tokens", 512)},
                {"key": "llm.prediction.stopStrings", "value": STOP_STRINGS},
                {"key": "llm.prediction.llama.cpuThreads", "value": int(cfg.get("nThreads") or 0)},
                {"key": "llm.prediction.contextPrefill", "value": []},
                {"key": "llm.prediction.tools", "value": {"type": "none"}},
                {"key": "llm.prediction.promptTemplate", "value": {"type": "none"}},
            ]
        },
        "stats": {
            "stopReason": stop_reason_final,
            "tokensPerSecond": tok_per_sec,
            "numGpuLayers": int(cfg.get("nGpuLayers") or 0),
            "timeToFirstTokenSec": round((ttft_ms or 0) / 1000.0, 3),
            "totalTimeSec": round(t_end - t_start, 3),
            "promptTokensCount": input_tokens_est,
            "predictedTokensCount": out_tokens,
            "totalTokensCount": total_tokens,
            "budget": budget_view or {},
            "timings": timings_payload,
            "error": error_text or None,
        },
        "budget_view": (budget_view or {}),
        "_derived": {
            "context": {
                "modelCtx": nctx,
                "clampMargin": clamp,
                "inputTokensEst": inp_est,
                "outBudgetChosen": out_chosen,
                "outActual": out_actual,
                "outShown": out_shown,
                "usedCtx": used_ctx,
                "ctxPct": ctx_pct,
            },
            "rag": {
                "ragDelta": rag_delta,
                "ragPctOfInput": rag_pct_of_input,
            },
            "web": {
                "webPre": web_pre,
            },
            "timing": {
                "accountedPreTtftSec": pre_accounted,
                "unattributedPreTtftSec": unattributed,
                "preModelSec": timings_payload.get("preModelSec"),
                "modelQueueSec": timings_payload.get("modelQueueSec"),
                "genSec": gen_secs,
                "ttftSec": round((ttft_ms or 0) / 1000.0, 3),
            },
            "throughput": {
                "encodeTokPerSec": encode_tps,
                "decodeTokPerSec": decode_tps,
                "overallTokPerSec": overall_tps,
            },
        },
    }


async def watch_disconnect(request, stop_ev):
    if await request.is_disconnected():
        stop_ev.set()
        return
    while not stop_ev.is_set():
        await asyncio.sleep(0.2)
        if await request.is_disconnected():
            stop_ev.set()
            break

# ===== aimodel/file_read/utils/text.py =====

from __future__ import annotations

import re

from ..core.logging import get_logger

log = get_logger(__name__)


def clean_ws(s: str | None) -> str:
    return " ".join((s or "").split())


def strip_wrappers(
    text: str, *, trim_whitespace: bool, split_on_blank: bool, header_regex: str | None
) -> str:
    t = text or ""
    if trim_whitespace:
        t = t.strip()
    if not header_regex and not split_on_blank:
        return t
    head = t
    if split_on_blank:
        head = t.split("\n\n", 1)[0]
    if header_regex:
        try:
            rx = re.compile(header_regex)
            out = []
            for ln in head.splitlines():
                if rx.match(ln):
                    break
                out.append(ln)
            core = " ".join(" ".join(out).split())
            return core if core else t
        except Exception:
            return head
    return head

# ===== aimodel/file_read/web/__init__.py =====



# ===== aimodel/file_read/web/brave.py =====

from __future__ import annotations

from ..core.logging import get_logger

log = get_logger(__name__)
import hashlib
import time
import urllib.parse
from typing import Any
import asyncio

from ..core.http import get_client
from ..core.settings import SETTINGS
from .orchestrator_common import _host
from .provider import SearchHit

_CACHE: dict[str, tuple[float, list[SearchHit]]] = {}


def _cache_key(query: str, base: str, key_marker: str) -> str:
    q = (query or "").strip().lower()
    b = (base or "").strip().lower()
    m = (key_marker or "").strip().lower()
    return f"{q}||{b}||{m}"


def _cache_get(key: str) -> list[SearchHit] | None:
    eff = SETTINGS.effective()
    ttl = int(eff["web_search_cache_ttl_sec"])
    v = _CACHE.get(key)
    if not v:
        return None
    ts, hits = v
    if time.time() - ts > ttl:
        _CACHE.pop(key, None)
        return None
    return hits


def _cache_set(key: str, hits: list[SearchHit]) -> None:
    _CACHE[key] = (time.time(), hits)


def _set_hits_telemetry(
    tel: dict[str, Any], all_hits: list[SearchHit], out: list[SearchHit]
) -> None:
    tel["hits"] = {
        "total": len(all_hits),
        "returned": len(out),
        "top": [f"{h.rank}:{_host(h.url)}:{(h.title or '')[:60]}" for h in out[:5]],
    }


def _build_url(base: str, q: str, k: int) -> str:
    params = {"q": q, "count": str(max(1, k))}
    return f"{base}?{urllib.parse.urlencode(params)}"


def _num(x: Any) -> int | None:
    try:
        return int(str(x))
    except Exception:
        return None


class BraveProvider:
    async def search(
        self,
        query: str,
        k: int = 3,
        telemetry: dict[str, Any] | None = None,
        xid: str | None = None,
        stop_ev: asyncio.Event | None = None,
    ) -> list[SearchHit]:
        t_start = time.perf_counter()
        eff = SETTINGS.effective()
        q_norm = (query or "").strip()

        # fast-cancel
        if stop_ev is not None and stop_ev.is_set():
            if telemetry is not None:
                telemetry.update({"query": q_norm, "k": int(k), "supersetK": int(k), "cancelled": True})
            return []

        if not q_norm:
            if telemetry is not None:
                telemetry.update(
                    {
                        "query": q_norm,
                        "k": int(k),
                        "supersetK": int(k),
                        "elapsedSec": round(time.perf_counter() - t_start, 6),
                        "cache": {"hit": False},
                    }
                )
            return []

        superset_k = max(int(k), int(eff["web_search_cache_superset_k"]))
        tel: dict[str, Any] = {"query": q_norm, "k": int(k), "supersetK": superset_k}

        brave_base = (
            eff.get("brave_api_base") or "https://api.search.brave.com/res/v1/web/search"
        ).strip()
        key = (SETTINGS.get("brave_api_key", "") or "").strip()
        key_hash = hashlib.sha1(key.encode("utf-8")).hexdigest()[:8] if key else "nokey"

        ckey = _cache_key(q_norm, brave_base, key_hash)
        t_cache = time.perf_counter()
        cached = _cache_get(ckey)
        tel["cache"] = {
            "hit": cached is not None,
            "elapsedSec": round(time.perf_counter() - t_cache, 6),
        }
        if cached is not None:
            if telemetry is not None:
                out = cached[:k]
                _set_hits_telemetry(tel, cached, out)
                tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
                telemetry.update(tel)
            return cached[:k]

        # fast-cancel before network
        if stop_ev is not None and stop_ev.is_set():
            if telemetry is not None:
                tel["cancelled"] = True
                tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
                telemetry.update(tel)
            return []

        hits: list[SearchHit] = []
        prov_info: dict[str, Any] = {"available": True}
        t_fetch = time.perf_counter()

        log.debug("BRAVE cfg", {"base": brave_base, "byok": True, "keyHash": key_hash})
        url = _build_url(brave_base, q_norm, superset_k)
        headers: dict[str, str] = {}
        if key:
            headers["X-Subscription-Token"] = key
        log.debug("BRAVE headers", {"hasKey": bool(key)})

        if not key:
            prov_info["errorType"] = "Unauthorized"
            prov_info["errorMsg"] = "No Brave API key configured in settings"
        else:
            try:
                timeout = float(eff.get("web_fetch_timeout_sec", 8))
                log.debug("BRAVE call", {"url": url, "timeoutSec": timeout})
                client = await get_client()
                # if cancel triggers here, asyncio.CancelledError will bubble up
                r = await client.get(url, headers=headers, timeout=timeout, follow_redirects=True)
                log.debug(
                    "BRAVE resp",
                    {
                        "status": r.status_code,
                        "len": len(r.text or ""),
                        "preview": r.text[:200] if r.text else "",
                    },
                )
                r.raise_for_status()
                data = r.json()

                rate = {
                    "minute": {
                        "limit": _num(r.headers.get("X-RateLimit-Limit-Minute")),
                        "remaining": _num(r.headers.get("X-RateLimit-Remaining-Minute")),
                        "resetMs": _num(r.headers.get("X-RateLimit-Reset-Minute")),
                    },
                    "day": {
                        "limit": _num(r.headers.get("X-RateLimit-Limit-Day")),
                        "remaining": _num(r.headers.get("X-RateLimit-Remaining-Day")),
                        "resetMs": _num(r.headers.get("X-RateLimit-Reset-Day")),
                    },
                }
                prov_info["rate"] = rate

                web = (data or {}).get("web") or {}
                results = web.get("results") or []
                for i, item in enumerate(results[:superset_k], start=1):
                    title = (item.get("title") or "").strip()
                    url_i = (item.get("url") or "").strip()
                    snippet = (item.get("description") or "").strip() or None
                    if not url_i:
                        continue
                    hits.append(SearchHit(title=title or url_i, url=url_i, snippet=snippet, rank=i))

                prov_info["errorType"] = None
                prov_info["errorMsg"] = None

            except Exception as e:
                log.debug("BRAVE exception", {"type": type(e).__name__, "msg": str(e)})
                prov_info["errorType"] = type(e).__name__
                prov_info["errorMsg"] = str(e)

        prov_info["elapsedSec"] = round(time.perf_counter() - t_fetch, 6)
        tel["provider"] = prov_info

        _cache_set(ckey, hits)
        out = hits[:k]
        _set_hits_telemetry(tel, hits, out)
        tel["elapsedSec"] = round(time.perf_counter() - t_start, 6)
        if telemetry is not None:
            telemetry.update(tel)
        return out

# ===== aimodel/file_read/web/fetch.py =====

from __future__ import annotations

import asyncio
import time
from typing import Any
from urllib.parse import urlparse

from ..core.http import ExternalServiceError, get_client
from ..core.logging import get_logger
from ..core.settings import SETTINGS

log = get_logger(__name__)

try:
    from readability import Document
except Exception:
    Document = None  # optional

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None  # optional

try:
    from selectolax.parser import HTMLParser
except Exception:
    HTMLParser = None  # optional


def _req(key: str):
    return SETTINGS[key]


def _ua() -> str:
    return str(_req("web_fetch_user_agent"))


def _timeout() -> float:
    return float(_req("web_fetch_timeout_sec"))


def _max_chars() -> int:
    return int(_req("web_fetch_max_chars"))


def _max_bytes() -> int:
    return int(_req("web_fetch_max_bytes"))


def _max_parallel() -> int:
    return max(1, int(_req("web_fetch_max_parallel")))


# -------------------- Adaptive cooldown (generic, no host hardcoding) --------------------
# host -> (fail_count, cooldown_until_ts)
_BAD_HOSTS: dict[str, tuple[int, float]] = {}


def _now() -> float:
    return time.time()


def _host_of(u: str) -> str:
    try:
        return (urlparse(u).hostname or "").lower()
    except Exception:
        return ""


def _cooldown_secs(fails: int) -> float:
    base = 15 * 60.0
    cap = 24 * 60 * 60.0
    return min(cap, base * (2 ** max(0, fails - 1)))


def _mark_bad(host: str) -> None:
    if not host:
        return
    fails, _until = _BAD_HOSTS.get(host, (0, 0.0))
    fails += 1
    _BAD_HOSTS[host] = (fails, _now() + _cooldown_secs(fails))


def _mark_good(host: str) -> None:
    if not host:
        return
    if host in _BAD_HOSTS:
        fails, _until = _BAD_HOSTS[host]
        fails = max(0, fails - 1)
        if fails == 0:
            _BAD_HOSTS.pop(host, None)
        else:
            _BAD_HOSTS[host] = (fails, _now() + _cooldown_secs(fails))


def _is_on_cooldown(host: str) -> bool:
    ent = _BAD_HOSTS.get(host)
    return bool(ent and ent[1] > _now())


# ----------------------------------------------------------------------------------------


async def _read_capped_bytes(resp, cap_bytes: int) -> bytes:
    out = bytearray()
    async for chunk in resp.aiter_bytes():
        if not chunk:
            continue
        remaining = cap_bytes - len(out)
        if remaining <= 0:
            break
        out.extend(chunk[:remaining])
        if len(out) >= cap_bytes:
            break
    return bytes(out)


def _extract_text_from_html(raw_html: str, url: str) -> str:
    html = raw_html or ""
    if Document is not None:
        try:
            doc = Document(html)
            summary_html = doc.summary(html_partial=True) or ""
            if summary_html:
                if BeautifulSoup is not None:
                    soup = BeautifulSoup(summary_html, "lxml")
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        return txt
        except Exception:
            pass
    if HTMLParser is not None:
        try:
            tree = HTMLParser(html)
            for bad in ("script", "style", "noscript"):
                for n in tree.tags(bad):
                    n.decompose()
            txt = (
                tree.body.text(separator=" ", strip=True)
                if tree.body
                else tree.text(separator=" ", strip=True)
            )
            if txt:
                return txt
        except Exception:
            pass
    if BeautifulSoup is not None:
        try:
            soup = BeautifulSoup(html, "lxml")
            for s in soup(["script", "style", "noscript"]):
                s.extract()
            txt = soup.get_text(" ", strip=True)
            if txt:
                return txt
        except Exception:
            pass
    return html


async def fetch_clean(
    url: str,
    timeout_s: float | None = None,
    max_chars: int | None = None,
    max_bytes: int | None = None,
    telemetry: dict[str, Any] | None = None,
    stop_ev: asyncio.Event | None = None,
) -> tuple[str, int, str]:
    # fast-cancel before network
    if stop_ev is not None and stop_ev.is_set():
        raise ExternalServiceError(service="fetch", url=url, detail="cancelled")

    t0 = time.perf_counter()
    timeout = _timeout() if timeout_s is None else float(timeout_s)
    cap_chars = _max_chars() if max_chars is None else int(max_chars)
    cap_bytes = _max_bytes() if max_bytes is None else int(max_bytes)

    headers = {"User-Agent": _ua()}
    client = await get_client()
    try:
        r = await client.get(
            url,
            follow_redirects=True,
            timeout=timeout,
         